{"path":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","commits":[{"id":"f20bb72b0dfa147c6f1fcd7693102c63a2714eae","date":1303767270,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"/dev/null","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = resultList.size();\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0663cc678850ea2c51151f9fd217342ea35b8568","date":1303828523,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":null,"sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = resultList.size();\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"21486a8058ee8d7503c7d7a5e55b6c3a218d0942","date":1303841712,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"/dev/null","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = resultList.size();\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a405e749df166cf8c456ac9381f77f6c99a6270","date":1303842176,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":null,"sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = resultList.size();\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8f944ac3fe3f9d40d825177507fb381d2b106b3","date":1303868525,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"/dev/null","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = resultList.size();\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061","487de3f55283f58d7e02a16993f8be55bbe32061","487de3f55283f58d7e02a16993f8be55bbe32061","843b845d397272dbafe8b80ebb8f9336d94568ef"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d5df8e07c035d62d982894b439322da40e0938","date":1303923139,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"/dev/null","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = resultList.size();\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"/dev/null","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = resultList.size();\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"843b845d397272dbafe8b80ebb8f9336d94568ef","date":1305726695,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = resultList.size();\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":["f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061","487de3f55283f58d7e02a16993f8be55bbe32061","487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = resultList.size();\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"/dev/null","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","date":1308670974,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061","487de3f55283f58d7e02a16993f8be55bbe32061","487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd9cc9d77712aba3662f24632df7539ab75e3667","date":1309095238,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061","487de3f55283f58d7e02a16993f8be55bbe32061","487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":["487de3f55283f58d7e02a16993f8be55bbe32061","487de3f55283f58d7e02a16993f8be55bbe32061","487de3f55283f58d7e02a16993f8be55bbe32061"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seek(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromDeletedDocs = MultiFields.getDeletedDocs(fromSearcher.getIndexReader());\n      Bits toDeletedDocs = fromSearcher == toSearcher ? fromDeletedDocs : MultiFields.getDeletedDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = StringHelper.intern(fromField);\n      fromDeState.deletedDocs = fromDeletedDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = StringHelper.intern(toField);\n      toDeState.deletedDocs = toDeletedDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip deletedDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seek(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use deletedDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.deletedDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","pathOld":"solr/src/java/org/apache/solr/search/JoinQuery[JoinQParserPlugin].JoinQueryWeight#getDocSet().mjava","sourceNew":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","sourceOld":"    public DocSet getDocSet() throws IOException {\n      OpenBitSet resultBits = null;\n\n      // minimum docFreq to use the cache\n      int minDocFreqFrom = Math.max(5, fromSearcher.maxDoc() >> 13);\n      int minDocFreqTo = Math.max(5, toSearcher.maxDoc() >> 13);\n\n      // use a smaller size than normal since we will need to sort and dedup the results\n      int maxSortedIntSize = Math.max(10, toSearcher.maxDoc() >> 10);\n\n      DocSet fromSet = fromSearcher.getDocSet(q);\n      fromSetSize = fromSet.size();\n\n      List<DocSet> resultList = new ArrayList<DocSet>(10);\n\n      // make sure we have a set that is fast for random access, if we will use it for that\n      DocSet fastForRandomSet = fromSet;\n      if (minDocFreqFrom>0 && fromSet instanceof SortedIntDocSet) {\n        SortedIntDocSet sset = (SortedIntDocSet)fromSet;\n        fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n      }\n\n      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());\n      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());\n      if (fromFields == null) return DocSet.EMPTY;\n      Terms terms = fromFields.terms(fromField);\n      Terms toTerms = toFields.terms(toField);\n      if (terms == null || toTerms==null) return DocSet.EMPTY;\n      String prefixStr = TrieField.getMainValuePrefix(fromSearcher.getSchema().getFieldType(fromField));\n      BytesRef prefix = prefixStr == null ? null : new BytesRef(prefixStr);\n\n      BytesRef term = null;\n      TermsEnum  termsEnum = terms.iterator();\n      TermsEnum  toTermsEnum = toTerms.iterator();\n      SolrIndexSearcher.DocsEnumState fromDeState = null;\n      SolrIndexSearcher.DocsEnumState toDeState = null;\n\n      if (prefix == null) {\n        term = termsEnum.next();\n      } else {\n        if (termsEnum.seekCeil(prefix, true) != TermsEnum.SeekStatus.END) {\n          term = termsEnum.term();\n        }\n      }\n\n      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());\n      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());\n\n      fromDeState = new SolrIndexSearcher.DocsEnumState();\n      fromDeState.fieldName = fromField;\n      fromDeState.liveDocs = fromLiveDocs;\n      fromDeState.termsEnum = termsEnum;\n      fromDeState.docsEnum = null;\n      fromDeState.minSetSizeCached = minDocFreqFrom;\n\n      toDeState = new SolrIndexSearcher.DocsEnumState();\n      toDeState.fieldName = toField;\n      toDeState.liveDocs = toLiveDocs;\n      toDeState.termsEnum = toTermsEnum;\n      toDeState.docsEnum = null;\n      toDeState.minSetSizeCached = minDocFreqTo;\n\n      while (term != null) {\n        if (prefix != null && !term.startsWith(prefix))\n          break;\n\n        fromTermCount++;\n\n        boolean intersects = false;\n        int freq = termsEnum.docFreq();\n        fromTermTotalDf++;\n\n        if (freq < minDocFreqFrom) {\n          fromTermDirectCount++;\n          // OK to skip liveDocs, since we check for intersection with docs matching query\n          fromDeState.docsEnum = fromDeState.termsEnum.docs(null, fromDeState.docsEnum);\n          DocsEnum docsEnum = fromDeState.docsEnum;\n\n          if (docsEnum instanceof MultiDocsEnum) {\n            MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n            int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n            outer: for (int subindex = 0; subindex<numSubs; subindex++) {\n              MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.docsEnum == null) continue;\n              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n              int base = sub.slice.start;\n              for (;;) {\n                int nDocs = sub.docsEnum.read();\n                if (nDocs == 0) break;\n                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                int end = bulk.docs.offset + nDocs;\n                for (int i=bulk.docs.offset; i<end; i++) {\n                  if (fastForRandomSet.exists(docArr[i]+base)) {\n                    intersects = true;\n                    break outer;\n                  }\n                }\n              }\n            }\n          } else {\n            // this should be the same bulk result object if sharing of the docsEnum succeeded\n            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n            outer: for (;;) {\n              int nDocs = docsEnum.read();\n              if (nDocs == 0) break;\n              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n              int end = bulk.docs.offset + nDocs;\n              for (int i=bulk.docs.offset; i<end; i++) {\n                if (fastForRandomSet.exists(docArr[i])) {\n                  intersects = true;\n                  break outer;\n                }\n              }\n            }\n          }\n        } else {\n          // use the filter cache\n          DocSet fromTermSet = fromSearcher.getDocSet(fromDeState);\n          intersects = fromSet.intersects(fromTermSet);\n        }\n\n        if (intersects) {\n          fromTermHits++;\n          fromTermHitsTotalDf++;\n          TermsEnum.SeekStatus status = toTermsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.END) break;\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            toTermHits++;\n            int df = toTermsEnum.docFreq();\n            toTermHitsTotalDf += df;\n            if (resultBits==null && df + resultListDocs > maxSortedIntSize && resultList.size() > 0) {\n              resultBits = new OpenBitSet(toSearcher.maxDoc());\n            }\n\n            // if we don't have a bitset yet, or if the resulting set will be too large\n            // use the filterCache to get a DocSet\n            if (toTermsEnum.docFreq() >= minDocFreqTo || resultBits == null) {\n              // use filter cache\n              DocSet toTermSet = toSearcher.getDocSet(toDeState);\n              resultListDocs += toTermSet.size();\n              if (resultBits != null) {\n                toTermSet.setBitsOn(resultBits);\n              } else {\n                if (toTermSet instanceof BitDocSet) {\n                  resultBits = (OpenBitSet)((BitDocSet)toTermSet).bits.clone();\n                } else {\n                  resultList.add(toTermSet);\n                }\n              }\n            } else {\n              toTermDirectCount++;\n\n              // need to use liveDocs here so we don't map to any deleted ones\n              toDeState.docsEnum = toDeState.termsEnum.docs(toDeState.liveDocs, toDeState.docsEnum);\n              DocsEnum docsEnum = toDeState.docsEnum;              \n\n              if (docsEnum instanceof MultiDocsEnum) {\n                MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();\n                int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();\n                for (int subindex = 0; subindex<numSubs; subindex++) {\n                  MultiDocsEnum.EnumWithSlice sub = subs[subindex];\n                  if (sub.docsEnum == null) continue;\n                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();\n                  int base = sub.slice.start;\n                  for (;;) {\n                    int nDocs = sub.docsEnum.read();\n                    if (nDocs == 0) break;\n                    resultListDocs += nDocs;\n                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                    int end = bulk.docs.offset + nDocs;\n                    for (int i=bulk.docs.offset; i<end; i++) {\n                      resultBits.fastSet(docArr[i]+base);\n                    }\n                  }\n                }\n              } else {\n                // this should be the same bulk result object if sharing of the docsEnum succeeded\n                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();\n\n                for (;;) {\n                  int nDocs = docsEnum.read();\n                  if (nDocs == 0) break;\n                  resultListDocs += nDocs;\n                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.\n                  int end = bulk.docs.offset + nDocs;\n                  for (int i=bulk.docs.offset; i<end; i++) {\n                    resultBits.fastSet(docArr[i]);\n                  }\n                }\n              }\n            }\n\n          }\n        }\n\n        term = termsEnum.next();\n      }\n\n      smallSetsDeferred = resultList.size();\n\n      if (resultBits != null) {\n        for (DocSet set : resultList) {\n          set.setBitsOn(resultBits);\n        }\n        return new BitDocSet(resultBits);\n      }\n\n      if (resultList.size()==0) {\n        return DocSet.EMPTY;\n      }\n\n      if (resultList.size() == 1) {\n        return resultList.get(0);\n      }\n\n      int sz = 0;\n\n      for (DocSet set : resultList)\n        sz += set.size();\n\n      int[] docs = new int[sz];\n      int pos = 0;\n      for (DocSet set : resultList) {\n        System.arraycopy(((SortedIntDocSet)set).getDocs(), 0, docs, pos, set.size());\n        pos += set.size();\n      }\n      Arrays.sort(docs);\n      int[] dedup = new int[sz];\n      pos = 0;\n      int last = -1;\n      for (int doc : docs) {\n        if (doc != last)\n          dedup[pos++] = doc;\n        last = doc;\n      }\n\n      if (pos != dedup.length) {\n        dedup = Arrays.copyOf(dedup, pos);\n      }\n\n      return new SortedIntDocSet(dedup, dedup.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"d4d5df8e07c035d62d982894b439322da40e0938":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"7a405e749df166cf8c456ac9381f77f6c99a6270":["21486a8058ee8d7503c7d7a5e55b6c3a218d0942"],"f20bb72b0dfa147c6f1fcd7693102c63a2714eae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["843b845d397272dbafe8b80ebb8f9336d94568ef"],"c26f00b574427b55127e869b935845554afde1fa":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["fd9cc9d77712aba3662f24632df7539ab75e3667"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"0663cc678850ea2c51151f9fd217342ea35b8568":["f20bb72b0dfa147c6f1fcd7693102c63a2714eae"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","fd9cc9d77712aba3662f24632df7539ab75e3667"],"f8f944ac3fe3f9d40d825177507fb381d2b106b3":["7a405e749df166cf8c456ac9381f77f6c99a6270"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["843b845d397272dbafe8b80ebb8f9336d94568ef","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["2553b00f699380c64959ccb27991289aae87be2e","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["135621f3a0670a9394eb563224a3b76cc4dddc0f","843b845d397272dbafe8b80ebb8f9336d94568ef"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","843b845d397272dbafe8b80ebb8f9336d94568ef"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"843b845d397272dbafe8b80ebb8f9336d94568ef":["f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"fd9cc9d77712aba3662f24632df7539ab75e3667":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"],"21486a8058ee8d7503c7d7a5e55b6c3a218d0942":["0663cc678850ea2c51151f9fd217342ea35b8568"]},"commit2Childs":{"d4d5df8e07c035d62d982894b439322da40e0938":[],"7a405e749df166cf8c456ac9381f77f6c99a6270":["f8f944ac3fe3f9d40d825177507fb381d2b106b3"],"f20bb72b0dfa147c6f1fcd7693102c63a2714eae":["0663cc678850ea2c51151f9fd217342ea35b8568"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["fd9cc9d77712aba3662f24632df7539ab75e3667"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["c26f00b574427b55127e869b935845554afde1fa","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","a258fbb26824fd104ed795e5d9033d2d040049ee"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c3a8a449466c1ff7ce2274fe73dab487256964b4"],"0663cc678850ea2c51151f9fd217342ea35b8568":["21486a8058ee8d7503c7d7a5e55b6c3a218d0942"],"2553b00f699380c64959ccb27991289aae87be2e":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"f8f944ac3fe3f9d40d825177507fb381d2b106b3":["d4d5df8e07c035d62d982894b439322da40e0938","135621f3a0670a9394eb563224a3b76cc4dddc0f","843b845d397272dbafe8b80ebb8f9336d94568ef"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":[],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d4d5df8e07c035d62d982894b439322da40e0938","f20bb72b0dfa147c6f1fcd7693102c63a2714eae","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"843b845d397272dbafe8b80ebb8f9336d94568ef":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","d083e83f225b11e5fdd900e83d26ddb385b6955c","c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233"],"fd9cc9d77712aba3662f24632df7539ab75e3667":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","2553b00f699380c64959ccb27991289aae87be2e"],"21486a8058ee8d7503c7d7a5e55b6c3a218d0942":["7a405e749df166cf8c456ac9381f77f6c99a6270"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d4d5df8e07c035d62d982894b439322da40e0938","d083e83f225b11e5fdd900e83d26ddb385b6955c","c3a8a449466c1ff7ce2274fe73dab487256964b4","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}