{"path":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testCleanupOldIndexDirectories().mjava","commits":[{"id":"e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0","date":1433439403,"type":0,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testCleanupOldIndexDirectories().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testCleanupOldIndexDirectories() throws Exception {\n\n    HdfsDirectoryFactory hdfsFactory = new HdfsDirectoryFactory();\n\n    System.setProperty(\"solr.hdfs.home\", HdfsTestUtil.getURI(dfsCluster) + \"/solr1\");\n    hdfsFactory.init(new NamedList<>());\n    String dataHome = hdfsFactory.getDataHome(new MockCoreDescriptor());\n    assertTrue(dataHome.endsWith(\"/solr1/mock/data\"));\n    System.clearProperty(\"solr.hdfs.home\");\n\n    FileSystem hdfs = dfsCluster.getFileSystem();\n\n    org.apache.hadoop.fs.Path dataHomePath = new org.apache.hadoop.fs.Path(dataHome);\n    org.apache.hadoop.fs.Path currentIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index\");\n    assertTrue(!hdfs.isDirectory(currentIndexDirPath));\n    hdfs.mkdirs(currentIndexDirPath);\n    assertTrue(hdfs.isDirectory(currentIndexDirPath));\n\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n    org.apache.hadoop.fs.Path oldIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index.\"+timestamp1);\n    assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n    hdfs.mkdirs(oldIndexDirPath);\n    assertTrue(hdfs.isDirectory(oldIndexDirPath));\n\n    hdfsFactory.cleanupOldIndexDirectories(dataHomePath.toString(), currentIndexDirPath.toString());\n\n    assertTrue(hdfs.isDirectory(currentIndexDirPath));\n    assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["ad28156288ac00b91352582904d97e6653205757","add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad28156288ac00b91352582904d97e6653205757","date":1486850922,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testCleanupOldIndexDirectories().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testCleanupOldIndexDirectories().mjava","sourceNew":"  @Test\n  public void testCleanupOldIndexDirectories() throws Exception {\n\n    HdfsDirectoryFactory hdfsFactory = new HdfsDirectoryFactory();\n\n    System.setProperty(\"solr.hdfs.home\", HdfsTestUtil.getURI(dfsCluster) + \"/solr1\");\n    hdfsFactory.init(new NamedList<>());\n    String dataHome = hdfsFactory.getDataHome(new MockCoreDescriptor());\n    assertTrue(dataHome.endsWith(\"/solr1/mock/data\"));\n    System.clearProperty(\"solr.hdfs.home\");\n\n    FileSystem hdfs = dfsCluster.getFileSystem();\n\n    org.apache.hadoop.fs.Path dataHomePath = new org.apache.hadoop.fs.Path(dataHome);\n    org.apache.hadoop.fs.Path currentIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index\");\n    assertTrue(!hdfs.isDirectory(currentIndexDirPath));\n    hdfs.mkdirs(currentIndexDirPath);\n    assertTrue(hdfs.isDirectory(currentIndexDirPath));\n\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n    org.apache.hadoop.fs.Path oldIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index.\"+timestamp1);\n    assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n    hdfs.mkdirs(oldIndexDirPath);\n    assertTrue(hdfs.isDirectory(oldIndexDirPath));\n\n    hdfsFactory.cleanupOldIndexDirectories(dataHomePath.toString(), currentIndexDirPath.toString(), false);\n\n    assertTrue(hdfs.isDirectory(currentIndexDirPath));\n    assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n  }\n\n","sourceOld":"  @Test\n  public void testCleanupOldIndexDirectories() throws Exception {\n\n    HdfsDirectoryFactory hdfsFactory = new HdfsDirectoryFactory();\n\n    System.setProperty(\"solr.hdfs.home\", HdfsTestUtil.getURI(dfsCluster) + \"/solr1\");\n    hdfsFactory.init(new NamedList<>());\n    String dataHome = hdfsFactory.getDataHome(new MockCoreDescriptor());\n    assertTrue(dataHome.endsWith(\"/solr1/mock/data\"));\n    System.clearProperty(\"solr.hdfs.home\");\n\n    FileSystem hdfs = dfsCluster.getFileSystem();\n\n    org.apache.hadoop.fs.Path dataHomePath = new org.apache.hadoop.fs.Path(dataHome);\n    org.apache.hadoop.fs.Path currentIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index\");\n    assertTrue(!hdfs.isDirectory(currentIndexDirPath));\n    hdfs.mkdirs(currentIndexDirPath);\n    assertTrue(hdfs.isDirectory(currentIndexDirPath));\n\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n    org.apache.hadoop.fs.Path oldIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index.\"+timestamp1);\n    assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n    hdfs.mkdirs(oldIndexDirPath);\n    assertTrue(hdfs.isDirectory(oldIndexDirPath));\n\n    hdfsFactory.cleanupOldIndexDirectories(dataHomePath.toString(), currentIndexDirPath.toString());\n\n    assertTrue(hdfs.isDirectory(currentIndexDirPath));\n    assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n  }\n\n","bugFix":["e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f15af35d55d70c34451f9df5edeaeff6b31f8cbe","date":1519625627,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testCleanupOldIndexDirectories().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testCleanupOldIndexDirectories().mjava","sourceNew":"  @Test\n  public void testCleanupOldIndexDirectories() throws Exception {\n\n    try (HdfsDirectoryFactory hdfsFactory = new HdfsDirectoryFactory()) {\n\n      System.setProperty(\"solr.hdfs.home\", HdfsTestUtil.getURI(dfsCluster) + \"/solr1\");\n      hdfsFactory.init(new NamedList<>());\n      String dataHome = hdfsFactory.getDataHome(new MockCoreDescriptor());\n      assertTrue(dataHome.endsWith(\"/solr1/mock/data\"));\n      System.clearProperty(\"solr.hdfs.home\");\n\n      FileSystem hdfs = dfsCluster.getFileSystem();\n\n      org.apache.hadoop.fs.Path dataHomePath = new org.apache.hadoop.fs.Path(dataHome);\n      org.apache.hadoop.fs.Path currentIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index\");\n      assertTrue(!hdfs.isDirectory(currentIndexDirPath));\n      hdfs.mkdirs(currentIndexDirPath);\n      assertTrue(hdfs.isDirectory(currentIndexDirPath));\n\n      String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n      org.apache.hadoop.fs.Path oldIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index.\" + timestamp1);\n      assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n      hdfs.mkdirs(oldIndexDirPath);\n      assertTrue(hdfs.isDirectory(oldIndexDirPath));\n\n      hdfsFactory.cleanupOldIndexDirectories(dataHomePath.toString(), currentIndexDirPath.toString(), false);\n\n      assertTrue(hdfs.isDirectory(currentIndexDirPath));\n      assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testCleanupOldIndexDirectories() throws Exception {\n\n    HdfsDirectoryFactory hdfsFactory = new HdfsDirectoryFactory();\n\n    System.setProperty(\"solr.hdfs.home\", HdfsTestUtil.getURI(dfsCluster) + \"/solr1\");\n    hdfsFactory.init(new NamedList<>());\n    String dataHome = hdfsFactory.getDataHome(new MockCoreDescriptor());\n    assertTrue(dataHome.endsWith(\"/solr1/mock/data\"));\n    System.clearProperty(\"solr.hdfs.home\");\n\n    FileSystem hdfs = dfsCluster.getFileSystem();\n\n    org.apache.hadoop.fs.Path dataHomePath = new org.apache.hadoop.fs.Path(dataHome);\n    org.apache.hadoop.fs.Path currentIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index\");\n    assertTrue(!hdfs.isDirectory(currentIndexDirPath));\n    hdfs.mkdirs(currentIndexDirPath);\n    assertTrue(hdfs.isDirectory(currentIndexDirPath));\n\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n    org.apache.hadoop.fs.Path oldIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index.\"+timestamp1);\n    assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n    hdfs.mkdirs(oldIndexDirPath);\n    assertTrue(hdfs.isDirectory(oldIndexDirPath));\n\n    hdfsFactory.cleanupOldIndexDirectories(dataHomePath.toString(), currentIndexDirPath.toString(), false);\n\n    assertTrue(hdfs.isDirectory(currentIndexDirPath));\n    assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n  }\n\n","bugFix":null,"bugIntro":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f","date":1552317217,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testCleanupOldIndexDirectories().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testCleanupOldIndexDirectories().mjava","sourceNew":"  @Test\n  public void testCleanupOldIndexDirectories() throws Exception {\n    try (HdfsDirectoryFactory hdfsFactory = new HdfsDirectoryFactory()) {\n      System.setProperty(\"solr.hdfs.home\", HdfsTestUtil.getURI(dfsCluster) + \"/solr1\");\n      hdfsFactory.init(new NamedList<>());\n      String dataHome = hdfsFactory.getDataHome(new MockCoreDescriptor());\n      assertTrue(dataHome.endsWith(\"/solr1/mock/data\"));\n      System.clearProperty(\"solr.hdfs.home\");\n\n      FileSystem hdfs = dfsCluster.getFileSystem();\n\n      org.apache.hadoop.fs.Path dataHomePath = new org.apache.hadoop.fs.Path(dataHome);\n      org.apache.hadoop.fs.Path currentIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index\");\n      assertTrue(!hdfs.isDirectory(currentIndexDirPath));\n      hdfs.mkdirs(currentIndexDirPath);\n      assertTrue(hdfs.isDirectory(currentIndexDirPath));\n\n      String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n      org.apache.hadoop.fs.Path oldIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index.\" + timestamp1);\n      assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n      hdfs.mkdirs(oldIndexDirPath);\n      assertTrue(hdfs.isDirectory(oldIndexDirPath));\n\n      hdfsFactory.cleanupOldIndexDirectories(dataHomePath.toString(), currentIndexDirPath.toString(), false);\n\n      assertTrue(hdfs.isDirectory(currentIndexDirPath));\n      assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testCleanupOldIndexDirectories() throws Exception {\n\n    try (HdfsDirectoryFactory hdfsFactory = new HdfsDirectoryFactory()) {\n\n      System.setProperty(\"solr.hdfs.home\", HdfsTestUtil.getURI(dfsCluster) + \"/solr1\");\n      hdfsFactory.init(new NamedList<>());\n      String dataHome = hdfsFactory.getDataHome(new MockCoreDescriptor());\n      assertTrue(dataHome.endsWith(\"/solr1/mock/data\"));\n      System.clearProperty(\"solr.hdfs.home\");\n\n      FileSystem hdfs = dfsCluster.getFileSystem();\n\n      org.apache.hadoop.fs.Path dataHomePath = new org.apache.hadoop.fs.Path(dataHome);\n      org.apache.hadoop.fs.Path currentIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index\");\n      assertTrue(!hdfs.isDirectory(currentIndexDirPath));\n      hdfs.mkdirs(currentIndexDirPath);\n      assertTrue(hdfs.isDirectory(currentIndexDirPath));\n\n      String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n      org.apache.hadoop.fs.Path oldIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index.\" + timestamp1);\n      assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n      hdfs.mkdirs(oldIndexDirPath);\n      assertTrue(hdfs.isDirectory(oldIndexDirPath));\n\n      hdfsFactory.cleanupOldIndexDirectories(dataHomePath.toString(), currentIndexDirPath.toString(), false);\n\n      assertTrue(hdfs.isDirectory(currentIndexDirPath));\n      assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"add53de9835b2cd1a7a80b4e0036afee171c9fdf","date":1552937136,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testCleanupOldIndexDirectories().mjava","pathOld":"solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest#testCleanupOldIndexDirectories().mjava","sourceNew":"  @Test\n  public void testCleanupOldIndexDirectories() throws Exception {\n    try (HdfsDirectoryFactory hdfsFactory = new HdfsDirectoryFactory()) {\n      System.setProperty(HdfsDirectoryFactory.HDFS_HOME, HdfsTestUtil.getURI(dfsCluster) + \"/solr1\");\n      hdfsFactory.init(new NamedList<>());\n      String dataHome = hdfsFactory.getDataHome(new MockCoreDescriptor());\n      assertTrue(dataHome.endsWith(\"/solr1/mock/data\"));\n      System.clearProperty(HdfsDirectoryFactory.HDFS_HOME);\n\n      try(FileSystem hdfs = FileSystem.get(HdfsTestUtil.getClientConfiguration(dfsCluster))) {\n        org.apache.hadoop.fs.Path dataHomePath = new org.apache.hadoop.fs.Path(dataHome);\n        org.apache.hadoop.fs.Path currentIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index\");\n        assertFalse(checkHdfsDirectory(hdfs,currentIndexDirPath));\n        hdfs.mkdirs(currentIndexDirPath);\n        assertTrue(checkHdfsDirectory(hdfs, currentIndexDirPath));\n\n        String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n        org.apache.hadoop.fs.Path oldIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index.\" + timestamp1);\n        assertFalse(checkHdfsDirectory(hdfs,oldIndexDirPath));\n        hdfs.mkdirs(oldIndexDirPath);\n        assertTrue(checkHdfsDirectory(hdfs, oldIndexDirPath));\n\n        hdfsFactory.cleanupOldIndexDirectories(dataHomePath.toString(), currentIndexDirPath.toString(), false);\n\n        assertTrue(checkHdfsDirectory(hdfs, currentIndexDirPath));\n        assertFalse(checkHdfsDirectory(hdfs, oldIndexDirPath));\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testCleanupOldIndexDirectories() throws Exception {\n    try (HdfsDirectoryFactory hdfsFactory = new HdfsDirectoryFactory()) {\n      System.setProperty(\"solr.hdfs.home\", HdfsTestUtil.getURI(dfsCluster) + \"/solr1\");\n      hdfsFactory.init(new NamedList<>());\n      String dataHome = hdfsFactory.getDataHome(new MockCoreDescriptor());\n      assertTrue(dataHome.endsWith(\"/solr1/mock/data\"));\n      System.clearProperty(\"solr.hdfs.home\");\n\n      FileSystem hdfs = dfsCluster.getFileSystem();\n\n      org.apache.hadoop.fs.Path dataHomePath = new org.apache.hadoop.fs.Path(dataHome);\n      org.apache.hadoop.fs.Path currentIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index\");\n      assertTrue(!hdfs.isDirectory(currentIndexDirPath));\n      hdfs.mkdirs(currentIndexDirPath);\n      assertTrue(hdfs.isDirectory(currentIndexDirPath));\n\n      String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n      org.apache.hadoop.fs.Path oldIndexDirPath = new org.apache.hadoop.fs.Path(dataHomePath, \"index.\" + timestamp1);\n      assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n      hdfs.mkdirs(oldIndexDirPath);\n      assertTrue(hdfs.isDirectory(oldIndexDirPath));\n\n      hdfsFactory.cleanupOldIndexDirectories(dataHomePath.toString(), currentIndexDirPath.toString(), false);\n\n      assertTrue(hdfs.isDirectory(currentIndexDirPath));\n      assertTrue(!hdfs.isDirectory(oldIndexDirPath));\n    }\n  }\n\n","bugFix":["f15af35d55d70c34451f9df5edeaeff6b31f8cbe","e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ad28156288ac00b91352582904d97e6653205757":["e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0"],"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f":["f15af35d55d70c34451f9df5edeaeff6b31f8cbe"],"e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f15af35d55d70c34451f9df5edeaeff6b31f8cbe":["ad28156288ac00b91352582904d97e6653205757"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"add53de9835b2cd1a7a80b4e0036afee171c9fdf":["2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f"]},"commit2Childs":{"ad28156288ac00b91352582904d97e6653205757":["f15af35d55d70c34451f9df5edeaeff6b31f8cbe"],"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0":["ad28156288ac00b91352582904d97e6653205757"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0"],"f15af35d55d70c34451f9df5edeaeff6b31f8cbe":["2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"add53de9835b2cd1a7a80b4e0036afee171c9fdf":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}