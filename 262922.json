{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","sourceNew":"  /** Returns the next token in the stream, or null at EOS. */\n  @Override\n  public boolean incrementToken() throws IOException {\n    clearAttributes();\n    if (!started) {\n      started = true;\n      gramSize = minGram;\n      char[] chars = new char[1024];\n      charsRead = 0;\n      // TODO: refactor to a shared readFully somewhere:\n      while (charsRead < chars.length) {\n        int inc = input.read(chars, charsRead, chars.length-charsRead);\n        if (inc == -1) {\n          break;\n        }\n        charsRead += inc;\n      }\n      inStr = new String(chars, 0, charsRead).trim();  // remove any trailing empty strings \n\n      if (charsRead == chars.length) {\n        // Read extra throwaway chars so that on end() we\n        // report the correct offset:\n        char[] throwaway = new char[1024];\n        while(true) {\n          final int inc = input.read(throwaway, 0, throwaway.length);\n          if (inc == -1) {\n            break;\n          }\n          charsRead += inc;\n        }\n      }\n\n      inLen = inStr.length();\n      if (inLen == 0) {\n        return false;\n      }\n    }\n\n    if (pos+gramSize > inLen) {            // if we hit the end of the string\n      pos = 0;                           // reset to beginning of string\n      gramSize++;                        // increase n-gram size\n      if (gramSize > maxGram)            // we are done\n        return false;\n      if (pos+gramSize > inLen)\n        return false;\n    }\n\n    int oldPos = pos;\n    pos++;\n    termAtt.setEmpty().append(inStr, oldPos, oldPos+gramSize);\n    offsetAtt.setOffset(correctOffset(oldPos), correctOffset(oldPos+gramSize));\n    return true;\n  }\n\n","sourceOld":"  /** Returns the next token in the stream, or null at EOS. */\n  @Override\n  public boolean incrementToken() throws IOException {\n    clearAttributes();\n    if (!started) {\n      started = true;\n      gramSize = minGram;\n      char[] chars = new char[1024];\n      charsRead = 0;\n      // TODO: refactor to a shared readFully somewhere:\n      while (charsRead < chars.length) {\n        int inc = input.read(chars, charsRead, chars.length-charsRead);\n        if (inc == -1) {\n          break;\n        }\n        charsRead += inc;\n      }\n      inStr = new String(chars, 0, charsRead).trim();  // remove any trailing empty strings \n\n      if (charsRead == chars.length) {\n        // Read extra throwaway chars so that on end() we\n        // report the correct offset:\n        char[] throwaway = new char[1024];\n        while(true) {\n          final int inc = input.read(throwaway, 0, throwaway.length);\n          if (inc == -1) {\n            break;\n          }\n          charsRead += inc;\n        }\n      }\n\n      inLen = inStr.length();\n      if (inLen == 0) {\n        return false;\n      }\n    }\n\n    if (pos+gramSize > inLen) {            // if we hit the end of the string\n      pos = 0;                           // reset to beginning of string\n      gramSize++;                        // increase n-gram size\n      if (gramSize > maxGram)            // we are done\n        return false;\n      if (pos+gramSize > inLen)\n        return false;\n    }\n\n    int oldPos = pos;\n    pos++;\n    termAtt.setEmpty().append(inStr, oldPos, oldPos+gramSize);\n    offsetAtt.setOffset(correctOffset(oldPos), correctOffset(oldPos+gramSize));\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4c807c4005aae1acaf5cebc9af40883985fb89a8","date":1366974206,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","sourceNew":"  /** Returns the next token in the stream, or null at EOS. */\n  @Override\n  public boolean incrementToken() throws IOException {\n    clearAttributes();\n\n    // compact\n    if (bufferStart >= buffer.length - maxGram) {\n      System.arraycopy(buffer, bufferStart, buffer, 0, bufferEnd - bufferStart);\n      bufferEnd -= bufferStart;\n      bufferStart = 0;\n\n      // fill in remaining space\n      if (!exhausted) {\n        // TODO: refactor to a shared readFully\n        while (bufferEnd < buffer.length) {\n          final int read = input.read(buffer, bufferEnd, buffer.length - bufferEnd);\n          if (read == -1) {\n            exhausted = true;\n            break;\n          }\n          bufferEnd += read;\n        }\n      }\n    }\n\n    // should we go to the next offset?\n    if (gramSize > maxGram || bufferStart + gramSize > bufferEnd) {\n      bufferStart++;\n      offset++;\n      gramSize = minGram;\n    }\n\n    // are there enough chars remaining?\n    if (bufferStart + gramSize > bufferEnd) {\n      return false;\n    }\n\n    termAtt.copyBuffer(buffer, bufferStart, gramSize);\n    posIncAtt.setPositionIncrement(1);\n    posLenAtt.setPositionLength(1);\n    offsetAtt.setOffset(correctOffset(offset), correctOffset(offset + gramSize));\n    ++gramSize;\n    return true;\n  }\n\n","sourceOld":"  /** Returns the next token in the stream, or null at EOS. */\n  @Override\n  public boolean incrementToken() throws IOException {\n    clearAttributes();\n    if (!started) {\n      started = true;\n      gramSize = minGram;\n      char[] chars = new char[1024];\n      charsRead = 0;\n      // TODO: refactor to a shared readFully somewhere:\n      while (charsRead < chars.length) {\n        int inc = input.read(chars, charsRead, chars.length-charsRead);\n        if (inc == -1) {\n          break;\n        }\n        charsRead += inc;\n      }\n      inStr = new String(chars, 0, charsRead).trim();  // remove any trailing empty strings \n\n      if (charsRead == chars.length) {\n        // Read extra throwaway chars so that on end() we\n        // report the correct offset:\n        char[] throwaway = new char[1024];\n        while(true) {\n          final int inc = input.read(throwaway, 0, throwaway.length);\n          if (inc == -1) {\n            break;\n          }\n          charsRead += inc;\n        }\n      }\n\n      inLen = inStr.length();\n      if (inLen == 0) {\n        return false;\n      }\n    }\n\n    if (pos+gramSize > inLen) {            // if we hit the end of the string\n      pos = 0;                           // reset to beginning of string\n      gramSize++;                        // increase n-gram size\n      if (gramSize > maxGram)            // we are done\n        return false;\n      if (pos+gramSize > inLen)\n        return false;\n    }\n\n    int oldPos = pos;\n    pos++;\n    termAtt.setEmpty().append(inStr, oldPos, oldPos+gramSize);\n    offsetAtt.setOffset(correctOffset(oldPos), correctOffset(oldPos+gramSize));\n    return true;\n  }\n\n","bugFix":["a82fda1447250ff156ff3b862d94a99bf0a3c23c","c46b5eed1428b2cecc6851b67142702486279f89","a7347509fad0711ac30cb15a746e9a3830a38ebd","9b5756469957918cac40a831acec9cf01c8c2bb3","8c983d9f75169f8df08cc7d8006298cddc144075","1a6997833904932b5332aede01bfd0f7ce4df318"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","date":1371043069,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n\n    // termination of this loop is guaranteed by the fact that every iteration\n    // either advances the buffer (calls consumes()) or increases gramSize\n    while (true) {\n      // compact\n      if (bufferStart >= bufferEnd - maxGram - 1 && !exhausted) {\n        System.arraycopy(buffer, bufferStart, buffer, 0, bufferEnd - bufferStart);\n        bufferEnd -= bufferStart;\n        lastCheckedChar -= bufferStart;\n        lastNonTokenChar -= bufferStart;\n        bufferStart = 0;\n\n        // fill in remaining space\n        exhausted = !charUtils.fill(charBuffer, input, buffer.length - bufferEnd);\n        // convert to code points\n        bufferEnd += charUtils.toCodePoints(charBuffer.getBuffer(), 0, charBuffer.getLength(), buffer, bufferEnd);\n      }\n\n      // should we go to the next offset?\n      if (gramSize > maxGram || (bufferStart + gramSize) > bufferEnd) {\n        if (bufferStart + 1 + minGram > bufferEnd) {\n          assert exhausted;\n          return false;\n        }\n        consume();\n        gramSize = minGram;\n      }\n\n      updateLastNonTokenChar();\n\n      // retry if the token to be emitted was going to not only contain token chars\n      final boolean termContainsNonTokenChar = lastNonTokenChar >= bufferStart && lastNonTokenChar < (bufferStart + gramSize);\n      final boolean isEdgeAndPreviousCharIsTokenChar = edgesOnly && lastNonTokenChar != bufferStart - 1;\n      if (termContainsNonTokenChar || isEdgeAndPreviousCharIsTokenChar) {\n        consume();\n        gramSize = minGram;\n        continue;\n      }\n\n      final int length = charUtils.toChars(buffer, bufferStart, gramSize, termAtt.buffer(), 0);\n      termAtt.setLength(length);\n      posIncAtt.setPositionIncrement(1);\n      posLenAtt.setPositionLength(1);\n      offsetAtt.setOffset(correctOffset(offset), correctOffset(offset + length));\n      ++gramSize;\n      return true;\n    }\n  }\n\n","sourceOld":"  /** Returns the next token in the stream, or null at EOS. */\n  @Override\n  public boolean incrementToken() throws IOException {\n    clearAttributes();\n\n    // compact\n    if (bufferStart >= buffer.length - maxGram) {\n      System.arraycopy(buffer, bufferStart, buffer, 0, bufferEnd - bufferStart);\n      bufferEnd -= bufferStart;\n      bufferStart = 0;\n\n      // fill in remaining space\n      if (!exhausted) {\n        // TODO: refactor to a shared readFully\n        while (bufferEnd < buffer.length) {\n          final int read = input.read(buffer, bufferEnd, buffer.length - bufferEnd);\n          if (read == -1) {\n            exhausted = true;\n            break;\n          }\n          bufferEnd += read;\n        }\n      }\n    }\n\n    // should we go to the next offset?\n    if (gramSize > maxGram || bufferStart + gramSize > bufferEnd) {\n      bufferStart++;\n      offset++;\n      gramSize = minGram;\n    }\n\n    // are there enough chars remaining?\n    if (bufferStart + gramSize > bufferEnd) {\n      return false;\n    }\n\n    termAtt.copyBuffer(buffer, bufferStart, gramSize);\n    posIncAtt.setPositionIncrement(1);\n    posLenAtt.setPositionLength(1);\n    offsetAtt.setOffset(correctOffset(offset), correctOffset(offset + gramSize));\n    ++gramSize;\n    return true;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc","date":1465824262,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n\n    // termination of this loop is guaranteed by the fact that every iteration\n    // either advances the buffer (calls consumes()) or increases gramSize\n    while (true) {\n      // compact\n      if (bufferStart >= bufferEnd - maxGram - 1 && !exhausted) {\n        System.arraycopy(buffer, bufferStart, buffer, 0, bufferEnd - bufferStart);\n        bufferEnd -= bufferStart;\n        lastCheckedChar -= bufferStart;\n        lastNonTokenChar -= bufferStart;\n        bufferStart = 0;\n\n        // fill in remaining space\n        exhausted = !CharacterUtils.fill(charBuffer, input, buffer.length - bufferEnd);\n        // convert to code points\n        bufferEnd += CharacterUtils.toCodePoints(charBuffer.getBuffer(), 0, charBuffer.getLength(), buffer, bufferEnd);\n      }\n\n      // should we go to the next offset?\n      if (gramSize > maxGram || (bufferStart + gramSize) > bufferEnd) {\n        if (bufferStart + 1 + minGram > bufferEnd) {\n          assert exhausted;\n          return false;\n        }\n        consume();\n        gramSize = minGram;\n      }\n\n      updateLastNonTokenChar();\n\n      // retry if the token to be emitted was going to not only contain token chars\n      final boolean termContainsNonTokenChar = lastNonTokenChar >= bufferStart && lastNonTokenChar < (bufferStart + gramSize);\n      final boolean isEdgeAndPreviousCharIsTokenChar = edgesOnly && lastNonTokenChar != bufferStart - 1;\n      if (termContainsNonTokenChar || isEdgeAndPreviousCharIsTokenChar) {\n        consume();\n        gramSize = minGram;\n        continue;\n      }\n\n      final int length = CharacterUtils.toChars(buffer, bufferStart, gramSize, termAtt.buffer(), 0);\n      termAtt.setLength(length);\n      posIncAtt.setPositionIncrement(1);\n      posLenAtt.setPositionLength(1);\n      offsetAtt.setOffset(correctOffset(offset), correctOffset(offset + length));\n      ++gramSize;\n      return true;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n\n    // termination of this loop is guaranteed by the fact that every iteration\n    // either advances the buffer (calls consumes()) or increases gramSize\n    while (true) {\n      // compact\n      if (bufferStart >= bufferEnd - maxGram - 1 && !exhausted) {\n        System.arraycopy(buffer, bufferStart, buffer, 0, bufferEnd - bufferStart);\n        bufferEnd -= bufferStart;\n        lastCheckedChar -= bufferStart;\n        lastNonTokenChar -= bufferStart;\n        bufferStart = 0;\n\n        // fill in remaining space\n        exhausted = !charUtils.fill(charBuffer, input, buffer.length - bufferEnd);\n        // convert to code points\n        bufferEnd += charUtils.toCodePoints(charBuffer.getBuffer(), 0, charBuffer.getLength(), buffer, bufferEnd);\n      }\n\n      // should we go to the next offset?\n      if (gramSize > maxGram || (bufferStart + gramSize) > bufferEnd) {\n        if (bufferStart + 1 + minGram > bufferEnd) {\n          assert exhausted;\n          return false;\n        }\n        consume();\n        gramSize = minGram;\n      }\n\n      updateLastNonTokenChar();\n\n      // retry if the token to be emitted was going to not only contain token chars\n      final boolean termContainsNonTokenChar = lastNonTokenChar >= bufferStart && lastNonTokenChar < (bufferStart + gramSize);\n      final boolean isEdgeAndPreviousCharIsTokenChar = edgesOnly && lastNonTokenChar != bufferStart - 1;\n      if (termContainsNonTokenChar || isEdgeAndPreviousCharIsTokenChar) {\n        consume();\n        gramSize = minGram;\n        continue;\n      }\n\n      final int length = charUtils.toChars(buffer, bufferStart, gramSize, termAtt.buffer(), 0);\n      termAtt.setLength(length);\n      posIncAtt.setPositionIncrement(1);\n      posLenAtt.setPositionLength(1);\n      offsetAtt.setOffset(correctOffset(offset), correctOffset(offset + length));\n      ++gramSize;\n      return true;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79","date":1465913303,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n\n    // termination of this loop is guaranteed by the fact that every iteration\n    // either advances the buffer (calls consumes()) or increases gramSize\n    while (true) {\n      // compact\n      if (bufferStart >= bufferEnd - maxGram - 1 && !exhausted) {\n        System.arraycopy(buffer, bufferStart, buffer, 0, bufferEnd - bufferStart);\n        bufferEnd -= bufferStart;\n        lastCheckedChar -= bufferStart;\n        lastNonTokenChar -= bufferStart;\n        bufferStart = 0;\n\n        // fill in remaining space\n        exhausted = !CharacterUtils.fill(charBuffer, input, buffer.length - bufferEnd);\n        // convert to code points\n        bufferEnd += CharacterUtils.toCodePoints(charBuffer.getBuffer(), 0, charBuffer.getLength(), buffer, bufferEnd);\n      }\n\n      // should we go to the next offset?\n      if (gramSize > maxGram || (bufferStart + gramSize) > bufferEnd) {\n        if (bufferStart + 1 + minGram > bufferEnd) {\n          assert exhausted;\n          return false;\n        }\n        consume();\n        gramSize = minGram;\n      }\n\n      updateLastNonTokenChar();\n\n      // retry if the token to be emitted was going to not only contain token chars\n      final boolean termContainsNonTokenChar = lastNonTokenChar >= bufferStart && lastNonTokenChar < (bufferStart + gramSize);\n      final boolean isEdgeAndPreviousCharIsTokenChar = edgesOnly && lastNonTokenChar != bufferStart - 1;\n      if (termContainsNonTokenChar || isEdgeAndPreviousCharIsTokenChar) {\n        consume();\n        gramSize = minGram;\n        continue;\n      }\n\n      final int length = CharacterUtils.toChars(buffer, bufferStart, gramSize, termAtt.buffer(), 0);\n      termAtt.setLength(length);\n      posIncAtt.setPositionIncrement(1);\n      posLenAtt.setPositionLength(1);\n      offsetAtt.setOffset(correctOffset(offset), correctOffset(offset + length));\n      ++gramSize;\n      return true;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n\n    // termination of this loop is guaranteed by the fact that every iteration\n    // either advances the buffer (calls consumes()) or increases gramSize\n    while (true) {\n      // compact\n      if (bufferStart >= bufferEnd - maxGram - 1 && !exhausted) {\n        System.arraycopy(buffer, bufferStart, buffer, 0, bufferEnd - bufferStart);\n        bufferEnd -= bufferStart;\n        lastCheckedChar -= bufferStart;\n        lastNonTokenChar -= bufferStart;\n        bufferStart = 0;\n\n        // fill in remaining space\n        exhausted = !charUtils.fill(charBuffer, input, buffer.length - bufferEnd);\n        // convert to code points\n        bufferEnd += charUtils.toCodePoints(charBuffer.getBuffer(), 0, charBuffer.getLength(), buffer, bufferEnd);\n      }\n\n      // should we go to the next offset?\n      if (gramSize > maxGram || (bufferStart + gramSize) > bufferEnd) {\n        if (bufferStart + 1 + minGram > bufferEnd) {\n          assert exhausted;\n          return false;\n        }\n        consume();\n        gramSize = minGram;\n      }\n\n      updateLastNonTokenChar();\n\n      // retry if the token to be emitted was going to not only contain token chars\n      final boolean termContainsNonTokenChar = lastNonTokenChar >= bufferStart && lastNonTokenChar < (bufferStart + gramSize);\n      final boolean isEdgeAndPreviousCharIsTokenChar = edgesOnly && lastNonTokenChar != bufferStart - 1;\n      if (termContainsNonTokenChar || isEdgeAndPreviousCharIsTokenChar) {\n        consume();\n        gramSize = minGram;\n        continue;\n      }\n\n      final int length = charUtils.toChars(buffer, bufferStart, gramSize, termAtt.buffer(), 0);\n      termAtt.setLength(length);\n      posIncAtt.setPositionIncrement(1);\n      posLenAtt.setPositionLength(1);\n      offsetAtt.setOffset(correctOffset(offset), correctOffset(offset + length));\n      ++gramSize;\n      return true;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n\n    // termination of this loop is guaranteed by the fact that every iteration\n    // either advances the buffer (calls consumes()) or increases gramSize\n    while (true) {\n      // compact\n      if (bufferStart >= bufferEnd - maxGram - 1 && !exhausted) {\n        System.arraycopy(buffer, bufferStart, buffer, 0, bufferEnd - bufferStart);\n        bufferEnd -= bufferStart;\n        lastCheckedChar -= bufferStart;\n        lastNonTokenChar -= bufferStart;\n        bufferStart = 0;\n\n        // fill in remaining space\n        exhausted = !CharacterUtils.fill(charBuffer, input, buffer.length - bufferEnd);\n        // convert to code points\n        bufferEnd += CharacterUtils.toCodePoints(charBuffer.getBuffer(), 0, charBuffer.getLength(), buffer, bufferEnd);\n      }\n\n      // should we go to the next offset?\n      if (gramSize > maxGram || (bufferStart + gramSize) > bufferEnd) {\n        if (bufferStart + 1 + minGram > bufferEnd) {\n          assert exhausted;\n          return false;\n        }\n        consume();\n        gramSize = minGram;\n      }\n\n      updateLastNonTokenChar();\n\n      // retry if the token to be emitted was going to not only contain token chars\n      final boolean termContainsNonTokenChar = lastNonTokenChar >= bufferStart && lastNonTokenChar < (bufferStart + gramSize);\n      final boolean isEdgeAndPreviousCharIsTokenChar = edgesOnly && lastNonTokenChar != bufferStart - 1;\n      if (termContainsNonTokenChar || isEdgeAndPreviousCharIsTokenChar) {\n        consume();\n        gramSize = minGram;\n        continue;\n      }\n\n      final int length = CharacterUtils.toChars(buffer, bufferStart, gramSize, termAtt.buffer(), 0);\n      termAtt.setLength(length);\n      posIncAtt.setPositionIncrement(1);\n      posLenAtt.setPositionLength(1);\n      offsetAtt.setOffset(correctOffset(offset), correctOffset(offset + length));\n      ++gramSize;\n      return true;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    clearAttributes();\n\n    // termination of this loop is guaranteed by the fact that every iteration\n    // either advances the buffer (calls consumes()) or increases gramSize\n    while (true) {\n      // compact\n      if (bufferStart >= bufferEnd - maxGram - 1 && !exhausted) {\n        System.arraycopy(buffer, bufferStart, buffer, 0, bufferEnd - bufferStart);\n        bufferEnd -= bufferStart;\n        lastCheckedChar -= bufferStart;\n        lastNonTokenChar -= bufferStart;\n        bufferStart = 0;\n\n        // fill in remaining space\n        exhausted = !charUtils.fill(charBuffer, input, buffer.length - bufferEnd);\n        // convert to code points\n        bufferEnd += charUtils.toCodePoints(charBuffer.getBuffer(), 0, charBuffer.getLength(), buffer, bufferEnd);\n      }\n\n      // should we go to the next offset?\n      if (gramSize > maxGram || (bufferStart + gramSize) > bufferEnd) {\n        if (bufferStart + 1 + minGram > bufferEnd) {\n          assert exhausted;\n          return false;\n        }\n        consume();\n        gramSize = minGram;\n      }\n\n      updateLastNonTokenChar();\n\n      // retry if the token to be emitted was going to not only contain token chars\n      final boolean termContainsNonTokenChar = lastNonTokenChar >= bufferStart && lastNonTokenChar < (bufferStart + gramSize);\n      final boolean isEdgeAndPreviousCharIsTokenChar = edgesOnly && lastNonTokenChar != bufferStart - 1;\n      if (termContainsNonTokenChar || isEdgeAndPreviousCharIsTokenChar) {\n        consume();\n        gramSize = minGram;\n        continue;\n      }\n\n      final int length = charUtils.toChars(buffer, bufferStart, gramSize, termAtt.buffer(), 0);\n      termAtt.setLength(length);\n      posIncAtt.setPositionIncrement(1);\n      posLenAtt.setPositionLength(1);\n      offsetAtt.setOffset(correctOffset(offset), correctOffset(offset + length));\n      ++gramSize;\n      return true;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["4c807c4005aae1acaf5cebc9af40883985fb89a8"],"fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4c807c4005aae1acaf5cebc9af40883985fb89a8":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["4c807c4005aae1acaf5cebc9af40883985fb89a8"],"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc","57dc82c7b33cd580e7ab5179019bc78f3d7f8e79","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc":["57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"4c807c4005aae1acaf5cebc9af40883985fb89a8":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"],"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}