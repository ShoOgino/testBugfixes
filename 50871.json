{"path":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","commits":[{"id":"1ea2ca01fc0b1375f71a47571ffacc7924742b30","date":1185994483,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 100;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker\",\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (Iterator it = stats.iterator(); it.hasNext();) {\n      TaskStats stat = (TaskStats) it.next();\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      termDocs.seek(terms.term());\n      while(termDocs.next())\n        totalTokenCount2 += termDocs.freq();\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6944b9fa6d8ef96b83ae2d3a4332d03b3857355b","date":1245355139,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 100;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource\",\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (Iterator it = stats.iterator(); it.hasNext();) {\n      TaskStats stat = (TaskStats) it.next();\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      termDocs.seek(terms.term());\n      while(termDocs.next())\n        totalTokenCount2 += termDocs.freq();\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 100;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker\",\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (Iterator it = stats.iterator(); it.hasNext();) {\n      TaskStats stat = (TaskStats) it.next();\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      termDocs.seek(terms.term());\n      while(termDocs.next())\n        totalTokenCount2 += termDocs.freq();\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4256bc1b3c94786287ccdfc751230374521843cf","date":1254612273,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 100;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource\",\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (Iterator it = stats.iterator(); it.hasNext();) {\n      TaskStats stat = (TaskStats) it.next();\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      termDocs.seek(terms.term());\n      while(termDocs.next())\n        totalTokenCount2 += termDocs.freq();\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 100;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource\",\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (Iterator it = stats.iterator(); it.hasNext();) {\n      TaskStats stat = (TaskStats) it.next();\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory());\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      termDocs.seek(terms.term());\n      while(termDocs.next())\n        totalTokenCount2 += termDocs.freq();\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90cb6b3f4e5652555b614adc90204287fbebd27c","date":1259494272,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (Iterator it = stats.iterator(); it.hasNext();) {\n      TaskStats stat = (TaskStats) it.next();\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      termDocs.seek(terms.term());\n      while(termDocs.next())\n        totalTokenCount2 += termDocs.freq();\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 100;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource\",\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (Iterator it = stats.iterator(); it.hasNext();) {\n      TaskStats stat = (TaskStats) it.next();\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      termDocs.seek(terms.term());\n      while(termDocs.next())\n        totalTokenCount2 += termDocs.freq();\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7848880b3c06f09f0f3ac50d0854b16efb0b815e","date":1260006234,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      termDocs.seek(terms.term());\n      while(termDocs.next())\n        totalTokenCount2 += termDocs.freq();\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (Iterator it = stats.iterator(); it.hasNext();) {\n      TaskStats stat = (TaskStats) it.next();\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      termDocs.seek(terms.term());\n      while(termDocs.next())\n        totalTokenCount2 += termDocs.freq();\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19902de501347481fdd1e781868986601e2a7c7b","date":1263326777,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      Term term = terms.term();\n      /* not-tokenized, but indexed field */\n      if (term != null && term.field() != DocMaker.ID_FIELD) { \n        termDocs.seek(terms.term());\n        while (termDocs.next())\n          totalTokenCount2 += termDocs.freq();\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      termDocs.seek(terms.term());\n      while(termDocs.next())\n        totalTokenCount2 += termDocs.freq();\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      Term term = terms.term();\n      /* not-tokenized, but indexed field */\n      if (term != null && term.field() != DocMaker.ID_FIELD) { \n        termDocs.seek(terms.term());\n        while (termDocs.next())\n          totalTokenCount2 += termDocs.freq();\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      Term term = terms.term();\n      /* not-tokenized, but indexed field */\n      if (term != null && term.field() != DocMaker.ID_FIELD) { \n        termDocs.seek(terms.term());\n        while (termDocs.next())\n          totalTokenCount2 += termDocs.freq();\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1ea2ca01fc0b1375f71a47571ffacc7924742b30":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"90cb6b3f4e5652555b614adc90204287fbebd27c":["4256bc1b3c94786287ccdfc751230374521843cf"],"6944b9fa6d8ef96b83ae2d3a4332d03b3857355b":["1ea2ca01fc0b1375f71a47571ffacc7924742b30"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"19902de501347481fdd1e781868986601e2a7c7b":["7848880b3c06f09f0f3ac50d0854b16efb0b815e"],"7848880b3c06f09f0f3ac50d0854b16efb0b815e":["90cb6b3f4e5652555b614adc90204287fbebd27c"],"4256bc1b3c94786287ccdfc751230374521843cf":["6944b9fa6d8ef96b83ae2d3a4332d03b3857355b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["19902de501347481fdd1e781868986601e2a7c7b"]},"commit2Childs":{"1ea2ca01fc0b1375f71a47571ffacc7924742b30":["6944b9fa6d8ef96b83ae2d3a4332d03b3857355b"],"90cb6b3f4e5652555b614adc90204287fbebd27c":["7848880b3c06f09f0f3ac50d0854b16efb0b815e"],"6944b9fa6d8ef96b83ae2d3a4332d03b3857355b":["4256bc1b3c94786287ccdfc751230374521843cf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1ea2ca01fc0b1375f71a47571ffacc7924742b30"],"4256bc1b3c94786287ccdfc751230374521843cf":["90cb6b3f4e5652555b614adc90204287fbebd27c"],"19902de501347481fdd1e781868986601e2a7c7b":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"7848880b3c06f09f0f3ac50d0854b16efb0b815e":["19902de501347481fdd1e781868986601e2a7c7b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}