{"path":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","commits":[{"id":"a870f9917149dc600c4ad4417d615c1795de5864","date":1445975387,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      numValues++;\n    }\n    if (minLength == maxLength) {\n      // no index needed: direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene50DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      numValues++;\n    }\n    if (minLength == maxLength) {\n      // no index needed: direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene50DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52a55f5afb81e4ca7ec581bf1a7b8218204e8af7","date":1472142029,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":null,"sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      numValues++;\n    }\n    if (minLength == maxLength) {\n      // no index needed: direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene50DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a","date":1472163016,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":null,"sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      numValues++;\n    }\n    if (minLength == maxLength) {\n      // no index needed: direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene50DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":null,"sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      numValues++;\n    }\n    if (minLength == maxLength) {\n      // no index needed: direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene50DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a870f9917149dc600c4ad4417d615c1795de5864":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a":["a870f9917149dc600c4ad4417d615c1795de5864","52a55f5afb81e4ca7ec581bf1a7b8218204e8af7"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a870f9917149dc600c4ad4417d615c1795de5864","e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a"],"52a55f5afb81e4ca7ec581bf1a7b8218204e8af7":["a870f9917149dc600c4ad4417d615c1795de5864"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a870f9917149dc600c4ad4417d615c1795de5864"],"a870f9917149dc600c4ad4417d615c1795de5864":["e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","52a55f5afb81e4ca7ec581bf1a7b8218204e8af7"],"e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"52a55f5afb81e4ca7ec581bf1a7b8218204e8af7":["e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}