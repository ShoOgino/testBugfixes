{"path":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","commits":[{"id":"a851824c09818632c94eba41e60ef5e72e323c8e","date":1337355760,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(MutableFieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(MutableFieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18","date":1339188570,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a","date":1341524239,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n      \n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, (StorableField) field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","date":1341839195,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      fieldsWriter.addField(field, fp.fieldInfo);\n      \n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, (StorableField) field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n      \n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, (StorableField) field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a7e4907084808af8fdb14b9809e6dceaccf6867b","date":1343473006,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      fieldsWriter.addField(field, fp.fieldInfo);\n      \n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      fieldsWriter.addField(field, fp.fieldInfo);\n      \n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, (StorableField) field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      fieldsWriter.addField(field, fp.fieldInfo);\n      \n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"084ac3d057e654193ccee4e8f65eab8bc24104a8","date":1346787217,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      fieldsWriter.addField(field, fp.fieldInfo);\n      \n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6647091125f681395cbde9bb2b7b947cc4ef9bb3","date":1352400554,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      // nocommit the DV indexing should be just another\n      // consumer in the chain, not stuck inside here?  this\n      // source should just \"dispatch\"\n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        switch(dvType) {\n        case BYTES_VAR_STRAIGHT:\n          fp.addBytesDVField(docState.docID, field.binaryValue());\n          break;\n        default:\n          break;\n        }\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9b637fb447c5b4257f6b4532d84ca91e456c1f2a","date":1352405059,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      // nocommit the DV indexing should be just another\n      // consumer in the chain, not stuck inside here?  this\n      // source should just \"dispatch\"\n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        switch(dvType) {\n        case BYTES_VAR_STRAIGHT:\n          fp.addBytesDVField(docState.docID, field.binaryValue());\n          break;\n        case VAR_INTS:\n        case FIXED_INTS_8:\n        case FIXED_INTS_16:\n        case FIXED_INTS_32:\n        case FIXED_INTS_64:\n          fp.addNumberDVField(docState.docID, field.numericValue());\n          break;\n        default:\n          break;\n        }\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      // nocommit the DV indexing should be just another\n      // consumer in the chain, not stuck inside here?  this\n      // source should just \"dispatch\"\n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        switch(dvType) {\n        case BYTES_VAR_STRAIGHT:\n          fp.addBytesDVField(docState.docID, field.binaryValue());\n          break;\n        default:\n          break;\n        }\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"71df1db89d3a713f022b58111aafd14a4b352da0","date":1352479848,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      // nocommit the DV indexing should be just another\n      // consumer in the chain, not stuck inside here?  this\n      // source should just \"dispatch\"\n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        switch(dvType) {\n        case BYTES_VAR_STRAIGHT:\n        case BYTES_FIXED_STRAIGHT:\n          fp.addBytesDVField(docState.docID, field.binaryValue());\n          break;\n        case BYTES_VAR_SORTED:\n        case BYTES_FIXED_SORTED:\n        case BYTES_VAR_DEREF:\n        case BYTES_FIXED_DEREF:\n          fp.addSortedBytesDVField(docState.docID, field.binaryValue());\n          break;\n        case VAR_INTS:\n        case FIXED_INTS_8:\n        case FIXED_INTS_16:\n        case FIXED_INTS_32:\n        case FIXED_INTS_64:\n          fp.addNumberDVField(docState.docID, field.numericValue());\n          break;\n        default:\n          break;\n        }\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      // nocommit the DV indexing should be just another\n      // consumer in the chain, not stuck inside here?  this\n      // source should just \"dispatch\"\n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        switch(dvType) {\n        case BYTES_VAR_STRAIGHT:\n          fp.addBytesDVField(docState.docID, field.binaryValue());\n          break;\n        case VAR_INTS:\n        case FIXED_INTS_8:\n        case FIXED_INTS_16:\n        case FIXED_INTS_32:\n        case FIXED_INTS_64:\n          fp.addNumberDVField(docState.docID, field.numericValue());\n          break;\n        default:\n          break;\n        }\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6295f14d43685811599f8a8f02a63d75ec6bd8fe","date":1353248103,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      // nocommit the DV indexing should be just another\n      // consumer in the chain, not stuck inside here?  this\n      // source should just \"dispatch\"\n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        switch(dvType) {\n        case BYTES_VAR_STRAIGHT:\n        case BYTES_FIXED_STRAIGHT:\n          fp.addBytesDVField(docState.docID, field.binaryValue());\n          break;\n        case BYTES_VAR_SORTED:\n        case BYTES_FIXED_SORTED:\n        case BYTES_VAR_DEREF:\n        case BYTES_FIXED_DEREF:\n          fp.addSortedBytesDVField(docState.docID, field.binaryValue());\n          break;\n        case VAR_INTS:\n        case FIXED_INTS_8:\n        case FIXED_INTS_16:\n        case FIXED_INTS_32:\n        case FIXED_INTS_64:\n          fp.addNumberDVField(docState.docID, field.numericValue());\n          break;\n        case FLOAT_32:\n          fp.addFloatDVField(docState.docID, field.numericValue());\n          break;\n        case FLOAT_64:\n          fp.addDoubleDVField(docState.docID, field.numericValue());\n          break;\n        default:\n          break;\n        }\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      // nocommit the DV indexing should be just another\n      // consumer in the chain, not stuck inside here?  this\n      // source should just \"dispatch\"\n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        switch(dvType) {\n        case BYTES_VAR_STRAIGHT:\n        case BYTES_FIXED_STRAIGHT:\n          fp.addBytesDVField(docState.docID, field.binaryValue());\n          break;\n        case BYTES_VAR_SORTED:\n        case BYTES_FIXED_SORTED:\n        case BYTES_VAR_DEREF:\n        case BYTES_FIXED_DEREF:\n          fp.addSortedBytesDVField(docState.docID, field.binaryValue());\n          break;\n        case VAR_INTS:\n        case FIXED_INTS_8:\n        case FIXED_INTS_16:\n        case FIXED_INTS_32:\n        case FIXED_INTS_64:\n          fp.addNumberDVField(docState.docID, field.numericValue());\n          break;\n        default:\n          break;\n        }\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"127981e5a1e1d1425c5fdc816ceacf753ca70ee4","date":1354205321,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n      \n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      // nocommit the DV indexing should be just another\n      // consumer in the chain, not stuck inside here?  this\n      // source should just \"dispatch\"\n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        switch(dvType) {\n        case BYTES_VAR_STRAIGHT:\n        case BYTES_FIXED_STRAIGHT:\n          fp.addBytesDVField(docState.docID, field.binaryValue());\n          break;\n        case BYTES_VAR_SORTED:\n        case BYTES_FIXED_SORTED:\n        case BYTES_VAR_DEREF:\n        case BYTES_FIXED_DEREF:\n          fp.addSortedBytesDVField(docState.docID, field.binaryValue());\n          break;\n        case VAR_INTS:\n        case FIXED_INTS_8:\n        case FIXED_INTS_16:\n        case FIXED_INTS_32:\n        case FIXED_INTS_64:\n          fp.addNumberDVField(docState.docID, field.numericValue());\n          break;\n        case FLOAT_32:\n          fp.addFloatDVField(docState.docID, field.numericValue());\n          break;\n        case FLOAT_64:\n          fp.addDoubleDVField(docState.docID, field.numericValue());\n          break;\n        default:\n          break;\n        }\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0837ab0472feecb3a54260729d845f839e1cbd72","date":1358283639,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n      \n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n      if (ft.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      \n      final DocValues.Type dvType = ft.docValueType();\n      if (dvType != null) {\n        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,\n            docState, fp.fieldInfo);\n        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;\n        if (docValuesConsumer.compatibility == null) {\n          consumer.add(docState.docID, field);\n          docValuesConsumer.compatibility = new TypeCompatibility(dvType,\n              consumer.getValueSize());\n        } else if (docValuesConsumer.compatibility.isCompatible(dvType,\n            TypePromoter.getValueSize(dvType, field.binaryValue()))) {\n          consumer.add(docState.docID, field);\n        } else {\n          docValuesConsumer.compatibility.isCompatible(dvType,\n              TypePromoter.getValueSize(dvType, field.binaryValue()));\n          TypeCompatibility compatibility = docValuesConsumer.compatibility;\n          throw new IllegalArgumentException(\"Incompatible DocValues type: \"\n              + dvType.name() + \" size: \"\n              + TypePromoter.getValueSize(dvType, field.binaryValue())\n              + \" expected: \" + \" type: \" + compatibility.getBaseType()\n              + \" size: \" + compatibility.getBaseSize());\n        }\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a","5350389bf83287111f7760b9e3db3af8e3648474","135621f3a0670a9394eb563224a3b76cc4dddc0f","fa0f44f887719e97183771e977cfc4bfb485b766","a7e4907084808af8fdb14b9809e6dceaccf6867b","99351c613f288821fa2b1fa505fe5cbab9ab0600","cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18","084ac3d057e654193ccee4e8f65eab8bc24104a8"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dcc555744b1a581a4beccd0b75f8d3fe49735a2f","date":1367588265,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.introSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"426a4760316fc52cf79e191cadfcb328dfc2d1ca","date":1394042725,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.introSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.introSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.introSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.introSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":null,"sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.introSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":null,"sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.introSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":4,"author":"Dawid Weiss","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos.Builder).mjava","sourceNew":null,"sourceOld":"  @Override\n  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    storedConsumer.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc.indexableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n\n      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);\n\n      fp.addField(field);\n    }\n\n    for (StorableField field: docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType ft = field.fieldType();\n      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);\n      storedConsumer.addField(docState.docID, field, fieldInfo);\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.introSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f","426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"6295f14d43685811599f8a8f02a63d75ec6bd8fe":["71df1db89d3a713f022b58111aafd14a4b352da0"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["426a4760316fc52cf79e191cadfcb328dfc2d1ca","3394716f52b34ab259ad5247e7595d9f9db6e935"],"426a4760316fc52cf79e191cadfcb328dfc2d1ca":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f"],"71df1db89d3a713f022b58111aafd14a4b352da0":["9b637fb447c5b4257f6b4532d84ca91e456c1f2a"],"6647091125f681395cbde9bb2b7b947cc4ef9bb3":["084ac3d057e654193ccee4e8f65eab8bc24104a8"],"a851824c09818632c94eba41e60ef5e72e323c8e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["426a4760316fc52cf79e191cadfcb328dfc2d1ca","52c7e49be259508735752fba88085255014a6ecf"],"cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"1d028314cced5858683a1bb4741423d0f934257b":["cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18","a7e4907084808af8fdb14b9809e6dceaccf6867b"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a851824c09818632c94eba41e60ef5e72e323c8e"],"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a":["cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["084ac3d057e654193ccee4e8f65eab8bc24104a8","0837ab0472feecb3a54260729d845f839e1cbd72"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"127981e5a1e1d1425c5fdc816ceacf753ca70ee4":["6295f14d43685811599f8a8f02a63d75ec6bd8fe"],"dcc555744b1a581a4beccd0b75f8d3fe49735a2f":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"0837ab0472feecb3a54260729d845f839e1cbd72":["127981e5a1e1d1425c5fdc816ceacf753ca70ee4"],"9b637fb447c5b4257f6b4532d84ca91e456c1f2a":["6647091125f681395cbde9bb2b7b947cc4ef9bb3"],"52c7e49be259508735752fba88085255014a6ecf":["426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"a7e4907084808af8fdb14b9809e6dceaccf6867b":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"084ac3d057e654193ccee4e8f65eab8bc24104a8":["1d028314cced5858683a1bb4741423d0f934257b"]},"commit2Childs":{"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["a7e4907084808af8fdb14b9809e6dceaccf6867b"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"6295f14d43685811599f8a8f02a63d75ec6bd8fe":["127981e5a1e1d1425c5fdc816ceacf753ca70ee4"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"426a4760316fc52cf79e191cadfcb328dfc2d1ca":["96ea64d994d340044e0d57aeb6a5871539d10ca5","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"71df1db89d3a713f022b58111aafd14a4b352da0":["6295f14d43685811599f8a8f02a63d75ec6bd8fe"],"6647091125f681395cbde9bb2b7b947cc4ef9bb3":["9b637fb447c5b4257f6b4532d84ca91e456c1f2a"],"a851824c09818632c94eba41e60ef5e72e323c8e":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18":["1d028314cced5858683a1bb4741423d0f934257b","33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a"],"1d028314cced5858683a1bb4741423d0f934257b":["084ac3d057e654193ccee4e8f65eab8bc24104a8"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18"],"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a851824c09818632c94eba41e60ef5e72e323c8e","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"127981e5a1e1d1425c5fdc816ceacf753ca70ee4":["0837ab0472feecb3a54260729d845f839e1cbd72"],"dcc555744b1a581a4beccd0b75f8d3fe49735a2f":["96ea64d994d340044e0d57aeb6a5871539d10ca5","426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"9b637fb447c5b4257f6b4532d84ca91e456c1f2a":["71df1db89d3a713f022b58111aafd14a4b352da0"],"0837ab0472feecb3a54260729d845f839e1cbd72":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"a7e4907084808af8fdb14b9809e6dceaccf6867b":["1d028314cced5858683a1bb4741423d0f934257b"],"084ac3d057e654193ccee4e8f65eab8bc24104a8":["6647091125f681395cbde9bb2b7b947cc4ef9bb3","d4d69c535930b5cce125cff868d40f6373dc27d4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}