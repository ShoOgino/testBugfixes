{"path":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","commits":[{"id":"06805da26538ed636bd89b10c2699cc3834032ae","date":1395132972,"type":0,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"/dev/null","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateBinaryDocValue(updateTerm, \"f\" + field, toBytes(value));\n      writer.updateBinaryDocValue(updateTerm, \"cf\" + field, toBytes(value * 2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    BytesRef scratch = new BytesRef();\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j, scratch), getValue(f, j, scratch) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateBinaryDocValue(updateTerm, \"f\" + field, toBytes(value));\n      writer.updateBinaryDocValue(updateTerm, \"cf\" + field, toBytes(value * 2));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    BytesRef scratch = new BytesRef();\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j, scratch), getValue(f, j, scratch) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateBinaryDocValue(updateTerm, \"f\" + field, toBytes(value));\n      writer.updateBinaryDocValue(updateTerm, \"cf\" + field, toBytes(value * 2));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    BytesRef scratch = new BytesRef();\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j, scratch), getValue(f, j, scratch) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70285ef5917fa2c8feec026d4be4d9c20fa89162","date":1401366288,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    BytesRef scratch = new BytesRef();\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j, scratch), getValue(f, j, scratch) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateBinaryDocValue(updateTerm, \"f\" + field, toBytes(value));\n      writer.updateBinaryDocValue(updateTerm, \"cf\" + field, toBytes(value * 2));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    BytesRef scratch = new BytesRef();\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j, scratch), getValue(f, j, scratch) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","date":1401983689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    BytesRef scratch = new BytesRef();\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j, scratch), getValue(f, j, scratch) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.shutdown();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        LeafReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (AtomicReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        AtomicReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a87ce200bba7d88024e2f1c4012212072ce8a5ae","date":1417031281,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        LeafReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        LeafReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        LeafReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf), getValue(f) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        LeafReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        LeafReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf), getValue(f) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        LeafReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates#testTonsOfUpdates().mjava","sourceNew":"  @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        LeafReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(j, f.nextDoc());\n          assertEquals(j, cf.nextDoc());\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf), getValue(f) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","sourceOld":"  @Nightly\n  public void testTonsOfUpdates() throws Exception {\n    // LUCENE-5248: make sure that when there are many updates, we don't use too much RAM\n    Directory dir = newDirectory();\n    final Random random = random();\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));\n    conf.setRAMBufferSizeMB(IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB);\n    conf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH); // don't flush by doc\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    // test data: lots of documents (few 10Ks) and lots of update terms (few hundreds)\n    final int numDocs = atLeast(20000);\n    final int numBinaryFields = atLeast(5);\n    final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs\n    Set<String> updateTerms = new HashSet<>();\n    while (updateTerms.size() < numTerms) {\n      updateTerms.add(TestUtil.randomSimpleString(random));\n    }\n\n//    System.out.println(\"numDocs=\" + numDocs + \" numBinaryFields=\" + numBinaryFields + \" numTerms=\" + numTerms);\n    \n    // build a large index with many BDV fields and update terms\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numUpdateTerms = TestUtil.nextInt(random, 1, numTerms / 10);\n      for (int j = 0; j < numUpdateTerms; j++) {\n        doc.add(new StringField(\"upd\", RandomPicks.randomFrom(random, updateTerms), Store.NO));\n      }\n      for (int j = 0; j < numBinaryFields; j++) {\n        long val = random.nextInt();\n        doc.add(new BinaryDocValuesField(\"f\" + j, toBytes(val)));\n        doc.add(new BinaryDocValuesField(\"cf\" + j, toBytes(val * 2)));\n      }\n      writer.addDocument(doc);\n    }\n    \n    writer.commit(); // commit so there's something to apply to\n    \n    // set to flush every 2048 bytes (approximately every 12 updates), so we get\n    // many flushes during binary updates\n    writer.getConfig().setRAMBufferSizeMB(2048.0 / 1024 / 1024);\n    final int numUpdates = atLeast(100);\n//    System.out.println(\"numUpdates=\" + numUpdates);\n    for (int i = 0; i < numUpdates; i++) {\n      int field = random.nextInt(numBinaryFields);\n      Term updateTerm = new Term(\"upd\", RandomPicks.randomFrom(random, updateTerms));\n      long value = random.nextInt();\n      writer.updateDocValues(updateTerm, new BinaryDocValuesField(\"f\" + field, toBytes(value)), \n          new BinaryDocValuesField(\"cf\" + field, toBytes(value * 2)));\n    }\n\n    writer.close();\n    \n    DirectoryReader reader = DirectoryReader.open(dir);\n    for (LeafReaderContext context : reader.leaves()) {\n      for (int i = 0; i < numBinaryFields; i++) {\n        LeafReader r = context.reader();\n        BinaryDocValues f = r.getBinaryDocValues(\"f\" + i);\n        BinaryDocValues cf = r.getBinaryDocValues(\"cf\" + i);\n        for (int j = 0; j < r.maxDoc(); j++) {\n          assertEquals(\"reader=\" + r + \", field=f\" + i + \", doc=\" + j, getValue(cf, j), getValue(f, j) * 2);\n        }\n      }\n    }\n    reader.close();\n    \n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["70285ef5917fa2c8feec026d4be4d9c20fa89162"],"06805da26538ed636bd89b10c2699cc3834032ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"70285ef5917fa2c8feec026d4be4d9c20fa89162":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a87ce200bba7d88024e2f1c4012212072ce8a5ae","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a87ce200bba7d88024e2f1c4012212072ce8a5ae":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["a87ce200bba7d88024e2f1c4012212072ce8a5ae","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["a87ce200bba7d88024e2f1c4012212072ce8a5ae"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["06805da26538ed636bd89b10c2699cc3834032ae"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"06805da26538ed636bd89b10c2699cc3834032ae":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"70285ef5917fa2c8feec026d4be4d9c20fa89162":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["a87ce200bba7d88024e2f1c4012212072ce8a5ae"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"a87ce200bba7d88024e2f1c4012212072ce8a5ae":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["06805da26538ed636bd89b10c2699cc3834032ae"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["70285ef5917fa2c8feec026d4be4d9c20fa89162"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}