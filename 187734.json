{"path":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","commits":[{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":1,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":1,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public DocumentsWriter.DocWriter processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    assert docFieldProcessor.docWriter.writer.testPoint(\"DocumentsWriter.ThreadState.init start\");\n\n    fieldCount = 0;\n    \n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName))\n        fp = fp.next;\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                               field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                               field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      if (fp.fieldCount == fp.fields.length) {\n        Fieldable[] newArray = new Fieldable[fp.fields.length*2];\n        System.arraycopy(fp.fields, 0, newArray, 0, fp.fieldCount);\n        fp.fields = newArray;\n      }\n\n      fp.fields[fp.fieldCount++] = field;\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\"); \n      docState.maxTermPrefix = null;\n    }\n\n    final DocumentsWriter.DocWriter one = fieldsWriter.finishDocument();\n    final DocumentsWriter.DocWriter two = consumer.finishDocument();\n    if (one == null) {\n      return two;\n    } else if (two == null) {\n      return one;\n    } else {\n      PerDoc both = getPerDoc();\n      both.docID = docState.docID;\n      assert one.docID == docState.docID;\n      assert two.docID == docState.docID;\n      both.one = one;\n      both.two = two;\n      return both;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo, fieldInfos);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e691ed72f2b1a463c0c263e323928b91976a5066","date":1304346142,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba1f0e2f00b4449f4f1fc7473a8287cb532d631e","date":1304347497,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo, fieldInfos);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo, fieldInfos);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    quickSort(fields, 0, fieldCount-1);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fe2fc74577855eadfb5eae3153c2fffdaaf791","date":1305237079,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo, fieldInfos);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo, fieldInfos);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n\n    for(int i=0;i<fieldCount;i++)\n      fields[i].consumer.processFields(fields[i].fields, fields[i].fieldCount);\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5ebb742ffd3fae76a577a68de595d711f43e6944","date":1305546231,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo, fieldInfos);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e8d7ba2175f47e280231533f7d3016249cea88b","date":1307711934,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2afd23a6f1242190c3409d8d81d5c5912d607fc9","date":1310477482,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getIndexOptions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getIndexOptions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getOmitTermFreqAndPositions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.indexed(), field.storeTermVectors(),\n                                              field.storeTermVectorPositions(), field.storeTermVectorOffsets(),\n                                              field.omitNorms(), false, field.indexOptions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.indexed(), field.storeTermVectors(),\n                               field.storeTermVectorPositions(), field.storeTermVectorOffsets(),\n                               field.omitNorms(), false, field.indexOptions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final PerDocFieldValues docValues = field.docValues();\n      if (docValues != null) {\n        docValuesConsumer(docState, fp.fieldInfo).add(docState.docID, docValues);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    final Document doc = docState.doc;\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    final List<Fieldable> docFields = doc.getFields();\n    final int numDocFields = docFields.size();\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(int i=0;i<numDocFields;i++) {\n      Fieldable field = docFields.get(i);\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.isIndexed(), field.isTermVectorStored(),\n                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                                      field.getOmitNorms(), false, field.getIndexOptions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2)\n          rehash();\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.isIndexed(), field.isTermVectorStored(),\n                            field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),\n                            field.getOmitNorms(), false, field.getIndexOptions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.isStored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      if (field.hasDocValues()) {\n        final DocValuesConsumer docValuesConsumer = docValuesConsumer(docState, fp.fieldInfo);\n        docValuesConsumer.add(docState.docID, field.getDocValues());\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6eb141f80638abdb6ffaa5149877f36ea39b6ad5","date":1315714072,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType(), false, field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType(), false, field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final PerDocFieldValues docValues = field.docValues();\n      if (docValues != null) {\n        docValuesConsumer(docState, fp.fieldInfo).add(docState.docID, docValues);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.indexed(), field.storeTermVectors(),\n                                              field.storeTermVectorPositions(), field.storeTermVectorOffsets(),\n                                              field.omitNorms(), false, field.indexOptions(), field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.indexed(), field.storeTermVectors(),\n                               field.storeTermVectorPositions(), field.storeTermVectorOffsets(),\n                               field.omitNorms(), false, field.indexOptions(), field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final PerDocFieldValues docValues = field.docValues();\n      if (docValues != null) {\n        docValuesConsumer(docState, fp.fieldInfo).add(docState.docID, docValues);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ecd41ba3cdd1b4d825aa53d1987e4360cf45cb05","date":1323284809,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final PerDocFieldValues docValues = field.docValues();\n      if (docValues != null) {\n        docValuesConsumer(field.docValuesType(), docState, fp.fieldInfo).add(docState.docID, docValues);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType(), false, field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType(), false, field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final PerDocFieldValues docValues = field.docValues();\n      if (docValues != null) {\n        docValuesConsumer(docState, fp.fieldInfo).add(docState.docID, docValues);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6cc9891f94df3a67986dfa9d463a3e2d76bbce7d","date":1323541054,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValue docValue = field.docValue();\n      if (docValue != null) {\n        docValuesConsumer(field.docValueType(), docState, fp.fieldInfo).add(docState.docID, docValue);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final PerDocFieldValues docValues = field.docValues();\n      if (docValues != null) {\n        docValuesConsumer(field.docValuesType(), docState, fp.fieldInfo).add(docState.docID, docValues);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c5df35ab57c223ea11aec64b53bf611904f3dced","date":1323640545,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType(), false, field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType(), false, field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final PerDocFieldValues docValues = field.docValues();\n      if (docValues != null) {\n        docValuesConsumer(docState, fp.fieldInfo).add(docState.docID, docValues);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType(), false, field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType(), false, field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final PerDocFieldValues docValues = field.docValues();\n      if (docValues != null) {\n        docValuesConsumer(docState, fp.fieldInfo).add(docState.docID, docValues);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValue docValue = field.docValue();\n      if (docValue != null) {\n        docValuesConsumer(field.docValueType(), docState, fp.fieldInfo).add(docState.docID, docValue);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValue docValue = field.docValue();\n      if (docValue != null) {\n        docValuesConsumer(field.docValueType(), docState, fp.fieldInfo).add(docState.docID, docValue);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream != null) {\n      docState.infoStream.println(\"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d638301ad1cfcae567b681b893bc8781f0ee48a5","date":1323801546,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValue docValue = field.docValue();\n      if (docValue != null) {\n        docValuesConsumer(field.docValueType(), docState, fp.fieldInfo).add(docState.docID, docValue);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType(), false, field.docValuesType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType(), false, field.docValuesType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final PerDocFieldValues docValues = field.docValues();\n      if (docValues != null) {\n        docValuesConsumer(docState, fp.fieldInfo).add(docState.docID, docValues);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fa0f44f887719e97183771e977cfc4bfb485b766","date":1326668713,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValue docValue = field.docValue();\n      if (docValue != null) {\n        docValuesConsumer(field.docValueType(), docState, fp.fieldInfo).add(docState.docID, docValue);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":["6cc9891f94df3a67986dfa9d463a3e2d76bbce7d"],"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#processDocument(FieldInfos).mjava","sourceNew":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument(FieldInfos fieldInfos) throws IOException {\n\n    consumer.startDocument();\n    fieldsWriter.startDocument();\n\n    fieldCount = 0;\n\n    final int thisFieldGen = fieldGen++;\n\n    // Absorb any new fields first seen in this document.\n    // Also absorb any changes to fields we had already\n    // seen before (eg suddenly turning on norms or\n    // vectors, etc.):\n\n    for(IndexableField field : docState.doc) {\n      final String fieldName = field.name();\n\n      // Make sure we have a PerField allocated\n      final int hashPos = fieldName.hashCode() & hashMask;\n      DocFieldProcessorPerField fp = fieldHash[hashPos];\n      while(fp != null && !fp.fieldInfo.name.equals(fieldName)) {\n        fp = fp.next;\n      }\n\n      if (fp == null) {\n\n        // TODO FI: we need to genericize the \"flags\" that a\n        // field holds, and, how these flags are merged; it\n        // needs to be more \"pluggable\" such that if I want\n        // to have a new \"thing\" my Fields can do, I can\n        // easily add it\n        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());\n\n        fp = new DocFieldProcessorPerField(this, fi);\n        fp.next = fieldHash[hashPos];\n        fieldHash[hashPos] = fp;\n        totalFieldCount++;\n\n        if (totalFieldCount >= fieldHash.length/2) {\n          rehash();\n        }\n      } else {\n        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());\n      }\n\n      if (thisFieldGen != fp.lastGen) {\n\n        // First time we're seeing this field for this doc\n        fp.fieldCount = 0;\n\n        if (fieldCount == fields.length) {\n          final int newSize = fields.length*2;\n          DocFieldProcessorPerField newArray[] = new DocFieldProcessorPerField[newSize];\n          System.arraycopy(fields, 0, newArray, 0, fieldCount);\n          fields = newArray;\n        }\n\n        fields[fieldCount++] = fp;\n        fp.lastGen = thisFieldGen;\n      }\n\n      fp.addField(field);\n\n      if (field.fieldType().stored()) {\n        fieldsWriter.addField(field, fp.fieldInfo);\n      }\n      final DocValues.Type dvType = field.fieldType().docValueType();\n      if (dvType != null) {\n        docValuesConsumer(dvType, docState, fp.fieldInfo).add(docState.docID, field);\n      }\n    }\n\n    // If we are writing vectors then we must visit\n    // fields in sorted order so they are written in\n    // sorted order.  TODO: we actually only need to\n    // sort the subset of fields that have vectors\n    // enabled; we could save [small amount of] CPU\n    // here.\n    ArrayUtil.quickSort(fields, 0, fieldCount, fieldsComp);\n    for(int i=0;i<fieldCount;i++) {\n      final DocFieldProcessorPerField perField = fields[i];\n      perField.consumer.processFields(perField.fields, perField.fieldCount);\n    }\n\n    if (docState.maxTermPrefix != null && docState.infoStream.isEnabled(\"IW\")) {\n      docState.infoStream.message(\"IW\", \"WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + docState.maxTermPrefix + \"...'\");\n      docState.maxTermPrefix = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"c5df35ab57c223ea11aec64b53bf611904f3dced":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"2afd23a6f1242190c3409d8d81d5c5912d607fc9":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","2e8d7ba2175f47e280231533f7d3016249cea88b"],"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["e691ed72f2b1a463c0c263e323928b91976a5066"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["fa0f44f887719e97183771e977cfc4bfb485b766"],"6cc9891f94df3a67986dfa9d463a3e2d76bbce7d":["ecd41ba3cdd1b4d825aa53d1987e4360cf45cb05"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["6cc9891f94df3a67986dfa9d463a3e2d76bbce7d","c5df35ab57c223ea11aec64b53bf611904f3dced"],"6eb141f80638abdb6ffaa5149877f36ea39b6ad5":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["c5df35ab57c223ea11aec64b53bf611904f3dced","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"5ebb742ffd3fae76a577a68de595d711f43e6944":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791","5ebb742ffd3fae76a577a68de595d711f43e6944"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["bde51b089eb7f86171eb3406e38a274743f9b7ac","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e691ed72f2b1a463c0c263e323928b91976a5066":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fa0f44f887719e97183771e977cfc4bfb485b766":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["ba1f0e2f00b4449f4f1fc7473a8287cb532d631e","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["2afd23a6f1242190c3409d8d81d5c5912d607fc9"],"ba1f0e2f00b4449f4f1fc7473a8287cb532d631e":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e691ed72f2b1a463c0c263e323928b91976a5066"],"ecd41ba3cdd1b4d825aa53d1987e4360cf45cb05":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"c5df35ab57c223ea11aec64b53bf611904f3dced":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","d638301ad1cfcae567b681b893bc8781f0ee48a5"],"2afd23a6f1242190c3409d8d81d5c5912d607fc9":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":[],"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["2e8d7ba2175f47e280231533f7d3016249cea88b","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6cc9891f94df3a67986dfa9d463a3e2d76bbce7d":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"6eb141f80638abdb6ffaa5149877f36ea39b6ad5":["c5df35ab57c223ea11aec64b53bf611904f3dced","ecd41ba3cdd1b4d825aa53d1987e4360cf45cb05"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["ba1f0e2f00b4449f4f1fc7473a8287cb532d631e"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e691ed72f2b1a463c0c263e323928b91976a5066"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["fa0f44f887719e97183771e977cfc4bfb485b766"],"5ebb742ffd3fae76a577a68de595d711f43e6944":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["2afd23a6f1242190c3409d8d81d5c5912d607fc9","a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"e691ed72f2b1a463c0c263e323928b91976a5066":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791","ba1f0e2f00b4449f4f1fc7473a8287cb532d631e"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["135621f3a0670a9394eb563224a3b76cc4dddc0f","b3e06be49006ecac364d39d12b9c9f74882f9b9f","c0ef0193974807e4bddf5432a6b0287fe4d6c9df","a3776dccca01c11e7046323cfad46a3b4a471233","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"fa0f44f887719e97183771e977cfc4bfb485b766":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["5ebb742ffd3fae76a577a68de595d711f43e6944"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["6eb141f80638abdb6ffaa5149877f36ea39b6ad5"],"ecd41ba3cdd1b4d825aa53d1987e4360cf45cb05":["6cc9891f94df3a67986dfa9d463a3e2d76bbce7d"],"ba1f0e2f00b4449f4f1fc7473a8287cb532d631e":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}