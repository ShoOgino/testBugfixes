{"path":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","commits":[{"id":"027bee21e09164c9ee230395405076d1e0034b30","date":1401521821,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.getDocCount();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(writer));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos) throws IOException {\n    if (verbose()) {\n      message(\"findForcedDeletesMerges infos=\" + writer.get().segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.get().getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.get().numDeletedDocs(info))/info.info.getDocCount();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending());\n\n    if (verbose()) {\n      message(\"eligible=\" + eligible);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose()) {\n        message(\"add merge=\" + writer.get().segString(merge.segments));\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(writer));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.getDocCount();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(writer));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(writer));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.getDocCount();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(writer));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(writer));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.getDocCount();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(writer));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(writer));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(writer));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(writer));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0f10353be871363159f724b6419748c9b89c5c0","date":1501592481,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","date":1502192746,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    Collections.sort(eligible, new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5399733c0eeb73827ff5b8aee51c110ec540e6f0","date":1511756706,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Set<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f0e53d133180083c87dcb8114b087cfe6bfc7bce","date":1511856078,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Set<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ae87c7be37e537f40fa3bb2c35fa4a368d12a72","date":1523453225,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Set<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletesToMerge(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Set<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43345f1452f9510f8aaadae6156fe0c834e7d957","date":1523483670,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Set<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletesToMerge(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Set<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d28f215464f76024caf026606f8ea51a5319c53","date":1527226629,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,MergeContext).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy#findForcedDeletesMerges(SegmentInfos,IndexWriter).mjava","sourceNew":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, MergeContext mergeContext) throws IOException {\n    if (verbose(mergeContext)) {\n      message(\"findForcedDeletesMerges infos=\" + segString(mergeContext, infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, mergeContext);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Set<SegmentCommitInfo> merging = mergeContext.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      int delCount = mergeContext.numDeletesToMerge(info);\n      assert assertDelCount(delCount, info);\n      double pctDeletes = 100.*((double) delCount)/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(mergeContext, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(mergeContext)) {\n      message(\"eligible=\" + eligible, mergeContext);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(mergeContext)) {\n        message(\"add merge=\" + segString(mergeContext, merge.segments), mergeContext);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","sourceOld":"  @Override\n  public MergeSpecification findForcedDeletesMerges(SegmentInfos infos, IndexWriter writer) throws IOException {\n    if (verbose(writer)) {\n      message(\"findForcedDeletesMerges infos=\" + writer.segString(infos) + \" forceMergeDeletesPctAllowed=\" + forceMergeDeletesPctAllowed, writer);\n    }\n    final List<SegmentCommitInfo> eligible = new ArrayList<>();\n    final Set<SegmentCommitInfo> merging = writer.getMergingSegments();\n    for(SegmentCommitInfo info : infos) {\n      double pctDeletes = 100.*((double) writer.numDeletesToMerge(info))/info.info.maxDoc();\n      if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {\n        eligible.add(info);\n      }\n    }\n\n    if (eligible.size() == 0) {\n      return null;\n    }\n\n    // The size can change concurrently while we are running here, because deletes\n    // are now applied concurrently, and this can piss off TimSort!  So we\n    // call size() once per segment and sort by that:\n    Map<SegmentCommitInfo,Long> sizeInBytes = getSegmentSizes(writer, infos.asList());\n\n    eligible.sort(new SegmentByteSizeDescending(sizeInBytes));\n\n    if (verbose(writer)) {\n      message(\"eligible=\" + eligible, writer);\n    }\n\n    int start = 0;\n    MergeSpecification spec = null;\n\n    while(start < eligible.size()) {\n      // Don't enforce max merged size here: app is explicitly\n      // calling forceMergeDeletes, and knows this may take a\n      // long time / produce big segments (like forceMerge):\n      final int end = Math.min(start + maxMergeAtOnceExplicit, eligible.size());\n      if (spec == null) {\n        spec = new MergeSpecification();\n      }\n\n      final OneMerge merge = new OneMerge(eligible.subList(start, end));\n      if (verbose(writer)) {\n        message(\"add merge=\" + writer.segString(merge.segments), writer);\n      }\n      spec.add(merge);\n      start = end;\n    }\n\n    return spec;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5399733c0eeb73827ff5b8aee51c110ec540e6f0":["c0f10353be871363159f724b6419748c9b89c5c0"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b0267c69e2456a3477a1ad785723f2135da3117e"],"b0267c69e2456a3477a1ad785723f2135da3117e":["027bee21e09164c9ee230395405076d1e0034b30"],"1d28f215464f76024caf026606f8ea51a5319c53":["43345f1452f9510f8aaadae6156fe0c834e7d957"],"c0f10353be871363159f724b6419748c9b89c5c0":["28288370235ed02234a64753cdbf0c6ec096304a"],"43345f1452f9510f8aaadae6156fe0c834e7d957":["f0e53d133180083c87dcb8114b087cfe6bfc7bce","9ae87c7be37e537f40fa3bb2c35fa4a368d12a72"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["b0267c69e2456a3477a1ad785723f2135da3117e","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["b0267c69e2456a3477a1ad785723f2135da3117e","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["027bee21e09164c9ee230395405076d1e0034b30","b0267c69e2456a3477a1ad785723f2135da3117e"],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","c0f10353be871363159f724b6419748c9b89c5c0"],"b06445ae1731e049327712db0454e5643ca9b7fe":["027bee21e09164c9ee230395405076d1e0034b30","b0267c69e2456a3477a1ad785723f2135da3117e"],"9ae87c7be37e537f40fa3bb2c35fa4a368d12a72":["f0e53d133180083c87dcb8114b087cfe6bfc7bce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f0e53d133180083c87dcb8114b087cfe6bfc7bce":["c0f10353be871363159f724b6419748c9b89c5c0","5399733c0eeb73827ff5b8aee51c110ec540e6f0"],"027bee21e09164c9ee230395405076d1e0034b30":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1d28f215464f76024caf026606f8ea51a5319c53"]},"commit2Childs":{"5399733c0eeb73827ff5b8aee51c110ec540e6f0":["f0e53d133180083c87dcb8114b087cfe6bfc7bce"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"b0267c69e2456a3477a1ad785723f2135da3117e":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe"],"c0f10353be871363159f724b6419748c9b89c5c0":["5399733c0eeb73827ff5b8aee51c110ec540e6f0","7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","f0e53d133180083c87dcb8114b087cfe6bfc7bce"],"1d28f215464f76024caf026606f8ea51a5319c53":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"43345f1452f9510f8aaadae6156fe0c834e7d957":["1d28f215464f76024caf026606f8ea51a5319c53"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["7a23cf16c8fa265dc0a564adcabb55e3f054e0ac"],"28288370235ed02234a64753cdbf0c6ec096304a":["c0f10353be871363159f724b6419748c9b89c5c0"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":[],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"9ae87c7be37e537f40fa3bb2c35fa4a368d12a72":["43345f1452f9510f8aaadae6156fe0c834e7d957"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["027bee21e09164c9ee230395405076d1e0034b30"],"f0e53d133180083c87dcb8114b087cfe6bfc7bce":["43345f1452f9510f8aaadae6156fe0c834e7d957","9ae87c7be37e537f40fa3bb2c35fa4a368d12a72"],"027bee21e09164c9ee230395405076d1e0034b30":["b0267c69e2456a3477a1ad785723f2135da3117e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","b06445ae1731e049327712db0454e5643ca9b7fe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}