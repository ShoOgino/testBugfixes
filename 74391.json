{"path":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","commits":[{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForOptimize((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["3cc749c053615f5871f3b95715fe292f34e70a53"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cc749c053615f5871f3b95715fe292f34e70a53":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"3cc749c053615f5871f3b95715fe292f34e70a53":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["3cc749c053615f5871f3b95715fe292f34e70a53"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}