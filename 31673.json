{"path":"src/test/org/apache/solr/analysis/TestSynonymMap#testBigramTokenizer().mjava","commits":[{"id":"ed0158ac307bee4b81f4c26ebe88cddc950f46db","date":1211204318,"type":0,"author":"Koji Sekiguchi","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestSynonymMap#testBigramTokenizer().mjava","pathOld":"/dev/null","sourceNew":"  public void testBigramTokenizer() throws Exception {\n\tSynonymMap synMap;\n\t\n\t// prepare bi-gram tokenizer factory\n\tBaseTokenizerFactory tf = new NGramTokenizerFactory();\n\tMap<String, String> args = new HashMap<String, String>();\n\targs.put(\"minGramSize\",\"2\");\n\targs.put(\"maxGramSize\",\"2\");\n\ttf.init( args );\n\n    // (ab)->(bc)->(cd)->[ef][fg][gh]\n    List<String> rules = new ArrayList<String>();\n    rules.add( \"abcd=>efgh\" );\n    synMap = new SynonymMap( true );\n    SynonymFilterFactory.parseRules( rules, synMap, \"=>\", \",\", true, tf);\n    assertEquals( 1, synMap.submap.size() );\n    assertEquals( 1, getSubSynonymMap( synMap, \"ab\" ).submap.size() );\n    assertEquals( 1, getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ).submap.size() );\n    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ), \"cd\", \"ef\" );\n    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ), \"cd\", \"fg\" );\n    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ), \"cd\", \"gh\" );\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestSynonymMap#testBigramTokenizer().mjava","pathOld":"src/test/org/apache/solr/analysis/TestSynonymMap#testBigramTokenizer().mjava","sourceNew":"  public void testBigramTokenizer() throws Exception {\n\tSynonymMap synMap;\n\t\n\t// prepare bi-gram tokenizer factory\n\tBaseTokenizerFactory tf = new NGramTokenizerFactory();\n\tMap<String, String> args = new HashMap<String, String>();\n\targs.put(\"minGramSize\",\"2\");\n\targs.put(\"maxGramSize\",\"2\");\n\ttf.init( args );\n\n    // (ab)->(bc)->(cd)->[ef][fg][gh]\n    List<String> rules = new ArrayList<String>();\n    rules.add( \"abcd=>efgh\" );\n    synMap = new SynonymMap( true );\n    SynonymFilterFactory.parseRules( rules, synMap, \"=>\", \",\", true, tf);\n    assertEquals( 1, synMap.submap.size() );\n    assertEquals( 1, getSubSynonymMap( synMap, \"ab\" ).submap.size() );\n    assertEquals( 1, getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ).submap.size() );\n    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ), \"cd\", \"ef\" );\n    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ), \"cd\", \"fg\" );\n    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ), \"cd\", \"gh\" );\n  }\n\n","sourceOld":"  public void testBigramTokenizer() throws Exception {\n\tSynonymMap synMap;\n\t\n\t// prepare bi-gram tokenizer factory\n\tBaseTokenizerFactory tf = new NGramTokenizerFactory();\n\tMap<String, String> args = new HashMap<String, String>();\n\targs.put(\"minGramSize\",\"2\");\n\targs.put(\"maxGramSize\",\"2\");\n\ttf.init( args );\n\n    // (ab)->(bc)->(cd)->[ef][fg][gh]\n    List<String> rules = new ArrayList<String>();\n    rules.add( \"abcd=>efgh\" );\n    synMap = new SynonymMap( true );\n    SynonymFilterFactory.parseRules( rules, synMap, \"=>\", \",\", true, tf);\n    assertEquals( 1, synMap.submap.size() );\n    assertEquals( 1, getSubSynonymMap( synMap, \"ab\" ).submap.size() );\n    assertEquals( 1, getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ).submap.size() );\n    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ), \"cd\", \"ef\" );\n    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ), \"cd\", \"fg\" );\n    assertTokIncludes( getSubSynonymMap( getSubSynonymMap( synMap, \"ab\" ), \"bc\" ), \"cd\", \"gh\" );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ed0158ac307bee4b81f4c26ebe88cddc950f46db":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ad94625fb8d088209f46650c8097196fec67f00c":["ed0158ac307bee4b81f4c26ebe88cddc950f46db"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"ed0158ac307bee4b81f4c26ebe88cddc950f46db":["ad94625fb8d088209f46650c8097196fec67f00c"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ed0158ac307bee4b81f4c26ebe88cddc950f46db"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}