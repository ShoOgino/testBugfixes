{"path":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","commits":[{"id":"a4df60738409662c962b4ed3201d830cd3c14530","date":1114991851,"type":1,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              position++;\n              String name = t.termText();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              position++;\n              String name = t.termText();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    final Token reusableToken = new Token();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n              position += (nextToken.getPositionIncrement() - 1);\n              position++;\n              String name = nextToken.term();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              position++;\n              String name = t.termText();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7b6cdc70e097da94da79a655ed8f94477ff69f5","date":1220815360,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map tokenMap = new HashMap();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Iterator fields = doc.getFields().iterator();\n    final Token reusableToken = new Token();\n    while (fields.hasNext()) {\n      Field field = (Field) fields.next();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n              position += (nextToken.getPositionIncrement() - 1);\n              position++;\n              String name = nextToken.term();\n              Integer Count = (Integer) tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Hashtable tokenHash = new Hashtable();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Enumeration fields = doc.fields();\n    final Token reusableToken = new Token();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n              position += (nextToken.getPositionIncrement() - 1);\n              position++;\n              String name = nextToken.term();\n              Integer Count = (Integer) tokenHash.get(name);\n              if (Count == null) { // not in there yet\n                tokenHash.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenHash.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedHashtableEntries(tokenHash);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c64e0c064f4d1397144dc064e5eb82f2e074357b","date":1247479561,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map tokenMap = new HashMap();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    Iterator fields = doc.getFields().iterator();\n    final Token reusableToken = new Token();\n    while (fields.hasNext()) {\n      Field field = (Field) fields.next();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n              position += (nextToken.getPositionIncrement() - 1);\n              position++;\n              String name = nextToken.term();\n              Integer Count = (Integer) tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map tokenMap = new HashMap();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = new StandardAnalyzer();\n    Iterator fields = doc.getFields().iterator();\n    final Token reusableToken = new Token();\n    while (fields.hasNext()) {\n      Field field = (Field) fields.next();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n              position += (nextToken.getPositionIncrement() - 1);\n              position++;\n              String name = nextToken.term();\n              Integer Count = (Integer) tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b5756469957918cac40a831acec9cf01c8c2bb3","date":1249167152,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map tokenMap = new HashMap();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    Iterator fields = doc.getFields().iterator();\n    final Token reusableToken = new Token();\n    while (fields.hasNext()) {\n      Field field = (Field) fields.next();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = (Integer) tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map tokenMap = new HashMap();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    Iterator fields = doc.getFields().iterator();\n    final Token reusableToken = new Token();\n    while (fields.hasNext()) {\n      Field field = (Field) fields.next();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n              position += (nextToken.getPositionIncrement() - 1);\n              position++;\n              String name = nextToken.term();\n              Integer Count = (Integer) tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map tokenMap = new HashMap();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    Iterator fields = doc.getFields().iterator();\n    final Token reusableToken = new Token();\n    while (fields.hasNext()) {\n      Field field = (Field) fields.next();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = (Integer) tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map tokenMap = new HashMap();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    Iterator fields = doc.getFields().iterator();\n    final Token reusableToken = new Token();\n    while (fields.hasNext()) {\n      Field field = (Field) fields.next();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = (Integer) tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bcde5e3f23911110baa101ed062b544162825b5","date":1254521804,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map tokenMap = new HashMap();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    Iterator fields = doc.getFields().iterator();\n    final Token reusableToken = new Token();\n    while (fields.hasNext()) {\n      Field field = (Field) fields.next();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = (Integer) tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map tokenMap = new HashMap();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    Iterator fields = doc.getFields().iterator();\n    final Token reusableToken = new Token();\n    while (fields.hasNext()) {\n      Field field = (Field) fields.next();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = (Integer) tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, new Integer(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, new Integer(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6358eb123cf2e9cc195c57e5ac4f3cf2f15cee94","date":1257897915,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map tokenMap = new HashMap();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    Iterator fields = doc.getFields().iterator();\n    final Token reusableToken = new Token();\n    while (fields.hasNext()) {\n      Field field = (Field) fields.next();\n      String fieldName = field.name();\n\n\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = (Integer) tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Entry[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Entry currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","pathOld":"contrib/lucli/src/java/lucli/LuceneMethods#invertDocument(Document).mjava","sourceNew":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","sourceOld":"  // Copied from DocumentWriter\n  // Tokenizes the fields of a document into Postings.\n  private void invertDocument(Document doc)\n    throws IOException {\n\n    Map<String,Integer> tokenMap = new HashMap<String,Integer>();\n    final int maxFieldLength = 10000;\n\n    Analyzer analyzer = createAnalyzer();\n    for (Fieldable field : doc.getFields()) {\n      String fieldName = field.name();\n      if (field.isIndexed()) {\n        if (field.isTokenized()) {     // un-tokenized field\n          Reader reader;        // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          int position = 0;\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n          PositionIncrementAttribute posIncrAtt = stream.addAttribute(PositionIncrementAttribute.class);\n          \n          try {\n            while (stream.incrementToken()) {\n              position += (posIncrAtt.getPositionIncrement() - 1);\n              position++;\n              String name = termAtt.term();\n              Integer Count = tokenMap.get(name);\n              if (Count == null) { // not in there yet\n                tokenMap.put(name, Integer.valueOf(1)); //first one\n              } else {\n                int count = Count.intValue();\n                tokenMap.put(name, Integer.valueOf(count + 1));\n              }\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n      }\n    }\n    Map.Entry<String,Integer>[] sortedHash = getSortedMapEntries(tokenMap);\n    for (int ii = 0; ii < sortedHash.length && ii < 10; ii++) {\n      Map.Entry<String,Integer> currentEntry = sortedHash[ii];\n      message((ii + 1) + \":\" + currentEntry.getKey() + \" \" + currentEntry.getValue());\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["a4df60738409662c962b4ed3201d830cd3c14530"],"6bcde5e3f23911110baa101ed062b544162825b5":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6358eb123cf2e9cc195c57e5ac4f3cf2f15cee94":["6bcde5e3f23911110baa101ed062b544162825b5"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["c64e0c064f4d1397144dc064e5eb82f2e074357b"],"c64e0c064f4d1397144dc064e5eb82f2e074357b":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["6358eb123cf2e9cc195c57e5ac4f3cf2f15cee94"],"a4df60738409662c962b4ed3201d830cd3c14530":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"6bcde5e3f23911110baa101ed062b544162825b5":["6358eb123cf2e9cc195c57e5ac4f3cf2f15cee94"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a4df60738409662c962b4ed3201d830cd3c14530"],"6358eb123cf2e9cc195c57e5ac4f3cf2f15cee94":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"c64e0c064f4d1397144dc064e5eb82f2e074357b":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["6bcde5e3f23911110baa101ed062b544162825b5"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["c64e0c064f4d1397144dc064e5eb82f2e074357b"],"a4df60738409662c962b4ed3201d830cd3c14530":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}