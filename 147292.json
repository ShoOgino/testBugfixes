{"path":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","commits":[{"id":"f6b1e64caa933f6fb3c0494afd6ca2597f55cc91","date":1470238980,"type":0,"author":"jbernste","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","pathOld":"/dev/null","sourceNew":"    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = MultiFields.getFields(searcher.getIndexReader()).terms(trainingParams.feature);\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["3185d4c8bb14af74e2ef0bde19f22e33b954b568"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3b013574eedcdbac35dc7e35b0ee616ffc38895d","date":1470897818,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","pathOld":"/dev/null","sourceNew":"    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = MultiFields.getFields(searcher.getIndexReader()).terms(trainingParams.feature);\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","pathOld":"/dev/null","sourceNew":"    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = MultiFields.getFields(searcher.getIndexReader()).terms(trainingParams.feature);\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3185d4c8bb14af74e2ef0bde19f22e33b954b568","date":1497065963,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","sourceNew":"    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = ((SolrIndexSearcher)searcher).getSlowAtomicReader().terms(trainingParams.feature);\n      TermsEnum termsEnum = terms == null ? TermsEnum.EMPTY : terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","sourceOld":"    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = MultiFields.getFields(searcher.getIndexReader()).terms(trainingParams.feature);\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","bugFix":["f6b1e64caa933f6fb3c0494afd6ca2597f55cc91"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","sourceNew":"    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = ((SolrIndexSearcher)searcher).getSlowAtomicReader().terms(trainingParams.feature);\n      TermsEnum termsEnum = terms == null ? TermsEnum.EMPTY : terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","sourceOld":"    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = MultiFields.getFields(searcher.getIndexReader()).terms(trainingParams.feature);\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","sourceNew":"    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = ((SolrIndexSearcher)searcher).getSlowAtomicReader().terms(trainingParams.feature);\n      TermsEnum termsEnum = terms == null ? TermsEnum.EMPTY : terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","sourceOld":"    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = MultiFields.getFields(searcher.getIndexReader()).terms(trainingParams.feature);\n      TermsEnum termsEnum = terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"50dfd19525c8d73e856dca6edb64b7aea074037f","date":1591579225,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/TextLogisticRegressionQParserPlugin.TextLogisticRegressionCollector#finish().mjava","sourceNew":"    @SuppressWarnings({\"unchecked\"})\n    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = ((SolrIndexSearcher)searcher).getSlowAtomicReader().terms(trainingParams.feature);\n      TermsEnum termsEnum = terms == null ? TermsEnum.EMPTY : terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      @SuppressWarnings({\"rawtypes\"})\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","sourceOld":"    public void finish() throws IOException {\n\n      Map<Integer, double[]> docVectors = new HashMap<>();\n      Terms terms = ((SolrIndexSearcher)searcher).getSlowAtomicReader().terms(trainingParams.feature);\n      TermsEnum termsEnum = terms == null ? TermsEnum.EMPTY : terms.iterator();\n      PostingsEnum postingsEnum = null;\n      int termIndex = 0;\n      for (String termStr : trainingParams.terms) {\n        BytesRef term = new BytesRef(termStr);\n        if (termsEnum.seekExact(term)) {\n          postingsEnum = termsEnum.postings(postingsEnum);\n          while (postingsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n            int docId = postingsEnum.docID();\n            if (docsSet.get(docId)) {\n              double[] vector = docVectors.get(docId);\n              if (vector == null) {\n                vector = new double[trainingParams.terms.length+1];\n                vector[0] = 1.0;\n                docVectors.put(docId, vector);\n              }\n              vector[termIndex + 1] = trainingParams.idfs[termIndex] * (1.0 + Math.log(postingsEnum.freq()));\n            }\n          }\n        }\n        termIndex++;\n      }\n\n      for (Map.Entry<Integer, double[]> entry : docVectors.entrySet()) {\n        double[] vector = entry.getValue();\n        int outcome = 0;\n        if (positiveDocsSet.get(entry.getKey())) {\n          outcome = 1;\n        }\n        double sig = sigmoid(sum(multiply(vector, weights)));\n        double error = sig - outcome;\n        double lastSig = sigmoid(sum(multiply(vector, trainingParams.weights)));\n        totalError += Math.abs(lastSig - outcome);\n        classificationEvaluation.count(outcome,  lastSig >= trainingParams.threshold ? 1 : 0);\n\n        workingDeltas = multiply(error * trainingParams.alpha, vector);\n\n        for(int i = 0; i< workingDeltas.length; i++) {\n          weights[i] -= workingDeltas[i];\n        }\n      }\n\n      NamedList analytics = new NamedList();\n      rbsp.rsp.add(\"logit\", analytics);\n\n      List<Double> outWeights = new ArrayList<>();\n      for(Double d : weights) {\n        outWeights.add(d);\n      }\n\n      analytics.add(\"weights\", outWeights);\n      analytics.add(\"error\", totalError);\n      analytics.add(\"evaluation\", classificationEvaluation.toMap());\n      analytics.add(\"feature\", trainingParams.feature);\n      analytics.add(\"positiveLabel\", trainingParams.positiveLabel);\n      if(this.delegate instanceof DelegatingCollector) {\n        ((DelegatingCollector)this.delegate).finish();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3185d4c8bb14af74e2ef0bde19f22e33b954b568":["f6b1e64caa933f6fb3c0494afd6ca2597f55cc91"],"50dfd19525c8d73e856dca6edb64b7aea074037f":["28288370235ed02234a64753cdbf0c6ec096304a"],"f6b1e64caa933f6fb3c0494afd6ca2597f55cc91":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"28288370235ed02234a64753cdbf0c6ec096304a":["f6b1e64caa933f6fb3c0494afd6ca2597f55cc91","3185d4c8bb14af74e2ef0bde19f22e33b954b568"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["f6b1e64caa933f6fb3c0494afd6ca2597f55cc91","3185d4c8bb14af74e2ef0bde19f22e33b954b568"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f6b1e64caa933f6fb3c0494afd6ca2597f55cc91"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f6b1e64caa933f6fb3c0494afd6ca2597f55cc91"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["50dfd19525c8d73e856dca6edb64b7aea074037f"]},"commit2Childs":{"3185d4c8bb14af74e2ef0bde19f22e33b954b568":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"50dfd19525c8d73e856dca6edb64b7aea074037f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f6b1e64caa933f6fb3c0494afd6ca2597f55cc91":["3185d4c8bb14af74e2ef0bde19f22e33b954b568","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f6b1e64caa933f6fb3c0494afd6ca2597f55cc91","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"28288370235ed02234a64753cdbf0c6ec096304a":["50dfd19525c8d73e856dca6edb64b7aea074037f"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3b013574eedcdbac35dc7e35b0ee616ffc38895d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}