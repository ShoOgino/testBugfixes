{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","commits":[{"id":"fd4c4b2a55a46d2c45e3de4514f08e33215caa21","date":1288902330,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"/dev/null","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"/dev/null","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56","date":1290598569,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n      ((LogMergePolicy) writer.getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n      ((LogMergePolicy) writer.getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"/dev/null","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7d16aff6229cca84309d03d047cd718946bd4b43","date":1296516600,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      Directory dir = newDirectory();\n\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c5b026d03cbbb03ca4c0b97d14e9839682281dc","date":1323049298,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir, true);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testSimulatedCorruptIndex2().mjava","sourceNew":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","sourceOld":"  // Simulate a corrupt index by removing one of the cfs\n  // files and make sure we get an IOException trying to\n  // open the index:\n  public void testSimulatedCorruptIndex2() throws IOException {\n      MockDirectoryWrapper dir = newDirectory();\n      dir.setCheckIndexOnClose(false); // we are corrupting it!\n      IndexWriter writer = null;\n\n      writer  = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMergePolicy(newLogMergePolicy(true))\n      );\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);\n\n      // add 100 documents\n      for (int i = 0; i < 100; i++) {\n          addDoc(writer);\n      }\n\n      // close\n      writer.close();\n\n      long gen = SegmentInfos.getCurrentSegmentGeneration(dir);\n      assertTrue(\"segment generation should be > 0 but got \" + gen, gen > 0);\n\n      String[] files = dir.listAll();\n      boolean corrupted = false;\n      for(int i=0;i<files.length;i++) {\n        if (files[i].endsWith(\".cfs\")) {\n          dir.deleteFile(files[i]);\n          corrupted = true;\n          break;\n        }\n      }\n      assertTrue(\"failed to find cfs file to remove\", corrupted);\n\n      IndexReader reader = null;\n      try {\n        reader = IndexReader.open(dir);\n        fail(\"reader did not hit IOException on opening a corrupt index\");\n      } catch (Exception e) {\n      }\n      if (reader != null) {\n        reader.close();\n      }\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["f2c5f0cb44df114db4228c8f77861714b5cabaea","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"85a883878c0af761245ab048babc63d099f835f3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","fd4c4b2a55a46d2c45e3de4514f08e33215caa21"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"962d04139994fce5193143ef35615499a9a96d78":["bde51b089eb7f86171eb3406e38a274743f9b7ac","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56":["fd4c4b2a55a46d2c45e3de4514f08e33215caa21"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"7d16aff6229cca84309d03d047cd718946bd4b43":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["7d16aff6229cca84309d03d047cd718946bd4b43"],"a3776dccca01c11e7046323cfad46a3b4a471233":["7d16aff6229cca84309d03d047cd718946bd4b43","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["3bb13258feba31ab676502787ab2e1779f129b7a","7d16aff6229cca84309d03d047cd718946bd4b43"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fd4c4b2a55a46d2c45e3de4514f08e33215caa21":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["f2c5f0cb44df114db4228c8f77861714b5cabaea","1c5b026d03cbbb03ca4c0b97d14e9839682281dc"],"3bb13258feba31ab676502787ab2e1779f129b7a":["85a883878c0af761245ab048babc63d099f835f3","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","7d16aff6229cca84309d03d047cd718946bd4b43"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56"]},"commit2Childs":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"85a883878c0af761245ab048babc63d099f835f3":["3bb13258feba31ab676502787ab2e1779f129b7a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"962d04139994fce5193143ef35615499a9a96d78":[],"5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"7d16aff6229cca84309d03d047cd718946bd4b43":["f2c5f0cb44df114db4228c8f77861714b5cabaea","a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","135621f3a0670a9394eb563224a3b76cc4dddc0f","1c5b026d03cbbb03ca4c0b97d14e9839682281dc","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["85a883878c0af761245ab048babc63d099f835f3","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","fd4c4b2a55a46d2c45e3de4514f08e33215caa21"],"fd4c4b2a55a46d2c45e3de4514f08e33215caa21":["85a883878c0af761245ab048babc63d099f835f3","5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["962d04139994fce5193143ef35615499a9a96d78"],"3bb13258feba31ab676502787ab2e1779f129b7a":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","7d16aff6229cca84309d03d047cd718946bd4b43","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}