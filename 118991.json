{"path":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","commits":[{"id":"3ee067e27d1cce6d2d5d64280007410c2e1a38d8","date":1001438945,"type":0,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","pathOld":"/dev/null","sourceNew":"\t/**\n\t * Creates a TokenStream which tokenizes all the text in the provided Reader.\n\t *\n\t * @return  A TokenStream build from a StandardTokenizer filtered with\n\t * \t\t\tStandardFilter, StopFilter, GermanStemFilter and LowerCaseFilter.\n\t */\n\tpublic final TokenStream tokenStream(String fieldName, Reader reader) {\n\t\tTokenStream result = new StandardTokenizer( reader );\n\t\tresult = new StandardFilter( result );\n\t\tresult = new StopFilter( result, stoptable );\n\t\tresult = new GermanStemFilter( result, excltable );\n\t\t// Convert to lowercase after stemming!\n\t\tresult = new LowerCaseFilter( result );\n\t\treturn result;\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b027969b494a02d6f7a03a43537cc6935afce2a","date":1008019104,"type":3,"author":"gschwarz","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","pathOld":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","sourceNew":"\t/**\n\t * Creates a TokenStream which tokenizes all the text in the provided Reader.\n\t *\n\t * @return  A TokenStream build from a StandardTokenizer filtered with\n\t * \t\t\tStandardFilter, StopFilter, GermanStemFilter and LowerCaseFilter\n\t */\n\tpublic final TokenStream tokenStream( String fieldName, Reader reader ) {\n\t\tTokenStream result = new StandardTokenizer( reader );\n\t\tresult = new StandardFilter( result );\n\t\tresult = new StopFilter( result, stoptable );\n\t\tresult = new GermanStemFilter( result, excltable );\n\t\t// Convert to lowercase after stemming!\n\t\tresult = new LowerCaseFilter( result );\n\t\treturn result;\n\t}\n\n","sourceOld":"\t/**\n\t * Creates a TokenStream which tokenizes all the text in the provided Reader.\n\t *\n\t * @return  A TokenStream build from a StandardTokenizer filtered with\n\t * \t\t\tStandardFilter, StopFilter, GermanStemFilter and LowerCaseFilter.\n\t */\n\tpublic final TokenStream tokenStream(String fieldName, Reader reader) {\n\t\tTokenStream result = new StandardTokenizer( reader );\n\t\tresult = new StandardFilter( result );\n\t\tresult = new StopFilter( result, stoptable );\n\t\tresult = new GermanStemFilter( result, excltable );\n\t\t// Convert to lowercase after stemming!\n\t\tresult = new LowerCaseFilter( result );\n\t\treturn result;\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30b05243fdaa0f85180178b8e76bbdc3f18c45ee","date":1027292437,"type":3,"author":"gschwarz","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","pathOld":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","sourceNew":"\t/**\n\t * Creates a TokenStream which tokenizes all the text in the provided Reader.\n\t *\n\t * @return  A TokenStream build from a StandardTokenizer filtered with\n\t * \t\t\tStandardFilter, StopFilter, GermanStemFilter and LowerCaseFilter\n\t */\n\tpublic TokenStream tokenStream( String fieldName, Reader reader ) {\n\t\tTokenStream result = new StandardTokenizer( reader );\n\t\tresult = new StandardFilter( result );\n\t\tresult = new StopFilter( result, stoptable );\n\t\tresult = new GermanStemFilter( result, excltable );\n\t\treturn result;\n\t}\n\n","sourceOld":"\t/**\n\t * Creates a TokenStream which tokenizes all the text in the provided Reader.\n\t *\n\t * @return  A TokenStream build from a StandardTokenizer filtered with\n\t * \t\t\tStandardFilter, StopFilter, GermanStemFilter and LowerCaseFilter\n\t */\n\tpublic final TokenStream tokenStream( String fieldName, Reader reader ) {\n\t\tTokenStream result = new StandardTokenizer( reader );\n\t\tresult = new StandardFilter( result );\n\t\tresult = new StopFilter( result, stoptable );\n\t\tresult = new GermanStemFilter( result, excltable );\n\t\t// Convert to lowercase after stemming!\n\t\tresult = new LowerCaseFilter( result );\n\t\treturn result;\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f1b51af49aa8e1c86a2453fc1d1b618effb6ec8e","date":1029691996,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","pathOld":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","sourceNew":"    /**\n     * Creates a TokenStream which tokenizes all the text in the provided Reader.\n     *\n     * @return  A TokenStream build from a StandardTokenizer filtered with\n     *\t\tStandardFilter, StopFilter, GermanStemFilter and LowerCaseFilter\n     */\n    public TokenStream tokenStream( String fieldName, Reader reader )\n    {\n\tTokenStream result = new StandardTokenizer( reader );\n\tresult = new StandardFilter( result );\n\tresult = new StopFilter( result, stoptable );\n\tresult = new GermanStemFilter( result, excltable );\n\treturn result;\n    }\n\n","sourceOld":"\t/**\n\t * Creates a TokenStream which tokenizes all the text in the provided Reader.\n\t *\n\t * @return  A TokenStream build from a StandardTokenizer filtered with\n\t * \t\t\tStandardFilter, StopFilter, GermanStemFilter and LowerCaseFilter\n\t */\n\tpublic TokenStream tokenStream( String fieldName, Reader reader ) {\n\t\tTokenStream result = new StandardTokenizer( reader );\n\t\tresult = new StandardFilter( result );\n\t\tresult = new StopFilter( result, stoptable );\n\t\tresult = new GermanStemFilter( result, excltable );\n\t\treturn result;\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6f803c3734eaaa82b6c977624b842b77946391af","date":1043860735,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","pathOld":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","sourceNew":"    /**\n     * Creates a TokenStream which tokenizes all the text in the provided Reader.\n     *\n     * @return  A TokenStream build from a StandardTokenizer filtered with\n     *\t\tStandardFilter, StopFilter, GermanStemFilter\n     */\n    public TokenStream tokenStream( String fieldName, Reader reader )\n    {\n\tTokenStream result = new StandardTokenizer( reader );\n\tresult = new StandardFilter( result );\n\tresult = new StopFilter( result, stoptable );\n\tresult = new GermanStemFilter( result, excltable );\n\treturn result;\n    }\n\n","sourceOld":"    /**\n     * Creates a TokenStream which tokenizes all the text in the provided Reader.\n     *\n     * @return  A TokenStream build from a StandardTokenizer filtered with\n     *\t\tStandardFilter, StopFilter, GermanStemFilter and LowerCaseFilter\n     */\n    public TokenStream tokenStream( String fieldName, Reader reader )\n    {\n\tTokenStream result = new StandardTokenizer( reader );\n\tresult = new StandardFilter( result );\n\tresult = new StopFilter( result, stoptable );\n\tresult = new GermanStemFilter( result, excltable );\n\treturn result;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"add40802dfe0640e177eeb8b42c2c8c928ec85cc","date":1065658132,"type":3,"author":"Erik Hatcher","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","pathOld":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","sourceNew":"    /**\n     * Creates a TokenStream which tokenizes all the text in the provided Reader.\n     *\n     * @return  A TokenStream build from a StandardTokenizer filtered with\n     *\t\tStandardFilter, StopFilter, GermanStemFilter\n     */\n    public TokenStream tokenStream( String fieldName, Reader reader )\n    {\n\tTokenStream result = new StandardTokenizer( reader );\n\tresult = new StandardFilter( result );\n  // shouldn't there be a lowercaser before stop word filtering?\n  result = new StopFilter( result, stoptable );\n\tresult = new GermanStemFilter( result, excltable );\n\treturn result;\n    }\n\n","sourceOld":"    /**\n     * Creates a TokenStream which tokenizes all the text in the provided Reader.\n     *\n     * @return  A TokenStream build from a StandardTokenizer filtered with\n     *\t\tStandardFilter, StopFilter, GermanStemFilter\n     */\n    public TokenStream tokenStream( String fieldName, Reader reader )\n    {\n\tTokenStream result = new StandardTokenizer( reader );\n\tresult = new StandardFilter( result );\n\tresult = new StopFilter( result, stoptable );\n\tresult = new GermanStemFilter( result, excltable );\n\treturn result;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d14dd1f81165dd4704a95f7427639ada7c3047f7","date":1079084628,"type":3,"author":"Erik Hatcher","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","pathOld":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","sourceNew":"    /**\n     * Creates a TokenStream which tokenizes all the text in the provided Reader.\n     *\n     * @return  A TokenStream build from a StandardTokenizer filtered with\n     *\t\tStandardFilter, StopFilter, GermanStemFilter\n     */\n    public TokenStream tokenStream( String fieldName, Reader reader )\n    {\n\tTokenStream result = new StandardTokenizer( reader );\n\tresult = new StandardFilter( result );\n  // shouldn't there be a lowercaser before stop word filtering?\n  result = new StopFilter( result, stopSet );\n\tresult = new GermanStemFilter( result, exclusionSet );\n\treturn result;\n    }\n\n","sourceOld":"    /**\n     * Creates a TokenStream which tokenizes all the text in the provided Reader.\n     *\n     * @return  A TokenStream build from a StandardTokenizer filtered with\n     *\t\tStandardFilter, StopFilter, GermanStemFilter\n     */\n    public TokenStream tokenStream( String fieldName, Reader reader )\n    {\n\tTokenStream result = new StandardTokenizer( reader );\n\tresult = new StandardFilter( result );\n  // shouldn't there be a lowercaser before stop word filtering?\n  result = new StopFilter( result, stoptable );\n\tresult = new GermanStemFilter( result, excltable );\n\treturn result;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac9d61a38ae43a4deda807b7f4afcca3a1abc0da","date":1079084717,"type":3,"author":"Erik Hatcher","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","pathOld":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","sourceNew":"  /**\n   * Creates a TokenStream which tokenizes all the text in the provided Reader.\n   *\n   * @return A TokenStream build from a StandardTokenizer filtered with\n   *         StandardFilter, StopFilter, GermanStemFilter\n   */\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    TokenStream result = new StandardTokenizer(reader);\n    result = new StandardFilter(result);\n// shouldn't there be a lowercaser before stop word filtering?\n    result = new StopFilter(result, stopSet);\n    result = new GermanStemFilter(result, exclusionSet);\n    return result;\n  }\n\n","sourceOld":"    /**\n     * Creates a TokenStream which tokenizes all the text in the provided Reader.\n     *\n     * @return  A TokenStream build from a StandardTokenizer filtered with\n     *\t\tStandardFilter, StopFilter, GermanStemFilter\n     */\n    public TokenStream tokenStream( String fieldName, Reader reader )\n    {\n\tTokenStream result = new StandardTokenizer( reader );\n\tresult = new StandardFilter( result );\n  // shouldn't there be a lowercaser before stop word filtering?\n  result = new StopFilter( result, stopSet );\n\tresult = new GermanStemFilter( result, exclusionSet );\n\treturn result;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4230eef3a047e2a85e989e7ced62bf7fd4a9f859","date":1080661498,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","pathOld":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","sourceNew":"  /**\n   * Creates a TokenStream which tokenizes all the text in the provided Reader.\n   *\n   * @return A TokenStream build from a StandardTokenizer filtered with\n   *         StandardFilter, LowerCaseFilter, StopFilter, GermanStemFilter\n   */\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    TokenStream result = new StandardTokenizer(reader);\n    result = new StandardFilter(result);\n    result = new LowerCaseFilter(result);\n    result = new StopFilter(result, stopSet);\n    result = new GermanStemFilter(result, exclusionSet);\n    return result;\n  }\n\n","sourceOld":"  /**\n   * Creates a TokenStream which tokenizes all the text in the provided Reader.\n   *\n   * @return A TokenStream build from a StandardTokenizer filtered with\n   *         StandardFilter, StopFilter, GermanStemFilter\n   */\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    TokenStream result = new StandardTokenizer(reader);\n    result = new StandardFilter(result);\n// shouldn't there be a lowercaser before stop word filtering?\n    result = new StopFilter(result, stopSet);\n    result = new GermanStemFilter(result, exclusionSet);\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eb502dc71e908fb2c30e64b73e1f7e7b6238f5a2","date":1092688309,"type":4,"author":"Daniel Naber","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/analysis/de/GermanAnalyzer#tokenStream(String,Reader).mjava","sourceNew":null,"sourceOld":"  /**\n   * Creates a TokenStream which tokenizes all the text in the provided Reader.\n   *\n   * @return A TokenStream build from a StandardTokenizer filtered with\n   *         StandardFilter, LowerCaseFilter, StopFilter, GermanStemFilter\n   */\n  public TokenStream tokenStream(String fieldName, Reader reader) {\n    TokenStream result = new StandardTokenizer(reader);\n    result = new StandardFilter(result);\n    result = new LowerCaseFilter(result);\n    result = new StopFilter(result, stopSet);\n    result = new GermanStemFilter(result, exclusionSet);\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"add40802dfe0640e177eeb8b42c2c8c928ec85cc":["6f803c3734eaaa82b6c977624b842b77946391af"],"3ee067e27d1cce6d2d5d64280007410c2e1a38d8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"30b05243fdaa0f85180178b8e76bbdc3f18c45ee":["9b027969b494a02d6f7a03a43537cc6935afce2a"],"eb502dc71e908fb2c30e64b73e1f7e7b6238f5a2":["4230eef3a047e2a85e989e7ced62bf7fd4a9f859"],"4230eef3a047e2a85e989e7ced62bf7fd4a9f859":["ac9d61a38ae43a4deda807b7f4afcca3a1abc0da"],"6f803c3734eaaa82b6c977624b842b77946391af":["f1b51af49aa8e1c86a2453fc1d1b618effb6ec8e"],"f1b51af49aa8e1c86a2453fc1d1b618effb6ec8e":["30b05243fdaa0f85180178b8e76bbdc3f18c45ee"],"9b027969b494a02d6f7a03a43537cc6935afce2a":["3ee067e27d1cce6d2d5d64280007410c2e1a38d8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ac9d61a38ae43a4deda807b7f4afcca3a1abc0da":["d14dd1f81165dd4704a95f7427639ada7c3047f7"],"d14dd1f81165dd4704a95f7427639ada7c3047f7":["add40802dfe0640e177eeb8b42c2c8c928ec85cc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["eb502dc71e908fb2c30e64b73e1f7e7b6238f5a2"]},"commit2Childs":{"add40802dfe0640e177eeb8b42c2c8c928ec85cc":["d14dd1f81165dd4704a95f7427639ada7c3047f7"],"3ee067e27d1cce6d2d5d64280007410c2e1a38d8":["9b027969b494a02d6f7a03a43537cc6935afce2a"],"30b05243fdaa0f85180178b8e76bbdc3f18c45ee":["f1b51af49aa8e1c86a2453fc1d1b618effb6ec8e"],"6f803c3734eaaa82b6c977624b842b77946391af":["add40802dfe0640e177eeb8b42c2c8c928ec85cc"],"eb502dc71e908fb2c30e64b73e1f7e7b6238f5a2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4230eef3a047e2a85e989e7ced62bf7fd4a9f859":["eb502dc71e908fb2c30e64b73e1f7e7b6238f5a2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3ee067e27d1cce6d2d5d64280007410c2e1a38d8"],"9b027969b494a02d6f7a03a43537cc6935afce2a":["30b05243fdaa0f85180178b8e76bbdc3f18c45ee"],"f1b51af49aa8e1c86a2453fc1d1b618effb6ec8e":["6f803c3734eaaa82b6c977624b842b77946391af"],"ac9d61a38ae43a4deda807b7f4afcca3a1abc0da":["4230eef3a047e2a85e989e7ced62bf7fd4a9f859"],"d14dd1f81165dd4704a95f7427639ada7c3047f7":["ac9d61a38ae43a4deda807b7f4afcca3a1abc0da"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}