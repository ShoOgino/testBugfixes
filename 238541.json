{"path":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","commits":[{"id":"2b84d416bbd661ae4b2a28d103bdfccb851e00de","date":1458041762,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#switchToOffline().mjava","sourceNew":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, longOrds, \"spill\");\n    tempInput = offlinePointWriter.out;\n    PointReader reader = heapPointWriter.getReader(0);\n    for(int i=0;i<pointCount;i++) {\n      boolean hasNext = reader.next();\n      assert hasNext;\n      offlinePointWriter.append(reader.packedValue(), i, heapPointWriter.docIDs[i]);\n    }\n\n    heapPointWriter = null;\n  }\n\n","sourceOld":"  /** If the current segment has too many points then we switchover to temp files / offline sort. */\n  private void switchToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, longOrds, \"switch\");\n    tempInput = offlinePointWriter.out;\n    PointReader reader = heapPointWriter.getReader(0);\n    for(int i=0;i<pointCount;i++) {\n      boolean hasNext = reader.next();\n      assert hasNext;\n      offlinePointWriter.append(reader.packedValue(), i, heapPointWriter.docIDs[i]);\n    }\n\n    heapPointWriter = null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"950b7a6881d14da782b60444c11295e3ec50d41a","date":1458379095,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","sourceNew":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, longOrds, \"spill\");\n    tempInput = offlinePointWriter.out;\n    PointReader reader = heapPointWriter.getReader(0, pointCount);\n    for(int i=0;i<pointCount;i++) {\n      boolean hasNext = reader.next();\n      assert hasNext;\n      offlinePointWriter.append(reader.packedValue(), i, heapPointWriter.docIDs[i]);\n    }\n\n    heapPointWriter = null;\n  }\n\n","sourceOld":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, longOrds, \"spill\");\n    tempInput = offlinePointWriter.out;\n    PointReader reader = heapPointWriter.getReader(0);\n    for(int i=0;i<pointCount;i++) {\n      boolean hasNext = reader.next();\n      assert hasNext;\n      offlinePointWriter.append(reader.packedValue(), i, heapPointWriter.docIDs[i]);\n    }\n\n    heapPointWriter = null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"416f9e28900210be57b69bc12e2954fb98ed7ebe","date":1458479803,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","sourceNew":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, longOrds, \"spill\", singleValuePerDoc);\n    tempInput = offlinePointWriter.out;\n    PointReader reader = heapPointWriter.getReader(0, pointCount);\n    for(int i=0;i<pointCount;i++) {\n      boolean hasNext = reader.next();\n      assert hasNext;\n      offlinePointWriter.append(reader.packedValue(), i, heapPointWriter.docIDs[i]);\n    }\n\n    heapPointWriter = null;\n  }\n\n","sourceOld":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, longOrds, \"spill\");\n    tempInput = offlinePointWriter.out;\n    PointReader reader = heapPointWriter.getReader(0, pointCount);\n    for(int i=0;i<pointCount;i++) {\n      boolean hasNext = reader.next();\n      assert hasNext;\n      offlinePointWriter.append(reader.packedValue(), i, heapPointWriter.docIDs[i]);\n    }\n\n    heapPointWriter = null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40","date":1458553787,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","sourceNew":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, longOrds, \"spill\", 0, singleValuePerDoc);\n    tempInput = offlinePointWriter.out;\n    PointReader reader = heapPointWriter.getReader(0, pointCount);\n    for(int i=0;i<pointCount;i++) {\n      boolean hasNext = reader.next();\n      assert hasNext;\n      offlinePointWriter.append(reader.packedValue(), i, heapPointWriter.docIDs[i]);\n    }\n\n    heapPointWriter = null;\n  }\n\n","sourceOld":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, longOrds, \"spill\", singleValuePerDoc);\n    tempInput = offlinePointWriter.out;\n    PointReader reader = heapPointWriter.getReader(0, pointCount);\n    for(int i=0;i<pointCount;i++) {\n      boolean hasNext = reader.next();\n      assert hasNext;\n      offlinePointWriter.append(reader.packedValue(), i, heapPointWriter.docIDs[i]);\n    }\n\n    heapPointWriter = null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"78bdc7d6906146edb12a1a6c1f765ba680ed5124","date":1549523533,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","sourceNew":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, \"spill\", 0);\n    tempInput = offlinePointWriter.out;\n    scratchBytesRef1.length = packedBytesLength;\n    for(int i=0;i<pointCount;i++) {\n      heapPointWriter.getPackedValueSlice(i, scratchBytesRef1);\n      offlinePointWriter.append(scratchBytesRef1, heapPointWriter.docIDs[i]);\n    }\n    heapPointWriter = null;\n  }\n\n","sourceOld":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, longOrds, \"spill\", 0, singleValuePerDoc);\n    tempInput = offlinePointWriter.out;\n    PointReader reader = heapPointWriter.getReader(0, pointCount);\n    for(int i=0;i<pointCount;i++) {\n      boolean hasNext = reader.next();\n      assert hasNext;\n      offlinePointWriter.append(reader.packedValue(), i, heapPointWriter.docIDs[i]);\n    }\n\n    heapPointWriter = null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c2344a1c769566d8c85cffcacc5e55153fa54b86","date":1550661298,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","sourceNew":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, \"spill\", 0);\n    tempInput = offlinePointWriter.out;\n    for(int i=0;i<pointCount;i++) {\n      offlinePointWriter.append(heapPointWriter.getPackedValueSlice(i));\n    }\n    heapPointWriter = null;\n  }\n\n","sourceOld":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, \"spill\", 0);\n    tempInput = offlinePointWriter.out;\n    scratchBytesRef1.length = packedBytesLength;\n    for(int i=0;i<pointCount;i++) {\n      heapPointWriter.getPackedValueSlice(i, scratchBytesRef1);\n      offlinePointWriter.append(scratchBytesRef1, heapPointWriter.docIDs[i]);\n    }\n    heapPointWriter = null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"76a51551f05a6c96a115b5a656837ecc8fd0b1ff","date":1551422476,"type":4,"author":"iverase","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#spillToOffline().mjava","sourceNew":null,"sourceOld":"  /** If the current segment has too many points then we spill over to temp files / offline sort. */\n  private void spillToOffline() throws IOException {\n\n    // For each .add we just append to this input file, then in .finish we sort this input and resursively build the tree:\n    offlinePointWriter = new OfflinePointWriter(tempDir, tempFileNamePrefix, packedBytesLength, \"spill\", 0);\n    tempInput = offlinePointWriter.out;\n    for(int i=0;i<pointCount;i++) {\n      offlinePointWriter.append(heapPointWriter.getPackedValueSlice(i));\n    }\n    heapPointWriter = null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40":["416f9e28900210be57b69bc12e2954fb98ed7ebe"],"c2344a1c769566d8c85cffcacc5e55153fa54b86":["78bdc7d6906146edb12a1a6c1f765ba680ed5124"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"78bdc7d6906146edb12a1a6c1f765ba680ed5124":["51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40"],"416f9e28900210be57b69bc12e2954fb98ed7ebe":["950b7a6881d14da782b60444c11295e3ec50d41a"],"950b7a6881d14da782b60444c11295e3ec50d41a":["2b84d416bbd661ae4b2a28d103bdfccb851e00de"],"2b84d416bbd661ae4b2a28d103bdfccb851e00de":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"76a51551f05a6c96a115b5a656837ecc8fd0b1ff":["c2344a1c769566d8c85cffcacc5e55153fa54b86"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["76a51551f05a6c96a115b5a656837ecc8fd0b1ff"]},"commit2Childs":{"51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40":["78bdc7d6906146edb12a1a6c1f765ba680ed5124"],"c2344a1c769566d8c85cffcacc5e55153fa54b86":["76a51551f05a6c96a115b5a656837ecc8fd0b1ff"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["2b84d416bbd661ae4b2a28d103bdfccb851e00de"],"78bdc7d6906146edb12a1a6c1f765ba680ed5124":["c2344a1c769566d8c85cffcacc5e55153fa54b86"],"416f9e28900210be57b69bc12e2954fb98ed7ebe":["51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40"],"950b7a6881d14da782b60444c11295e3ec50d41a":["416f9e28900210be57b69bc12e2954fb98ed7ebe"],"2b84d416bbd661ae4b2a28d103bdfccb851e00de":["950b7a6881d14da782b60444c11295e3ec50d41a"],"76a51551f05a6c96a115b5a656837ecc8fd0b1ff":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}