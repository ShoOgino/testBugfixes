{"path":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","commits":[{"id":"91109046a59c58ee0ee5d0d2767b08d1f30d6702","date":1000830588,"type":0,"author":"Jason van Zyl","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"/dev/null","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public final synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n    int minSegment = segmentInfos.size();\n    int segmentsAddedSinceMerge = 0;\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n\tsegmentInfos.addElement(sis.info(j));\t  // add each info\n\n\t// merge whenever mergeFactor segments have been added\n\tif (++segmentsAddedSinceMerge == mergeFactor) {\n\t  mergeSegments(minSegment++, false);\n\t  segmentsAddedSinceMerge = 0;\n\t}\n      }\n    }\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e745568eabfe0f163a0eddb52ef01a6a8404656","date":1012321816,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public final synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n\tsegmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public final synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n    int minSegment = segmentInfos.size();\n    int segmentsAddedSinceMerge = 0;\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n\tsegmentInfos.addElement(sis.info(j));\t  // add each info\n\n\t// merge whenever mergeFactor segments have been added\n\tif (++segmentsAddedSinceMerge == mergeFactor) {\n\t  mergeSegments(minSegment++, false);\n\t  segmentsAddedSinceMerge = 0;\n\t}\n      }\n    }\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9c7454619ea6a0710272c1dd947345cee64489f6","date":1026927484,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n\tsegmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public final synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n\tsegmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":null,"bugIntro":["dbb18b6a222f2507f22fab7cc7eed06658d59772"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1","date":1064527311,"type":3,"author":"Dmitry Serebrennikov","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n        segmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n\tsegmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"db628922a5eb84f4c7e097a23b99c6fcfc46e084","date":1117731958,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n        segmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n    \n    // merge newly added segments in log(n) passes\n    while (segmentInfos.size() > start+mergeFactor) {\n      for (int base = start+1; base < segmentInfos.size(); base++) {\n        int end = Math.min(segmentInfos.size(), base+mergeFactor);\n        if (end-base > 1)\n          mergeSegments(base, end);\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n        segmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":null,"bugIntro":["1507a324c1f939ed71e01297733a49b9c36e5688","6eb6723414c7578e3be2fa28b281a224547cdf93"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6eb6723414c7578e3be2fa28b281a224547cdf93","date":1144287642,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n        segmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n    \n    // merge newly added segments in log(n) passes\n    while (segmentInfos.size() > start+mergeFactor) {\n      for (int base = start; base < segmentInfos.size(); base++) {\n        int end = Math.min(segmentInfos.size(), base+mergeFactor);\n        if (end-base > 1)\n          mergeSegments(base, end);\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n        segmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n    \n    // merge newly added segments in log(n) passes\n    while (segmentInfos.size() > start+mergeFactor) {\n      for (int base = start+1; base < segmentInfos.size(); base++) {\n        int end = Math.min(segmentInfos.size(), base+mergeFactor);\n        if (end-base > 1)\n          mergeSegments(base, end);\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":["db628922a5eb84f4c7e097a23b99c6fcfc46e084"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1507a324c1f939ed71e01297733a49b9c36e5688","date":1155783141,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n        segmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n\n    // merge newly added segments in log(n) passes\n    while (segmentInfos.size() > start+mergeFactor) {\n      for (int base = start; base < segmentInfos.size(); base++) {\n        int end = Math.min(segmentInfos.size(), base+mergeFactor);\n        if (end-base > 1)\n          mergeSegments(base, end);\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n    // testInvariants();\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n        segmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n    \n    // merge newly added segments in log(n) passes\n    while (segmentInfos.size() > start+mergeFactor) {\n      for (int base = start; base < segmentInfos.size(); base++) {\n        int end = Math.min(segmentInfos.size(), base+mergeFactor);\n        if (end-base > 1)\n          mergeSegments(base, end);\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":["db628922a5eb84f4c7e097a23b99c6fcfc46e084"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d7052f725a053aa55424f966831826f61b798bf1","date":1158258681,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n        segmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n\n    // merge newly added segments in log(n) passes\n    while (segmentInfos.size() > start+mergeFactor) {\n      for (int base = start; base < segmentInfos.size(); base++) {\n        int end = Math.min(segmentInfos.size(), base+mergeFactor);\n        if (end-base > 1)\n          mergeSegments(segmentInfos, base, end);\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n        segmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n\n    // merge newly added segments in log(n) passes\n    while (segmentInfos.size() > start+mergeFactor) {\n      for (int base = start; base < segmentInfos.size(); base++) {\n        int end = Math.min(segmentInfos.size(), base+mergeFactor);\n        if (end-base > 1)\n          mergeSegments(base, end);\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n    // testInvariants();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"eeefd99c477417e5c7c574228461ebafe92469d4","date":1166460329,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (eg disk full), then either no indexes will have\n   * been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  Exact usage could be\n   * less but will depend on many factors.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      // merge newly added segments in log(n) passes\n      while (segmentInfos.size() > start+mergeFactor) {\n        for (int base = start; base < segmentInfos.size(); base++) {\n          int end = Math.min(segmentInfos.size(), base+mergeFactor);\n          if (end-base > 1) {\n            mergeSegments(segmentInfos, base, end);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(Directory[] dirs)\n      throws IOException {\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    for (int i = 0; i < dirs.length; i++) {\n      SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n      sis.read(dirs[i]);\n      for (int j = 0; j < sis.size(); j++) {\n        segmentInfos.addElement(sis.info(j));\t  // add each info\n      }\n    }\n\n    // merge newly added segments in log(n) passes\n    while (segmentInfos.size() > start+mergeFactor) {\n      for (int base = start; base < segmentInfos.size(); base++) {\n        int end = Math.min(segmentInfos.size(), base+mergeFactor);\n        if (end-base > 1)\n          mergeSegments(segmentInfos, base, end);\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":null,"bugIntro":["1b54a9bc667895a2095a886184bf69a3179e63df"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8f7478f1d67a81bf80f28067595be0383022d65b","date":1167857941,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      // merge newly added segments in log(n) passes\n      while (segmentInfos.size() > start+mergeFactor) {\n        for (int base = start; base < segmentInfos.size(); base++) {\n          int end = Math.min(segmentInfos.size(), base+mergeFactor);\n          if (end-base > 1) {\n            mergeSegments(segmentInfos, base, end);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (eg disk full), then either no indexes will have\n   * been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  Exact usage could be\n   * less but will depend on many factors.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      // merge newly added segments in log(n) passes\n      while (segmentInfos.size() > start+mergeFactor) {\n        for (int base = start; base < segmentInfos.size(); base++) {\n          int end = Math.min(segmentInfos.size(), base+mergeFactor);\n          if (end-base > 1) {\n            mergeSegments(segmentInfos, base, end);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b54a9bc667895a2095a886184bf69a3179e63df","date":1172088096,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      // merge newly added segments in log(n) passes\n      while (segmentInfos.size() > start+mergeFactor) {\n        for (int base = start; base < segmentInfos.size(); base++) {\n          int end = Math.min(segmentInfos.size(), base+mergeFactor);\n          if (end-base > 1) {\n            mergeSegments(segmentInfos, base, end);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      // merge newly added segments in log(n) passes\n      while (segmentInfos.size() > start+mergeFactor) {\n        for (int base = start; base < segmentInfos.size(); base++) {\n          int end = Math.min(segmentInfos.size(), base+mergeFactor);\n          if (end-base > 1) {\n            mergeSegments(segmentInfos, base, end);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":["eeefd99c477417e5c7c574228461ebafe92469d4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"328c1568e471f0c6eaa49ec00334ca59e573710f","date":1173897963,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      // merge newly added segments in log(n) passes\n      while (segmentInfos.size() > start+mergeFactor) {\n        for (int base = start; base < segmentInfos.size(); base++) {\n          int end = Math.min(segmentInfos.size(), base+mergeFactor);\n          if (end-base > 1) {\n            mergeSegments(segmentInfos, base, end);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      // merge newly added segments in log(n) passes\n      while (segmentInfos.size() > start+mergeFactor) {\n        for (int base = start; base < segmentInfos.size(); base++) {\n          int end = Math.min(segmentInfos.size(), base+mergeFactor);\n          if (end-base > 1) {\n            mergeSegments(segmentInfos, base, end);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      // merge newly added segments in log(n) passes\n      while (segmentInfos.size() > start+mergeFactor) {\n        for (int base = start; base < segmentInfos.size(); base++) {\n          int end = Math.min(segmentInfos.size(), base+mergeFactor);\n          if (end-base > 1) {\n            mergeSegments(base, end);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      // merge newly added segments in log(n) passes\n      while (segmentInfos.size() > start+mergeFactor) {\n        for (int base = start; base < segmentInfos.size(); base++) {\n          int end = Math.min(segmentInfos.size(), base+mergeFactor);\n          if (end-base > 1) {\n            mergeSegments(segmentInfos, base, end);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    flush();\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      optimize();\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      // merge newly added segments in log(n) passes\n      while (segmentInfos.size() > start+mergeFactor) {\n        for (int base = start; base < segmentInfos.size(); base++) {\n          int end = Math.min(segmentInfos.size(), base+mergeFactor);\n          if (end-base > 1) {\n            mergeSegments(base, end);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n\n    optimize();\t\t\t\t\t  // final cleanup\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"67006a60923e2124212d3baa0d29b444bcbd8373","date":1191425052,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexes\");\n    flush();\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      optimize();\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    flush();\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      optimize();\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2607e6466921754dbb3ebd17f098a4e9f1fc4877","date":1192019178,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexes\");\n    flush();\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      optimize();\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexes\");\n    flush();\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      optimize();\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6a1f29c9b1051488fd5fa7d56c98db5f4388408","date":1196281221,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexes\");\n    flush();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      optimize();\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexes\");\n    flush();\n\n    int start = segmentInfos.size();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      optimize();\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e82780afe6097066eb5befb86e9432f077667e3d","date":1202756169,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexes\");\n    flush(true, false);\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      optimize();\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexes\");\n    flush();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      optimize();\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63","date":1204234542,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        for (int i = 0; i < dirs.length; i++) {\n          SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            segmentInfos.addElement(sis.info(j));\t  // add each info\n          }\n        }\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexes\");\n    flush(true, false);\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          segmentInfos.addElement(sis.info(j));\t  // add each info\n        }\n      }\n\n      optimize();\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["dbb18b6a222f2507f22fab7cc7eed06658d59772","cd488f50316362b01a7f67b11a96796b9652e3e5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be","date":1204801324,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = 0;\n        for (int i = 0; i < dirs.length; i++) {\n          SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            final SegmentInfo info = sis.info(j);\n            docCount += info.docCount;\n            segmentInfos.addElement(info);\t  // add each info\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        for (int i = 0; i < dirs.length; i++) {\n          SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            segmentInfos.addElement(sis.info(j));\t  // add each info\n          }\n        }\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","bugFix":null,"bugIntro":["dbb18b6a222f2507f22fab7cc7eed06658d59772"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dbb18b6a222f2507f22fab7cc7eed06658d59772","date":1204804366,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              segmentInfos.addElement(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = 0;\n        for (int i = 0; i < dirs.length; i++) {\n          SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            final SegmentInfo info = sis.info(j);\n            docCount += info.docCount;\n            segmentInfos.addElement(info);\t  // add each info\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","bugFix":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63","9c7454619ea6a0710272c1dd947345cee64489f6","a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"268d26e9bb23249b6a418a01d52fcbe19ee33a1f","date":1219498325,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              segmentInfos.addElement(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>After this completes, the index is optimized.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>If an Exception is hit, it's still possible that all\n   * indexes were successfully added.  This happens when the\n   * Exception is hit when trying to build a CFS file.  In\n   * this case, one segment in the index will be in non-CFS\n   * format, even when using compound file format.</p>\n   *\n   * <p>Also note that on an Exception, the index may still\n   * have been partially or fully optimized even though none\n   * of the input indexes were added. </p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   *\n   * <p>See <a target=\"_top\"\n   * href=\"http://issues.apache.org/jira/browse/LUCENE-702\">LUCENE-702</a>\n   * for details.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              segmentInfos.addElement(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2586f96f60332eb97ecd2934b0763791462568b2","date":1220116589,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    \n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              assert !segmentInfos.contains(info);\n              segmentInfos.addElement(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              segmentInfos.addElement(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7b6cdc70e097da94da79a655ed8f94477ff69f5","date":1220815360,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    \n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              assert !segmentInfos.contains(info);\n              segmentInfos.add(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    \n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              assert !segmentInfos.contains(info);\n              segmentInfos.addElement(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9665d17707cc21b1db995118ff36129723139ab","date":1225384420,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    \n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              assert !segmentInfos.contains(info);\n              segmentInfos.add(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    \n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              assert !segmentInfos.contains(info);\n              segmentInfos.add(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c4ff8864209d2e972cb4393600c26082f9a6533d","date":1239297466,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    \n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              assert !segmentInfos.contains(info);\n              segmentInfos.add(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    \n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              assert !segmentInfos.contains(info);\n              segmentInfos.add(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cd488f50316362b01a7f67b11a96796b9652e3e5","date":1241121034,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    \n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              assert !segmentInfos.contains(info);\n              segmentInfos.add(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory[])\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    \n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              assert !segmentInfos.contains(info);\n              segmentInfos.add(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7","date":1255555265,"type":4,"author":"Michael Busch","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(Directory[]).mjava","sourceNew":null,"sourceOld":"  /** Merges all segments from an array of indexes into this index.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @deprecated Use {@link #addIndexesNoOptimize} instead,\n   * then separately call {@link #optimize} afterwards if\n   * you need to.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(Directory[] dirs)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    \n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n\n      if (infoStream != null)\n        message(\"flush at addIndexes\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n          for (int i = 0; i < dirs.length; i++) {\n            SegmentInfos sis = new SegmentInfos();\t  // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              final SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              assert !segmentInfos.contains(info);\n              segmentInfos.add(info);\t  // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        optimize();\n\n        success = true;\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(Directory[])\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"328c1568e471f0c6eaa49ec00334ca59e573710f":["1b54a9bc667895a2095a886184bf69a3179e63df"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["e9665d17707cc21b1db995118ff36129723139ab"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["2586f96f60332eb97ecd2934b0763791462568b2"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["328c1568e471f0c6eaa49ec00334ca59e573710f"],"3e745568eabfe0f163a0eddb52ef01a6a8404656":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"dbb18b6a222f2507f22fab7cc7eed06658d59772":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"eeefd99c477417e5c7c574228461ebafe92469d4":["d7052f725a053aa55424f966831826f61b798bf1"],"1507a324c1f939ed71e01297733a49b9c36e5688":["6eb6723414c7578e3be2fa28b281a224547cdf93"],"8f7478f1d67a81bf80f28067595be0383022d65b":["eeefd99c477417e5c7c574228461ebafe92469d4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2607e6466921754dbb3ebd17f098a4e9f1fc4877":["67006a60923e2124212d3baa0d29b444bcbd8373"],"2586f96f60332eb97ecd2934b0763791462568b2":["268d26e9bb23249b6a418a01d52fcbe19ee33a1f"],"268d26e9bb23249b6a418a01d52fcbe19ee33a1f":["dbb18b6a222f2507f22fab7cc7eed06658d59772"],"d7052f725a053aa55424f966831826f61b798bf1":["1507a324c1f939ed71e01297733a49b9c36e5688"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1b54a9bc667895a2095a886184bf69a3179e63df":["8f7478f1d67a81bf80f28067595be0383022d65b"],"db628922a5eb84f4c7e097a23b99c6fcfc46e084":["8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1"],"67006a60923e2124212d3baa0d29b444bcbd8373":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["cd488f50316362b01a7f67b11a96796b9652e3e5"],"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1":["9c7454619ea6a0710272c1dd947345cee64489f6"],"b6a1f29c9b1051488fd5fa7d56c98db5f4388408":["2607e6466921754dbb3ebd17f098a4e9f1fc4877"],"6eb6723414c7578e3be2fa28b281a224547cdf93":["db628922a5eb84f4c7e097a23b99c6fcfc46e084"],"e9665d17707cc21b1db995118ff36129723139ab":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"cd488f50316362b01a7f67b11a96796b9652e3e5":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63":["e82780afe6097066eb5befb86e9432f077667e3d"],"9c7454619ea6a0710272c1dd947345cee64489f6":["3e745568eabfe0f163a0eddb52ef01a6a8404656"],"e82780afe6097066eb5befb86e9432f077667e3d":["b6a1f29c9b1051488fd5fa7d56c98db5f4388408"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"]},"commit2Childs":{"328c1568e471f0c6eaa49ec00334ca59e573710f":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["cd488f50316362b01a7f67b11a96796b9652e3e5"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["e9665d17707cc21b1db995118ff36129723139ab"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["dbb18b6a222f2507f22fab7cc7eed06658d59772"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"3e745568eabfe0f163a0eddb52ef01a6a8404656":["9c7454619ea6a0710272c1dd947345cee64489f6"],"dbb18b6a222f2507f22fab7cc7eed06658d59772":["268d26e9bb23249b6a418a01d52fcbe19ee33a1f"],"eeefd99c477417e5c7c574228461ebafe92469d4":["8f7478f1d67a81bf80f28067595be0383022d65b"],"1507a324c1f939ed71e01297733a49b9c36e5688":["d7052f725a053aa55424f966831826f61b798bf1"],"8f7478f1d67a81bf80f28067595be0383022d65b":["1b54a9bc667895a2095a886184bf69a3179e63df"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"2607e6466921754dbb3ebd17f098a4e9f1fc4877":["b6a1f29c9b1051488fd5fa7d56c98db5f4388408"],"2586f96f60332eb97ecd2934b0763791462568b2":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"d7052f725a053aa55424f966831826f61b798bf1":["eeefd99c477417e5c7c574228461ebafe92469d4"],"268d26e9bb23249b6a418a01d52fcbe19ee33a1f":["2586f96f60332eb97ecd2934b0763791462568b2"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["3e745568eabfe0f163a0eddb52ef01a6a8404656"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["67006a60923e2124212d3baa0d29b444bcbd8373"],"1b54a9bc667895a2095a886184bf69a3179e63df":["328c1568e471f0c6eaa49ec00334ca59e573710f"],"db628922a5eb84f4c7e097a23b99c6fcfc46e084":["6eb6723414c7578e3be2fa28b281a224547cdf93"],"67006a60923e2124212d3baa0d29b444bcbd8373":["2607e6466921754dbb3ebd17f098a4e9f1fc4877"],"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1":["db628922a5eb84f4c7e097a23b99c6fcfc46e084"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b6a1f29c9b1051488fd5fa7d56c98db5f4388408":["e82780afe6097066eb5befb86e9432f077667e3d"],"6eb6723414c7578e3be2fa28b281a224547cdf93":["1507a324c1f939ed71e01297733a49b9c36e5688"],"e9665d17707cc21b1db995118ff36129723139ab":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"cd488f50316362b01a7f67b11a96796b9652e3e5":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"],"9c7454619ea6a0710272c1dd947345cee64489f6":["8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1"],"e82780afe6097066eb5befb86e9432f077667e3d":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}