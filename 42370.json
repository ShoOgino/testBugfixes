{"path":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","commits":[{"id":"a371aa649cc243e82cb8677ca960a1e0232ecedf","date":1393605574,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(int,Set[String],String).mjava","sourceNew":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator(TermsEnum.EMPTY);\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || docTerm.startsWith(prefixToken)) {\n\n        DocsAndPositionsEnum docPosEnum = it.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","sourceOld":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator(TermsEnum.EMPTY);\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || docTerm.startsWith(prefixToken)) {\n\n        DocsAndPositionsEnum docPosEnum = it.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79854637616b791a00f39ee3d5257ea093804ddb","date":1422697309,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","sourceNew":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator(TermsEnum.EMPTY);\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || (prefixToken != null && docTerm.startsWith(prefixToken))) {\n \n        DocsAndPositionsEnum docPosEnum = it.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","sourceOld":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator(TermsEnum.EMPTY);\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || docTerm.startsWith(prefixToken)) {\n\n        DocsAndPositionsEnum docPosEnum = it.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","bugFix":["f13ec1b606a28789743a563929e7c556e8218297"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","sourceNew":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator(TermsEnum.EMPTY);\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || (prefixToken != null && docTerm.startsWith(prefixToken))) {\n \n        PostingsEnum docPosEnum = it.postings(null, null, PostingsEnum.FLAG_OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","sourceOld":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator(TermsEnum.EMPTY);\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || (prefixToken != null && docTerm.startsWith(prefixToken))) {\n \n        DocsAndPositionsEnum docPosEnum = it.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","sourceNew":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator(TermsEnum.EMPTY);\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || (prefixToken != null && docTerm.startsWith(prefixToken))) {\n \n        PostingsEnum docPosEnum = it.postings(null, null, PostingsEnum.OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","sourceOld":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator(TermsEnum.EMPTY);\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || (prefixToken != null && docTerm.startsWith(prefixToken))) {\n \n        PostingsEnum docPosEnum = it.postings(null, null, PostingsEnum.FLAG_OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","sourceNew":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator();\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || (prefixToken != null && docTerm.startsWith(prefixToken))) {\n \n        PostingsEnum docPosEnum = it.postings(null, null, PostingsEnum.OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","sourceOld":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator(TermsEnum.EMPTY);\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || (prefixToken != null && docTerm.startsWith(prefixToken))) {\n \n        PostingsEnum docPosEnum = it.postings(null, null, PostingsEnum.OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester#createCoefficient(IndexSearcher,int,Set[String],String).mjava","sourceNew":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator();\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || (prefixToken != null && docTerm.startsWith(prefixToken))) {\n \n        PostingsEnum docPosEnum = it.postings(null, PostingsEnum.OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","sourceOld":"  /**\n   * Create the coefficient to transform the weight.\n   *\n   * @param doc id of the document\n   * @param matchedTokens tokens found in the query\n   * @param prefixToken unfinished token in the query\n   * @return the coefficient\n   * @throws IOException If there are problems reading term vectors from the underlying Lucene index.\n   */\n  private double createCoefficient(IndexSearcher searcher, int doc, Set<String> matchedTokens, String prefixToken) throws IOException {\n\n    Terms tv = searcher.getIndexReader().getTermVector(doc, TEXT_FIELD_NAME);\n    TermsEnum it = tv.iterator();\n\n    Integer position = Integer.MAX_VALUE;\n    BytesRef term;\n    // find the closest token position\n    while ((term = it.next()) != null) {\n\n      String docTerm = term.utf8ToString();\n\n      if (matchedTokens.contains(docTerm) || (prefixToken != null && docTerm.startsWith(prefixToken))) {\n \n        PostingsEnum docPosEnum = it.postings(null, null, PostingsEnum.OFFSETS);\n        docPosEnum.nextDoc();\n\n        // use the first occurrence of the term\n        int p = docPosEnum.nextPosition();\n        if (p < position) {\n          position = p;\n        }\n      }\n    }\n\n    // create corresponding coefficient based on position\n    return calculateCoefficient(position);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"79854637616b791a00f39ee3d5257ea093804ddb":["a371aa649cc243e82cb8677ca960a1e0232ecedf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a371aa649cc243e82cb8677ca960a1e0232ecedf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"51f5280f31484820499077f41fcdfe92d527d9dc":["79854637616b791a00f39ee3d5257ea093804ddb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f4464508ee83288c8c4585b533f9faaa93aa314"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"79854637616b791a00f39ee3d5257ea093804ddb":["51f5280f31484820499077f41fcdfe92d527d9dc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a371aa649cc243e82cb8677ca960a1e0232ecedf"],"a371aa649cc243e82cb8677ca960a1e0232ecedf":["79854637616b791a00f39ee3d5257ea093804ddb"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}