{"path":"lucene/analysis/nori/src/java/org/apache/lucene/analysis/ko/KoreanTokenizer#backtrace(Position,int).mjava","commits":[{"id":"8493925b2e70246f0961df584c01a8c2e61ee52f","date":1523611602,"type":0,"author":"Jim Ferenczi","isMerge":false,"pathNew":"lucene/analysis/nori/src/java/org/apache/lucene/analysis/ko/KoreanTokenizer#backtrace(Position,int).mjava","pathOld":"/dev/null","sourceNew":"  // the pending list.  The pending list is then in-reverse\n  // (last token should be returned first).\n  private void backtrace(final Position endPosData, final int fromIDX) {\n    final int endPos = endPosData.pos;\n\n    if (VERBOSE) {\n      System.out.println(\"\\n  backtrace: endPos=\" + endPos + \" pos=\" + pos + \"; \" + (pos - lastBackTracePos) + \" characters; last=\" + lastBackTracePos + \" cost=\" + endPosData.costs[fromIDX]);\n    }\n\n    final char[] fragment = buffer.get(lastBackTracePos, endPos-lastBackTracePos);\n\n    if (dotOut != null) {\n      dotOut.onBacktrace(this, positions, lastBackTracePos, endPosData, fromIDX, fragment, end);\n    }\n\n    int pos = endPos;\n    int bestIDX = fromIDX;\n\n    // TODO: sort of silly to make Token instances here; the\n    // back trace has all info needed to generate the\n    // token.  So, we could just directly set the attrs,\n    // from the backtrace, in incrementToken w/o ever\n    // creating Token; we'd have to defer calling freeBefore\n    // until after the backtrace was fully \"consumed\" by\n    // incrementToken.\n\n    while (pos > lastBackTracePos) {\n      //System.out.println(\"BT: back pos=\" + pos + \" bestIDX=\" + bestIDX);\n      final Position posData = positions.get(pos);\n      assert bestIDX < posData.count;\n\n      int backPos = posData.backPos[bestIDX];\n      int backWordPos = posData.backWordPos[bestIDX];\n      assert backPos >= lastBackTracePos: \"backPos=\" + backPos + \" vs lastBackTracePos=\" + lastBackTracePos;\n      // the length of the word without the whitespaces at the beginning.\n      int length = pos - backWordPos;\n      Type backType = posData.backType[bestIDX];\n      int backID = posData.backID[bestIDX];\n      int nextBestIDX = posData.backIndex[bestIDX];\n      // the start of the word after the whitespace at the beginning.\n      final int fragmentOffset = backWordPos - lastBackTracePos;\n      assert fragmentOffset >= 0;\n\n      final Dictionary dict = getDict(backType);\n\n      if (outputUnknownUnigrams && backType == Type.UNKNOWN) {\n        // outputUnknownUnigrams converts unknown word into unigrams:\n        for (int i = length - 1; i >= 0; i--) {\n          int charLen = 1;\n          if (i > 0 && Character.isLowSurrogate(fragment[fragmentOffset + i])) {\n            i--;\n            charLen = 2;\n          }\n          final DictionaryToken token = new DictionaryToken(Type.UNKNOWN,\n              unkDictionary,\n              CharacterDefinition.NGRAM,\n              fragment,\n              fragmentOffset+i,\n              charLen,\n              backWordPos+i,\n              backWordPos+i+charLen\n          );\n          if (shouldFilterToken(token) == false) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          }\n        }\n      } else {\n        final DictionaryToken token = new DictionaryToken(backType,\n            dict,\n            backID,\n            fragment,\n            fragmentOffset,\n            length,\n            backWordPos,\n            backWordPos + length\n        );\n        if (token.getPOSType() == POS.Type.MORPHEME || mode == DecompoundMode.NONE) {\n          if (shouldFilterToken(token) == false) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          }\n        } else {\n          Dictionary.Morpheme[] morphemes = token.getMorphemes();\n          if (morphemes == null) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          } else {\n            int endOffset = backWordPos + length;\n            int posLen = 0;\n            // decompose the compound\n            for (int i = morphemes.length - 1; i >= 0; i--) {\n              final Dictionary.Morpheme morpheme = morphemes[i];\n              final Token compoundToken;\n              if (token.getPOSType() == POS.Type.COMPOUND) {\n                assert endOffset - morpheme.surfaceForm.length() >= 0;\n                compoundToken = new DecompoundToken(morpheme.posTag, morpheme.surfaceForm,\n                    endOffset - morpheme.surfaceForm.length(), endOffset);\n              } else {\n                compoundToken = new DecompoundToken(morpheme.posTag, morpheme.surfaceForm, token.getStartOffset(), token.getEndOffset());\n              }\n              if (i == 0 && mode == DecompoundMode.MIXED) {\n                compoundToken.setPositionIncrement(0);\n              }\n              ++ posLen;\n              endOffset -= morpheme.surfaceForm.length();\n              pending.add(compoundToken);\n              if (VERBOSE) {\n                System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n              }\n            }\n            if (mode == DecompoundMode.MIXED) {\n              token.setPositionLength(Math.max(1, posLen));\n              pending.add(token);\n              if (VERBOSE) {\n                System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n              }\n            }\n          }\n        }\n      }\n\n      pos = backPos;\n      bestIDX = nextBestIDX;\n    }\n\n    lastBackTracePos = endPos;\n\n    if (VERBOSE) {\n      System.out.println(\"  freeBefore pos=\" + endPos);\n    }\n    // Notify the circular buffers that we are done with\n    // these positions:\n    buffer.freeBefore(endPos);\n    positions.freeBefore(endPos);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c6453827f947004a68ad9db7418781e9df2f660","date":1523626811,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/analysis/nori/src/java/org/apache/lucene/analysis/ko/KoreanTokenizer#backtrace(Position,int).mjava","pathOld":"/dev/null","sourceNew":"  // the pending list.  The pending list is then in-reverse\n  // (last token should be returned first).\n  private void backtrace(final Position endPosData, final int fromIDX) {\n    final int endPos = endPosData.pos;\n\n    if (VERBOSE) {\n      System.out.println(\"\\n  backtrace: endPos=\" + endPos + \" pos=\" + pos + \"; \" + (pos - lastBackTracePos) + \" characters; last=\" + lastBackTracePos + \" cost=\" + endPosData.costs[fromIDX]);\n    }\n\n    final char[] fragment = buffer.get(lastBackTracePos, endPos-lastBackTracePos);\n\n    if (dotOut != null) {\n      dotOut.onBacktrace(this, positions, lastBackTracePos, endPosData, fromIDX, fragment, end);\n    }\n\n    int pos = endPos;\n    int bestIDX = fromIDX;\n\n    // TODO: sort of silly to make Token instances here; the\n    // back trace has all info needed to generate the\n    // token.  So, we could just directly set the attrs,\n    // from the backtrace, in incrementToken w/o ever\n    // creating Token; we'd have to defer calling freeBefore\n    // until after the backtrace was fully \"consumed\" by\n    // incrementToken.\n\n    while (pos > lastBackTracePos) {\n      //System.out.println(\"BT: back pos=\" + pos + \" bestIDX=\" + bestIDX);\n      final Position posData = positions.get(pos);\n      assert bestIDX < posData.count;\n\n      int backPos = posData.backPos[bestIDX];\n      int backWordPos = posData.backWordPos[bestIDX];\n      assert backPos >= lastBackTracePos: \"backPos=\" + backPos + \" vs lastBackTracePos=\" + lastBackTracePos;\n      // the length of the word without the whitespaces at the beginning.\n      int length = pos - backWordPos;\n      Type backType = posData.backType[bestIDX];\n      int backID = posData.backID[bestIDX];\n      int nextBestIDX = posData.backIndex[bestIDX];\n      // the start of the word after the whitespace at the beginning.\n      final int fragmentOffset = backWordPos - lastBackTracePos;\n      assert fragmentOffset >= 0;\n\n      final Dictionary dict = getDict(backType);\n\n      if (outputUnknownUnigrams && backType == Type.UNKNOWN) {\n        // outputUnknownUnigrams converts unknown word into unigrams:\n        for (int i = length - 1; i >= 0; i--) {\n          int charLen = 1;\n          if (i > 0 && Character.isLowSurrogate(fragment[fragmentOffset + i])) {\n            i--;\n            charLen = 2;\n          }\n          final DictionaryToken token = new DictionaryToken(Type.UNKNOWN,\n              unkDictionary,\n              CharacterDefinition.NGRAM,\n              fragment,\n              fragmentOffset+i,\n              charLen,\n              backWordPos+i,\n              backWordPos+i+charLen\n          );\n          if (shouldFilterToken(token) == false) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          }\n        }\n      } else {\n        final DictionaryToken token = new DictionaryToken(backType,\n            dict,\n            backID,\n            fragment,\n            fragmentOffset,\n            length,\n            backWordPos,\n            backWordPos + length\n        );\n        if (token.getPOSType() == POS.Type.MORPHEME || mode == DecompoundMode.NONE) {\n          if (shouldFilterToken(token) == false) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          }\n        } else {\n          Dictionary.Morpheme[] morphemes = token.getMorphemes();\n          if (morphemes == null) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          } else {\n            int endOffset = backWordPos + length;\n            int posLen = 0;\n            // decompose the compound\n            for (int i = morphemes.length - 1; i >= 0; i--) {\n              final Dictionary.Morpheme morpheme = morphemes[i];\n              final Token compoundToken;\n              if (token.getPOSType() == POS.Type.COMPOUND) {\n                assert endOffset - morpheme.surfaceForm.length() >= 0;\n                compoundToken = new DecompoundToken(morpheme.posTag, morpheme.surfaceForm,\n                    endOffset - morpheme.surfaceForm.length(), endOffset);\n              } else {\n                compoundToken = new DecompoundToken(morpheme.posTag, morpheme.surfaceForm, token.getStartOffset(), token.getEndOffset());\n              }\n              if (i == 0 && mode == DecompoundMode.MIXED) {\n                compoundToken.setPositionIncrement(0);\n              }\n              ++ posLen;\n              endOffset -= morpheme.surfaceForm.length();\n              pending.add(compoundToken);\n              if (VERBOSE) {\n                System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n              }\n            }\n            if (mode == DecompoundMode.MIXED) {\n              token.setPositionLength(Math.max(1, posLen));\n              pending.add(token);\n              if (VERBOSE) {\n                System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n              }\n            }\n          }\n        }\n      }\n\n      pos = backPos;\n      bestIDX = nextBestIDX;\n    }\n\n    lastBackTracePos = endPos;\n\n    if (VERBOSE) {\n      System.out.println(\"  freeBefore pos=\" + endPos);\n    }\n    // Notify the circular buffers that we are done with\n    // these positions:\n    buffer.freeBefore(endPos);\n    positions.freeBefore(endPos);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9a6a8e09e9e2d9479c718093a32f3a8c19455af","date":1558962924,"type":3,"author":"Namgyu Kim","isMerge":false,"pathNew":"lucene/analysis/nori/src/java/org/apache/lucene/analysis/ko/KoreanTokenizer#backtrace(Position,int).mjava","pathOld":"lucene/analysis/nori/src/java/org/apache/lucene/analysis/ko/KoreanTokenizer#backtrace(Position,int).mjava","sourceNew":"  // the pending list.  The pending list is then in-reverse\n  // (last token should be returned first).\n  private void backtrace(final Position endPosData, final int fromIDX) {\n    final int endPos = endPosData.pos;\n\n    if (VERBOSE) {\n      System.out.println(\"\\n  backtrace: endPos=\" + endPos + \" pos=\" + pos + \"; \" + (pos - lastBackTracePos) + \" characters; last=\" + lastBackTracePos + \" cost=\" + endPosData.costs[fromIDX]);\n    }\n\n    final char[] fragment = buffer.get(lastBackTracePos, endPos-lastBackTracePos);\n\n    if (dotOut != null) {\n      dotOut.onBacktrace(this, positions, lastBackTracePos, endPosData, fromIDX, fragment, end);\n    }\n\n    int pos = endPos;\n    int bestIDX = fromIDX;\n\n    // TODO: sort of silly to make Token instances here; the\n    // back trace has all info needed to generate the\n    // token.  So, we could just directly set the attrs,\n    // from the backtrace, in incrementToken w/o ever\n    // creating Token; we'd have to defer calling freeBefore\n    // until after the backtrace was fully \"consumed\" by\n    // incrementToken.\n\n    while (pos > lastBackTracePos) {\n      //System.out.println(\"BT: back pos=\" + pos + \" bestIDX=\" + bestIDX);\n      final Position posData = positions.get(pos);\n      assert bestIDX < posData.count;\n\n      int backPos = posData.backPos[bestIDX];\n      int backWordPos = posData.backWordPos[bestIDX];\n      assert backPos >= lastBackTracePos: \"backPos=\" + backPos + \" vs lastBackTracePos=\" + lastBackTracePos;\n      // the length of the word without the whitespaces at the beginning.\n      int length = pos - backWordPos;\n      Type backType = posData.backType[bestIDX];\n      int backID = posData.backID[bestIDX];\n      int nextBestIDX = posData.backIndex[bestIDX];\n      // the start of the word after the whitespace at the beginning.\n      final int fragmentOffset = backWordPos - lastBackTracePos;\n      assert fragmentOffset >= 0;\n\n      final Dictionary dict = getDict(backType);\n\n      if (outputUnknownUnigrams && backType == Type.UNKNOWN) {\n        // outputUnknownUnigrams converts unknown word into unigrams:\n        for (int i = length - 1; i >= 0; i--) {\n          int charLen = 1;\n          if (i > 0 && Character.isLowSurrogate(fragment[fragmentOffset + i])) {\n            i--;\n            charLen = 2;\n          }\n          final DictionaryToken token = new DictionaryToken(Type.UNKNOWN,\n              unkDictionary,\n              CharacterDefinition.NGRAM,\n              fragment,\n              fragmentOffset+i,\n              charLen,\n              backWordPos+i,\n              backWordPos+i+charLen\n          );\n          pending.add(token);\n          if (VERBOSE) {\n            System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n          }\n        }\n      } else {\n        final DictionaryToken token = new DictionaryToken(backType,\n            dict,\n            backID,\n            fragment,\n            fragmentOffset,\n            length,\n            backWordPos,\n            backWordPos + length\n        );\n        if (token.getPOSType() == POS.Type.MORPHEME || mode == DecompoundMode.NONE) {\n          if (shouldFilterToken(token) == false) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          }\n        } else {\n          Dictionary.Morpheme[] morphemes = token.getMorphemes();\n          if (morphemes == null) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          } else {\n            int endOffset = backWordPos + length;\n            int posLen = 0;\n            // decompose the compound\n            for (int i = morphemes.length - 1; i >= 0; i--) {\n              final Dictionary.Morpheme morpheme = morphemes[i];\n              final Token compoundToken;\n              if (token.getPOSType() == POS.Type.COMPOUND) {\n                assert endOffset - morpheme.surfaceForm.length() >= 0;\n                compoundToken = new DecompoundToken(morpheme.posTag, morpheme.surfaceForm,\n                    endOffset - morpheme.surfaceForm.length(), endOffset);\n              } else {\n                compoundToken = new DecompoundToken(morpheme.posTag, morpheme.surfaceForm, token.getStartOffset(), token.getEndOffset());\n              }\n              if (i == 0 && mode == DecompoundMode.MIXED) {\n                compoundToken.setPositionIncrement(0);\n              }\n              ++ posLen;\n              endOffset -= morpheme.surfaceForm.length();\n              pending.add(compoundToken);\n              if (VERBOSE) {\n                System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n              }\n            }\n            if (mode == DecompoundMode.MIXED) {\n              token.setPositionLength(Math.max(1, posLen));\n              pending.add(token);\n              if (VERBOSE) {\n                System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n              }\n            }\n          }\n        }\n      }\n      if (discardPunctuation == false && backWordPos != backPos) {\n        // Add a token for whitespaces between terms\n        int offset = backPos - lastBackTracePos;\n        int len = backWordPos - backPos;\n        //System.out.println(offset + \" \" + fragmentOffset + \" \" + len + \" \" + backWordPos + \" \" + backPos);\n        unkDictionary.lookupWordIds(characterDefinition.getCharacterClass(' '), wordIdRef);\n        DictionaryToken spaceToken = new DictionaryToken(Type.UNKNOWN, unkDictionary,\n            wordIdRef.ints[wordIdRef.offset], fragment, offset, len, backPos, backPos+len);\n        pending.add(spaceToken);\n      }\n\n      pos = backPos;\n      bestIDX = nextBestIDX;\n    }\n\n    lastBackTracePos = endPos;\n\n    if (VERBOSE) {\n      System.out.println(\"  freeBefore pos=\" + endPos);\n    }\n    // Notify the circular buffers that we are done with\n    // these positions:\n    buffer.freeBefore(endPos);\n    positions.freeBefore(endPos);\n  }\n\n","sourceOld":"  // the pending list.  The pending list is then in-reverse\n  // (last token should be returned first).\n  private void backtrace(final Position endPosData, final int fromIDX) {\n    final int endPos = endPosData.pos;\n\n    if (VERBOSE) {\n      System.out.println(\"\\n  backtrace: endPos=\" + endPos + \" pos=\" + pos + \"; \" + (pos - lastBackTracePos) + \" characters; last=\" + lastBackTracePos + \" cost=\" + endPosData.costs[fromIDX]);\n    }\n\n    final char[] fragment = buffer.get(lastBackTracePos, endPos-lastBackTracePos);\n\n    if (dotOut != null) {\n      dotOut.onBacktrace(this, positions, lastBackTracePos, endPosData, fromIDX, fragment, end);\n    }\n\n    int pos = endPos;\n    int bestIDX = fromIDX;\n\n    // TODO: sort of silly to make Token instances here; the\n    // back trace has all info needed to generate the\n    // token.  So, we could just directly set the attrs,\n    // from the backtrace, in incrementToken w/o ever\n    // creating Token; we'd have to defer calling freeBefore\n    // until after the backtrace was fully \"consumed\" by\n    // incrementToken.\n\n    while (pos > lastBackTracePos) {\n      //System.out.println(\"BT: back pos=\" + pos + \" bestIDX=\" + bestIDX);\n      final Position posData = positions.get(pos);\n      assert bestIDX < posData.count;\n\n      int backPos = posData.backPos[bestIDX];\n      int backWordPos = posData.backWordPos[bestIDX];\n      assert backPos >= lastBackTracePos: \"backPos=\" + backPos + \" vs lastBackTracePos=\" + lastBackTracePos;\n      // the length of the word without the whitespaces at the beginning.\n      int length = pos - backWordPos;\n      Type backType = posData.backType[bestIDX];\n      int backID = posData.backID[bestIDX];\n      int nextBestIDX = posData.backIndex[bestIDX];\n      // the start of the word after the whitespace at the beginning.\n      final int fragmentOffset = backWordPos - lastBackTracePos;\n      assert fragmentOffset >= 0;\n\n      final Dictionary dict = getDict(backType);\n\n      if (outputUnknownUnigrams && backType == Type.UNKNOWN) {\n        // outputUnknownUnigrams converts unknown word into unigrams:\n        for (int i = length - 1; i >= 0; i--) {\n          int charLen = 1;\n          if (i > 0 && Character.isLowSurrogate(fragment[fragmentOffset + i])) {\n            i--;\n            charLen = 2;\n          }\n          final DictionaryToken token = new DictionaryToken(Type.UNKNOWN,\n              unkDictionary,\n              CharacterDefinition.NGRAM,\n              fragment,\n              fragmentOffset+i,\n              charLen,\n              backWordPos+i,\n              backWordPos+i+charLen\n          );\n          if (shouldFilterToken(token) == false) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          }\n        }\n      } else {\n        final DictionaryToken token = new DictionaryToken(backType,\n            dict,\n            backID,\n            fragment,\n            fragmentOffset,\n            length,\n            backWordPos,\n            backWordPos + length\n        );\n        if (token.getPOSType() == POS.Type.MORPHEME || mode == DecompoundMode.NONE) {\n          if (shouldFilterToken(token) == false) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          }\n        } else {\n          Dictionary.Morpheme[] morphemes = token.getMorphemes();\n          if (morphemes == null) {\n            pending.add(token);\n            if (VERBOSE) {\n              System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n            }\n          } else {\n            int endOffset = backWordPos + length;\n            int posLen = 0;\n            // decompose the compound\n            for (int i = morphemes.length - 1; i >= 0; i--) {\n              final Dictionary.Morpheme morpheme = morphemes[i];\n              final Token compoundToken;\n              if (token.getPOSType() == POS.Type.COMPOUND) {\n                assert endOffset - morpheme.surfaceForm.length() >= 0;\n                compoundToken = new DecompoundToken(morpheme.posTag, morpheme.surfaceForm,\n                    endOffset - morpheme.surfaceForm.length(), endOffset);\n              } else {\n                compoundToken = new DecompoundToken(morpheme.posTag, morpheme.surfaceForm, token.getStartOffset(), token.getEndOffset());\n              }\n              if (i == 0 && mode == DecompoundMode.MIXED) {\n                compoundToken.setPositionIncrement(0);\n              }\n              ++ posLen;\n              endOffset -= morpheme.surfaceForm.length();\n              pending.add(compoundToken);\n              if (VERBOSE) {\n                System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n              }\n            }\n            if (mode == DecompoundMode.MIXED) {\n              token.setPositionLength(Math.max(1, posLen));\n              pending.add(token);\n              if (VERBOSE) {\n                System.out.println(\"    add token=\" + pending.get(pending.size() - 1));\n              }\n            }\n          }\n        }\n      }\n\n      pos = backPos;\n      bestIDX = nextBestIDX;\n    }\n\n    lastBackTracePos = endPos;\n\n    if (VERBOSE) {\n      System.out.println(\"  freeBefore pos=\" + endPos);\n    }\n    // Notify the circular buffers that we are done with\n    // these positions:\n    buffer.freeBefore(endPos);\n    positions.freeBefore(endPos);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c9a6a8e09e9e2d9479c718093a32f3a8c19455af":["5c6453827f947004a68ad9db7418781e9df2f660"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8493925b2e70246f0961df584c01a8c2e61ee52f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c9a6a8e09e9e2d9479c718093a32f3a8c19455af"],"5c6453827f947004a68ad9db7418781e9df2f660":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","8493925b2e70246f0961df584c01a8c2e61ee52f"]},"commit2Childs":{"c9a6a8e09e9e2d9479c718093a32f3a8c19455af":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8493925b2e70246f0961df584c01a8c2e61ee52f","5c6453827f947004a68ad9db7418781e9df2f660"],"8493925b2e70246f0961df584c01a8c2e61ee52f":["5c6453827f947004a68ad9db7418781e9df2f660"],"5c6453827f947004a68ad9db7418781e9df2f660":["c9a6a8e09e9e2d9479c718093a32f3a8c19455af"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}