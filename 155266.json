{"path":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","commits":[{"id":"b8acf0807ca5f38beda8e0f7d5ab46ff39f81200","date":1358521790,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/SimpleDVConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c099601f244fdad1fbc76e4fff4e2c25a1424d00","date":1359585010,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          int sourceOrd = top.ord;\n          // nocommit: do this\n          //   ordToReaderID.add(readerId);\n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          // nocommit: do this\n          //   top.ordDeltas.add(delta);\n          \n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"33feaba5d317bf4ee3eec58c327ed68858c9cac9","date":1359587256,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.lastOrd;\n             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          int sourceOrd = top.ord;\n          // nocommit: do this\n          //   ordToReaderID.add(readerId);\n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          // nocommit: do this\n          //   top.ordDeltas.add(delta);\n          \n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"67445ba6570b72d1e617d3a85562eabf92790274","date":1359593235,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.ord;             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.lastOrd;\n             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"495f733847d5d2f8d5f4aeb77bb42bf38ad011a7","date":1359593346,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.ord;             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.ord;             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0957acf04dd58e07d48286537fe0e924f34e070a","date":1359597575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.ord;             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          ord++;\n        }\n\n        long signedDelta = (ord-1) - top.ord; // global ord space - segment ord space\n        // fill in any holes for unused ords, then finally the value we want (segOrdToMergedOrd[top.ord])\n        // TODO: is there a better way...\n        while (top.segOrdToMergedOrd.size() <= top.ord) {\n          top.segOrdToMergedOrd.add(signedDelta);\n        }\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.ord;             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9eafdf27a0bda3d70664dd39f3a1683d8416dcf","date":1359644871,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // TODO: we can unload the bits/packed ints to disk to reduce\n        // transient ram spike... most of these just require iterators\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.ord;             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          if (lastTerm == null) {\n            lastTerm = BytesRef.deepCopyOf(top.scratch);\n          } else {\n            lastTerm.copyBytes(top.scratch);\n          }\n          ord++;\n        }\n\n        long signedDelta = (ord-1) - top.ord; // global ord space - segment ord space\n        // fill in any holes for unused ords, then finally the value we want (segOrdToMergedOrd[top.ord])\n        // TODO: is there a better way...\n        while (top.segOrdToMergedOrd.size() <= top.ord) {\n          top.segOrdToMergedOrd.add(signedDelta);\n        }\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.ord;             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          ord++;\n        }\n\n        long signedDelta = (ord-1) - top.ord; // global ord space - segment ord space\n        // fill in any holes for unused ords, then finally the value we want (segOrdToMergedOrd[top.ord])\n        // TODO: is there a better way...\n        while (top.segOrdToMergedOrd.size() <= top.ord) {\n          top.segOrdToMergedOrd.add(signedDelta);\n        }\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"/dev/null","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // TODO: we can unload the bits/packed ints to disk to reduce\n        // transient ram spike... most of these just require iterators\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.ord;             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          if (lastTerm == null) {\n            lastTerm = BytesRef.deepCopyOf(top.scratch);\n          } else {\n            lastTerm.copyBytes(top.scratch);\n          }\n          ord++;\n        }\n\n        long signedDelta = (ord-1) - top.ord; // global ord space - segment ord space\n        // fill in any holes for unused ords, then finally the value we want (segOrdToMergedOrd[top.ord])\n        // TODO: is there a better way...\n        while (top.segOrdToMergedOrd.size() <= top.ord) {\n          top.segOrdToMergedOrd.add(signedDelta);\n        }\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f366ce28775e2b8ea4e06355009471328711666d","date":1360551293,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":null,"sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // TODO: we can unload the bits/packed ints to disk to reduce\n        // transient ram spike... most of these just require iterators\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.ord;             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          if (lastTerm == null) {\n            lastTerm = BytesRef.deepCopyOf(top.scratch);\n          } else {\n            lastTerm.copyBytes(top.scratch);\n          }\n          ord++;\n        }\n\n        long signedDelta = (ord-1) - top.ord; // global ord space - segment ord space\n        // fill in any holes for unused ords, then finally the value we want (segOrdToMergedOrd[top.ord])\n        // TODO: is there a better way...\n        while (top.segOrdToMergedOrd.size() <= top.ord) {\n          top.segOrdToMergedOrd.add(signedDelta);\n        }\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ddbb72a33557d2b5bc22ee95daf3281c43560502","date":1361334582,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":null,"sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.segmentID = readerIDX;\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // TODO: we can unload the bits/packed ints to disk to reduce\n        // transient ram spike... most of these just require iterators\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n          q.add(segState);\n        }\n      }\n\n      int lastOrds[] = new int[segStates.size()];\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          // a new unique term: record its segment ID / sourceOrd pair\n          int readerId = top.segmentID;\n          ordToReaderId.add(readerId);\n\n          int sourceOrd = top.ord;             \n          int delta = sourceOrd - lastOrds[readerId];\n          lastOrds[readerId] = sourceOrd;\n          top.ordDeltas.add(delta);\n          \n          if (lastTerm == null) {\n            lastTerm = BytesRef.deepCopyOf(top.scratch);\n          } else {\n            lastTerm.copyBytes(top.scratch);\n          }\n          ord++;\n        }\n\n        long signedDelta = (ord-1) - top.ord; // global ord space - segment ord space\n        // fill in any holes for unused ords, then finally the value we want (segOrdToMergedOrd[top.ord])\n        // TODO: is there a better way...\n        while (top.segOrdToMergedOrd.size() <= top.ord) {\n          top.segOrdToMergedOrd.add(signedDelta);\n        }\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)\n      for (SegmentState state : segStates) {\n        state.liveTerms = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b8acf0807ca5f38beda8e0f7d5ab46ff39f81200":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e9eafdf27a0bda3d70664dd39f3a1683d8416dcf"],"0957acf04dd58e07d48286537fe0e924f34e070a":["495f733847d5d2f8d5f4aeb77bb42bf38ad011a7"],"67445ba6570b72d1e617d3a85562eabf92790274":["33feaba5d317bf4ee3eec58c327ed68858c9cac9"],"c099601f244fdad1fbc76e4fff4e2c25a1424d00":["b8acf0807ca5f38beda8e0f7d5ab46ff39f81200"],"e9eafdf27a0bda3d70664dd39f3a1683d8416dcf":["0957acf04dd58e07d48286537fe0e924f34e070a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f366ce28775e2b8ea4e06355009471328711666d":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"33feaba5d317bf4ee3eec58c327ed68858c9cac9":["c099601f244fdad1fbc76e4fff4e2c25a1424d00"],"ddbb72a33557d2b5bc22ee95daf3281c43560502":["d4d69c535930b5cce125cff868d40f6373dc27d4","f366ce28775e2b8ea4e06355009471328711666d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ddbb72a33557d2b5bc22ee95daf3281c43560502"],"495f733847d5d2f8d5f4aeb77bb42bf38ad011a7":["67445ba6570b72d1e617d3a85562eabf92790274"]},"commit2Childs":{"b8acf0807ca5f38beda8e0f7d5ab46ff39f81200":["c099601f244fdad1fbc76e4fff4e2c25a1424d00"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["f366ce28775e2b8ea4e06355009471328711666d","ddbb72a33557d2b5bc22ee95daf3281c43560502"],"0957acf04dd58e07d48286537fe0e924f34e070a":["e9eafdf27a0bda3d70664dd39f3a1683d8416dcf"],"67445ba6570b72d1e617d3a85562eabf92790274":["495f733847d5d2f8d5f4aeb77bb42bf38ad011a7"],"c099601f244fdad1fbc76e4fff4e2c25a1424d00":["33feaba5d317bf4ee3eec58c327ed68858c9cac9"],"e9eafdf27a0bda3d70664dd39f3a1683d8416dcf":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b8acf0807ca5f38beda8e0f7d5ab46ff39f81200","d4d69c535930b5cce125cff868d40f6373dc27d4"],"f366ce28775e2b8ea4e06355009471328711666d":["ddbb72a33557d2b5bc22ee95daf3281c43560502"],"33feaba5d317bf4ee3eec58c327ed68858c9cac9":["67445ba6570b72d1e617d3a85562eabf92790274"],"ddbb72a33557d2b5bc22ee95daf3281c43560502":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"495f733847d5d2f8d5f4aeb77bb42bf38ad011a7":["0957acf04dd58e07d48286537fe0e924f34e070a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}