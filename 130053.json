{"path":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","commits":[{"id":"afdf6ff5b1319d8ae254212f1203a6233ed3e1dc","date":1461010196,"type":0,"author":"jbernste","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"/dev/null","sourceNew":"    public List<Tuple> call() {\n\n      Map joinParams = new HashMap();\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = (String)queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n\n      joinParams.putAll(queryParams);\n      joinParams.put(\"fl\", buf.toString());\n      joinParams.put(\"qt\", \"/export\");\n      joinParams.put(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      for(String node : nodes) {\n        nodeQuery.append(node).append(\" \");\n      }\n\n      String q = traverseTo + \":(\" + nodeQuery.toString().trim() + \")\";\n\n\n      joinParams.put(\"q\", q);\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e15955b4980562a0c1c81d08654904f3fadb83b","date":1461068916,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"/dev/null","sourceNew":"    public List<Tuple> call() {\n\n      Map joinParams = new HashMap();\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = (String)queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n\n      joinParams.putAll(queryParams);\n      joinParams.put(\"fl\", buf.toString());\n      joinParams.put(\"qt\", \"/export\");\n      joinParams.put(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      for(String node : nodes) {\n        nodeQuery.append(node).append(\" \");\n      }\n\n      String q = traverseTo + \":(\" + nodeQuery.toString().trim() + \")\";\n\n\n      joinParams.put(\"q\", q);\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"94ac27a7f2b3e27d17f14b7aeeb3fcd450b7cc96","date":1461702806,"type":3,"author":"jbernste","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","sourceNew":"    public List<Tuple> call() {\n\n      Map joinParams = new HashMap();\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n\n      joinParams.putAll(queryParams);\n      joinParams.put(\"fl\", buf.toString());\n      joinParams.put(\"qt\", \"/export\");\n      joinParams.put(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinParams.put(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinParams.put(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":"    public List<Tuple> call() {\n\n      Map joinParams = new HashMap();\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = (String)queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n\n      joinParams.putAll(queryParams);\n      joinParams.put(\"fl\", buf.toString());\n      joinParams.put(\"qt\", \"/export\");\n      joinParams.put(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      for(String node : nodes) {\n        nodeQuery.append(node).append(\" \");\n      }\n\n      String q = traverseTo + \":(\" + nodeQuery.toString().trim() + \")\";\n\n\n      joinParams.put(\"q\", q);\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"826d15444ddf61716dc768c229cd54b2c2ccce1c","date":1462822652,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","sourceNew":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams(SolrParams.toMultiMap(new NamedList(queryParams)));\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":"    public List<Tuple> call() {\n\n      Map joinParams = new HashMap();\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n\n      joinParams.putAll(queryParams);\n      joinParams.put(\"fl\", buf.toString());\n      joinParams.put(\"qt\", \"/export\");\n      joinParams.put(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinParams.put(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinParams.put(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e66a459d38c1c4a2f97128433dab546f683a9fed","date":1462873476,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","sourceNew":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams(SolrParams.toMultiMap(new NamedList(queryParams)));\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":"    public List<Tuple> call() {\n\n      Map joinParams = new HashMap();\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n\n      joinParams.putAll(queryParams);\n      joinParams.put(\"fl\", buf.toString());\n      joinParams.put(\"qt\", \"/export\");\n      joinParams.put(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinParams.put(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinParams.put(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","sourceNew":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams(SolrParams.toMultiMap(new NamedList(queryParams)));\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":"    public List<Tuple> call() {\n\n      Map joinParams = new HashMap();\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n\n      joinParams.putAll(queryParams);\n      joinParams.put(\"fl\", buf.toString());\n      joinParams.put(\"qt\", \"/export\");\n      joinParams.put(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinParams.put(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinParams.put(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","sourceNew":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams(SolrParams.toMultiMap(new NamedList(queryParams)));\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":"    public List<Tuple> call() {\n\n      Map joinParams = new HashMap();\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n\n      joinParams.putAll(queryParams);\n      joinParams.put(\"fl\", buf.toString());\n      joinParams.put(\"qt\", \"/export\");\n      joinParams.put(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinParams.put(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinParams.put(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01624b85de12fb02335810bdf325124e59040772","date":1490254940,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","sourceNew":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams(SolrParams.toMultiMap(new NamedList(queryParams)));\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(SORT, gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams(SolrParams.toMultiMap(new NamedList(queryParams)));\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6f4c5d3859373c3a74734e85efa122b17514e3e8","date":1490280013,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","sourceNew":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams(SolrParams.toMultiMap(new NamedList(queryParams)));\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(SORT, gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams(SolrParams.toMultiMap(new NamedList(queryParams)));\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(\"sort\", gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"129e5b874f99ae4e04f39c337c940dda690f2d38","date":1524504409,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","sourceNew":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams();\n      queryParams.forEach(joinSParams::add);\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(SORT, gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams(SolrParams.toMultiMap(new NamedList(queryParams)));\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(SORT, gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4f6e13eb95702dc635119d2ccd4fa5fafcc88d","date":1592006776,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/graph/GatherNodesStream.JoinRunner#call().mjava","sourceNew":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet<>();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams();\n      queryParams.forEach(joinSParams::add);\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(SORT, gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","sourceOld":"    public List<Tuple> call() {\n\n\n      Set<String> flSet = new HashSet();\n      flSet.add(gather);\n      flSet.add(traverseTo);\n\n      //Add the metric columns\n\n      if(metrics != null) {\n        for(Metric metric : metrics) {\n          for(String column : metric.getColumns()) {\n            flSet.add(column);\n          }\n        }\n      }\n\n      if(queryParams.containsKey(\"fl\")) {\n        String flString = queryParams.get(\"fl\");\n        String[] flArray = flString.split(\",\");\n        for(String f : flArray) {\n          flSet.add(f.trim());\n        }\n      }\n\n      Iterator<String> it = flSet.iterator();\n      StringBuilder buf = new StringBuilder();\n      while(it.hasNext()) {\n        buf.append(it.next());\n        if(it.hasNext()) {\n          buf.append(\",\");\n        }\n      }\n      \n      ModifiableSolrParams joinSParams = new ModifiableSolrParams();\n      queryParams.forEach(joinSParams::add);\n      joinSParams.set(\"fl\", buf.toString());\n      joinSParams.set(\"qt\", \"/export\");\n      joinSParams.set(SORT, gather + \" asc,\"+traverseTo +\" asc\");\n\n      StringBuffer nodeQuery = new StringBuffer();\n\n      boolean comma = false;\n      for(String node : nodes) {\n        if(comma) {\n          nodeQuery.append(\",\");\n        }\n        nodeQuery.append(node);\n        comma = true;\n      }\n\n      if(maxDocFreq > -1) {\n        String docFreqParam = \" maxDocFreq=\"+maxDocFreq;\n        joinSParams.set(\"q\", \"{!graphTerms f=\" + traverseTo + docFreqParam + \"}\" + nodeQuery.toString());\n      } else {\n        joinSParams.set(\"q\", \"{!terms f=\" + traverseTo+\"}\" + nodeQuery.toString());\n      }\n\n      TupleStream stream = null;\n      try {\n        stream = new UniqueStream(new CloudSolrStream(zkHost, collection, joinSParams), new MultipleFieldEqualitor(new FieldEqualitor(gather), new FieldEqualitor(traverseTo)));\n        stream.setStreamContext(streamContext);\n        stream.open();\n        BATCH:\n        while (true) {\n          Tuple tuple = stream.read();\n          if (tuple.EOF) {\n            break BATCH;\n          }\n\n          edges.add(tuple);\n        }\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      } finally {\n        try {\n          stream.close();\n        } catch(Exception ce) {\n          throw new RuntimeException(ce);\n        }\n      }\n      return edges;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"afdf6ff5b1319d8ae254212f1203a6233ed3e1dc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0f4f6e13eb95702dc635119d2ccd4fa5fafcc88d":["129e5b874f99ae4e04f39c337c940dda690f2d38"],"6f4c5d3859373c3a74734e85efa122b17514e3e8":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"94ac27a7f2b3e27d17f14b7aeeb3fcd450b7cc96":["0e15955b4980562a0c1c81d08654904f3fadb83b"],"01624b85de12fb02335810bdf325124e59040772":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"826d15444ddf61716dc768c229cd54b2c2ccce1c":["94ac27a7f2b3e27d17f14b7aeeb3fcd450b7cc96"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["94ac27a7f2b3e27d17f14b7aeeb3fcd450b7cc96","d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["94ac27a7f2b3e27d17f14b7aeeb3fcd450b7cc96","826d15444ddf61716dc768c229cd54b2c2ccce1c"],"129e5b874f99ae4e04f39c337c940dda690f2d38":["01624b85de12fb02335810bdf325124e59040772"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e66a459d38c1c4a2f97128433dab546f683a9fed":["94ac27a7f2b3e27d17f14b7aeeb3fcd450b7cc96","826d15444ddf61716dc768c229cd54b2c2ccce1c"],"0e15955b4980562a0c1c81d08654904f3fadb83b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","afdf6ff5b1319d8ae254212f1203a6233ed3e1dc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f4f6e13eb95702dc635119d2ccd4fa5fafcc88d"]},"commit2Childs":{"afdf6ff5b1319d8ae254212f1203a6233ed3e1dc":["0e15955b4980562a0c1c81d08654904f3fadb83b"],"0f4f6e13eb95702dc635119d2ccd4fa5fafcc88d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6f4c5d3859373c3a74734e85efa122b17514e3e8":[],"94ac27a7f2b3e27d17f14b7aeeb3fcd450b7cc96":["826d15444ddf61716dc768c229cd54b2c2ccce1c","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","e66a459d38c1c4a2f97128433dab546f683a9fed"],"01624b85de12fb02335810bdf325124e59040772":["129e5b874f99ae4e04f39c337c940dda690f2d38"],"826d15444ddf61716dc768c229cd54b2c2ccce1c":["d470c8182e92b264680e34081b75e70a9f2b3c89","e66a459d38c1c4a2f97128433dab546f683a9fed"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d470c8182e92b264680e34081b75e70a9f2b3c89":["6f4c5d3859373c3a74734e85efa122b17514e3e8","01624b85de12fb02335810bdf325124e59040772","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"129e5b874f99ae4e04f39c337c940dda690f2d38":["0f4f6e13eb95702dc635119d2ccd4fa5fafcc88d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["afdf6ff5b1319d8ae254212f1203a6233ed3e1dc","0e15955b4980562a0c1c81d08654904f3fadb83b"],"e66a459d38c1c4a2f97128433dab546f683a9fed":[],"0e15955b4980562a0c1c81d08654904f3fadb83b":["94ac27a7f2b3e27d17f14b7aeeb3fcd450b7cc96"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["6f4c5d3859373c3a74734e85efa122b17514e3e8","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","e66a459d38c1c4a2f97128433dab546f683a9fed","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}