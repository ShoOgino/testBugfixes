{"path":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","commits":[{"id":"44fcbde6fb2ac44ee3b45e013e54a42911e689ff","date":1292065621,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":"  private synchronized boolean applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      any = true;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e175058b6aef7c3f9971e918edcc7ae7ef4347d1","date":1292497297,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","sourceNew":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  private synchronized boolean applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      any = true;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":1,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,int).mjava","sourceNew":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  private final synchronized boolean applyDeletes(IndexReader reader, int docIDStart)\n    throws CorruptIndexException, IOException {\n\n    final int docEnd = docIDStart + reader.maxDoc();\n    boolean any = false;\n\n    assert checkDeleteTerm(null);\n\n    // Delete by term\n    if (deletesFlushed.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return false;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            int limit = entry.getValue().getNum();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              any = true;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletesFlushed.docIDs) {\n      int docID = docIdInt.intValue();\n      if (docID >= docIDStart && docID < docEnd) {\n        reader.deleteDocument(docID-docIDStart);\n        any = true;\n      }\n    }\n\n    // Delete by query\n    if (deletesFlushed.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (((long) docIDStart) + doc >= limit)\n                break;\n              reader.deleteDocument(doc);\n              any = true;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"/dev/null","sourceNew":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n\n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n\n      String currentField = null;\n      DocsEnum docs = null;\n\n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n\n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n\n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2dadf0f3286a34a0fee6e788ffce88624bf2984e","date":1294260428,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","sourceNew":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      \n      final ReaderContext readerContext = searcher.getTopReaderContext();\n      assert readerContext.isAtomic;\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          \n          Scorer scorer = weight.scorer(readerContext, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a10b98ef1ef4bf9e38d2e07a9e425a916afa8705","date":1294747166,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","sourceNew":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      assert searcher.getTopReaderContext().isAtomic;\n      final AtomicReaderContext readerContext = (AtomicReaderContext) searcher.getTopReaderContext();\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          \n          Scorer scorer = weight.scorer(readerContext, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      \n      final ReaderContext readerContext = searcher.getTopReaderContext();\n      assert readerContext.isAtomic;\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          \n          Scorer scorer = weight.scorer(readerContext, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fa1a999d6674423e5c4ac858b410283f6fe03f20","date":1294868331,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","sourceNew":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      assert searcher.getTopReaderContext().isAtomic;\n      final AtomicReaderContext readerContext = (AtomicReaderContext) searcher.getTopReaderContext();\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(readerContext, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      assert searcher.getTopReaderContext().isAtomic;\n      final AtomicReaderContext readerContext = (AtomicReaderContext) searcher.getTopReaderContext();\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          \n          Scorer scorer = weight.scorer(readerContext, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dc63f17f42c64d6ccc8c361cfcdf074f115f770c","date":1294930751,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","sourceNew":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      assert searcher.getTopReaderContext().isAtomic;\n      final AtomicReaderContext readerContext = (AtomicReaderContext) searcher.getTopReaderContext();\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(readerContext, Weight.ScorerContext.def());\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      assert searcher.getTopReaderContext().isAtomic;\n      final AtomicReaderContext readerContext = (AtomicReaderContext) searcher.getTopReaderContext();\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(readerContext, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","sourceNew":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n\n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n\n      String currentField = null;\n      DocsEnum docs = null;\n\n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n\n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n\n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      assert searcher.getTopReaderContext().isAtomic;\n      final AtomicReaderContext readerContext = (AtomicReaderContext) searcher.getTopReaderContext();\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(readerContext, Weight.ScorerContext.def());\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n\n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n\n      String currentField = null;\n      DocsEnum docs = null;\n\n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n\n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n\n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/BufferedDeletesStream#applyDeletes(BufferedDeletes,SegmentReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","sourceNew":"  private synchronized long applyDeletes(BufferedDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      assert searcher.getTopReaderContext().isAtomic;\n      final AtomicReaderContext readerContext = (AtomicReaderContext) searcher.getTopReaderContext();\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(readerContext, Weight.ScorerContext.def());\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      assert searcher.getTopReaderContext().isAtomic;\n      final AtomicReaderContext readerContext = (AtomicReaderContext) searcher.getTopReaderContext();\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(readerContext, Weight.ScorerContext.def());\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","sourceNew":null,"sourceOld":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n    \n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n        \n      String currentField = null;\n      DocsEnum docs = null;\n        \n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n          \n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n          \n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n            \n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(reader, true, false);\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":4,"author":"Michael Busch","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/BufferedDeletes#applyDeletes(SegmentDeletes,SegmentReader).mjava","sourceNew":null,"sourceOld":"  private synchronized long applyDeletes(SegmentDeletes deletes, SegmentReader reader) throws IOException {\n\n    long delCount = 0;\n\n    assert checkDeleteTerm(null);\n\n    if (deletes.terms.size() > 0) {\n      Fields fields = reader.fields();\n      if (fields == null) {\n        // This reader has no postings\n        return 0;\n      }\n\n      TermsEnum termsEnum = null;\n\n      String currentField = null;\n      DocsEnum docs = null;\n\n      for (Entry<Term,Integer> entry: deletes.terms.entrySet()) {\n        Term term = entry.getKey();\n        // Since we visit terms sorted, we gain performance\n        // by re-using the same TermsEnum and seeking only\n        // forwards\n        if (term.field() != currentField) {\n          assert currentField == null || currentField.compareTo(term.field()) < 0;\n          currentField = term.field();\n          Terms terms = fields.terms(currentField);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum == null) {\n          continue;\n        }\n        assert checkDeleteTerm(term);\n\n        if (termsEnum.seek(term.bytes(), false) == TermsEnum.SeekStatus.FOUND) {\n          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n\n          if (docsEnum != null) {\n            docs = docsEnum;\n            final int limit = entry.getValue();\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS || docID >= limit) {\n                break;\n              }\n              reader.deleteDocument(docID);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // Delete by docID\n    for (Integer docIdInt : deletes.docIDs) {\n      int docID = docIdInt.intValue();\n      reader.deleteDocument(docID);\n      delCount++;\n    }\n\n    // Delete by query\n    if (deletes.queries.size() > 0) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n      assert searcher.getTopReaderContext().isAtomic;\n      final AtomicReaderContext readerContext = (AtomicReaderContext) searcher.getTopReaderContext();\n      try {\n        for (Entry<Query, Integer> entry : deletes.queries.entrySet()) {\n          Query query = entry.getKey();\n          int limit = entry.getValue().intValue();\n          Weight weight = query.weight(searcher);\n          Scorer scorer = weight.scorer(readerContext, Weight.ScorerContext.def());\n          if (scorer != null) {\n            while(true)  {\n              int doc = scorer.nextDoc();\n              if (doc >= limit)\n                break;\n\n              reader.deleteDocument(doc);\n              // TODO: we could/should change\n              // reader.deleteDocument to return boolean\n              // true if it did in fact delete, because here\n              // we could be deleting an already-deleted doc\n              // which makes this an upper bound:\n              delCount++;\n            }\n          }\n        }\n      } finally {\n        searcher.close();\n      }\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e175058b6aef7c3f9971e918edcc7ae7ef4347d1"],"e175058b6aef7c3f9971e918edcc7ae7ef4347d1":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e175058b6aef7c3f9971e918edcc7ae7ef4347d1"],"dc63f17f42c64d6ccc8c361cfcdf074f115f770c":["fa1a999d6674423e5c4ac858b410283f6fe03f20"],"c19f985e36a65cc969e8e564fe337a0d41512075":["dc63f17f42c64d6ccc8c361cfcdf074f115f770c"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","c19f985e36a65cc969e8e564fe337a0d41512075"],"2dadf0f3286a34a0fee6e788ffce88624bf2984e":["e175058b6aef7c3f9971e918edcc7ae7ef4347d1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fa1a999d6674423e5c4ac858b410283f6fe03f20":["a10b98ef1ef4bf9e38d2e07a9e425a916afa8705"],"a10b98ef1ef4bf9e38d2e07a9e425a916afa8705":["2dadf0f3286a34a0fee6e788ffce88624bf2984e"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","dc63f17f42c64d6ccc8c361cfcdf074f115f770c"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["868da859b43505d9d2a023bfeae6dd0c795f5295","c19f985e36a65cc969e8e564fe337a0d41512075"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c19f985e36a65cc969e8e564fe337a0d41512075"]},"commit2Childs":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["e175058b6aef7c3f9971e918edcc7ae7ef4347d1"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"e175058b6aef7c3f9971e918edcc7ae7ef4347d1":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","2dadf0f3286a34a0fee6e788ffce88624bf2984e"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"dc63f17f42c64d6ccc8c361cfcdf074f115f770c":["c19f985e36a65cc969e8e564fe337a0d41512075","868da859b43505d9d2a023bfeae6dd0c795f5295"],"c19f985e36a65cc969e8e564fe337a0d41512075":["29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"29ef99d61cda9641b6250bf9567329a6e65f901d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff","ab5cb6a74aefb78aa0569857970b9151dfe2e787","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"2dadf0f3286a34a0fee6e788ffce88624bf2984e":["a10b98ef1ef4bf9e38d2e07a9e425a916afa8705"],"fa1a999d6674423e5c4ac858b410283f6fe03f20":["dc63f17f42c64d6ccc8c361cfcdf074f115f770c"],"a10b98ef1ef4bf9e38d2e07a9e425a916afa8705":["fa1a999d6674423e5c4ac858b410283f6fe03f20"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}