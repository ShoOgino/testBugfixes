{"path":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","commits":[{"id":"b1add9ddc0005b07550d4350720aac22dc9886b3","date":1295549635,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"/dev/null","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd1bfe3cedf815c14939d170d53031c88eb5c444","date":1295896578,"type":0,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"/dev/null","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"/dev/null","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762","date":1297938719,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f1bdbf92da222965b46c0a942c3857ba56e5c638","date":1298297608,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0515d905e854ae1fd47f03a4cf6dcb025bbc19aa","date":1310121126,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent &= (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60ba444201d2570214b6fcf1d15600dc1a01f548","date":1313868045,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        state.blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + state.blockTermCount);\n        if (state.blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termCount = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.FieldReader.SegmentTermsEnum#nextBlock().mjava","sourceNew":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","sourceOld":"      /* Does initial decode of next block of terms; this\n         doesn't actually decode the docFreq, totalTermFreq,\n         postings details (frq/prx offset, etc.) metadata;\n         it just loads them as byte[] blobs which are then      \n         decoded on-demand if the metadata is ever requested\n         for any term in this block.  This enables terms-only\n         intensive consumes (eg certain MTQs, respelling) to\n         not pay the price of decoding metadata they won't\n         use. */\n      private boolean nextBlock() throws IOException {\n\n        // TODO: we still lazy-decode the byte[] for each\n        // term (the suffix), but, if we decoded\n        // all N terms up front then seeking could do a fast\n        // bsearch w/in the block...\n\n        //System.out.println(\"BTR.nextBlock() fp=\" + in.getFilePointer() + \" this=\" + this);\n        state.blockFilePointer = in.getFilePointer();\n        blockTermCount = in.readVInt();\n        //System.out.println(\"  blockTermCount=\" + blockTermCount);\n        if (blockTermCount == 0) {\n          return false;\n        }\n        termBlockPrefix = in.readVInt();\n\n        // term suffixes:\n        int len = in.readVInt();\n        if (termSuffixes.length < len) {\n          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  termSuffixes len=\" + len);\n        in.readBytes(termSuffixes, 0, len);\n        termSuffixesReader.reset(termSuffixes, 0, len);\n\n        // docFreq, totalTermFreq\n        len = in.readVInt();\n        if (docFreqBytes.length < len) {\n          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];\n        }\n        //System.out.println(\"  freq bytes len=\" + len);\n        in.readBytes(docFreqBytes, 0, len);\n        freqReader.reset(docFreqBytes, 0, len);\n        metaDataUpto = 0;\n\n        state.termBlockOrd = 0;\n\n        postingsReader.readTermsBlock(in, fieldInfo, state);\n\n        blocksSinceSeek++;\n        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());\n        //System.out.println(\"  indexIsCurrent=\" + indexIsCurrent);\n\n        return true;\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"60ba444201d2570214b6fcf1d15600dc1a01f548":["0515d905e854ae1fd47f03a4cf6dcb025bbc19aa"],"0515d905e854ae1fd47f03a4cf6dcb025bbc19aa":["4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762"],"fd1bfe3cedf815c14939d170d53031c88eb5c444":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["29ef99d61cda9641b6250bf9567329a6e65f901d","4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762"],"4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762":["b1add9ddc0005b07550d4350720aac22dc9886b3"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b1add9ddc0005b07550d4350720aac22dc9886b3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762","0515d905e854ae1fd47f03a4cf6dcb025bbc19aa"],"b1add9ddc0005b07550d4350720aac22dc9886b3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["fd1bfe3cedf815c14939d170d53031c88eb5c444","4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"]},"commit2Childs":{"60ba444201d2570214b6fcf1d15600dc1a01f548":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"0515d905e854ae1fd47f03a4cf6dcb025bbc19aa":["60ba444201d2570214b6fcf1d15600dc1a01f548","5d004d0e0b3f65bb40da76d476d659d7888270e8"],"fd1bfe3cedf815c14939d170d53031c88eb5c444":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":[],"4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762":["0515d905e854ae1fd47f03a4cf6dcb025bbc19aa","f1bdbf92da222965b46c0a942c3857ba56e5c638","5d004d0e0b3f65bb40da76d476d659d7888270e8","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["f1bdbf92da222965b46c0a942c3857ba56e5c638"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["fd1bfe3cedf815c14939d170d53031c88eb5c444","29ef99d61cda9641b6250bf9567329a6e65f901d","b1add9ddc0005b07550d4350720aac22dc9886b3"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"b1add9ddc0005b07550d4350720aac22dc9886b3":["4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762","29ef99d61cda9641b6250bf9567329a6e65f901d"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f1bdbf92da222965b46c0a942c3857ba56e5c638","5d004d0e0b3f65bb40da76d476d659d7888270e8","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}