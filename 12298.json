{"path":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","commits":[{"id":"ff2d7326b1f013c8da9bad45b1e98a3d16c38575","date":1330406992,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"/dev/null","sourceNew":"  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean succesfulRecovery = false;\n\n    UpdateLog ulog;\n    try {\n      ulog = core.getUpdateHandler().getUpdateLog();\n      if (ulog == null) {\n        SolrException.log(log, \"No UpdateLog found - cannot recover\");\n        recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n            core.getCoreDescriptor());\n        return;\n      }\n    } finally {\n      core.close();\n    }\n\n    List<Long> startingRecentVersions;\n    UpdateLog.RecentUpdates startingRecentUpdates = ulog.getRecentUpdates();\n    try {\n      startingRecentVersions = startingRecentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      startingRecentUpdates.close();\n    }\n\n    List<Long> reallyStartingVersions = ulog.getStartingVersions();\n\n\n    if (reallyStartingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = reallyStartingVersions.size() > 0 ? reallyStartingVersions.get(0) : 0;\n\n      for (; oldIdx<startingRecentVersions.size(); oldIdx++) {\n        if (startingRecentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n      }\n\n      // TODO: only log at debug level in the future (or move to oldIdx > 0 block)\n      log.info(\"###### startupVersions=\" + reallyStartingVersions);\n      log.info(\"###### currentVersions=\" + startingRecentVersions);\n    }\n    \n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.\n      startingRecentVersions = reallyStartingVersions;\n    }\n\n    boolean firstTime = true;\n\n    while (!succesfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      core = cc.getCore(coreName);\n      if (core == null) {\n        SolrException.log(log, \"SolrCore not found - cannot recover:\" + coreName);\n        return;\n      }\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n \n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        \n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n        \n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName); \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        \n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(startingRecentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was succesful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was succesful - registering as Active \"\n            // + zkController.getNodeName());\n            \n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n            \n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            succesfulRecovery = true;\n            close = true;\n            return;\n          }\n          \n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        \n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n          \n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n          \n          replay(ulog);\n          replayed = true;\n          \n          log.info(\"Recovery was succesful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          succesfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n        \n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover\", t);\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n      \n      if (!succesfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n          \n          SolrException.log(log, \"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              \n            } else {\n              // TODO: for now, give up after X tries - should we do more?\n              core = cc.getCore(coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } finally {\n                if (core != null) {\n                  core.close();\n                }\n              }\n            }\n            break;\n          }\n          \n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n        \n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n    \n      \n      log.info(\"Finished recovery process\");\n      \n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a6378064655e76cd7b908b1cab4ce425b384b508","3c3f78fc552394c5911fc8e26627b33263967e83","bb62cc3362417c3e5136f2f26d34a1072ad633eb","e8fa677b7b2405d2c2b902646dbae8f5fe34b60e","e99829242bceda4cf974ec0eb5d82d713615b3da","f56da6f4f15d95f318d2d6ac2a39a9183dfecff2","1525b4dfbc0d413b8d7247da232009778e624836"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0fcbbde821566094ce01864ce5f7824a3fe2072e","date":1330487528,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    try {\n      ulog = core.getUpdateHandler().getUpdateLog();\n      if (ulog == null) {\n        SolrException.log(log, \"No UpdateLog found - cannot recover\");\n        recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n            core.getCoreDescriptor());\n        return;\n      }\n    } finally {\n      core.close();\n    }\n\n    List<Long> startingRecentVersions;\n    UpdateLog.RecentUpdates startingRecentUpdates = ulog.getRecentUpdates();\n    try {\n      startingRecentVersions = startingRecentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      startingRecentUpdates.close();\n    }\n\n    List<Long> reallyStartingVersions = ulog.getStartingVersions();\n\n\n    if (reallyStartingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = reallyStartingVersions.size() > 0 ? reallyStartingVersions.get(0) : 0;\n\n      for (; oldIdx<startingRecentVersions.size(); oldIdx++) {\n        if (startingRecentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n      }\n\n      // TODO: only log at debug level in the future (or move to oldIdx > 0 block)\n      log.info(\"###### startupVersions=\" + reallyStartingVersions);\n      log.info(\"###### currentVersions=\" + startingRecentVersions);\n    }\n    \n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.\n      startingRecentVersions = reallyStartingVersions;\n    }\n\n    boolean firstTime = true;\n\n    while (!successfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      core = cc.getCore(coreName);\n      if (core == null) {\n        SolrException.log(log, \"SolrCore not found - cannot recover:\" + coreName);\n        return;\n      }\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n \n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        \n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n        \n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName); \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        \n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(startingRecentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n            \n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n            \n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n          \n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        \n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n          \n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n          \n          replay(ulog);\n          replayed = true;\n          \n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n        \n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover\", t);\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n      \n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n          \n          SolrException.log(log, \"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              \n            } else {\n              // TODO: for now, give up after X tries - should we do more?\n              core = cc.getCore(coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } finally {\n                if (core != null) {\n                  core.close();\n                }\n              }\n            }\n            break;\n          }\n          \n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n        \n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n    \n      \n      log.info(\"Finished recovery process\");\n      \n    }\n  }\n\n","sourceOld":"  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean succesfulRecovery = false;\n\n    UpdateLog ulog;\n    try {\n      ulog = core.getUpdateHandler().getUpdateLog();\n      if (ulog == null) {\n        SolrException.log(log, \"No UpdateLog found - cannot recover\");\n        recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n            core.getCoreDescriptor());\n        return;\n      }\n    } finally {\n      core.close();\n    }\n\n    List<Long> startingRecentVersions;\n    UpdateLog.RecentUpdates startingRecentUpdates = ulog.getRecentUpdates();\n    try {\n      startingRecentVersions = startingRecentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      startingRecentUpdates.close();\n    }\n\n    List<Long> reallyStartingVersions = ulog.getStartingVersions();\n\n\n    if (reallyStartingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = reallyStartingVersions.size() > 0 ? reallyStartingVersions.get(0) : 0;\n\n      for (; oldIdx<startingRecentVersions.size(); oldIdx++) {\n        if (startingRecentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n      }\n\n      // TODO: only log at debug level in the future (or move to oldIdx > 0 block)\n      log.info(\"###### startupVersions=\" + reallyStartingVersions);\n      log.info(\"###### currentVersions=\" + startingRecentVersions);\n    }\n    \n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.\n      startingRecentVersions = reallyStartingVersions;\n    }\n\n    boolean firstTime = true;\n\n    while (!succesfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      core = cc.getCore(coreName);\n      if (core == null) {\n        SolrException.log(log, \"SolrCore not found - cannot recover:\" + coreName);\n        return;\n      }\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n \n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        \n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n        \n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName); \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        \n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(startingRecentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was succesful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was succesful - registering as Active \"\n            // + zkController.getNodeName());\n            \n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n            \n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            succesfulRecovery = true;\n            close = true;\n            return;\n          }\n          \n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        \n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n          \n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n          \n          replay(ulog);\n          replayed = true;\n          \n          log.info(\"Recovery was succesful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          succesfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n        \n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover\", t);\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n      \n      if (!succesfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n          \n          SolrException.log(log, \"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              \n            } else {\n              // TODO: for now, give up after X tries - should we do more?\n              core = cc.getCore(coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } finally {\n                if (core != null) {\n                  core.close();\n                }\n              }\n            }\n            break;\n          }\n          \n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n        \n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n    \n      \n      log.info(\"Finished recovery process\");\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7241f6cadcc9f475e0bf1eaae71c274e8a07b525","date":1330881818,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    try {\n      ulog = core.getUpdateHandler().getUpdateLog();\n      if (ulog == null) {\n        SolrException.log(log, \"No UpdateLog found - cannot recover\");\n        recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n            core.getCoreDescriptor());\n        return;\n      }\n    } finally {\n      core.close();\n    }\n\n    List<Long> startingRecentVersions;\n    UpdateLog.RecentUpdates startingRecentUpdates = ulog.getRecentUpdates();\n    try {\n      startingRecentVersions = startingRecentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      startingRecentUpdates.close();\n    }\n\n    List<Long> reallyStartingVersions = ulog.getStartingVersions();\n\n\n    if (reallyStartingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = reallyStartingVersions.size() > 0 ? reallyStartingVersions.get(0) : 0;\n\n      for (; oldIdx<startingRecentVersions.size(); oldIdx++) {\n        if (startingRecentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + startingRecentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + reallyStartingVersions);\n    }\n    \n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.\n      startingRecentVersions = reallyStartingVersions;\n    }\n\n    boolean firstTime = true;\n\n    while (!successfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      core = cc.getCore(coreName);\n      if (core == null) {\n        SolrException.log(log, \"SolrCore not found - cannot recover:\" + coreName);\n        return;\n      }\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n \n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        \n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n        \n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName); \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        \n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(startingRecentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n            \n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n            \n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n          \n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        \n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n          \n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n          \n          replay(ulog);\n          replayed = true;\n          \n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n        \n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover... closing core.\", t);\n      } finally {\n        core.close();\n      }\n      \n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n          \n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              \n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              // TODO: for now, give up after X tries - should we do more?\n              core = cc.getCore(coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } finally {\n                core.close();\n              }\n            }\n            break;\n          }\n          \n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n        \n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    try {\n      ulog = core.getUpdateHandler().getUpdateLog();\n      if (ulog == null) {\n        SolrException.log(log, \"No UpdateLog found - cannot recover\");\n        recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n            core.getCoreDescriptor());\n        return;\n      }\n    } finally {\n      core.close();\n    }\n\n    List<Long> startingRecentVersions;\n    UpdateLog.RecentUpdates startingRecentUpdates = ulog.getRecentUpdates();\n    try {\n      startingRecentVersions = startingRecentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      startingRecentUpdates.close();\n    }\n\n    List<Long> reallyStartingVersions = ulog.getStartingVersions();\n\n\n    if (reallyStartingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = reallyStartingVersions.size() > 0 ? reallyStartingVersions.get(0) : 0;\n\n      for (; oldIdx<startingRecentVersions.size(); oldIdx++) {\n        if (startingRecentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n      }\n\n      // TODO: only log at debug level in the future (or move to oldIdx > 0 block)\n      log.info(\"###### startupVersions=\" + reallyStartingVersions);\n      log.info(\"###### currentVersions=\" + startingRecentVersions);\n    }\n    \n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.\n      startingRecentVersions = reallyStartingVersions;\n    }\n\n    boolean firstTime = true;\n\n    while (!successfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      core = cc.getCore(coreName);\n      if (core == null) {\n        SolrException.log(log, \"SolrCore not found - cannot recover:\" + coreName);\n        return;\n      }\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n \n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        \n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n        \n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName); \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        \n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(startingRecentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n            \n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n            \n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n          \n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        \n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n          \n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n          \n          replay(ulog);\n          replayed = true;\n          \n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n        \n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover\", t);\n      } finally {\n        if (core != null) {\n          core.close();\n        }\n      }\n      \n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n          \n          SolrException.log(log, \"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              \n            } else {\n              // TODO: for now, give up after X tries - should we do more?\n              core = cc.getCore(coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } finally {\n                if (core != null) {\n                  core.close();\n                }\n              }\n            }\n            break;\n          }\n          \n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n        \n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n    \n      \n      log.info(\"Finished recovery process\");\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":["e99829242bceda4cf974ec0eb5d82d713615b3da"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8de248630034b5863f6795325312ed9b0e697794","date":1330887105,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> startingRecentVersions;\n    UpdateLog.RecentUpdates startingRecentUpdates = ulog.getRecentUpdates();\n    try {\n      startingRecentVersions = startingRecentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      startingRecentUpdates.close();\n    }\n\n    List<Long> reallyStartingVersions = ulog.getStartingVersions();\n\n\n    if (reallyStartingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = reallyStartingVersions.size() > 0 ? reallyStartingVersions.get(0) : 0;\n\n      for (; oldIdx<startingRecentVersions.size(); oldIdx++) {\n        if (startingRecentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + startingRecentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + reallyStartingVersions);\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.\n      startingRecentVersions = reallyStartingVersions;\n    }\n\n    boolean firstTime = true;\n\n    while (!successfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(startingRecentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    try {\n      ulog = core.getUpdateHandler().getUpdateLog();\n      if (ulog == null) {\n        SolrException.log(log, \"No UpdateLog found - cannot recover\");\n        recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n            core.getCoreDescriptor());\n        return;\n      }\n    } finally {\n      core.close();\n    }\n\n    List<Long> startingRecentVersions;\n    UpdateLog.RecentUpdates startingRecentUpdates = ulog.getRecentUpdates();\n    try {\n      startingRecentVersions = startingRecentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      startingRecentUpdates.close();\n    }\n\n    List<Long> reallyStartingVersions = ulog.getStartingVersions();\n\n\n    if (reallyStartingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = reallyStartingVersions.size() > 0 ? reallyStartingVersions.get(0) : 0;\n\n      for (; oldIdx<startingRecentVersions.size(); oldIdx++) {\n        if (startingRecentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + startingRecentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + reallyStartingVersions);\n    }\n    \n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.\n      startingRecentVersions = reallyStartingVersions;\n    }\n\n    boolean firstTime = true;\n\n    while (!successfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      core = cc.getCore(coreName);\n      if (core == null) {\n        SolrException.log(log, \"SolrCore not found - cannot recover:\" + coreName);\n        return;\n      }\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n \n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        \n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n        \n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName); \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        \n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(startingRecentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n            \n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n            \n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n          \n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        \n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n          \n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n          \n          replay(ulog);\n          replayed = true;\n          \n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n        \n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover... closing core.\", t);\n      } finally {\n        core.close();\n      }\n      \n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n          \n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              \n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              // TODO: for now, give up after X tries - should we do more?\n              core = cc.getCore(coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } finally {\n                core.close();\n              }\n            }\n            break;\n          }\n          \n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n        \n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":["a6378064655e76cd7b908b1cab4ce425b384b508","3c3f78fc552394c5911fc8e26627b33263967e83","d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1","e8fa677b7b2405d2c2b902646dbae8f5fe34b60e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":0,"author":"Ryan McKinley","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"/dev/null","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> startingRecentVersions;\n    UpdateLog.RecentUpdates startingRecentUpdates = ulog.getRecentUpdates();\n    try {\n      startingRecentVersions = startingRecentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      startingRecentUpdates.close();\n    }\n\n    List<Long> reallyStartingVersions = ulog.getStartingVersions();\n\n\n    if (reallyStartingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = reallyStartingVersions.size() > 0 ? reallyStartingVersions.get(0) : 0;\n\n      for (; oldIdx<startingRecentVersions.size(); oldIdx++) {\n        if (startingRecentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + startingRecentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + reallyStartingVersions);\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.\n      startingRecentVersions = reallyStartingVersions;\n    }\n\n    boolean firstTime = true;\n\n    while (!successfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(startingRecentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e99829242bceda4cf974ec0eb5d82d713615b3da","date":1337646971,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> startingRecentVersions;\n    UpdateLog.RecentUpdates startingRecentUpdates = ulog.getRecentUpdates();\n    try {\n      startingRecentVersions = startingRecentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      startingRecentUpdates.close();\n    }\n\n    List<Long> reallyStartingVersions = ulog.getStartingVersions();\n\n\n    if (reallyStartingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = reallyStartingVersions.size() > 0 ? reallyStartingVersions.get(0) : 0;\n\n      for (; oldIdx<startingRecentVersions.size(); oldIdx++) {\n        if (startingRecentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + startingRecentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + reallyStartingVersions);\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.\n      startingRecentVersions = reallyStartingVersions;\n    }\n\n    boolean firstTime = true;\n\n    while (!successfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(startingRecentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":["ff2d7326b1f013c8da9bad45b1e98a3d16c38575","7241f6cadcc9f475e0bf1eaae71c274e8a07b525"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e1e5b576a150f260cba5c0287b3764e42cab2fe7","date":1337700347,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5c76806dcdb4841b4f71ecfe9e9e95147f7201f2","date":1337787106,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // if (!isClosed()) Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n          for (int i = 0; i<Math.min(retries, 600); i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(START_TIMEOUT);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3599646b4d4c346cf74d334813488b8b337b5bf5","date":1337790261,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // if (!isClosed()) Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n          for (int i = 0; i<Math.min(retries, 600); i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(START_TIMEOUT);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> startingRecentVersions;\n    UpdateLog.RecentUpdates startingRecentUpdates = ulog.getRecentUpdates();\n    try {\n      startingRecentVersions = startingRecentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      startingRecentUpdates.close();\n    }\n\n    List<Long> reallyStartingVersions = ulog.getStartingVersions();\n\n\n    if (reallyStartingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = reallyStartingVersions.size() > 0 ? reallyStartingVersions.get(0) : 0;\n\n      for (; oldIdx<startingRecentVersions.size(); oldIdx++) {\n        if (startingRecentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + startingRecentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + reallyStartingVersions);\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.\n      startingRecentVersions = reallyStartingVersions;\n    }\n\n    boolean firstTime = true;\n\n    while (!successfulRecovery && !close && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(startingRecentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a5093a9e893633cc091cf2f729d7863671c2b715","date":1339132888,"type":3,"author":"Sami Siren","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // if (!isClosed()) Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n          for (int i = 0; i<Math.min(retries, 600); i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(START_TIMEOUT);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n                coreZkNodeName, coreName);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publishAsActive(baseUrl, core.getCoreDescriptor(),\n              coreZkNodeName, coreName);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // if (!isClosed()) Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n          for (int i = 0; i<Math.min(retries, 600); i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(START_TIMEOUT);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e53faaf24d27f16d0ec340d9a4109403ed918fcd","date":1343227661,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // if (!isClosed()) Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n          for (int i = 0; i<Math.min(retries, 600); i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(START_TIMEOUT);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":["234f41e4e127541225632264295a22ef1aef1d7c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // if (!isClosed()) Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n          for (int i = 0; i<Math.min(retries, 600); i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(START_TIMEOUT);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31e031f53c5dc379346e9162e3703c9a1fdca395","date":1343683836,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring\", t);\n      recentVersions = new ArrayList<Long>(0);\n    }finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":["f56da6f4f15d95f318d2d6ac2a39a9183dfecff2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring\", t);\n      recentVersions = new ArrayList<Long>(0);\n    }finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // if (!isClosed()) Thread.sleep(Math.min(START_TIMEOUT * retries, 60000));\n          for (int i = 0; i<Math.min(retries, 600); i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(START_TIMEOUT);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d3fe370ea74eec986c7277cfcc216c43d2ca8df","date":1344013889,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring\", t);\n      recentVersions = new ArrayList<Long>(0);\n    }finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync\");\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting replication recovery\");\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring\", t);\n      recentVersions = new ArrayList<Long>(0);\n    }finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"21f23e00493fe5b2bf0678fd0f0b4f4d166f20be","date":1344015106,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring\", t);\n      recentVersions = new ArrayList<Long>(0);\n    }finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync\");\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery\");\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring\", t);\n      recentVersions = new ArrayList<Long>(0);\n    }finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync\");\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting replication recovery\");\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8fd5be977c105554c6a7b68afcdbc511439723ab","date":1344115570,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring\", t);\n      recentVersions = new ArrayList<Long>(0);\n    }finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync\");\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery\");\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring\", t);\n      recentVersions = new ArrayList<Long>(0);\n    }finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ef9583322dbc8650e3cd32d936c0b3e31eefadba","date":1344482424,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. Core:\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. Core:\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. Core:\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" Core:\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. Core:\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. Core:\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. Core:\" + coreName);\n        log.info(\"Begin buffering updates. Core:\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. Core:\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover. Core:\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... Core:\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded. Core:\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"Core:\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. Core:\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. Core:\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring\", t);\n      recentVersions = new ArrayList<Long>(0);\n    }finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync\");\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery\");\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. Core:\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. Core:\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. Core:\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" Core:\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. Core:\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. Core:\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. Core:\" + coreName);\n        log.info(\"Begin buffering updates. Core:\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. Core:\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover. Core:\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... Core:\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded. Core:\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"Core:\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. Core:\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. Core:\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"Sync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"Sync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"40c478fcb4e093ac431407a8db2896ac9c867f33","date":1344540116,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. Core:\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. Core:\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. Core:\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" Core:\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. Core:\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. Core:\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. Core:\" + coreName);\n        log.info(\"Begin buffering updates. Core:\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. Core:\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover. Core:\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... Core:\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded. Core:\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"Core:\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. Core:\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. Core:\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":["f56da6f4f15d95f318d2d6ac2a39a9183dfecff2","1525b4dfbc0d413b8d7247da232009778e624836"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates();\n    try {\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring\", t);\n      recentVersions = new ArrayList<Long>(0);\n    }finally {\n      recentUpdates.close();\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync\");\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active\");\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication\");\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery\");\n        log.info(\"Begin buffering updates\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover.\", t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again...\");\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded.\");\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. Core:\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. Core:\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. Core:\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" Core:\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. Core:\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. Core:\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. Core:\" + coreName);\n        log.info(\"Begin buffering updates. Core:\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. Core:\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover. Core:\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... Core:\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded. Core:\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"Core:\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. Core:\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. Core:\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"66c64e8cfded6a585100e6430238faaf416f3fea","date":1344964603,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              SolrException.log(log, \"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":["a6378064655e76cd7b908b1cab4ce425b384b508","d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1","f56da6f4f15d95f318d2d6ac2a39a9183dfecff2","877f1e09b9299ce0757f4d83768da944803baf04"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              SolrException.log(log, \"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              SolrException.log(log, \"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        // first thing we just try to sync\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          log.error(\"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        log.error(\"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              log.error(\"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          log.error(\"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6013b4c7388f1627659c8f96c44abd10a294d3a6","date":1346343796,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              SolrException.log(log, \"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              SolrException.log(log, \"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              SolrException.log(log, \"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n\n    boolean firstTime = true;\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n            // System.out\n            // .println(\"Sync Recovery was successful - registering as Active \"\n            // + zkController.getNodeName());\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              SolrException.log(log, \"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6","date":1346692465,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            } else {\n              SolrException.log(log, \"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      int oldIdx = 0;  // index of the start of the old list in the current list\n      long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n\n      for (; oldIdx<recentVersions.size(); oldIdx++) {\n        if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n      }\n\n      if (oldIdx > 0) {\n        log.info(\"####### Found new versions added after startup: num=\" + oldIdx);\n        log.info(\"###### currentVersions=\" + recentVersions);\n      }\n\n      log.info(\"###### startupVersions=\" + startingVersions);\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n\n      if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime.\n        log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\" + coreName);\n        firstTime = false;    // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isClosed() && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n\n            } else {\n              SolrException.log(log, \"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":["3c3f78fc552394c5911fc8e26627b33263967e83","d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1","e8fa677b7b2405d2c2b902646dbae8f5fe34b60e","f56da6f4f15d95f318d2d6ac2a39a9183dfecff2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1","date":1346817835,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded. core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            } else {\n              SolrException.log(log, \"Recovery failed - max retries exceeded. core=\" + coreName);\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                  core.getCoreDescriptor());\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":["7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6","8de248630034b5863f6795325312ed9b0e697794","66c64e8cfded6a585100e6430238faaf416f3fea"],"bugIntro":["e8fa677b7b2405d2c2b902646dbae8f5fe34b60e","f56da6f4f15d95f318d2d6ac2a39a9183dfecff2","1525b4dfbc0d413b8d7247da232009778e624836"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fa64435b5902ce266c23755a4a00691a3285dab8","date":1347243290,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded. core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.get(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.get(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded. core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a6378064655e76cd7b908b1cab4ce425b384b508","date":1347656715,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded. core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n        //System.out.println(\"Sync Recovery was not successful - trying replication\");\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n//        // open a new IndexWriter - we don't want any background merges ongoing\n//        // also ensures something like NRTCachingDirectory is flushed\n//        boolean forceNewIndexDir = false;\n//        try {\n//          core.getUpdateHandler().newIndexWriter(false);\n//        } catch (Throwable t) {\n//          SolrException.log(log, \"Could not read the current index - replicating to a new directory\", t);\n//          // something is wrong with the index\n//          // we need to force using a new index directory\n//          forceNewIndexDir = true;\n//        }\n//        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded. core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":["8de248630034b5863f6795325312ed9b0e697794","66c64e8cfded6a585100e6430238faaf416f3fea","ff2d7326b1f013c8da9bad45b1e98a3d16c38575"],"bugIntro":["3c3f78fc552394c5911fc8e26627b33263967e83"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1525b4dfbc0d413b8d7247da232009778e624836","date":1351101135,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded. core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":["d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1","ff2d7326b1f013c8da9bad45b1e98a3d16c38575","40c478fcb4e093ac431407a8db2896ac9c867f33"],"bugIntro":["e8fa677b7b2405d2c2b902646dbae8f5fe34b60e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5e71cbdfcf34d779dd7e7ba148dfff6022f2005a","date":1351228731,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f2126b84bd093fa3d921582a109a0ee578c28126","date":1351522501,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries == INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded. core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56a558aa5aadd60ae850d1ab090098bc63bdfaf9","date":1355245333,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d29a9a7485f393c7a581569b5734d8f9054f6a92","date":1356210364,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderProps(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71f14bcc8e12885ab59981c9fdecebb7c2ef3ca0","date":1371567556,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"849494cf2f3a96af5c8c84995108ddd8456fcd04","date":1372277913,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core,\n              leaderprops, leaderUrl);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c52ad29218ee436d52c57bf0829b98acad9de379","date":1383401544,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (Boolean.getBoolean(\"solr.cloud.debug\")) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  System.out.println(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            // try {\n            // RefCounted<SolrIndexSearcher> searchHolder =\n            // core.getNewestSearcher(false);\n            // SolrIndexSearcher searcher = searchHolder.get();\n            // try {\n            // System.out.println(core.getCoreDescriptor().getCoreContainer().getZkController().getNodeName()\n            // + \" synched \"\n            // + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n            // } finally {\n            // searchHolder.decref();\n            // }\n            // } catch (Exception e) {\n            //\n            // }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(ulog);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n  }\n\n","bugFix":null,"bugIntro":["6c94d2661bc1c14426980ec7882e951fdcff08d0","877f1e09b9299ce0757f4d83768da944803baf04"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b17e61942d57c9c1c6dc8a926bce1c5c47882f8","date":1383401729,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (Boolean.getBoolean(\"solr.cloud.debug\")) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  System.err.println(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (Boolean.getBoolean(\"solr.cloud.debug\")) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  System.out.println(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"89b56ee224dbe29ee4436d91a7070ca418ffc4fb","date":1383492966,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (Boolean.getBoolean(\"solr.cloud.debug\")) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  System.err.println(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"97bd2b0da4beced82821b752b29576be986cf1ff","date":1387747012,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f56da6f4f15d95f318d2d6ac2a39a9183dfecff2","date":1389633998,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Throwable t) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, t);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, t);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, t);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error while trying to recover\", t);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Throwable t) {\n              SolrException.log(log, \"\", t);\n            }\n          }\n\n        }\n\n      } catch (Throwable t) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, t);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Throwable t) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", t);\n              }\n            }\n            break;\n          }\n\n        } catch (Throwable e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":["7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6","31e031f53c5dc379346e9162e3703c9a1fdca395","66c64e8cfded6a585100e6430238faaf416f3fea","d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1","ff2d7326b1f013c8da9bad45b1e98a3d16c38575","40c478fcb4e093ac431407a8db2896ac9c867f33"],"bugIntro":["3c3f78fc552394c5911fc8e26627b33263967e83","e8fa677b7b2405d2c2b902646dbae8f5fe34b60e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<Long>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<Long>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fee8f787196eb664b953b851d18c52f0d8c9e157","date":1395630304,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          replay(core);\n          replayed = true;\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":["877f1e09b9299ce0757f4d83768da944803baf04"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14d5815ecbef89580f5c48990bcd433f04f8563a","date":1399564106,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"234f41e4e127541225632264295a22ef1aef1d7c","date":1413998343,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a couple min\n          double loopCount = Math.min(Math.pow(2, retries), 600); \n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":["e53faaf24d27f16d0ec340d9a4109403ed918fcd"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e8fa677b7b2405d2c2b902646dbae8f5fe34b60e","date":1423420267,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          retries = INTERRUPTED;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          if (isClosed()) {\n            retries = INTERRUPTED;\n          }\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            if (retries >= INTERRUPTED) {\n              SolrException.log(log, \"Recovery failed - interrupted. core=\"\n                  + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            } else {\n              SolrException.log(log,\n                  \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n              try {\n                recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n                    core.getCoreDescriptor());\n              } catch (Exception e) {\n                SolrException.log(log,\n                    \"Could not publish that recovery failed\", e);\n              }\n            }\n            break;\n          }\n\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          retries = INTERRUPTED;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":["7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6","8de248630034b5863f6795325312ed9b0e697794","1525b4dfbc0d413b8d7247da232009778e624836","d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1","ff2d7326b1f013c8da9bad45b1e98a3d16c38575","f56da6f4f15d95f318d2d6ac2a39a9183dfecff2"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6f3c1f22c5fe0011e187dac3151422365ae857f3","date":1425728437,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"15e37b3faa8262545b88208e097ade36ce4e8ba8","date":1427040286,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6c94d2661bc1c14426980ec7882e951fdcff08d0","date":1427167177,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":["c52ad29218ee436d52c57bf0829b98acad9de379"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished\n        try {\n          Thread.sleep(2000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.numRecordsToKeep, false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a219f1dcad1700e84807666bdbd2b573e8de7021","date":1428130940,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), ZkStateReader.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(),\n                ZkStateReader.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), ZkStateReader.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":["3c3f78fc552394c5911fc8e26627b33263967e83"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"439c63ae5d22132fca810a0029a854e97d2c1a3e","date":1432733612,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n        \n        log.info(\"Begin buffering updates.\");\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active.\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover.\", e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process.\");\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover. core=\" + coreName);\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring. core=\" + coreName, e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions. core=\" + coreName, e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync. core=\"\n              + coreName);\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation. core=\"\n            + coreName, e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader! core=\" + coreName);\n          log.info(\"Finished recovery process. core=\" + coreName);\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \"+core.getName()+\" as recovering, leader is \"+leaderUrl+\" and I am \"+ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" core=\" + coreName + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active. core=\" + coreName);\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication. core=\" + coreName);\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery. core=\" + coreName);\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active. core=\" + coreName);\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \") core=\" + coreName);\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \"). core=\" + coreName);\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"core=\" + coreName, e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted. core=\" + coreName, e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process. core=\" + coreName);\n\n    \n  }\n\n","bugFix":null,"bugIntro":["3c3f78fc552394c5911fc8e26627b33263967e83"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"877f1e09b9299ce0757f4d83768da944803baf04","date":1433276115,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n        \n        log.info(\"Begin buffering updates.\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active.\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover.\", e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process.\");\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n        \n        log.info(\"Begin buffering updates.\");\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n          \n          replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active.\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover.\", e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n    log.info(\"Finished recovery process.\");\n\n    \n  }\n\n","bugFix":["c52ad29218ee436d52c57bf0829b98acad9de379","66c64e8cfded6a585100e6430238faaf416f3fea","fee8f787196eb664b953b851d18c52f0d8c9e157"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3c3f78fc552394c5911fc8e26627b33263967e83","date":1443796813,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n        }\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process.\");\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync Recovery was successful - registering as Active.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n\n            // sync success - register as active and return\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n            successfulRecovery = true;\n            close = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n        \n        log.info(\"Begin buffering updates.\");\n        ulog.bufferUpdates();\n        replayed = false;\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful - registering as Active.\");\n          // if there are pending recovery requests, don't advert as active\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          close = true;\n          successfulRecovery = true;\n          recoveryListener.recovered();\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        } finally {\n          if (!replayed) {\n            try {\n              ulog.dropBufferedUpdates();\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n            }\n          }\n\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover.\", e);\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process.\");\n\n    \n  }\n\n","bugFix":["8de248630034b5863f6795325312ed9b0e697794","7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6","a6378064655e76cd7b908b1cab4ce425b384b508","ff2d7326b1f013c8da9bad45b1e98a3d16c38575","439c63ae5d22132fca810a0029a854e97d2c1a3e","f56da6f4f15d95f318d2d6ac2a39a9183dfecff2","a219f1dcad1700e84807666bdbd2b573e8de7021"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a9186bf60d7c6f504d4d5b01cfee95dc4bd15e53","date":1449051812,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n        }\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process.\");\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    UpdateLog.RecentUpdates recentUpdates = null;\n    try {\n      recentUpdates = ulog.getRecentUpdates();\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    } finally {\n      if (recentUpdates != null) {\n        recentUpdates.close();\n      }\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n        }\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process.\");\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f49f14d65e01870e494b6f38fa88ea1a8011e4fe","date":1450213592,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          log.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process.\");\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n        }\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process.\");\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"536b47681314488eb94706248be6047a3f142841","date":1450372090,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          log.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=\", Boolean.toString(successfulRecovery));\n\n    \n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"Recovery was cancelled\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"Recovery was cancelled\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          log.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) break; // check if someone closed us\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process.\");\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"610f5499a87a7113d53e5b621b616890f002e9f1","date":1450873010,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(log, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(log, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current\n                        // list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions\n            .get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          log.info(\"####### Found new versions added after startup: num=\"\n              + oldIdx);\n          log.info(\"###### currentVersions=\" + recentVersions);\n        }\n        \n        log.info(\"###### startupVersions=\" + startingVersions);\n      } catch (Exception e) {\n        SolrException.log(log, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          log.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(log, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor()\n            .getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          log.warn(\"We have not yet recovered - but we are now the leader!\");\n          log.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        log.info(\"Begin buffering updates. core=\" + coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        log.info(\"Publishing state of core \" + core.getName() + \" as recovering, leader is \" + leaderUrl + \" and I am \"\n            + ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n\n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          log.info(\"Attempting to PeerSync from \" + leaderUrl + \" - recoveringAfterStartup=\"+recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            log.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            if (log.isDebugEnabled()) {\n              try {\n                RefCounted<SolrIndexSearcher> searchHolder = core\n                    .getNewestSearcher(false);\n                SolrIndexSearcher searcher = searchHolder.get();\n                try {\n                  log.debug(core.getCoreDescriptor()\n                      .getCoreContainer().getZkController().getNodeName()\n                      + \" synched \"\n                      + searcher.search(new MatchAllDocsQuery(), 1).totalHits);\n                } finally {\n                  searchHolder.decref();\n                }\n              } catch (Exception e) {\n                log.debug(\"Error in solrcloud_debug block\", e);\n              }\n            }\n            log.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          log.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          log.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        log.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          log.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(log, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(log, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          log.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          log.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            log.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            log.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          log.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(log, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(log, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(log, \"\", e);\n        }\n\n        try {\n          // start at 1 sec and work up to a min\n          double loopCount = Math.min(Math.pow(2, retries), 60);\n          log.info(\"Wait {} seconds before trying to recover again ({})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              log.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          log.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      log.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    log.info(\"Finished recovery process, successful=\", Boolean.toString(successfulRecovery));\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb62cc3362417c3e5136f2f26d34a1072ad633eb","date":1475149102,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":["ff2d7326b1f013c8da9bad45b1e98a3d16c38575"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7392b75d2c2f2aecf31188732a0764fe0dc74ade","date":1489420141,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(WAIT_FOR_UPDATES_WITH_STALE_STATE_PAUSE);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= MAX_RETRIES) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(STARTING_RECOVERY_DELAY);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"be320990bdc77e643388fa801e75017f19289c42","date":1489477067,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for realtimeReplicas mode\n    boolean firstTime = !onlyLeaderIndexes;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (onlyLeaderIndexes) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (onlyLeaderIndexes) {\n              zkController.startReplicationFromLeader(coreName);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f996f8177b9204bdc92f7164460c6cefad9ac99a","date":1489482690,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for realtimeReplicas mode\n    boolean firstTime = !onlyLeaderIndexes;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (onlyLeaderIndexes) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (onlyLeaderIndexes) {\n              zkController.startReplicationFromLeader(coreName);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab68488225b6a6c357dda72ed11dedca9914a192","date":1490013111,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for realtimeReplicas mode\n    boolean firstTime = !onlyLeaderIndexes;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (onlyLeaderIndexes) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (onlyLeaderIndexes) {\n              zkController.startReplicationFromLeader(coreName);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    boolean firstTime = true;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5413931a874939770ede57e7f5ce5c64c1d2366c","date":1494979390,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for realtimeReplicas mode\n    boolean firstTime = !onlyLeaderIndexes;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (onlyLeaderIndexes) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (onlyLeaderIndexes) {\n              zkController.startReplicationFromLeader(coreName);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for realtimeReplicas mode\n    boolean firstTime = !onlyLeaderIndexes;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (onlyLeaderIndexes) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (onlyLeaderIndexes) {\n              zkController.startReplicationFromLeader(coreName);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c5536d1fc2015d46bbeb8163f6f8b99483c7cc1f","date":1495081498,"type":3,"author":"Cao Manh Dat","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for realtimeReplicas mode\n    boolean firstTime = !onlyLeaderIndexes;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (onlyLeaderIndexes) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (onlyLeaderIndexes) {\n              zkController.startReplicationFromLeader(coreName);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for realtimeReplicas mode\n    boolean firstTime = !onlyLeaderIndexes;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (onlyLeaderIndexes) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (onlyLeaderIndexes) {\n              zkController.startReplicationFromLeader(coreName);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"61c45e99cf6676da48f19d7511c73712ad39402b","date":1495508331,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    if (core.getCoreDescriptor().getCloudDescriptor().requiresTransactionLog()) {\n      doSyncOrReplicateRecovery(core);\n    } else {\n      doReplicateOnlyRecovery(core);\n    }\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for realtimeReplicas mode\n    boolean firstTime = !onlyLeaderIndexes;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (onlyLeaderIndexes) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !Thread.currentThread().isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (onlyLeaderIndexes) {\n              zkController.startReplicationFromLeader(coreName);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    if (core.getCoreDescriptor().getCloudDescriptor().requiresTransactionLog()) {\n      doSyncOrReplicateRecovery(core);\n    } else {\n      doReplicateOnlyRecovery(core);\n    }\n  }\n\n","sourceOld":"  // TODO: perhaps make this grab a new core each time through the loop to handle core reloads?\n  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    boolean replayed = false;\n    boolean successfulRecovery = false;\n\n    UpdateLog ulog;\n    ulog = core.getUpdateHandler().getUpdateLog();\n    if (ulog == null) {\n      SolrException.log(LOG, \"No UpdateLog found - cannot recover.\");\n      recoveryFailed(core, zkController, baseUrl, coreZkNodeName,\n          core.getCoreDescriptor());\n      return;\n    }\n\n    // we temporary ignore peersync for realtimeReplicas mode\n    boolean firstTime = !onlyLeaderIndexes;\n\n    List<Long> recentVersions;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      recentVersions = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    } catch (Exception e) {\n      SolrException.log(LOG, \"Corrupt tlog - ignoring.\", e);\n      recentVersions = new ArrayList<>(0);\n    }\n\n    List<Long> startingVersions = ulog.getStartingVersions();\n\n    if (startingVersions != null && recoveringAfterStartup) {\n      try {\n        int oldIdx = 0; // index of the start of the old list in the current list\n        long firstStartingVersion = startingVersions.size() > 0 ? startingVersions.get(0) : 0;\n        \n        for (; oldIdx < recentVersions.size(); oldIdx++) {\n          if (recentVersions.get(oldIdx) == firstStartingVersion) break;\n        }\n        \n        if (oldIdx > 0) {\n          LOG.info(\"####### Found new versions added after startup: num=[{}]\", oldIdx);\n          LOG.info(\"###### currentVersions=[{}]\",recentVersions);\n        }\n        \n        LOG.info(\"###### startupVersions=[{}]\", startingVersions);\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error getting recent versions.\", e);\n        recentVersions = new ArrayList<>(0);\n      }\n    }\n\n    if (recoveringAfterStartup) {\n      // if we're recovering after startup (i.e. we have been down), then we need to know what the last versions were\n      // when we went down.  We may have received updates since then.\n      recentVersions = startingVersions;\n      try {\n        if ((ulog.getStartingOperation() & UpdateLog.FLAG_GAP) != 0) {\n          // last operation at the time of startup had the GAP flag set...\n          // this means we were previously doing a full index replication\n          // that probably didn't complete and buffering updates in the\n          // meantime.\n          LOG.info(\"Looks like a previous replication recovery did not complete - skipping peer sync.\");\n          firstTime = false; // skip peersync\n        }\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error trying to get ulog starting operation.\", e);\n        firstTime = false; // skip peersync\n      }\n    }\n\n    if (onlyLeaderIndexes) {\n      zkController.stopReplicationFromLeader(coreName);\n    }\n\n    Future<RecoveryInfo> replayFuture = null;\n    while (!successfulRecovery && !isInterrupted() && !isClosed()) { // don't use interruption or it will close channels though\n      try {\n        CloudDescriptor cloudDesc = core.getCoreDescriptor().getCloudDescriptor();\n        ZkNodeProps leaderprops = zkStateReader.getLeaderRetry(\n            cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      \n        final String leaderBaseUrl = leaderprops.getStr(ZkStateReader.BASE_URL_PROP);\n        final String leaderCoreName = leaderprops.getStr(ZkStateReader.CORE_NAME_PROP);\n\n        String leaderUrl = ZkCoreNodeProps.getCoreUrl(leaderBaseUrl, leaderCoreName);\n\n        String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);\n\n        boolean isLeader = leaderUrl.equals(ourUrl);\n        if (isLeader && !cloudDesc.isLeader()) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Cloud state still says we are leader.\");\n        }\n        if (cloudDesc.isLeader()) {\n          // we are now the leader - no one else must have been suitable\n          LOG.warn(\"We have not yet recovered - but we are now the leader!\");\n          LOG.info(\"Finished recovery process.\");\n          zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          return;\n        }\n        \n        LOG.info(\"Begin buffering updates. core=[{}]\", coreName);\n        ulog.bufferUpdates();\n        replayed = false;\n        \n        LOG.info(\"Publishing state of core [{}] as recovering, leader is [{}] and I am [{}]\", core.getName(), leaderUrl,\n            ourUrl);\n        zkController.publish(core.getCoreDescriptor(), Replica.State.RECOVERING);\n        \n        \n        final Slice slice = zkStateReader.getClusterState().getSlice(cloudDesc.getCollectionName(),\n            cloudDesc.getShardId());\n            \n        try {\n          prevSendPreRecoveryHttpUriRequest.abort();\n        } catch (NullPointerException e) {\n          // okay\n        }\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n\n        sendPrepRecoveryCmd(leaderBaseUrl, leaderCoreName, slice);\n        \n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        // we wait a bit so that any updates on the leader\n        // that started before they saw recovering state \n        // are sure to have finished (see SOLR-7141 for\n        // discussion around current value)\n        try {\n          Thread.sleep(waitForUpdatesWithStaleStatePauseMilliSeconds);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n        }\n\n        // first thing we just try to sync\n        if (firstTime) {\n          firstTime = false; // only try sync the first time through the loop\n          LOG.info(\"Attempting to PeerSync from [{}] - recoveringAfterStartup=[{}]\", leaderUrl, recoveringAfterStartup);\n          // System.out.println(\"Attempting to PeerSync from \" + leaderUrl\n          // + \" i am:\" + zkController.getNodeName());\n          PeerSync peerSync = new PeerSync(core,\n              Collections.singletonList(leaderUrl), ulog.getNumRecordsToKeep(), false, false);\n          peerSync.setStartingVersions(recentVersions);\n          boolean syncSuccess = peerSync.sync().isSuccess();\n          if (syncSuccess) {\n            SolrQueryRequest req = new LocalSolrQueryRequest(core,\n                new ModifiableSolrParams());\n            // force open a new searcher\n            core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n            LOG.info(\"PeerSync stage of recovery was successful.\");\n\n            // solrcloud_debug\n            cloudDebugLog(core, \"synced\");\n            \n            LOG.info(\"Replaying updates buffered during PeerSync.\");\n            replay(core);\n            replayed = true;\n            \n            // sync success\n            successfulRecovery = true;\n            return;\n          }\n\n          LOG.info(\"PeerSync Recovery was not successful - trying replication.\");\n        }\n\n        if (isClosed()) {\n          LOG.info(\"RecoveryStrategy has been closed\");\n          break;\n        }\n        \n        LOG.info(\"Starting Replication Recovery.\");\n\n        try {\n\n          replicate(zkController.getNodeName(), core, leaderprops);\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          replayFuture = replay(core);\n          replayed = true;\n          \n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n\n          LOG.info(\"Replication Recovery was successful.\");\n          successfulRecovery = true;\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted\", e);\n          close = true;\n        } catch (Exception e) {\n          SolrException.log(LOG, \"Error while trying to recover\", e);\n        }\n\n      } catch (Exception e) {\n        SolrException.log(LOG, \"Error while trying to recover. core=\" + coreName, e);\n      } finally {\n        if (!replayed) {\n          // dropBufferedUpdate()s currently only supports returning to ACTIVE state, which risks additional updates\n          // being added w/o UpdateLog.FLAG_GAP, hence losing the info on restart that we are not up-to-date.\n          // For now, ulog will simply remain in BUFFERING state, and an additional call to bufferUpdates() will\n          // reset our starting point for playback.\n          LOG.info(\"Replay not started, or was not successful... still buffering updates.\");\n\n          /** this prev code is retained in case we want to switch strategies.\n          try {\n            ulog.dropBufferedUpdates();\n          } catch (Exception e) {\n            SolrException.log(log, \"\", e);\n          }\n          **/\n        }\n        if (successfulRecovery) {\n          LOG.info(\"Registering as Active after recovery.\");\n          try {\n            if (onlyLeaderIndexes) {\n              zkController.startReplicationFromLeader(coreName);\n            }\n            zkController.publish(core.getCoreDescriptor(), Replica.State.ACTIVE);\n          } catch (Exception e) {\n            LOG.error(\"Could not publish as ACTIVE after succesful recovery\", e);\n            successfulRecovery = false;\n          }\n          \n          if (successfulRecovery) {\n            close = true;\n            recoveryListener.recovered();\n          }\n        }\n      }\n\n      if (!successfulRecovery) {\n        // lets pause for a moment and we need to try again...\n        // TODO: we don't want to retry for some problems?\n        // Or do a fall off retry...\n        try {\n\n          if (isClosed()) {\n            LOG.info(\"RecoveryStrategy has been closed\");\n            break;\n          }\n          \n          LOG.error(\"Recovery failed - trying again... (\" + retries + \")\");\n          \n          retries++;\n          if (retries >= maxRetries) {\n            SolrException.log(LOG, \"Recovery failed - max retries exceeded (\" + retries + \").\");\n            try {\n              recoveryFailed(core, zkController, baseUrl, coreZkNodeName, core.getCoreDescriptor());\n            } catch (Exception e) {\n              SolrException.log(LOG, \"Could not publish that recovery failed\", e);\n            }\n            break;\n          }\n        } catch (Exception e) {\n          SolrException.log(LOG, \"An error has occurred during recovery\", e);\n        }\n\n        try {\n          // Wait an exponential interval between retries, start at 5 seconds and work up to a minute.\n          // If we're at attempt >= 4, there's no point computing pow(2, retries) because the result \n          // will always be the minimum of the two (12). Since we sleep at 5 seconds sub-intervals in\n          // order to check if we were closed, 12 is chosen as the maximum loopCount (5s * 12 = 1m).\n          double loopCount = retries < 4 ? Math.min(Math.pow(2, retries), 12) : 12;\n          LOG.info(\"Wait [{}] seconds before trying to recover again (attempt={})\", loopCount, retries);\n          for (int i = 0; i < loopCount; i++) {\n            if (isClosed()) {\n              LOG.info(\"RecoveryStrategy has been closed\");\n              break; // check if someone closed us\n            }\n            Thread.sleep(startingRecoveryDelayMilliSeconds);\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          LOG.warn(\"Recovery was interrupted.\", e);\n          close = true;\n        }\n      }\n\n    }\n\n    // if replay was skipped (possibly to due pulling a full index from the leader),\n    // then we still need to update version bucket seeds after recovery\n    if (successfulRecovery && replayFuture == null) {\n      LOG.info(\"Updating version bucket highest from index after successful recovery.\");\n      core.seedVersionBuckets();\n    }\n\n    LOG.info(\"Finished recovery process, successful=[{}]\", Boolean.toString(successfulRecovery));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7ff8a6fbdc9c2d84bc6e9e71e1c738e49c441ac","date":1503580177,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  final public void doRecovery(SolrCore core) throws Exception {\n    if (core.getCoreDescriptor().getCloudDescriptor().requiresTransactionLog()) {\n      doSyncOrReplicateRecovery(core);\n    } else {\n      doReplicateOnlyRecovery(core);\n    }\n  }\n\n","sourceOld":"  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    if (core.getCoreDescriptor().getCloudDescriptor().requiresTransactionLog()) {\n      doSyncOrReplicateRecovery(core);\n    } else {\n      doReplicateOnlyRecovery(core);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b869898f50ca80263bac2e3ae0949f7700e5c977","date":1503580229,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    if (core.getCoreDescriptor().getCloudDescriptor().requiresTransactionLog()) {\n      doSyncOrReplicateRecovery(core);\n    } else {\n      doReplicateOnlyRecovery(core);\n    }\n  }\n\n","sourceOld":"  final public void doRecovery(SolrCore core) throws Exception {\n    if (core.getCoreDescriptor().getCloudDescriptor().requiresTransactionLog()) {\n      doSyncOrReplicateRecovery(core);\n    } else {\n      doReplicateOnlyRecovery(core);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85212dad4ed576c7f7e6c165ee19e597b7b4efc8","date":1507997740,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  final public void doRecovery(SolrCore core) throws Exception {\n    if (core.getCoreDescriptor().getCloudDescriptor().requiresTransactionLog()) {\n      doSyncOrReplicateRecovery(core);\n    } else {\n      doReplicateOnlyRecovery(core);\n    }\n  }\n\n","sourceOld":"  final public void doRecovery(SolrCore core) throws KeeperException, InterruptedException {\n    if (core.getCoreDescriptor().getCloudDescriptor().requiresTransactionLog()) {\n      doSyncOrReplicateRecovery(core);\n    } else {\n      doReplicateOnlyRecovery(core);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy#doRecovery(SolrCore).mjava","sourceNew":"  final public void doRecovery(SolrCore core) throws Exception {\n    // we can lose our core descriptor, so store it now\n    this.coreDescriptor = core.getCoreDescriptor();\n\n    if (this.coreDescriptor.getCloudDescriptor().requiresTransactionLog()) {\n      doSyncOrReplicateRecovery(core);\n    } else {\n      doReplicateOnlyRecovery(core);\n    }\n  }\n\n","sourceOld":"  final public void doRecovery(SolrCore core) throws Exception {\n    if (core.getCoreDescriptor().getCloudDescriptor().requiresTransactionLog()) {\n      doSyncOrReplicateRecovery(core);\n    } else {\n      doReplicateOnlyRecovery(core);\n    }\n  }\n\n","bugFix":["61c45e99cf6676da48f19d7511c73712ad39402b"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"610f5499a87a7113d53e5b621b616890f002e9f1":["536b47681314488eb94706248be6047a3f142841"],"1b17e61942d57c9c1c6dc8a926bce1c5c47882f8":["c52ad29218ee436d52c57bf0829b98acad9de379"],"97bd2b0da4beced82821b752b29576be986cf1ff":["89b56ee224dbe29ee4436d91a7070ca418ffc4fb"],"89b56ee224dbe29ee4436d91a7070ca418ffc4fb":["1b17e61942d57c9c1c6dc8a926bce1c5c47882f8"],"ff2d7326b1f013c8da9bad45b1e98a3d16c38575":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["d29a9a7485f393c7a581569b5734d8f9054f6a92","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","8de248630034b5863f6795325312ed9b0e697794"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["c7869f64c874ebf7f317d22c00baf2b6857797a6","66c64e8cfded6a585100e6430238faaf416f3fea"],"6c94d2661bc1c14426980ec7882e951fdcff08d0":["15e37b3faa8262545b88208e097ade36ce4e8ba8"],"d29a9a7485f393c7a581569b5734d8f9054f6a92":["56a558aa5aadd60ae850d1ab090098bc63bdfaf9"],"a9186bf60d7c6f504d4d5b01cfee95dc4bd15e53":["3c3f78fc552394c5911fc8e26627b33263967e83"],"d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1":["7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6"],"ef9583322dbc8650e3cd32d936c0b3e31eefadba":["21f23e00493fe5b2bf0678fd0f0b4f4d166f20be"],"5413931a874939770ede57e7f5ce5c64c1d2366c":["be320990bdc77e643388fa801e75017f19289c42"],"5e71cbdfcf34d779dd7e7ba148dfff6022f2005a":["1525b4dfbc0d413b8d7247da232009778e624836"],"a219f1dcad1700e84807666bdbd2b573e8de7021":["6c94d2661bc1c14426980ec7882e951fdcff08d0"],"31e031f53c5dc379346e9162e3703c9a1fdca395":["e53faaf24d27f16d0ec340d9a4109403ed918fcd"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["ab68488225b6a6c357dda72ed11dedca9914a192","61c45e99cf6676da48f19d7511c73712ad39402b"],"40c478fcb4e093ac431407a8db2896ac9c867f33":["ef9583322dbc8650e3cd32d936c0b3e31eefadba"],"aba371508186796cc6151d8223a5b4e16d02e26e":["a5093a9e893633cc091cf2f729d7863671c2b715","e53faaf24d27f16d0ec340d9a4109403ed918fcd"],"8de248630034b5863f6795325312ed9b0e697794":["7241f6cadcc9f475e0bf1eaae71c274e8a07b525"],"14d5815ecbef89580f5c48990bcd433f04f8563a":["fee8f787196eb664b953b851d18c52f0d8c9e157"],"f2126b84bd093fa3d921582a109a0ee578c28126":["a6378064655e76cd7b908b1cab4ce425b384b508","5e71cbdfcf34d779dd7e7ba148dfff6022f2005a"],"be320990bdc77e643388fa801e75017f19289c42":["7392b75d2c2f2aecf31188732a0764fe0dc74ade"],"f56da6f4f15d95f318d2d6ac2a39a9183dfecff2":["97bd2b0da4beced82821b752b29576be986cf1ff"],"ab68488225b6a6c357dda72ed11dedca9914a192":["7392b75d2c2f2aecf31188732a0764fe0dc74ade","f996f8177b9204bdc92f7164460c6cefad9ac99a"],"877f1e09b9299ce0757f4d83768da944803baf04":["439c63ae5d22132fca810a0029a854e97d2c1a3e"],"66c64e8cfded6a585100e6430238faaf416f3fea":["40c478fcb4e093ac431407a8db2896ac9c867f33"],"536b47681314488eb94706248be6047a3f142841":["f49f14d65e01870e494b6f38fa88ea1a8011e4fe"],"85212dad4ed576c7f7e6c165ee19e597b7b4efc8":["61c45e99cf6676da48f19d7511c73712ad39402b"],"e53faaf24d27f16d0ec340d9a4109403ed918fcd":["a5093a9e893633cc091cf2f729d7863671c2b715"],"bb62cc3362417c3e5136f2f26d34a1072ad633eb":["610f5499a87a7113d53e5b621b616890f002e9f1"],"6f3c1f22c5fe0011e187dac3151422365ae857f3":["e8fa677b7b2405d2c2b902646dbae8f5fe34b60e"],"21f23e00493fe5b2bf0678fd0f0b4f4d166f20be":["4d3fe370ea74eec986c7277cfcc216c43d2ca8df"],"05a14b2611ead08655a2b2bdc61632eb31316e57":["c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","6013b4c7388f1627659c8f96c44abd10a294d3a6"],"7241f6cadcc9f475e0bf1eaae71c274e8a07b525":["0fcbbde821566094ce01864ce5f7824a3fe2072e"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["71f14bcc8e12885ab59981c9fdecebb7c2ef3ca0"],"f996f8177b9204bdc92f7164460c6cefad9ac99a":["7392b75d2c2f2aecf31188732a0764fe0dc74ade"],"a5093a9e893633cc091cf2f729d7863671c2b715":["5c76806dcdb4841b4f71ecfe9e9e95147f7201f2"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["aba371508186796cc6151d8223a5b4e16d02e26e","ef9583322dbc8650e3cd32d936c0b3e31eefadba"],"7392b75d2c2f2aecf31188732a0764fe0dc74ade":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["5e71cbdfcf34d779dd7e7ba148dfff6022f2005a","d29a9a7485f393c7a581569b5734d8f9054f6a92"],"0fcbbde821566094ce01864ce5f7824a3fe2072e":["ff2d7326b1f013c8da9bad45b1e98a3d16c38575"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["85212dad4ed576c7f7e6c165ee19e597b7b4efc8"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","66c64e8cfded6a585100e6430238faaf416f3fea"],"e8fa677b7b2405d2c2b902646dbae8f5fe34b60e":["234f41e4e127541225632264295a22ef1aef1d7c"],"c5536d1fc2015d46bbeb8163f6f8b99483c7cc1f":["be320990bdc77e643388fa801e75017f19289c42","5413931a874939770ede57e7f5ce5c64c1d2366c"],"f49f14d65e01870e494b6f38fa88ea1a8011e4fe":["a9186bf60d7c6f504d4d5b01cfee95dc4bd15e53"],"439c63ae5d22132fca810a0029a854e97d2c1a3e":["a219f1dcad1700e84807666bdbd2b573e8de7021"],"6013b4c7388f1627659c8f96c44abd10a294d3a6":["66c64e8cfded6a585100e6430238faaf416f3fea"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["e8fa677b7b2405d2c2b902646dbae8f5fe34b60e","6c94d2661bc1c14426980ec7882e951fdcff08d0"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["8fd5be977c105554c6a7b68afcdbc511439723ab","40c478fcb4e093ac431407a8db2896ac9c867f33"],"5c76806dcdb4841b4f71ecfe9e9e95147f7201f2":["e1e5b576a150f260cba5c0287b3764e42cab2fe7"],"e1e5b576a150f260cba5c0287b3764e42cab2fe7":["e99829242bceda4cf974ec0eb5d82d713615b3da"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["610f5499a87a7113d53e5b621b616890f002e9f1","bb62cc3362417c3e5136f2f26d34a1072ad633eb"],"3599646b4d4c346cf74d334813488b8b337b5bf5":["8de248630034b5863f6795325312ed9b0e697794","5c76806dcdb4841b4f71ecfe9e9e95147f7201f2"],"56a558aa5aadd60ae850d1ab090098bc63bdfaf9":["5e71cbdfcf34d779dd7e7ba148dfff6022f2005a"],"e99829242bceda4cf974ec0eb5d82d713615b3da":["8de248630034b5863f6795325312ed9b0e697794"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["d6f074e73200c07d54f242d3880a8da5a35ff97b","40c478fcb4e093ac431407a8db2896ac9c867f33"],"234f41e4e127541225632264295a22ef1aef1d7c":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["f56da6f4f15d95f318d2d6ac2a39a9183dfecff2"],"61c45e99cf6676da48f19d7511c73712ad39402b":["c5536d1fc2015d46bbeb8163f6f8b99483c7cc1f"],"c7ff8a6fbdc9c2d84bc6e9e71e1c738e49c441ac":["e9017cf144952056066919f1ebc7897ff9bd71b1"],"7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6":["6013b4c7388f1627659c8f96c44abd10a294d3a6"],"4d3fe370ea74eec986c7277cfcc216c43d2ca8df":["31e031f53c5dc379346e9162e3703c9a1fdca395"],"fa64435b5902ce266c23755a4a00691a3285dab8":["d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1"],"b869898f50ca80263bac2e3ae0949f7700e5c977":["c7ff8a6fbdc9c2d84bc6e9e71e1c738e49c441ac"],"8fd5be977c105554c6a7b68afcdbc511439723ab":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","21f23e00493fe5b2bf0678fd0f0b4f4d166f20be"],"15e37b3faa8262545b88208e097ade36ce4e8ba8":["6f3c1f22c5fe0011e187dac3151422365ae857f3"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["610f5499a87a7113d53e5b621b616890f002e9f1","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"fee8f787196eb664b953b851d18c52f0d8c9e157":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"71f14bcc8e12885ab59981c9fdecebb7c2ef3ca0":["d29a9a7485f393c7a581569b5734d8f9054f6a92"],"a6378064655e76cd7b908b1cab4ce425b384b508":["fa64435b5902ce266c23755a4a00691a3285dab8"],"3c3f78fc552394c5911fc8e26627b33263967e83":["877f1e09b9299ce0757f4d83768da944803baf04"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["a5093a9e893633cc091cf2f729d7863671c2b715","31e031f53c5dc379346e9162e3703c9a1fdca395"],"c52ad29218ee436d52c57bf0829b98acad9de379":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"1525b4dfbc0d413b8d7247da232009778e624836":["a6378064655e76cd7b908b1cab4ce425b384b508"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"]},"commit2Childs":{"610f5499a87a7113d53e5b621b616890f002e9f1":["bb62cc3362417c3e5136f2f26d34a1072ad633eb","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"1b17e61942d57c9c1c6dc8a926bce1c5c47882f8":["89b56ee224dbe29ee4436d91a7070ca418ffc4fb"],"97bd2b0da4beced82821b752b29576be986cf1ff":["f56da6f4f15d95f318d2d6ac2a39a9183dfecff2"],"89b56ee224dbe29ee4436d91a7070ca418ffc4fb":["97bd2b0da4beced82821b752b29576be986cf1ff"],"ff2d7326b1f013c8da9bad45b1e98a3d16c38575":["0fcbbde821566094ce01864ce5f7824a3fe2072e"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"6c94d2661bc1c14426980ec7882e951fdcff08d0":["a219f1dcad1700e84807666bdbd2b573e8de7021","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"d29a9a7485f393c7a581569b5734d8f9054f6a92":["37a0f60745e53927c4c876cfe5b5a58170f0646c","d3fcb70cf561547c7bb1506e0cf32ca7b1287064","71f14bcc8e12885ab59981c9fdecebb7c2ef3ca0"],"a9186bf60d7c6f504d4d5b01cfee95dc4bd15e53":["f49f14d65e01870e494b6f38fa88ea1a8011e4fe"],"d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1":["fa64435b5902ce266c23755a4a00691a3285dab8"],"ef9583322dbc8650e3cd32d936c0b3e31eefadba":["40c478fcb4e093ac431407a8db2896ac9c867f33","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"5413931a874939770ede57e7f5ce5c64c1d2366c":["c5536d1fc2015d46bbeb8163f6f8b99483c7cc1f"],"5e71cbdfcf34d779dd7e7ba148dfff6022f2005a":["f2126b84bd093fa3d921582a109a0ee578c28126","d3fcb70cf561547c7bb1506e0cf32ca7b1287064","56a558aa5aadd60ae850d1ab090098bc63bdfaf9"],"a219f1dcad1700e84807666bdbd2b573e8de7021":["439c63ae5d22132fca810a0029a854e97d2c1a3e"],"31e031f53c5dc379346e9162e3703c9a1fdca395":["4d3fe370ea74eec986c7277cfcc216c43d2ca8df","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["c7ff8a6fbdc9c2d84bc6e9e71e1c738e49c441ac"],"40c478fcb4e093ac431407a8db2896ac9c867f33":["66c64e8cfded6a585100e6430238faaf416f3fea","c7869f64c874ebf7f317d22c00baf2b6857797a6","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"aba371508186796cc6151d8223a5b4e16d02e26e":["d6f074e73200c07d54f242d3880a8da5a35ff97b"],"8de248630034b5863f6795325312ed9b0e697794":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","3599646b4d4c346cf74d334813488b8b337b5bf5","e99829242bceda4cf974ec0eb5d82d713615b3da"],"14d5815ecbef89580f5c48990bcd433f04f8563a":["234f41e4e127541225632264295a22ef1aef1d7c"],"f2126b84bd093fa3d921582a109a0ee578c28126":[],"be320990bdc77e643388fa801e75017f19289c42":["5413931a874939770ede57e7f5ce5c64c1d2366c","c5536d1fc2015d46bbeb8163f6f8b99483c7cc1f"],"ab68488225b6a6c357dda72ed11dedca9914a192":["e9017cf144952056066919f1ebc7897ff9bd71b1"],"f56da6f4f15d95f318d2d6ac2a39a9183dfecff2":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"877f1e09b9299ce0757f4d83768da944803baf04":["3c3f78fc552394c5911fc8e26627b33263967e83"],"66c64e8cfded6a585100e6430238faaf416f3fea":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","6013b4c7388f1627659c8f96c44abd10a294d3a6"],"536b47681314488eb94706248be6047a3f142841":["610f5499a87a7113d53e5b621b616890f002e9f1"],"e53faaf24d27f16d0ec340d9a4109403ed918fcd":["31e031f53c5dc379346e9162e3703c9a1fdca395","aba371508186796cc6151d8223a5b4e16d02e26e"],"85212dad4ed576c7f7e6c165ee19e597b7b4efc8":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"bb62cc3362417c3e5136f2f26d34a1072ad633eb":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"21f23e00493fe5b2bf0678fd0f0b4f4d166f20be":["ef9583322dbc8650e3cd32d936c0b3e31eefadba","8fd5be977c105554c6a7b68afcdbc511439723ab"],"6f3c1f22c5fe0011e187dac3151422365ae857f3":["15e37b3faa8262545b88208e097ade36ce4e8ba8"],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"7241f6cadcc9f475e0bf1eaae71c274e8a07b525":["8de248630034b5863f6795325312ed9b0e697794"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c52ad29218ee436d52c57bf0829b98acad9de379"],"f996f8177b9204bdc92f7164460c6cefad9ac99a":["ab68488225b6a6c357dda72ed11dedca9914a192"],"a5093a9e893633cc091cf2f729d7863671c2b715":["aba371508186796cc6151d8223a5b4e16d02e26e","e53faaf24d27f16d0ec340d9a4109403ed918fcd","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"7392b75d2c2f2aecf31188732a0764fe0dc74ade":["be320990bdc77e643388fa801e75017f19289c42","ab68488225b6a6c357dda72ed11dedca9914a192","f996f8177b9204bdc92f7164460c6cefad9ac99a"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"0fcbbde821566094ce01864ce5f7824a3fe2072e":["7241f6cadcc9f475e0bf1eaae71c274e8a07b525"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["05a14b2611ead08655a2b2bdc61632eb31316e57"],"e8fa677b7b2405d2c2b902646dbae8f5fe34b60e":["6f3c1f22c5fe0011e187dac3151422365ae857f3","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"c5536d1fc2015d46bbeb8163f6f8b99483c7cc1f":["61c45e99cf6676da48f19d7511c73712ad39402b"],"f49f14d65e01870e494b6f38fa88ea1a8011e4fe":["536b47681314488eb94706248be6047a3f142841"],"439c63ae5d22132fca810a0029a854e97d2c1a3e":["877f1e09b9299ce0757f4d83768da944803baf04"],"6013b4c7388f1627659c8f96c44abd10a294d3a6":["05a14b2611ead08655a2b2bdc61632eb31316e57","7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ff2d7326b1f013c8da9bad45b1e98a3d16c38575","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab"],"5c76806dcdb4841b4f71ecfe9e9e95147f7201f2":["a5093a9e893633cc091cf2f729d7863671c2b715","3599646b4d4c346cf74d334813488b8b337b5bf5"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["7392b75d2c2f2aecf31188732a0764fe0dc74ade","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"e1e5b576a150f260cba5c0287b3764e42cab2fe7":["5c76806dcdb4841b4f71ecfe9e9e95147f7201f2"],"3599646b4d4c346cf74d334813488b8b337b5bf5":[],"56a558aa5aadd60ae850d1ab090098bc63bdfaf9":["d29a9a7485f393c7a581569b5734d8f9054f6a92"],"e99829242bceda4cf974ec0eb5d82d713615b3da":["e1e5b576a150f260cba5c0287b3764e42cab2fe7"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"234f41e4e127541225632264295a22ef1aef1d7c":["e8fa677b7b2405d2c2b902646dbae8f5fe34b60e"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["fee8f787196eb664b953b851d18c52f0d8c9e157"],"61c45e99cf6676da48f19d7511c73712ad39402b":["e9017cf144952056066919f1ebc7897ff9bd71b1","85212dad4ed576c7f7e6c165ee19e597b7b4efc8"],"c7ff8a6fbdc9c2d84bc6e9e71e1c738e49c441ac":["b869898f50ca80263bac2e3ae0949f7700e5c977"],"7157fdda82ae1a1cb77b012ef0eb72249ee2f3d6":["d8469c0c0b45f7e3da74918efc8ede2ad2efe2b1"],"4d3fe370ea74eec986c7277cfcc216c43d2ca8df":["21f23e00493fe5b2bf0678fd0f0b4f4d166f20be"],"fa64435b5902ce266c23755a4a00691a3285dab8":["a6378064655e76cd7b908b1cab4ce425b384b508"],"b869898f50ca80263bac2e3ae0949f7700e5c977":[],"8fd5be977c105554c6a7b68afcdbc511439723ab":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"15e37b3faa8262545b88208e097ade36ce4e8ba8":["6c94d2661bc1c14426980ec7882e951fdcff08d0"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"fee8f787196eb664b953b851d18c52f0d8c9e157":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"71f14bcc8e12885ab59981c9fdecebb7c2ef3ca0":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"a6378064655e76cd7b908b1cab4ce425b384b508":["f2126b84bd093fa3d921582a109a0ee578c28126","1525b4dfbc0d413b8d7247da232009778e624836"],"3c3f78fc552394c5911fc8e26627b33263967e83":["a9186bf60d7c6f504d4d5b01cfee95dc4bd15e53"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["8fd5be977c105554c6a7b68afcdbc511439723ab"],"c52ad29218ee436d52c57bf0829b98acad9de379":["1b17e61942d57c9c1c6dc8a926bce1c5c47882f8"],"1525b4dfbc0d413b8d7247da232009778e624836":["5e71cbdfcf34d779dd7e7ba148dfff6022f2005a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","b05c56a41b733e02a189c48895922b5bd8c7f3d1","f2126b84bd093fa3d921582a109a0ee578c28126","05a14b2611ead08655a2b2bdc61632eb31316e57","d3fcb70cf561547c7bb1506e0cf32ca7b1287064","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","3599646b4d4c346cf74d334813488b8b337b5bf5","b869898f50ca80263bac2e3ae0949f7700e5c977","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}