{"path":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  static byte[] getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random, dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random, 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random.nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    DocValues dv1 = MultiDocValues.getNormDocValues(ir1, field);\n    byte[] norms1 = dv1 == null ? null : (byte[]) dv1.getSource().getArray();\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    DocValues dv2 = getOnlySegmentReader(ir2).normValues(field);\n    byte[] norms2 = dv2 == null ? null : (byte[]) dv2.getSource().getArray();\n    \n    assertArrayEquals(norms1, norms2);\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  static byte[] getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random, dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random, 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random.nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    DocValues dv1 = MultiDocValues.getNormDocValues(ir1, field);\n    byte[] norms1 = dv1 == null ? null : (byte[]) dv1.getSource().getArray();\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    DocValues dv2 = getOnlySegmentReader(ir2).normValues(field);\n    byte[] norms2 = dv2 == null ? null : (byte[]) dv2.getSource().getArray();\n    \n    assertArrayEquals(norms1, norms2);\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  byte[] getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    DocValues dv1 = MultiDocValues.getNormDocValues(ir1, field);\n    byte[] norms1 = dv1 == null ? null : (byte[]) dv1.getSource().getArray();\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    DocValues dv2 = getOnlySegmentReader(ir2).normValues(field);\n    byte[] norms2 = dv2 == null ? null : (byte[]) dv2.getSource().getArray();\n    \n    assertArrayEquals(norms1, norms2);\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  static byte[] getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random, dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random, 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random.nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    DocValues dv1 = MultiDocValues.getNormDocValues(ir1, field);\n    byte[] norms1 = dv1 == null ? null : (byte[]) dv1.getSource().getArray();\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    DocValues dv2 = getOnlySegmentReader(ir2).normValues(field);\n    byte[] norms2 = dv2 == null ? null : (byte[]) dv2.getSource().getArray();\n    \n    assertArrayEquals(norms1, norms2);\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad7de846867bd14c63f9dd19df082f72c5ea9c54","date":1355517454,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    // nocommit remove\n    Assume.assumeTrue(_TestUtil.canUseSimpleNorms());\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiSimpleDocValues.simpleNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).simpleNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      assertEquals(norms1.size(), norms2.size());\n      for(int docID=0;docID<norms1.size();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  byte[] getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    DocValues dv1 = MultiDocValues.getNormDocValues(ir1, field);\n    byte[] norms1 = dv1 == null ? null : (byte[]) dv1.getSource().getArray();\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    DocValues dv2 = getOnlySegmentReader(ir2).normValues(field);\n    byte[] norms2 = dv2 == null ? null : (byte[]) dv2.getSource().getArray();\n    \n    assertArrayEquals(norms1, norms2);\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"32608e0a08e76fe8668cd1dcca0e7a8f6d7f3f0a","date":1357739321,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    // nocommit remove\n    Assume.assumeTrue(_TestUtil.canUseSimpleNorms());\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiSimpleDocValues.simpleNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).simpleNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    // nocommit remove\n    Assume.assumeTrue(_TestUtil.canUseSimpleNorms());\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiSimpleDocValues.simpleNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).simpleNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      assertEquals(norms1.size(), norms2.size());\n      for(int docID=0;docID<norms1.size();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0837ab0472feecb3a54260729d845f839e1cbd72","date":1358283639,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiSimpleDocValues.simpleNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).simpleNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    // nocommit remove\n    Assume.assumeTrue(_TestUtil.canUseSimpleNorms());\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiSimpleDocValues.simpleNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).simpleNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b8acf0807ca5f38beda8e0f7d5ab46ff39f81200","date":1358521790,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiSimpleDocValues.simpleNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).simpleNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  byte[] getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    DocValues dv1 = MultiDocValues.getNormDocValues(ir1, field);\n    byte[] norms1 = dv1 == null ? null : (byte[]) dv1.getSource().getArray();\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    DocValues dv2 = getOnlySegmentReader(ir2).normValues(field);\n    byte[] norms2 = dv2 == null ? null : (byte[]) dv2.getSource().getArray();\n    \n    assertArrayEquals(norms1, norms2);\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":["0f4b223b56d0c7927ae8baced5e1b1dd4c693b1d","629c38c4ae4e303d0617e05fbfe508140b32f0a3","7a54e23e03b47f3d568ab3020bdd386e4b2f0a05","5af906d697b0a05f75caf94b03a28adb76579c66"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = _TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.shutdown();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.shutdown();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.shutdown();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.shutdown();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","date":1457644139,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlyLeafReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":["b8acf0807ca5f38beda8e0f7d5ab46ff39f81200"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlyLeafReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      while (true) {\n        int norms1DocID = norms1.nextDoc();\n        int norms2DocID = norms2.nextDoc();\n        while (norms1DocID < norms2DocID) {\n          assertEquals(0, norms1.longValue());\n          norms1DocID = norms1.nextDoc();\n        }\n        while (norms2DocID < norms1DocID) {\n          assertEquals(0, norms2.longValue());\n          norms2DocID = norms2.nextDoc();\n        }\n        if (norms1.docID() == NO_MORE_DOCS) {\n          break;\n        }\n        assertEquals(norms1.longValue(), norms2.longValue());\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlyLeafReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlyLeafReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      while (true) {\n        int norms1DocID = norms1.nextDoc();\n        int norms2DocID = norms2.nextDoc();\n        while (norms1DocID < norms2DocID) {\n          assertEquals(0, norms1.longValue());\n          norms1DocID = norms1.nextDoc();\n        }\n        while (norms2DocID < norms1DocID) {\n          assertEquals(0, norms2.longValue());\n          norms2DocID = norms2.nextDoc();\n        }\n        if (norms1.docID() == NO_MORE_DOCS) {\n          break;\n        }\n        assertEquals(norms1.longValue(), norms2.longValue());\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlyLeafReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestOmitNorms#getNorms(String,Field,Field).mjava","sourceNew":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlyLeafReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      while (true) {\n        int norms1DocID = norms1.nextDoc();\n        int norms2DocID = norms2.nextDoc();\n        while (norms1DocID < norms2DocID) {\n          assertEquals(0, norms1.longValue());\n          norms1DocID = norms1.nextDoc();\n        }\n        while (norms2DocID < norms1DocID) {\n          assertEquals(0, norms2.longValue());\n          norms2DocID = norms2.nextDoc();\n        }\n        if (norms1.docID() == NO_MORE_DOCS) {\n          break;\n        }\n        assertEquals(norms1.longValue(), norms2.longValue());\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","sourceOld":"  /**\n   * Indexes at least 1 document with f1, and at least 1 document with f2.\n   * returns the norms for \"field\".\n   */\n  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n                              .setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n    \n    // add f1\n    Document d = new Document();\n    d.add(f1);\n    riw.addDocument(d);\n    \n    // add f2\n    d = new Document();\n    d.add(f2);\n    riw.addDocument(d);\n    \n    // add a mix of f1's and f2's\n    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n    for (int i = 0; i < numExtraDocs; i++) {\n      d = new Document();\n      d.add(random().nextBoolean() ? f1 : f2);\n      riw.addDocument(d);\n    }\n\n    IndexReader ir1 = riw.getReader();\n    // todo: generalize\n    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n    \n    // fully merge and validate MultiNorms against single segment.\n    riw.forceMerge(1);\n    DirectoryReader ir2 = riw.getReader();\n    NumericDocValues norms2 = getOnlyLeafReader(ir2).getNormValues(field);\n\n    if (norms1 == null) {\n      assertNull(norms2);\n    } else {\n      for(int docID=0;docID<ir1.maxDoc();docID++) {\n        assertEquals(norms1.get(docID), norms2.get(docID));\n      }\n    }\n    ir1.close();\n    ir2.close();\n    riw.close();\n    dir.close();\n    return norms1;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"32608e0a08e76fe8668cd1dcca0e7a8f6d7f3f0a":["ad7de846867bd14c63f9dd19df082f72c5ea9c54"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"b8acf0807ca5f38beda8e0f7d5ab46ff39f81200":["0837ab0472feecb3a54260729d845f839e1cbd72"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","b8acf0807ca5f38beda8e0f7d5ab46ff39f81200"],"ad7de846867bd14c63f9dd19df082f72c5ea9c54":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"0837ab0472feecb3a54260729d845f839e1cbd72":["32608e0a08e76fe8668cd1dcca0e7a8f6d7f3f0a"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"32608e0a08e76fe8668cd1dcca0e7a8f6d7f3f0a":["0837ab0472feecb3a54260729d845f839e1cbd72"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"b8acf0807ca5f38beda8e0f7d5ab46ff39f81200":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["6613659748fe4411a7dcf85266e55db1f95f7315"],"ad7de846867bd14c63f9dd19df082f72c5ea9c54":["32608e0a08e76fe8668cd1dcca0e7a8f6d7f3f0a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"0837ab0472feecb3a54260729d845f839e1cbd72":["b8acf0807ca5f38beda8e0f7d5ab46ff39f81200"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["d4d69c535930b5cce125cff868d40f6373dc27d4","ad7de846867bd14c63f9dd19df082f72c5ea9c54"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}