{"path":"solr/core/src/java/org/apache/solr/internal/csv/CSVParser#nextToken(Token).mjava","commits":[{"id":"80931fa1f3b04650bde7b20e61b02063a7093ced","date":1333130683,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/internal/csv/CSVParser#nextToken(Token).mjava","pathOld":"/dev/null","sourceNew":" /**\n   * Returns the next token.\n   * \n   * A token corresponds to a term, a record change or an\n   * end-of-file indicator.\n   * \n   * @param tkn an existing Token object to reuse. The caller is responsible to initialize the\n   * Token.\n   * @return the next token found\n   * @throws IOException on stream access error\n   */\n  protected Token nextToken(Token tkn) throws IOException {\n    wsBuf.clear(); // resuse\n    \n    // get the last read char (required for empty line detection)\n    int lastChar = in.readAgain();\n    \n    //  read the next char and set eol\n    /* note: unfourtunately isEndOfLine may consumes a character silently.\n     *       this has no effect outside of the method. so a simple workaround\n     *       is to call 'readAgain' on the stream...\n     *       uh: might using objects instead of base-types (jdk1.5 autoboxing!)\n     */\n    int c = in.read();\n    boolean eol = isEndOfLine(c);\n    c = in.readAgain();\n     \n    //  empty line detection: eol AND (last char was EOL or beginning)\n    while (strategy.getIgnoreEmptyLines() && eol \n      && (lastChar == '\\n' \n      || lastChar == ExtendedBufferedReader.UNDEFINED) \n      && !isEndOfFile(lastChar)) {\n      // go on char ahead ...\n      lastChar = c;\n      c = in.read();\n      eol = isEndOfLine(c);\n      c = in.readAgain();\n      // reached end of file without any content (empty line at the end)\n      if (isEndOfFile(c)) {\n        tkn.type = TT_EOF;\n        return tkn;\n      }\n    }\n\n    // did we reached eof during the last iteration already ? TT_EOF\n    if (isEndOfFile(lastChar) || (lastChar != strategy.getDelimiter() && isEndOfFile(c))) {\n      tkn.type = TT_EOF;\n      return tkn;\n    } \n    \n    //  important: make sure a new char gets consumed in each iteration\n    while (!tkn.isReady && tkn.type != TT_EOF) {\n      // ignore whitespaces at beginning of a token\n      while (strategy.getIgnoreLeadingWhitespaces() && isWhitespace(c) && !eol) {\n        wsBuf.append((char) c);\n        c = in.read();\n        eol = isEndOfLine(c);\n      }\n      // ok, start of token reached: comment, encapsulated, or token\n      if (c == strategy.getCommentStart()) {\n        // ignore everything till end of line and continue (incr linecount)\n        in.readLine();\n        tkn = nextToken(tkn.reset());\n      } else if (c == strategy.getDelimiter()) {\n        // empty token return TT_TOKEN(\"\")\n        tkn.type = TT_TOKEN;\n        tkn.isReady = true;\n      } else if (eol) {\n        // empty token return TT_EORECORD(\"\")\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EORECORD;\n        tkn.isReady = true;\n      } else if (c == strategy.getEncapsulator()) {\n        // consume encapsulated token\n        encapsulatedTokenLexer(tkn, c);\n      } else if (isEndOfFile(c)) {\n        // end of file return TT_EOF()\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EOF;\n        tkn.isReady = true;\n      } else {\n        // next token must be a simple token\n        // add removed blanks when not ignoring whitespace chars...\n        if (!strategy.getIgnoreLeadingWhitespaces()) {\n          tkn.content.append(wsBuf);\n        }\n        simpleTokenLexer(tkn, c);\n      }\n    }\n    return tkn;  \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0158ced21948b6626f733c1c42c1e18d94449789","date":1462994341,"type":3,"author":"Bartosz Krasi≈Ñski","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/internal/csv/CSVParser#nextToken(Token).mjava","pathOld":"solr/core/src/java/org/apache/solr/internal/csv/CSVParser#nextToken(Token).mjava","sourceNew":" /**\n   * Returns the next token.\n   * \n   * A token corresponds to a term, a record change or an\n   * end-of-file indicator.\n   * \n   * @param tkn an existing Token object to reuse. The caller is responsible to initialize the\n   * Token.\n   * @return the next token found\n   * @throws IOException on stream access error\n   */\n  protected Token nextToken(Token tkn) throws IOException {\n    wsBuf.clear(); // resuse\n    \n    // get the last read char (required for empty line detection)\n    int lastChar = in.readAgain();\n    \n    //  read the next char and set eol\n    /* note: unfortunately isEndOfLine may consumes a character silently.\n     *       this has no effect outside of the method. so a simple workaround\n     *       is to call 'readAgain' on the stream...\n     *       uh: might using objects instead of base-types (jdk1.5 autoboxing!)\n     */\n    int c = in.read();\n    boolean eol = isEndOfLine(c);\n    c = in.readAgain();\n     \n    //  empty line detection: eol AND (last char was EOL or beginning)\n    while (strategy.getIgnoreEmptyLines() && eol \n      && (lastChar == '\\n' \n      || lastChar == ExtendedBufferedReader.UNDEFINED) \n      && !isEndOfFile(lastChar)) {\n      // go on char ahead ...\n      lastChar = c;\n      c = in.read();\n      eol = isEndOfLine(c);\n      c = in.readAgain();\n      // reached end of file without any content (empty line at the end)\n      if (isEndOfFile(c)) {\n        tkn.type = TT_EOF;\n        return tkn;\n      }\n    }\n\n    // did we reached eof during the last iteration already ? TT_EOF\n    if (isEndOfFile(lastChar) || (lastChar != strategy.getDelimiter() && isEndOfFile(c))) {\n      tkn.type = TT_EOF;\n      return tkn;\n    } \n    \n    //  important: make sure a new char gets consumed in each iteration\n    while (!tkn.isReady && tkn.type != TT_EOF) {\n      // ignore whitespaces at beginning of a token\n      while (strategy.getIgnoreLeadingWhitespaces() && isWhitespace(c) && !eol) {\n        wsBuf.append((char) c);\n        c = in.read();\n        eol = isEndOfLine(c);\n      }\n      // ok, start of token reached: comment, encapsulated, or token\n      if (c == strategy.getCommentStart()) {\n        // ignore everything till end of line and continue (incr linecount)\n        in.readLine();\n        tkn = nextToken(tkn.reset());\n      } else if (c == strategy.getDelimiter()) {\n        // empty token return TT_TOKEN(\"\")\n        tkn.type = TT_TOKEN;\n        tkn.isReady = true;\n      } else if (eol) {\n        // empty token return TT_EORECORD(\"\")\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EORECORD;\n        tkn.isReady = true;\n      } else if (c == strategy.getEncapsulator()) {\n        // consume encapsulated token\n        encapsulatedTokenLexer(tkn, c);\n      } else if (isEndOfFile(c)) {\n        // end of file return TT_EOF()\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EOF;\n        tkn.isReady = true;\n      } else {\n        // next token must be a simple token\n        // add removed blanks when not ignoring whitespace chars...\n        if (!strategy.getIgnoreLeadingWhitespaces()) {\n          tkn.content.append(wsBuf);\n        }\n        simpleTokenLexer(tkn, c);\n      }\n    }\n    return tkn;  \n  }\n\n","sourceOld":" /**\n   * Returns the next token.\n   * \n   * A token corresponds to a term, a record change or an\n   * end-of-file indicator.\n   * \n   * @param tkn an existing Token object to reuse. The caller is responsible to initialize the\n   * Token.\n   * @return the next token found\n   * @throws IOException on stream access error\n   */\n  protected Token nextToken(Token tkn) throws IOException {\n    wsBuf.clear(); // resuse\n    \n    // get the last read char (required for empty line detection)\n    int lastChar = in.readAgain();\n    \n    //  read the next char and set eol\n    /* note: unfourtunately isEndOfLine may consumes a character silently.\n     *       this has no effect outside of the method. so a simple workaround\n     *       is to call 'readAgain' on the stream...\n     *       uh: might using objects instead of base-types (jdk1.5 autoboxing!)\n     */\n    int c = in.read();\n    boolean eol = isEndOfLine(c);\n    c = in.readAgain();\n     \n    //  empty line detection: eol AND (last char was EOL or beginning)\n    while (strategy.getIgnoreEmptyLines() && eol \n      && (lastChar == '\\n' \n      || lastChar == ExtendedBufferedReader.UNDEFINED) \n      && !isEndOfFile(lastChar)) {\n      // go on char ahead ...\n      lastChar = c;\n      c = in.read();\n      eol = isEndOfLine(c);\n      c = in.readAgain();\n      // reached end of file without any content (empty line at the end)\n      if (isEndOfFile(c)) {\n        tkn.type = TT_EOF;\n        return tkn;\n      }\n    }\n\n    // did we reached eof during the last iteration already ? TT_EOF\n    if (isEndOfFile(lastChar) || (lastChar != strategy.getDelimiter() && isEndOfFile(c))) {\n      tkn.type = TT_EOF;\n      return tkn;\n    } \n    \n    //  important: make sure a new char gets consumed in each iteration\n    while (!tkn.isReady && tkn.type != TT_EOF) {\n      // ignore whitespaces at beginning of a token\n      while (strategy.getIgnoreLeadingWhitespaces() && isWhitespace(c) && !eol) {\n        wsBuf.append((char) c);\n        c = in.read();\n        eol = isEndOfLine(c);\n      }\n      // ok, start of token reached: comment, encapsulated, or token\n      if (c == strategy.getCommentStart()) {\n        // ignore everything till end of line and continue (incr linecount)\n        in.readLine();\n        tkn = nextToken(tkn.reset());\n      } else if (c == strategy.getDelimiter()) {\n        // empty token return TT_TOKEN(\"\")\n        tkn.type = TT_TOKEN;\n        tkn.isReady = true;\n      } else if (eol) {\n        // empty token return TT_EORECORD(\"\")\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EORECORD;\n        tkn.isReady = true;\n      } else if (c == strategy.getEncapsulator()) {\n        // consume encapsulated token\n        encapsulatedTokenLexer(tkn, c);\n      } else if (isEndOfFile(c)) {\n        // end of file return TT_EOF()\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EOF;\n        tkn.isReady = true;\n      } else {\n        // next token must be a simple token\n        // add removed blanks when not ignoring whitespace chars...\n        if (!strategy.getIgnoreLeadingWhitespaces()) {\n          tkn.content.append(wsBuf);\n        }\n        simpleTokenLexer(tkn, c);\n      }\n    }\n    return tkn;  \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/internal/csv/CSVParser#nextToken(Token).mjava","pathOld":"solr/core/src/java/org/apache/solr/internal/csv/CSVParser#nextToken(Token).mjava","sourceNew":" /**\n   * Returns the next token.\n   * \n   * A token corresponds to a term, a record change or an\n   * end-of-file indicator.\n   * \n   * @param tkn an existing Token object to reuse. The caller is responsible to initialize the\n   * Token.\n   * @return the next token found\n   * @throws IOException on stream access error\n   */\n  protected Token nextToken(Token tkn) throws IOException {\n    wsBuf.clear(); // resuse\n    \n    // get the last read char (required for empty line detection)\n    int lastChar = in.readAgain();\n    \n    //  read the next char and set eol\n    /* note: unfortunately isEndOfLine may consumes a character silently.\n     *       this has no effect outside of the method. so a simple workaround\n     *       is to call 'readAgain' on the stream...\n     *       uh: might using objects instead of base-types (jdk1.5 autoboxing!)\n     */\n    int c = in.read();\n    boolean eol = isEndOfLine(c);\n    c = in.readAgain();\n     \n    //  empty line detection: eol AND (last char was EOL or beginning)\n    while (strategy.getIgnoreEmptyLines() && eol \n      && (lastChar == '\\n' \n      || lastChar == ExtendedBufferedReader.UNDEFINED) \n      && !isEndOfFile(lastChar)) {\n      // go on char ahead ...\n      lastChar = c;\n      c = in.read();\n      eol = isEndOfLine(c);\n      c = in.readAgain();\n      // reached end of file without any content (empty line at the end)\n      if (isEndOfFile(c)) {\n        tkn.type = TT_EOF;\n        return tkn;\n      }\n    }\n\n    // did we reached eof during the last iteration already ? TT_EOF\n    if (isEndOfFile(lastChar) || (lastChar != strategy.getDelimiter() && isEndOfFile(c))) {\n      tkn.type = TT_EOF;\n      return tkn;\n    } \n    \n    //  important: make sure a new char gets consumed in each iteration\n    while (!tkn.isReady && tkn.type != TT_EOF) {\n      // ignore whitespaces at beginning of a token\n      while (strategy.getIgnoreLeadingWhitespaces() && isWhitespace(c) && !eol) {\n        wsBuf.append((char) c);\n        c = in.read();\n        eol = isEndOfLine(c);\n      }\n      // ok, start of token reached: comment, encapsulated, or token\n      if (c == strategy.getCommentStart()) {\n        // ignore everything till end of line and continue (incr linecount)\n        in.readLine();\n        tkn = nextToken(tkn.reset());\n      } else if (c == strategy.getDelimiter()) {\n        // empty token return TT_TOKEN(\"\")\n        tkn.type = TT_TOKEN;\n        tkn.isReady = true;\n      } else if (eol) {\n        // empty token return TT_EORECORD(\"\")\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EORECORD;\n        tkn.isReady = true;\n      } else if (c == strategy.getEncapsulator()) {\n        // consume encapsulated token\n        encapsulatedTokenLexer(tkn, c);\n      } else if (isEndOfFile(c)) {\n        // end of file return TT_EOF()\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EOF;\n        tkn.isReady = true;\n      } else {\n        // next token must be a simple token\n        // add removed blanks when not ignoring whitespace chars...\n        if (!strategy.getIgnoreLeadingWhitespaces()) {\n          tkn.content.append(wsBuf);\n        }\n        simpleTokenLexer(tkn, c);\n      }\n    }\n    return tkn;  \n  }\n\n","sourceOld":" /**\n   * Returns the next token.\n   * \n   * A token corresponds to a term, a record change or an\n   * end-of-file indicator.\n   * \n   * @param tkn an existing Token object to reuse. The caller is responsible to initialize the\n   * Token.\n   * @return the next token found\n   * @throws IOException on stream access error\n   */\n  protected Token nextToken(Token tkn) throws IOException {\n    wsBuf.clear(); // resuse\n    \n    // get the last read char (required for empty line detection)\n    int lastChar = in.readAgain();\n    \n    //  read the next char and set eol\n    /* note: unfourtunately isEndOfLine may consumes a character silently.\n     *       this has no effect outside of the method. so a simple workaround\n     *       is to call 'readAgain' on the stream...\n     *       uh: might using objects instead of base-types (jdk1.5 autoboxing!)\n     */\n    int c = in.read();\n    boolean eol = isEndOfLine(c);\n    c = in.readAgain();\n     \n    //  empty line detection: eol AND (last char was EOL or beginning)\n    while (strategy.getIgnoreEmptyLines() && eol \n      && (lastChar == '\\n' \n      || lastChar == ExtendedBufferedReader.UNDEFINED) \n      && !isEndOfFile(lastChar)) {\n      // go on char ahead ...\n      lastChar = c;\n      c = in.read();\n      eol = isEndOfLine(c);\n      c = in.readAgain();\n      // reached end of file without any content (empty line at the end)\n      if (isEndOfFile(c)) {\n        tkn.type = TT_EOF;\n        return tkn;\n      }\n    }\n\n    // did we reached eof during the last iteration already ? TT_EOF\n    if (isEndOfFile(lastChar) || (lastChar != strategy.getDelimiter() && isEndOfFile(c))) {\n      tkn.type = TT_EOF;\n      return tkn;\n    } \n    \n    //  important: make sure a new char gets consumed in each iteration\n    while (!tkn.isReady && tkn.type != TT_EOF) {\n      // ignore whitespaces at beginning of a token\n      while (strategy.getIgnoreLeadingWhitespaces() && isWhitespace(c) && !eol) {\n        wsBuf.append((char) c);\n        c = in.read();\n        eol = isEndOfLine(c);\n      }\n      // ok, start of token reached: comment, encapsulated, or token\n      if (c == strategy.getCommentStart()) {\n        // ignore everything till end of line and continue (incr linecount)\n        in.readLine();\n        tkn = nextToken(tkn.reset());\n      } else if (c == strategy.getDelimiter()) {\n        // empty token return TT_TOKEN(\"\")\n        tkn.type = TT_TOKEN;\n        tkn.isReady = true;\n      } else if (eol) {\n        // empty token return TT_EORECORD(\"\")\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EORECORD;\n        tkn.isReady = true;\n      } else if (c == strategy.getEncapsulator()) {\n        // consume encapsulated token\n        encapsulatedTokenLexer(tkn, c);\n      } else if (isEndOfFile(c)) {\n        // end of file return TT_EOF()\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EOF;\n        tkn.isReady = true;\n      } else {\n        // next token must be a simple token\n        // add removed blanks when not ignoring whitespace chars...\n        if (!strategy.getIgnoreLeadingWhitespaces()) {\n          tkn.content.append(wsBuf);\n        }\n        simpleTokenLexer(tkn, c);\n      }\n    }\n    return tkn;  \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/internal/csv/CSVParser#nextToken(Token).mjava","pathOld":"solr/core/src/java/org/apache/solr/internal/csv/CSVParser#nextToken(Token).mjava","sourceNew":" /**\n   * Returns the next token.\n   * \n   * A token corresponds to a term, a record change or an\n   * end-of-file indicator.\n   * \n   * @param tkn an existing Token object to reuse. The caller is responsible to initialize the\n   * Token.\n   * @return the next token found\n   * @throws IOException on stream access error\n   */\n  protected Token nextToken(Token tkn) throws IOException {\n    wsBuf.clear(); // resuse\n    \n    // get the last read char (required for empty line detection)\n    int lastChar = in.readAgain();\n    \n    //  read the next char and set eol\n    /* note: unfortunately isEndOfLine may consumes a character silently.\n     *       this has no effect outside of the method. so a simple workaround\n     *       is to call 'readAgain' on the stream...\n     *       uh: might using objects instead of base-types (jdk1.5 autoboxing!)\n     */\n    int c = in.read();\n    boolean eol = isEndOfLine(c);\n    c = in.readAgain();\n     \n    //  empty line detection: eol AND (last char was EOL or beginning)\n    while (strategy.getIgnoreEmptyLines() && eol \n      && (lastChar == '\\n' \n      || lastChar == ExtendedBufferedReader.UNDEFINED) \n      && !isEndOfFile(lastChar)) {\n      // go on char ahead ...\n      lastChar = c;\n      c = in.read();\n      eol = isEndOfLine(c);\n      c = in.readAgain();\n      // reached end of file without any content (empty line at the end)\n      if (isEndOfFile(c)) {\n        tkn.type = TT_EOF;\n        return tkn;\n      }\n    }\n\n    // did we reached eof during the last iteration already ? TT_EOF\n    if (isEndOfFile(lastChar) || (lastChar != strategy.getDelimiter() && isEndOfFile(c))) {\n      tkn.type = TT_EOF;\n      return tkn;\n    } \n    \n    //  important: make sure a new char gets consumed in each iteration\n    while (!tkn.isReady && tkn.type != TT_EOF) {\n      // ignore whitespaces at beginning of a token\n      while (strategy.getIgnoreLeadingWhitespaces() && isWhitespace(c) && !eol) {\n        wsBuf.append((char) c);\n        c = in.read();\n        eol = isEndOfLine(c);\n      }\n      // ok, start of token reached: comment, encapsulated, or token\n      if (c == strategy.getCommentStart()) {\n        // ignore everything till end of line and continue (incr linecount)\n        in.readLine();\n        tkn = nextToken(tkn.reset());\n      } else if (c == strategy.getDelimiter()) {\n        // empty token return TT_TOKEN(\"\")\n        tkn.type = TT_TOKEN;\n        tkn.isReady = true;\n      } else if (eol) {\n        // empty token return TT_EORECORD(\"\")\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EORECORD;\n        tkn.isReady = true;\n      } else if (c == strategy.getEncapsulator()) {\n        // consume encapsulated token\n        encapsulatedTokenLexer(tkn, c);\n      } else if (isEndOfFile(c)) {\n        // end of file return TT_EOF()\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EOF;\n        tkn.isReady = true;\n      } else {\n        // next token must be a simple token\n        // add removed blanks when not ignoring whitespace chars...\n        if (!strategy.getIgnoreLeadingWhitespaces()) {\n          tkn.content.append(wsBuf);\n        }\n        simpleTokenLexer(tkn, c);\n      }\n    }\n    return tkn;  \n  }\n\n","sourceOld":" /**\n   * Returns the next token.\n   * \n   * A token corresponds to a term, a record change or an\n   * end-of-file indicator.\n   * \n   * @param tkn an existing Token object to reuse. The caller is responsible to initialize the\n   * Token.\n   * @return the next token found\n   * @throws IOException on stream access error\n   */\n  protected Token nextToken(Token tkn) throws IOException {\n    wsBuf.clear(); // resuse\n    \n    // get the last read char (required for empty line detection)\n    int lastChar = in.readAgain();\n    \n    //  read the next char and set eol\n    /* note: unfourtunately isEndOfLine may consumes a character silently.\n     *       this has no effect outside of the method. so a simple workaround\n     *       is to call 'readAgain' on the stream...\n     *       uh: might using objects instead of base-types (jdk1.5 autoboxing!)\n     */\n    int c = in.read();\n    boolean eol = isEndOfLine(c);\n    c = in.readAgain();\n     \n    //  empty line detection: eol AND (last char was EOL or beginning)\n    while (strategy.getIgnoreEmptyLines() && eol \n      && (lastChar == '\\n' \n      || lastChar == ExtendedBufferedReader.UNDEFINED) \n      && !isEndOfFile(lastChar)) {\n      // go on char ahead ...\n      lastChar = c;\n      c = in.read();\n      eol = isEndOfLine(c);\n      c = in.readAgain();\n      // reached end of file without any content (empty line at the end)\n      if (isEndOfFile(c)) {\n        tkn.type = TT_EOF;\n        return tkn;\n      }\n    }\n\n    // did we reached eof during the last iteration already ? TT_EOF\n    if (isEndOfFile(lastChar) || (lastChar != strategy.getDelimiter() && isEndOfFile(c))) {\n      tkn.type = TT_EOF;\n      return tkn;\n    } \n    \n    //  important: make sure a new char gets consumed in each iteration\n    while (!tkn.isReady && tkn.type != TT_EOF) {\n      // ignore whitespaces at beginning of a token\n      while (strategy.getIgnoreLeadingWhitespaces() && isWhitespace(c) && !eol) {\n        wsBuf.append((char) c);\n        c = in.read();\n        eol = isEndOfLine(c);\n      }\n      // ok, start of token reached: comment, encapsulated, or token\n      if (c == strategy.getCommentStart()) {\n        // ignore everything till end of line and continue (incr linecount)\n        in.readLine();\n        tkn = nextToken(tkn.reset());\n      } else if (c == strategy.getDelimiter()) {\n        // empty token return TT_TOKEN(\"\")\n        tkn.type = TT_TOKEN;\n        tkn.isReady = true;\n      } else if (eol) {\n        // empty token return TT_EORECORD(\"\")\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EORECORD;\n        tkn.isReady = true;\n      } else if (c == strategy.getEncapsulator()) {\n        // consume encapsulated token\n        encapsulatedTokenLexer(tkn, c);\n      } else if (isEndOfFile(c)) {\n        // end of file return TT_EOF()\n        //noop: tkn.content.append(\"\");\n        tkn.type = TT_EOF;\n        tkn.isReady = true;\n      } else {\n        // next token must be a simple token\n        // add removed blanks when not ignoring whitespace chars...\n        if (!strategy.getIgnoreLeadingWhitespaces()) {\n          tkn.content.append(wsBuf);\n        }\n        simpleTokenLexer(tkn, c);\n      }\n    }\n    return tkn;  \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"80931fa1f3b04650bde7b20e61b02063a7093ced":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["80931fa1f3b04650bde7b20e61b02063a7093ced","d470c8182e92b264680e34081b75e70a9f2b3c89"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["80931fa1f3b04650bde7b20e61b02063a7093ced","0158ced21948b6626f733c1c42c1e18d94449789"],"0158ced21948b6626f733c1c42c1e18d94449789":["80931fa1f3b04650bde7b20e61b02063a7093ced"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["80931fa1f3b04650bde7b20e61b02063a7093ced"],"80931fa1f3b04650bde7b20e61b02063a7093ced":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","0158ced21948b6626f733c1c42c1e18d94449789"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d470c8182e92b264680e34081b75e70a9f2b3c89":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"0158ced21948b6626f733c1c42c1e18d94449789":["d470c8182e92b264680e34081b75e70a9f2b3c89"]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}