{"path":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c71d502dea2f9d6ed3d8783f510ea3254435de9","date":1318266042,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        final Object arr;\n        if (ordReader.hasArray()) {\n          arr = ordReader.getArray();\n        } else {\n          arr = null;\n        }\n\n        if (arr instanceof int[]) {\n          int[] ords = (int[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof short[]) {\n          short[] ords = (short[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof byte[]) {\n          byte[] ords = (byte[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        if (ordReader instanceof Direct32) {\n          int[] ords = ((Direct32)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct16) {\n          short[] ords = ((Direct16)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (ordReader instanceof Direct8) {\n          byte[] ords = ((Direct8)ordReader).getArray();\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n\n      }\n    }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6620df8541b174097b1133a4fc370adb2e570524","date":1319544675,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        final Object arr;\n        if (ordReader.hasArray()) {\n          arr = ordReader.getArray();\n        } else {\n          arr = null;\n        }\n\n        if (arr instanceof int[]) {\n          int[] ords = (int[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof short[]) {\n          short[] ords = (short[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof byte[]) {\n          byte[] ords = (byte[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context);\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        final Object arr;\n        if (ordReader.hasArray()) {\n          arr = ordReader.getArray();\n        } else {\n          arr = null;\n        }\n\n        if (arr instanceof int[]) {\n          int[] ords = (int[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof short[]) {\n          short[] ords = (short[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof byte[]) {\n          byte[] ords = (byte[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96d207426bd26fa5c1014e26d21d87603aea68b7","date":1327944562,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        final Object arr;\n        if (ordReader.hasArray()) {\n          arr = ordReader.getArray();\n        } else {\n          arr = null;\n        }\n\n        if (arr instanceof int[]) {\n          int[] ords = (int[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof short[]) {\n          short[] ords = (short[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof byte[]) {\n          byte[] ords = (byte[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        final Object arr;\n        if (ordReader.hasArray()) {\n          arr = ordReader.getArray();\n        } else {\n          arr = null;\n        }\n\n        if (arr instanceof int[]) {\n          int[] ords = (int[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof short[]) {\n          short[] ords = (short[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof byte[]) {\n          byte[] ords = (byte[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        final Object arr;\n        if (ordReader.hasArray()) {\n          arr = ordReader.getArray();\n        } else {\n          arr = null;\n        }\n\n        if (arr instanceof int[]) {\n          int[] ords = (int[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof short[]) {\n          short[] ords = (short[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof byte[]) {\n          byte[] ords = (byte[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        final Object arr;\n        if (ordReader.hasArray()) {\n          arr = ordReader.getArray();\n        } else {\n          arr = null;\n        }\n\n        if (arr instanceof int[]) {\n          int[] ords = (int[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof short[]) {\n          short[] ords = (short[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof byte[]) {\n          byte[] ords = (byte[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6e2893fd5349134af382d33ccc3d84840394c6c1","date":1353682567,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        final Object arr;\n        if (ordReader.hasArray()) {\n          arr = ordReader.getArray();\n        } else {\n          arr = null;\n        }\n\n        if (arr instanceof int[]) {\n          int[] ords = (int[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof short[]) {\n          short[] ords = (short[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof byte[]) {\n          byte[] ords = (byte[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d7e5f3aa5935964617824d1f9b2599ddb334464","date":1353762831,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b","date":1359664357,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=0;\n        endTermIndex=si.numOrd();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        PackedInts.Reader ordReader = si.getDocToOrd();\n        int doc;\n\n        final Object arr;\n        if (ordReader.hasArray()) {\n          arr = ordReader.getArray();\n        } else {\n          arr = null;\n        }\n\n        if (arr instanceof int[]) {\n          int[] ords = (int[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc]]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc];\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof short[]) {\n          short[] ords = (short[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xffff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xffff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else if (arr instanceof byte[]) {\n          byte[] ords = (byte[]) arr;\n          if (prefix==null) {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[ords[doc] & 0xff]++;\n            }\n          } else {\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = ords[doc] & 0xff;\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        } else {\n          if (prefix==null) {\n            // specialized version when collecting counts for all terms\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              counts[si.getOrd(doc)]++;\n            }\n          } else {\n            // version that adjusts term numbers because we aren't collecting the full range\n            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n              int term = si.getOrd(doc);\n              int arrIdx = term-startTermIndex;\n              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n            }\n          }\n        }\n      }\n    }\n\n","bugFix":["98a8a68e6714cb8742c790308b9f5180d63417d4","c51c66468c7c8fd86e3d4162f5be31654d33e54c","3c71d502dea2f9d6ed3d8783f510ea3254435de9","be20f9fed1d3edcb1c84abcc39df87a90fab22df","627ce218a5a68018115c2deb6559b41e3665b8ab"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"19275ba31e621f6da1b83bf13af75233876fd3d4","date":1374846698,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","date":1376375609,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e28b14e7783d24ca69089f13ddadadbd2afdcb29","date":1399840701,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRef prefixRef = new BytesRef(prefix);\n        startTermIndex = si.lookupTerm(prefixRef);\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef);\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":["3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b","627ce218a5a68018115c2deb6559b41e3665b8ab"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c5280f6286c7546ab75b72c663f7bb1dc10e96","date":1427372570,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          counts[1+si.getOrd(doc)]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int term = si.getOrd(doc);\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          counts[1+si.getOrd(doc)]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int term = si.getOrd(doc);\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms>0) {\n        // count collection array only needs to be as big as the number of terms we are\n        // going to collect counts for.\n        final int[] counts = this.counts = new int[nTerms];\n        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n        DocIdSetIterator iter = idSet.iterator();\n\n\n        ////\n        int doc;\n\n        if (prefix==null) {\n          // specialized version when collecting counts for all terms\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            counts[1+si.getOrd(doc)]++;\n          }\n        } else {\n          // version that adjusts term numbers because we aren't collecting the full range\n          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n            int term = si.getOrd(doc);\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e469758fd4a573bda61113c4ca812fafa6beac85","date":1453243309,"type":3,"author":"Dennis Gove","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int t = 1+si.getOrd(doc);\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int term = si.getOrd(doc);\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          counts[1+si.getOrd(doc)]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int term = si.getOrd(doc);\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int t;\n          if (doc == si.docID()) {\n            t = 1+si.ordValue();\n          } else {\n            t = 0;\n          }\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int term;\n          if (doc == si.docID()) {\n            term = si.ordValue();\n          } else {\n            term = -1;\n          }\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int t = 1+si.getOrd(doc);\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int term = si.getOrd(doc);\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int t;\n          if (doc == si.docID()) {\n            t = 1+si.ordValue();\n          } else {\n            t = 0;\n          }\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int term;\n          if (doc == si.docID()) {\n            term = si.ordValue();\n          } else {\n            term = -1;\n          }\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int t = 1+si.getOrd(doc);\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int term = si.getOrd(doc);\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int t;\n          if (doc == si.docID()) {\n            t = 1+si.ordValue();\n          } else {\n            t = 0;\n          }\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int term;\n          if (doc == si.docID()) {\n            term = si.ordValue();\n          } else {\n            term = -1;\n          }\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int t = 1+si.getOrd(doc);\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int term = si.getOrd(doc);\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"11134e449dabe11d6d0ff6a564d84b82cbe93722","date":1477299083,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int t;\n          if (si.advanceExact(doc)) {\n            t = 1+si.ordValue();\n          } else {\n            t = 0;\n          }\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int term;\n          if (si.advanceExact(doc)) {\n            term = si.ordValue();\n          } else {\n            term = -1;\n          }\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int t;\n          if (doc == si.docID()) {\n            t = 1+si.ordValue();\n          } else {\n            t = 0;\n          }\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int term;\n          if (doc == si.docID()) {\n            term = si.ordValue();\n          } else {\n            term = -1;\n          }\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2714c85633b642b29871cf5ff8d17d3ba7bfd76","date":1477307753,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int t;\n          if (si.advanceExact(doc)) {\n            t = 1+si.ordValue();\n          } else {\n            t = 0;\n          }\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int term;\n          if (si.advanceExact(doc)) {\n            term = si.ordValue();\n          } else {\n            term = -1;\n          }\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int t;\n          if (doc == si.docID()) {\n            t = 1+si.ordValue();\n          } else {\n            t = 0;\n          }\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int term;\n          if (doc == si.docID()) {\n            term = si.ordValue();\n          } else {\n            term = -1;\n          }\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","pathOld":"solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.SegFacet#countTerms().mjava","sourceNew":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int t;\n          if (si.advanceExact(doc)) {\n            t = 1+si.ordValue();\n          } else {\n            t = 0;\n          }\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          int term;\n          if (si.advanceExact(doc)) {\n            term = si.ordValue();\n          } else {\n            term = -1;\n          }\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","sourceOld":"    void countTerms() throws IOException {\n      si = DocValues.getSorted(context.reader(), fieldName);\n      // SolrCore.log.info(\"reader= \" + reader + \"  FC=\" + System.identityHashCode(si));\n\n      if (prefix!=null) {\n        BytesRefBuilder prefixRef = new BytesRefBuilder();\n        prefixRef.copyChars(prefix);\n        startTermIndex = si.lookupTerm(prefixRef.get());\n        if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n        prefixRef.append(UnicodeUtil.BIG_TERM);\n        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end\n        endTermIndex = si.lookupTerm(prefixRef.get());\n        assert endTermIndex < 0;\n        endTermIndex = -endTermIndex-1;\n      } else {\n        startTermIndex=-1;\n        endTermIndex=si.getValueCount();\n      }\n      final int nTerms=endTermIndex-startTermIndex;\n      if (nTerms == 0) return;\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = this.counts = new int[nTerms];\n      DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs\n      DocIdSetIterator iter = idSet.iterator();\n\n      if (prefix==null) {\n        // specialized version when collecting counts for all terms\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int t;\n          if (doc == si.docID()) {\n            t = 1+si.ordValue();\n          } else {\n            t = 0;\n          }\n          hasAnyCount = hasAnyCount || t > 0; //counts[0] == missing counts\n          counts[t]++;\n        }\n      } else {\n        // version that adjusts term numbers because we aren't collecting the full range\n        int doc;\n        while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {\n          if (doc > si.docID()) {\n            si.advance(doc);\n          }\n          int term;\n          if (doc == si.docID()) {\n            term = si.ordValue();\n          } else {\n            term = -1;\n          }\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms){\n            counts[arrIdx]++;\n            hasAnyCount = true;\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e469758fd4a573bda61113c4ca812fafa6beac85":["52c5280f6286c7546ab75b72c663f7bb1dc10e96"],"6620df8541b174097b1133a4fc370adb2e570524":["3c71d502dea2f9d6ed3d8783f510ea3254435de9"],"52c5280f6286c7546ab75b72c663f7bb1dc10e96":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["6620df8541b174097b1133a4fc370adb2e570524"],"3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b":["9d7e5f3aa5935964617824d1f9b2599ddb334464"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"56572ec06f1407c066d6b7399413178b33176cd8":["19275ba31e621f6da1b83bf13af75233876fd3d4","93dd449115a9247533e44bab47e8429e5dccbc6d"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","52c5280f6286c7546ab75b72c663f7bb1dc10e96"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["e469758fd4a573bda61113c4ca812fafa6beac85","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["6e2893fd5349134af382d33ccc3d84840394c6c1"],"e28b14e7783d24ca69089f13ddadadbd2afdcb29":["19275ba31e621f6da1b83bf13af75233876fd3d4"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":["d4d69c535930b5cce125cff868d40f6373dc27d4","19275ba31e621f6da1b83bf13af75233876fd3d4"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["6620df8541b174097b1133a4fc370adb2e570524","96d207426bd26fa5c1014e26d21d87603aea68b7"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["e469758fd4a573bda61113c4ca812fafa6beac85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"3c71d502dea2f9d6ed3d8783f510ea3254435de9":["c26f00b574427b55127e869b935845554afde1fa"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["5cab9a86bd67202d20b6adc463008c8e982b070a","3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["19275ba31e621f6da1b83bf13af75233876fd3d4","e28b14e7783d24ca69089f13ddadadbd2afdcb29"],"11134e449dabe11d6d0ff6a564d84b82cbe93722":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["e469758fd4a573bda61113c4ca812fafa6beac85"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d2714c85633b642b29871cf5ff8d17d3ba7bfd76":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","11134e449dabe11d6d0ff6a564d84b82cbe93722"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d2714c85633b642b29871cf5ff8d17d3ba7bfd76"]},"commit2Childs":{"e469758fd4a573bda61113c4ca812fafa6beac85":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6620df8541b174097b1133a4fc370adb2e570524":["96d207426bd26fa5c1014e26d21d87603aea68b7","5cab9a86bd67202d20b6adc463008c8e982b070a"],"52c5280f6286c7546ab75b72c663f7bb1dc10e96":["e469758fd4a573bda61113c4ca812fafa6beac85","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"c26f00b574427b55127e869b935845554afde1fa":["3c71d502dea2f9d6ed3d8783f510ea3254435de9"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["56572ec06f1407c066d6b7399413178b33176cd8","e28b14e7783d24ca69089f13ddadadbd2afdcb29","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","93dd449115a9247533e44bab47e8429e5dccbc6d"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["52c5280f6286c7546ab75b72c663f7bb1dc10e96","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","11134e449dabe11d6d0ff6a564d84b82cbe93722","d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["3d5a5a419065ad4cdc70485f83fa9fe9373aaa8b"],"e28b14e7783d24ca69089f13ddadadbd2afdcb29":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":[],"6e2893fd5349134af382d33ccc3d84840394c6c1":["9d7e5f3aa5935964617824d1f9b2599ddb334464"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["6e2893fd5349134af382d33ccc3d84840394c6c1","d4d69c535930b5cce125cff868d40f6373dc27d4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"3c71d502dea2f9d6ed3d8783f510ea3254435de9":["6620df8541b174097b1133a4fc370adb2e570524"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["19275ba31e621f6da1b83bf13af75233876fd3d4","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","56572ec06f1407c066d6b7399413178b33176cd8"],"11134e449dabe11d6d0ff6a564d84b82cbe93722":["d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"d2714c85633b642b29871cf5ff8d17d3ba7bfd76":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["56572ec06f1407c066d6b7399413178b33176cd8","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","80d0e6d59ae23f4a6f30eaf40bfb40742300287f","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}