{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","commits":[{"id":"4ff25cb7bb787cbe9d05740c89a527ddd2617c16","date":1462702859,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom1() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    IndexWriterConfig iwc1 = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(new Random(seed), dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    IndexWriterConfig iwc2 = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(new Random(seed), dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a8ad56fe9e068c425e8ac38f2263714e167490c2","date":1462704115,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            final long value = values.get(i);\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        assertEquals(i, MultiDocValues.getNumericValues(reader, \"id\").get(topDocs.scoreDocs[0].doc));\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom1() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    IndexWriterConfig iwc1 = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(new Random(seed), dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    IndexWriterConfig iwc2 = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(new Random(seed), dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","bugFix":null,"bugIntro":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":0,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            final long value = values.get(i);\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        assertEquals(i, MultiDocValues.getNumericValues(reader, \"id\").get(topDocs.scoreDocs[0].doc));\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            final long value = values.get(i);\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        assertEquals(i, MultiDocValues.getNumericValues(reader, \"id\").get(topDocs.scoreDocs[0].doc));\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            final long value = values.get(i);\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        assertEquals(i, MultiDocValues.getNumericValues(reader, \"id\").get(topDocs.scoreDocs[0].doc));\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            final long value = values.get(i);\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        assertEquals(i, MultiDocValues.getNumericValues(reader, \"id\").get(topDocs.scoreDocs[0].doc));\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            final long value = values.get(i);\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        assertEquals(i, MultiDocValues.getNumericValues(reader, \"id\").get(topDocs.scoreDocs[0].doc));\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86a0a50d2d14aaee1e635bbec914468551f7f9a2","date":1482234306,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":["a8ad56fe9e068c425e8ac38f2263714e167490c2"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","date":1482251961,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n          assertNull(info.getIndexSort());\n          break;\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits.value);\n      } else {\n        assertEquals(1, topDocs.totalHits.value);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits);\n      } else {\n        assertEquals(1, topDocs.totalHits);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f","date":1579652839,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom1().mjava","sourceNew":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(200);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits.value);\n      } else {\n        assertEquals(1, topDocs.totalHits.value);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom1() throws IOException {\n    boolean withDeletes = random().nextBoolean();\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    Sort indexSort = new Sort(new SortField(\"foo\", SortField.Type.LONG));\n    iwc.setIndexSort(indexSort);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    final int numDocs = atLeast(1000);\n    final FixedBitSet deleted = new FixedBitSet(numDocs);\n    for (int i = 0; i < numDocs; ++i) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"foo\", random().nextInt(20)));\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      w.addDocument(doc);\n      if (random().nextInt(5) == 0) {\n        w.getReader().close();\n      } else if (random().nextInt(30) == 0) {\n        w.forceMerge(2);\n      } else if (random().nextInt(4) == 0) {\n        final int id = TestUtil.nextInt(random(), 0, i);\n        deleted.set(id);\n        w.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n      }\n    }\n\n    // Check that segments are sorted\n    DirectoryReader reader = w.getReader();\n    for (LeafReaderContext ctx : reader.leaves()) {\n      final SegmentReader leaf = (SegmentReader) ctx.reader();\n      SegmentInfo info = leaf.getSegmentInfo().info;\n      switch (info.getDiagnostics().get(IndexWriter.SOURCE)) {\n        case IndexWriter.SOURCE_FLUSH:\n        case IndexWriter.SOURCE_MERGE:\n          assertEquals(indexSort, info.getIndexSort());\n          final NumericDocValues values = leaf.getNumericDocValues(\"foo\");\n          long previous = Long.MIN_VALUE;\n          for (int i = 0; i < leaf.maxDoc(); ++i) {\n            assertEquals(i, values.nextDoc());\n            final long value = values.longValue();\n            assertTrue(value >= previous);\n            previous = value;\n          }\n          break;\n        default:\n          fail();\n      }\n    }\n\n    // Now check that the index is consistent\n    IndexSearcher searcher = newSearcher(reader);\n    for (int i = 0; i < numDocs; ++i) {\n      TermQuery termQuery = new TermQuery(new Term(\"id\", Integer.toString(i)));\n      final TopDocs topDocs = searcher.search(termQuery, 1);\n      if (deleted.get(i)) {\n        assertEquals(0, topDocs.totalHits.value);\n      } else {\n        assertEquals(1, topDocs.totalHits.value);\n        NumericDocValues values = MultiDocValues.getNumericValues(reader, \"id\");\n        assertEquals(topDocs.scoreDocs[0].doc, values.advance(topDocs.scoreDocs[0].doc));\n        assertEquals(i, values.longValue());\n        Document document = reader.document(topDocs.scoreDocs[0].doc);\n        assertEquals(Integer.toString(i), document.get(\"id\"));\n      }\n    }\n\n    reader.close();\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"0ad30c6a479e764150a3316e57263319775f1df2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3d33e731a93d4b57e662ff094f64f94a745422d4"],"4ff25cb7bb787cbe9d05740c89a527ddd2617c16":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0ad30c6a479e764150a3316e57263319775f1df2"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["d470c8182e92b264680e34081b75e70a9f2b3c89","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a8ad56fe9e068c425e8ac38f2263714e167490c2"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a8ad56fe9e068c425e8ac38f2263714e167490c2":["4ff25cb7bb787cbe9d05740c89a527ddd2617c16"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"]},"commit2Childs":{"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"4ff25cb7bb787cbe9d05740c89a527ddd2617c16":["a8ad56fe9e068c425e8ac38f2263714e167490c2"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0ad30c6a479e764150a3316e57263319775f1df2","4ff25cb7bb787cbe9d05740c89a527ddd2617c16","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","3d33e731a93d4b57e662ff094f64f94a745422d4"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","83788ad129a5154d5c6562c4e8ce3db48793aada"],"a8ad56fe9e068c425e8ac38f2263714e167490c2":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}