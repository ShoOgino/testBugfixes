{"path":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"modules/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      FacetArrays facetArrays = new FacetArrays(intArrayAllocator, floatArrayAllocator);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      FacetArrays facetArrays = new FacetArrays(intArrayAllocator, floatArrayAllocator);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a2548b7f050533ac9a884b31cab5fb6f0386fbb","date":1355233860,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      FacetArrays facetArrays = new FacetArrays(intArrayAllocator, floatArrayAllocator);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4c6c7f3cda7a0595cabd16e5e9107ca29852708","date":1355402234,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getCategoryListCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"487b6150786f5145006f5d0d38a5f514b4472319","date":1355684762,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getCategoryListCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getCategoryListCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":["89f15687f60bd49cd3d9de427e85c17fd9397d61"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getCategoryListCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      FacetArrays facetArrays = new FacetArrays(intArrayAllocator, floatArrayAllocator);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42f51b3ab4258ff4623227b0db011b8bb83db5c7","date":1358164991,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getCategoryListCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getCategoryListCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8917bfede3b4ca30f4305c1e391e9218959cd723","date":1358189662,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getCategoryListCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getCategoryListCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c2cd18c7da6f499a33f06fc89c07a463ec074c0","date":1358329431,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.getFacetIndexingParams());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getCategoryListCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4015cd39dff8d4dec562d909f9766debac53aa6","date":1358548736,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.getFacetIndexingParams());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getCategoryListCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f96e4a056f7ee1bafbfb8a06c5bd93f7708e560d","date":1358784296,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.getFacetIndexingParams());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"07155cdd910937cdf6877e48884d5782845c8b8b","date":1358796205,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.getFacetIndexingParams());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"571abba77e55fea386a38c0024f72ffa5b37a9ad","date":1360272747,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5b1ac4be5e40d7bda6ec0f850c933a95ca0642fd","date":1361836936,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          FacetResultNode root = new FacetResultNode();\n          root.ordinal = TaxonomyReader.INVALID_ORDINAL;\n          root.label = fr.categoryPath;\n          root.value = 0;\n          res.add(new FacetResult(fr, root, 0));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":["d1f7dc2d5ba61f478d9439f5b6afe27c8809422a"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d1f7dc2d5ba61f478d9439f5b6afe27c8809422a","date":1365621037,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          FacetResultNode root = new FacetResultNode();\n          root.ordinal = TaxonomyReader.INVALID_ORDINAL;\n          root.label = fr.categoryPath;\n          root.value = 0;\n          res.add(new FacetResult(fr, root, 0));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":["5b1ac4be5e40d7bda6ec0f850c933a95ca0642fd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6249cba93d7ad1bf6f5a225c34fbe3d547ed9f49","date":1375103250,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6","date":1375108983,"type":5,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          isUsingComplements = false;\n        } catch (IOException e) {\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":5,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          isUsingComplements = false;\n        } catch (IOException e) {\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["b89678825b68eccaf09e6ab71675fc0b0af1e099","487b6150786f5145006f5d0d38a5f514b4472319"],"5b1ac4be5e40d7bda6ec0f850c933a95ca0642fd":["571abba77e55fea386a38c0024f72ffa5b37a9ad"],"f96e4a056f7ee1bafbfb8a06c5bd93f7708e560d":["6c2cd18c7da6f499a33f06fc89c07a463ec074c0"],"42f51b3ab4258ff4623227b0db011b8bb83db5c7":["487b6150786f5145006f5d0d38a5f514b4472319"],"c4015cd39dff8d4dec562d909f9766debac53aa6":["8917bfede3b4ca30f4305c1e391e9218959cd723","6c2cd18c7da6f499a33f06fc89c07a463ec074c0"],"8917bfede3b4ca30f4305c1e391e9218959cd723":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","42f51b3ab4258ff4623227b0db011b8bb83db5c7"],"07155cdd910937cdf6877e48884d5782845c8b8b":["c4015cd39dff8d4dec562d909f9766debac53aa6","f96e4a056f7ee1bafbfb8a06c5bd93f7708e560d"],"571abba77e55fea386a38c0024f72ffa5b37a9ad":["f96e4a056f7ee1bafbfb8a06c5bd93f7708e560d"],"487b6150786f5145006f5d0d38a5f514b4472319":["d4c6c7f3cda7a0595cabd16e5e9107ca29852708"],"d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6":["6249cba93d7ad1bf6f5a225c34fbe3d547ed9f49"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2a2548b7f050533ac9a884b31cab5fb6f0386fbb":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"d4c6c7f3cda7a0595cabd16e5e9107ca29852708":["2a2548b7f050533ac9a884b31cab5fb6f0386fbb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d1f7dc2d5ba61f478d9439f5b6afe27c8809422a":["5b1ac4be5e40d7bda6ec0f850c933a95ca0642fd"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["d1f7dc2d5ba61f478d9439f5b6afe27c8809422a"],"6249cba93d7ad1bf6f5a225c34fbe3d547ed9f49":["d1f7dc2d5ba61f478d9439f5b6afe27c8809422a"],"6c2cd18c7da6f499a33f06fc89c07a463ec074c0":["42f51b3ab4258ff4623227b0db011b8bb83db5c7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["8917bfede3b4ca30f4305c1e391e9218959cd723"],"5b1ac4be5e40d7bda6ec0f850c933a95ca0642fd":["d1f7dc2d5ba61f478d9439f5b6afe27c8809422a"],"f96e4a056f7ee1bafbfb8a06c5bd93f7708e560d":["07155cdd910937cdf6877e48884d5782845c8b8b","571abba77e55fea386a38c0024f72ffa5b37a9ad"],"42f51b3ab4258ff4623227b0db011b8bb83db5c7":["8917bfede3b4ca30f4305c1e391e9218959cd723","6c2cd18c7da6f499a33f06fc89c07a463ec074c0"],"c4015cd39dff8d4dec562d909f9766debac53aa6":["07155cdd910937cdf6877e48884d5782845c8b8b"],"8917bfede3b4ca30f4305c1e391e9218959cd723":["c4015cd39dff8d4dec562d909f9766debac53aa6"],"07155cdd910937cdf6877e48884d5782845c8b8b":[],"571abba77e55fea386a38c0024f72ffa5b37a9ad":["5b1ac4be5e40d7bda6ec0f850c933a95ca0642fd"],"487b6150786f5145006f5d0d38a5f514b4472319":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","42f51b3ab4258ff4623227b0db011b8bb83db5c7"],"d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","2a2548b7f050533ac9a884b31cab5fb6f0386fbb"],"2a2548b7f050533ac9a884b31cab5fb6f0386fbb":["d4c6c7f3cda7a0595cabd16e5e9107ca29852708"],"d4c6c7f3cda7a0595cabd16e5e9107ca29852708":["487b6150786f5145006f5d0d38a5f514b4472319"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"d1f7dc2d5ba61f478d9439f5b6afe27c8809422a":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","6249cba93d7ad1bf6f5a225c34fbe3d547ed9f49"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"6c2cd18c7da6f499a33f06fc89c07a463ec074c0":["f96e4a056f7ee1bafbfb8a06c5bd93f7708e560d","c4015cd39dff8d4dec562d909f9766debac53aa6"],"6249cba93d7ad1bf6f5a225c34fbe3d547ed9f49":["d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["07155cdd910937cdf6877e48884d5782845c8b8b","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}