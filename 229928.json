{"path":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#runCloudTool(CloudSolrClient,CommandLine).mjava","commits":[{"id":"76bb93998d4d4fa60da28429640216a0a249111a","date":1549968795,"type":0,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#runCloudTool(CloudSolrClient,CommandLine).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected void runCloudTool(CloudSolrClient cloudSolrClient, CommandLine cli) throws Exception {\n      DistributedQueueFactory dummmyFactory = new DistributedQueueFactory() {\n        @Override\n        public DistributedQueue makeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"makeQueue\");\n        }\n\n        @Override\n        public void removeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"removeQueue\");\n        }\n      };\n      try (SolrClientCloudManager clientCloudManager = new SolrClientCloudManager(dummmyFactory, cloudSolrClient)) {\n        AutoScalingConfig config = null;\n        HashSet<String> liveNodes = new HashSet<>();\n        String configFile = cli.getOptionValue(\"a\");\n        if (configFile != null) {\n          log.info(\"- reading autoscaling config from \" + configFile);\n          config = new AutoScalingConfig(IOUtils.toByteArray(new FileInputStream(configFile)));\n        } else {\n          log.info(\"- reading autoscaling config from the cluster.\");\n          config = clientCloudManager.getDistribStateManager().getAutoScalingConfig();\n        }\n        log.info(\"- calculating suggestions...\");\n        long start = TimeSource.NANO_TIME.getTimeNs();\n        // collect live node names for optional redaction\n        liveNodes.addAll(clientCloudManager.getClusterStateProvider().getLiveNodes());\n        List<Suggester.SuggestionInfo> suggestions = PolicyHelper.getSuggestions(config, clientCloudManager);\n        long end = TimeSource.NANO_TIME.getTimeNs();\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n        log.info(\"- calculating diagnostics...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        // update the live nodes\n        liveNodes.addAll(clientCloudManager.getClusterStateProvider().getLiveNodes());\n        Policy.Session session = config.getPolicy().createSession(clientCloudManager);\n        MapWriter mw = PolicyHelper.getDiagnostics(session);\n        Map<String, Object> diagnostics = new LinkedHashMap<>();\n        mw.toMap(diagnostics);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n        boolean withSuggestions = cli.hasOption(\"s\");\n        boolean withDiagnostics = cli.hasOption(\"d\") || cli.hasOption(\"n\");\n        boolean withSortedNodes = cli.hasOption(\"n\");\n        boolean withClusterState = cli.hasOption(\"c\");\n        boolean withStats = cli.hasOption(\"stats\");\n        boolean redact = cli.hasOption(\"r\");\n        if (cli.hasOption(\"all\")) {\n          withSuggestions = true;\n          withDiagnostics = true;\n          withSortedNodes = true;\n          withClusterState = true;\n          withStats = true;\n        }\n        // prepare to redact also host names / IPs in base_url and other properties\n        Set<String> redactNames = new HashSet<>();\n        for (String nodeName : liveNodes) {\n          String urlString = Utils.getBaseUrlForNodeName(nodeName, \"http\");\n          try {\n            URL u = new URL(urlString);\n            // protocol format\n            redactNames.add(u.getHost() + \":\" + u.getPort());\n            // node name format\n            redactNames.add(u.getHost() + \"_\" + u.getPort() + \"_\");\n          } catch (MalformedURLException e) {\n            log.warn(\"Invalid URL for node name \" + nodeName + \", replacing including protocol and path\", e);\n            redactNames.add(urlString);\n            redactNames.add(Utils.getBaseUrlForNodeName(nodeName, \"https\"));\n          }\n        }\n        // redact collection names too\n        Set<String> redactCollections = new HashSet<>();\n        ClusterState clusterState = clientCloudManager.getClusterStateProvider().getClusterState();\n        clusterState.forEachCollection(coll -> redactCollections.add(coll.getName()));\n        if (!withSuggestions && !withDiagnostics) {\n          withSuggestions = true;\n        }\n        Map<String, Object> results = new LinkedHashMap<>();\n        if (withClusterState) {\n          Map<String, Object> map = new LinkedHashMap<>();\n          map.put(\"znodeVersion\", clusterState.getZNodeVersion());\n          map.put(\"liveNodes\", new TreeSet<>(clusterState.getLiveNodes()));\n          map.put(\"collections\", clusterState.getCollectionsMap());\n          results.put(\"CLUSTERSTATE\", map);\n        }\n        if (withStats) {\n          Map<String, Map<String, Number>> collStats = new TreeMap<>();\n          clusterState.forEachCollection(coll -> {\n            Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n            AtomicInteger numCores = new AtomicInteger();\n            HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n            coll.getSlices().forEach(s -> {\n              numCores.addAndGet(s.getReplicas().size());\n              s.getReplicas().forEach(r -> {\n                nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n                    .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n              });\n            });\n            int maxCoresPerNode = 0;\n            int minCoresPerNode = 0;\n            int maxActualShardsPerNode = 0;\n            int minActualShardsPerNode = 0;\n            int maxShardReplicasPerNode = 0;\n            int minShardReplicasPerNode = 0;\n            if (!nodes.isEmpty()) {\n              minCoresPerNode = Integer.MAX_VALUE;\n              minActualShardsPerNode = Integer.MAX_VALUE;\n              minShardReplicasPerNode = Integer.MAX_VALUE;\n              for (Map<String, AtomicInteger> counts : nodes.values()) {\n                int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n                for (AtomicInteger count : counts.values()) {\n                  if (count.get() > maxShardReplicasPerNode) {\n                    maxShardReplicasPerNode = count.get();\n                  }\n                  if (count.get() < minShardReplicasPerNode) {\n                    minShardReplicasPerNode = count.get();\n                  }\n                }\n                if (total > maxCoresPerNode) {\n                  maxCoresPerNode = total;\n                }\n                if (total < minCoresPerNode) {\n                  minCoresPerNode = total;\n                }\n                if (counts.size() > maxActualShardsPerNode) {\n                  maxActualShardsPerNode = counts.size();\n                }\n                if (counts.size() < minActualShardsPerNode) {\n                  minActualShardsPerNode = counts.size();\n                }\n              }\n            }\n            perColl.put(\"activeShards\", coll.getActiveSlices().size());\n            perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n            perColl.put(\"rf\", coll.getReplicationFactor());\n            perColl.put(\"maxShardsPerNode\", coll.getMaxShardsPerNode());\n            perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n            perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n            perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n            perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n            perColl.put(\"numCores\", numCores.get());\n            perColl.put(\"numNodes\", nodes.size());\n            perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n            perColl.put(\"minCoresPerNode\", minCoresPerNode);\n          });\n          Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n          for (Row row : session.getSortedNodes()) {\n            Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n            nodeStat.put(\"isLive\", row.isLive());\n            nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n            nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n            nodeStat.put(\"cores\", row.getVal(\"cores\", 0));\n            Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n            row.forEachReplica(ri -> {\n              Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n                  .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n              perReplica.put(\"INDEX.sizeInGB\", ri.getVariable(\"INDEX.sizeInGB\"));\n              perReplica.put(\"coreNode\", ri.getName());\n              if (ri.getBool(\"leader\", false)) {\n                perReplica.put(\"leader\", true);\n                Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n                    .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n                Number riSize = (Number)ri.getVariable(\"INDEX.sizeInGB\");\n                if (riSize != null) {\n                  totalSize += riSize.doubleValue();\n                  collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n                  Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n                  if (max == null) max = 0.0;\n                  if (riSize.doubleValue() > max) {\n                    collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n                  }\n                  Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n                  if (min == null) min = Double.MAX_VALUE;\n                  if (riSize.doubleValue() < min) {\n                    collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n                  }\n                }\n              }\n              nodeStat.put(\"replicas\", collReplicas);\n            });\n          }\n\n          // calculate average per shard\n          for (Map<String, Number> perColl : collStats.values()) {\n            Double avg = (Double)perColl.get(\"avgShardSize\");\n            if (avg != null) {\n              avg = avg / ((Number)perColl.get(\"activeShards\")).doubleValue();\n              perColl.put(\"avgShardSize\", avg);\n            }\n          }\n          Map<String, Object> stats = new LinkedHashMap<>();\n          results.put(\"STATISTICS\", stats);\n          stats.put(\"nodeStats\", nodeStats);\n          stats.put(\"collectionStats\", collStats);\n        }\n        if (withSuggestions) {\n          results.put(\"SUGGESTIONS\", suggestions);\n        }\n        if (!withSortedNodes) {\n          diagnostics.remove(\"sortedNodes\");\n        }\n        if (withDiagnostics) {\n          results.put(\"DIAGNOSTICS\", diagnostics);\n        }\n        String data = Utils.toJSONString(results);\n        if (redact) {\n          data = RedactionUtils.redactNames(redactCollections, COLL_REDACTION_PREFIX, data);\n          data = RedactionUtils.redactNames(redactNames, NODE_REDACTION_PREFIX, data);\n        }\n        stdout.println(data);\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"edbd289ce380f0c9e1b3f3d53a1f096b0fdfd518","date":1550140553,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#runCloudTool(CloudSolrClient,CommandLine).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#runCloudTool(CloudSolrClient,CommandLine).mjava","sourceNew":"    @Override\n    protected void runCloudTool(CloudSolrClient cloudSolrClient, CommandLine cli) throws Exception {\n      DistributedQueueFactory dummmyFactory = new DistributedQueueFactory() {\n        @Override\n        public DistributedQueue makeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"makeQueue\");\n        }\n\n        @Override\n        public void removeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"removeQueue\");\n        }\n      };\n      try (SolrClientCloudManager clientCloudManager = new SolrClientCloudManager(dummmyFactory, cloudSolrClient)) {\n        AutoScalingConfig config = null;\n        HashSet<String> liveNodes = new HashSet<>();\n        String configFile = cli.getOptionValue(\"a\");\n        if (configFile != null) {\n          log.info(\"- reading autoscaling config from \" + configFile);\n          config = new AutoScalingConfig(IOUtils.toByteArray(new FileInputStream(configFile)));\n        } else {\n          log.info(\"- reading autoscaling config from the cluster.\");\n          config = clientCloudManager.getDistribStateManager().getAutoScalingConfig();\n        }\n        log.info(\"- calculating suggestions...\");\n        long start = TimeSource.NANO_TIME.getTimeNs();\n        // collect live node names for optional redaction\n        liveNodes.addAll(clientCloudManager.getClusterStateProvider().getLiveNodes());\n        List<Suggester.SuggestionInfo> suggestions = PolicyHelper.getSuggestions(config, clientCloudManager);\n        long end = TimeSource.NANO_TIME.getTimeNs();\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n        log.info(\"- calculating diagnostics...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        // update the live nodes\n        liveNodes.addAll(clientCloudManager.getClusterStateProvider().getLiveNodes());\n        Policy.Session session = config.getPolicy().createSession(clientCloudManager);\n        MapWriter mw = PolicyHelper.getDiagnostics(session);\n        Map<String, Object> diagnostics = new LinkedHashMap<>();\n        mw.toMap(diagnostics);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n        boolean withSuggestions = cli.hasOption(\"s\");\n        boolean withDiagnostics = cli.hasOption(\"d\") || cli.hasOption(\"n\");\n        boolean withSortedNodes = cli.hasOption(\"n\");\n        boolean withClusterState = cli.hasOption(\"c\");\n        boolean withStats = cli.hasOption(\"stats\");\n        boolean redact = cli.hasOption(\"r\");\n        if (cli.hasOption(\"all\")) {\n          withSuggestions = true;\n          withDiagnostics = true;\n          withSortedNodes = true;\n          withClusterState = true;\n          withStats = true;\n        }\n        // prepare to redact also host names / IPs in base_url and other properties\n        Set<String> redactNames = new HashSet<>();\n        for (String nodeName : liveNodes) {\n          String urlString = Utils.getBaseUrlForNodeName(nodeName, \"http\");\n          try {\n            URL u = new URL(urlString);\n            // protocol format\n            redactNames.add(u.getHost() + \":\" + u.getPort());\n            // node name format\n            redactNames.add(u.getHost() + \"_\" + u.getPort() + \"_\");\n          } catch (MalformedURLException e) {\n            log.warn(\"Invalid URL for node name \" + nodeName + \", replacing including protocol and path\", e);\n            redactNames.add(urlString);\n            redactNames.add(Utils.getBaseUrlForNodeName(nodeName, \"https\"));\n          }\n        }\n        // redact collection names too\n        Set<String> redactCollections = new HashSet<>();\n        ClusterState clusterState = clientCloudManager.getClusterStateProvider().getClusterState();\n        clusterState.forEachCollection(coll -> redactCollections.add(coll.getName()));\n        if (!withSuggestions && !withDiagnostics) {\n          withSuggestions = true;\n        }\n        Map<String, Object> results = new LinkedHashMap<>();\n        if (withClusterState) {\n          Map<String, Object> map = new LinkedHashMap<>();\n          map.put(\"znodeVersion\", clusterState.getZNodeVersion());\n          map.put(\"liveNodes\", new TreeSet<>(clusterState.getLiveNodes()));\n          map.put(\"collections\", clusterState.getCollectionsMap());\n          results.put(\"CLUSTERSTATE\", map);\n        }\n        if (withStats) {\n          Map<String, Map<String, Number>> collStats = new TreeMap<>();\n          clusterState.forEachCollection(coll -> {\n            Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n            AtomicInteger numCores = new AtomicInteger();\n            HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n            coll.getSlices().forEach(s -> {\n              numCores.addAndGet(s.getReplicas().size());\n              s.getReplicas().forEach(r -> {\n                nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n                    .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n              });\n            });\n            int maxCoresPerNode = 0;\n            int minCoresPerNode = 0;\n            int maxActualShardsPerNode = 0;\n            int minActualShardsPerNode = 0;\n            int maxShardReplicasPerNode = 0;\n            int minShardReplicasPerNode = 0;\n            if (!nodes.isEmpty()) {\n              minCoresPerNode = Integer.MAX_VALUE;\n              minActualShardsPerNode = Integer.MAX_VALUE;\n              minShardReplicasPerNode = Integer.MAX_VALUE;\n              for (Map<String, AtomicInteger> counts : nodes.values()) {\n                int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n                for (AtomicInteger count : counts.values()) {\n                  if (count.get() > maxShardReplicasPerNode) {\n                    maxShardReplicasPerNode = count.get();\n                  }\n                  if (count.get() < minShardReplicasPerNode) {\n                    minShardReplicasPerNode = count.get();\n                  }\n                }\n                if (total > maxCoresPerNode) {\n                  maxCoresPerNode = total;\n                }\n                if (total < minCoresPerNode) {\n                  minCoresPerNode = total;\n                }\n                if (counts.size() > maxActualShardsPerNode) {\n                  maxActualShardsPerNode = counts.size();\n                }\n                if (counts.size() < minActualShardsPerNode) {\n                  minActualShardsPerNode = counts.size();\n                }\n              }\n            }\n            perColl.put(\"activeShards\", coll.getActiveSlices().size());\n            perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n            perColl.put(\"rf\", coll.getReplicationFactor());\n            perColl.put(\"maxShardsPerNode\", coll.getMaxShardsPerNode());\n            perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n            perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n            perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n            perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n            perColl.put(\"numCores\", numCores.get());\n            perColl.put(\"numNodes\", nodes.size());\n            perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n            perColl.put(\"minCoresPerNode\", minCoresPerNode);\n          });\n          Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n          Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n          for (Row row : session.getSortedNodes()) {\n            Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n            nodeStat.put(\"isLive\", row.isLive());\n            nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n            nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n            int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n            nodeStat.put(\"cores\", cores);\n            coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n            Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n            row.forEachReplica(ri -> {\n              Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n                  .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n              perReplica.put(\"INDEX.sizeInGB\", ri.getVariable(\"INDEX.sizeInGB\"));\n              perReplica.put(\"coreNode\", ri.getName());\n              if (ri.getBool(\"leader\", false)) {\n                perReplica.put(\"leader\", true);\n                Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n                    .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n                Number riSize = (Number)ri.getVariable(\"INDEX.sizeInGB\");\n                if (riSize != null) {\n                  totalSize += riSize.doubleValue();\n                  collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n                  Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n                  if (max == null) max = 0.0;\n                  if (riSize.doubleValue() > max) {\n                    collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n                  }\n                  Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n                  if (min == null) min = Double.MAX_VALUE;\n                  if (riSize.doubleValue() < min) {\n                    collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n                  }\n                }\n              }\n              nodeStat.put(\"replicas\", collReplicas);\n            });\n          }\n\n          // calculate average per shard\n          for (Map<String, Number> perColl : collStats.values()) {\n            Double avg = (Double)perColl.get(\"avgShardSize\");\n            if (avg != null) {\n              avg = avg / ((Number)perColl.get(\"activeShards\")).doubleValue();\n              perColl.put(\"avgShardSize\", avg);\n            }\n          }\n          Map<String, Object> stats = new LinkedHashMap<>();\n          results.put(\"STATISTICS\", stats);\n          stats.put(\"coresPerNodes\", coreStats);\n          stats.put(\"nodeStats\", nodeStats);\n          stats.put(\"collectionStats\", collStats);\n        }\n        if (withSuggestions) {\n          results.put(\"SUGGESTIONS\", suggestions);\n        }\n        if (!withSortedNodes) {\n          diagnostics.remove(\"sortedNodes\");\n        }\n        if (withDiagnostics) {\n          results.put(\"DIAGNOSTICS\", diagnostics);\n        }\n        String data = Utils.toJSONString(results);\n        if (redact) {\n          data = RedactionUtils.redactNames(redactCollections, COLL_REDACTION_PREFIX, data);\n          data = RedactionUtils.redactNames(redactNames, NODE_REDACTION_PREFIX, data);\n        }\n        stdout.println(data);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void runCloudTool(CloudSolrClient cloudSolrClient, CommandLine cli) throws Exception {\n      DistributedQueueFactory dummmyFactory = new DistributedQueueFactory() {\n        @Override\n        public DistributedQueue makeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"makeQueue\");\n        }\n\n        @Override\n        public void removeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"removeQueue\");\n        }\n      };\n      try (SolrClientCloudManager clientCloudManager = new SolrClientCloudManager(dummmyFactory, cloudSolrClient)) {\n        AutoScalingConfig config = null;\n        HashSet<String> liveNodes = new HashSet<>();\n        String configFile = cli.getOptionValue(\"a\");\n        if (configFile != null) {\n          log.info(\"- reading autoscaling config from \" + configFile);\n          config = new AutoScalingConfig(IOUtils.toByteArray(new FileInputStream(configFile)));\n        } else {\n          log.info(\"- reading autoscaling config from the cluster.\");\n          config = clientCloudManager.getDistribStateManager().getAutoScalingConfig();\n        }\n        log.info(\"- calculating suggestions...\");\n        long start = TimeSource.NANO_TIME.getTimeNs();\n        // collect live node names for optional redaction\n        liveNodes.addAll(clientCloudManager.getClusterStateProvider().getLiveNodes());\n        List<Suggester.SuggestionInfo> suggestions = PolicyHelper.getSuggestions(config, clientCloudManager);\n        long end = TimeSource.NANO_TIME.getTimeNs();\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n        log.info(\"- calculating diagnostics...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        // update the live nodes\n        liveNodes.addAll(clientCloudManager.getClusterStateProvider().getLiveNodes());\n        Policy.Session session = config.getPolicy().createSession(clientCloudManager);\n        MapWriter mw = PolicyHelper.getDiagnostics(session);\n        Map<String, Object> diagnostics = new LinkedHashMap<>();\n        mw.toMap(diagnostics);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n        boolean withSuggestions = cli.hasOption(\"s\");\n        boolean withDiagnostics = cli.hasOption(\"d\") || cli.hasOption(\"n\");\n        boolean withSortedNodes = cli.hasOption(\"n\");\n        boolean withClusterState = cli.hasOption(\"c\");\n        boolean withStats = cli.hasOption(\"stats\");\n        boolean redact = cli.hasOption(\"r\");\n        if (cli.hasOption(\"all\")) {\n          withSuggestions = true;\n          withDiagnostics = true;\n          withSortedNodes = true;\n          withClusterState = true;\n          withStats = true;\n        }\n        // prepare to redact also host names / IPs in base_url and other properties\n        Set<String> redactNames = new HashSet<>();\n        for (String nodeName : liveNodes) {\n          String urlString = Utils.getBaseUrlForNodeName(nodeName, \"http\");\n          try {\n            URL u = new URL(urlString);\n            // protocol format\n            redactNames.add(u.getHost() + \":\" + u.getPort());\n            // node name format\n            redactNames.add(u.getHost() + \"_\" + u.getPort() + \"_\");\n          } catch (MalformedURLException e) {\n            log.warn(\"Invalid URL for node name \" + nodeName + \", replacing including protocol and path\", e);\n            redactNames.add(urlString);\n            redactNames.add(Utils.getBaseUrlForNodeName(nodeName, \"https\"));\n          }\n        }\n        // redact collection names too\n        Set<String> redactCollections = new HashSet<>();\n        ClusterState clusterState = clientCloudManager.getClusterStateProvider().getClusterState();\n        clusterState.forEachCollection(coll -> redactCollections.add(coll.getName()));\n        if (!withSuggestions && !withDiagnostics) {\n          withSuggestions = true;\n        }\n        Map<String, Object> results = new LinkedHashMap<>();\n        if (withClusterState) {\n          Map<String, Object> map = new LinkedHashMap<>();\n          map.put(\"znodeVersion\", clusterState.getZNodeVersion());\n          map.put(\"liveNodes\", new TreeSet<>(clusterState.getLiveNodes()));\n          map.put(\"collections\", clusterState.getCollectionsMap());\n          results.put(\"CLUSTERSTATE\", map);\n        }\n        if (withStats) {\n          Map<String, Map<String, Number>> collStats = new TreeMap<>();\n          clusterState.forEachCollection(coll -> {\n            Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n            AtomicInteger numCores = new AtomicInteger();\n            HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n            coll.getSlices().forEach(s -> {\n              numCores.addAndGet(s.getReplicas().size());\n              s.getReplicas().forEach(r -> {\n                nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n                    .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n              });\n            });\n            int maxCoresPerNode = 0;\n            int minCoresPerNode = 0;\n            int maxActualShardsPerNode = 0;\n            int minActualShardsPerNode = 0;\n            int maxShardReplicasPerNode = 0;\n            int minShardReplicasPerNode = 0;\n            if (!nodes.isEmpty()) {\n              minCoresPerNode = Integer.MAX_VALUE;\n              minActualShardsPerNode = Integer.MAX_VALUE;\n              minShardReplicasPerNode = Integer.MAX_VALUE;\n              for (Map<String, AtomicInteger> counts : nodes.values()) {\n                int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n                for (AtomicInteger count : counts.values()) {\n                  if (count.get() > maxShardReplicasPerNode) {\n                    maxShardReplicasPerNode = count.get();\n                  }\n                  if (count.get() < minShardReplicasPerNode) {\n                    minShardReplicasPerNode = count.get();\n                  }\n                }\n                if (total > maxCoresPerNode) {\n                  maxCoresPerNode = total;\n                }\n                if (total < minCoresPerNode) {\n                  minCoresPerNode = total;\n                }\n                if (counts.size() > maxActualShardsPerNode) {\n                  maxActualShardsPerNode = counts.size();\n                }\n                if (counts.size() < minActualShardsPerNode) {\n                  minActualShardsPerNode = counts.size();\n                }\n              }\n            }\n            perColl.put(\"activeShards\", coll.getActiveSlices().size());\n            perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n            perColl.put(\"rf\", coll.getReplicationFactor());\n            perColl.put(\"maxShardsPerNode\", coll.getMaxShardsPerNode());\n            perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n            perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n            perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n            perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n            perColl.put(\"numCores\", numCores.get());\n            perColl.put(\"numNodes\", nodes.size());\n            perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n            perColl.put(\"minCoresPerNode\", minCoresPerNode);\n          });\n          Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n          for (Row row : session.getSortedNodes()) {\n            Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n            nodeStat.put(\"isLive\", row.isLive());\n            nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n            nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n            nodeStat.put(\"cores\", row.getVal(\"cores\", 0));\n            Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n            row.forEachReplica(ri -> {\n              Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n                  .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n              perReplica.put(\"INDEX.sizeInGB\", ri.getVariable(\"INDEX.sizeInGB\"));\n              perReplica.put(\"coreNode\", ri.getName());\n              if (ri.getBool(\"leader\", false)) {\n                perReplica.put(\"leader\", true);\n                Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n                    .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n                Number riSize = (Number)ri.getVariable(\"INDEX.sizeInGB\");\n                if (riSize != null) {\n                  totalSize += riSize.doubleValue();\n                  collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n                  Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n                  if (max == null) max = 0.0;\n                  if (riSize.doubleValue() > max) {\n                    collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n                  }\n                  Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n                  if (min == null) min = Double.MAX_VALUE;\n                  if (riSize.doubleValue() < min) {\n                    collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n                  }\n                }\n              }\n              nodeStat.put(\"replicas\", collReplicas);\n            });\n          }\n\n          // calculate average per shard\n          for (Map<String, Number> perColl : collStats.values()) {\n            Double avg = (Double)perColl.get(\"avgShardSize\");\n            if (avg != null) {\n              avg = avg / ((Number)perColl.get(\"activeShards\")).doubleValue();\n              perColl.put(\"avgShardSize\", avg);\n            }\n          }\n          Map<String, Object> stats = new LinkedHashMap<>();\n          results.put(\"STATISTICS\", stats);\n          stats.put(\"nodeStats\", nodeStats);\n          stats.put(\"collectionStats\", collStats);\n        }\n        if (withSuggestions) {\n          results.put(\"SUGGESTIONS\", suggestions);\n        }\n        if (!withSortedNodes) {\n          diagnostics.remove(\"sortedNodes\");\n        }\n        if (withDiagnostics) {\n          results.put(\"DIAGNOSTICS\", diagnostics);\n        }\n        String data = Utils.toJSONString(results);\n        if (redact) {\n          data = RedactionUtils.redactNames(redactCollections, COLL_REDACTION_PREFIX, data);\n          data = RedactionUtils.redactNames(redactNames, NODE_REDACTION_PREFIX, data);\n        }\n        stdout.println(data);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5","date":1556572478,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#runCloudTool(CloudSolrClient,CommandLine).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#runCloudTool(CloudSolrClient,CommandLine).mjava","sourceNew":"    @Override\n    protected void runCloudTool(CloudSolrClient cloudSolrClient, CommandLine cli) throws Exception {\n      DistributedQueueFactory dummmyFactory = new DistributedQueueFactory() {\n        @Override\n        public DistributedQueue makeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"makeQueue\");\n        }\n\n        @Override\n        public void removeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"removeQueue\");\n        }\n      };\n      try (SolrClientCloudManager realCloudManager = new SolrClientCloudManager(dummmyFactory, cloudSolrClient)) {\n        AutoScalingConfig config = null;\n        HashSet<String> liveNodes = new HashSet<>();\n        String configFile = cli.getOptionValue(\"a\");\n        if (configFile != null) {\n          if (verbose) {\n            log.info(\"- reading autoscaling config from \" + configFile);\n          }\n          config = new AutoScalingConfig(IOUtils.toByteArray(new FileInputStream(configFile)));\n        } else {\n          if (verbose) {\n            log.info(\"- reading autoscaling config from the cluster.\");\n          }\n          config = realCloudManager.getDistribStateManager().getAutoScalingConfig();\n        }\n        // freeze the cluster state\n        SimCloudManager cloudManager = SimCloudManager.createCluster(realCloudManager, TimeSource.get(\"simTime:50\"));\n        liveNodes.addAll(cloudManager.getClusterStateProvider().getLiveNodes());\n        boolean withSuggestions = cli.hasOption(\"s\");\n        boolean withDiagnostics = cli.hasOption(\"d\") || cli.hasOption(\"n\");\n        boolean withSortedNodes = cli.hasOption(\"n\");\n        boolean withClusterState = cli.hasOption(\"c\");\n        boolean withStats = cli.hasOption(\"stats\");\n        boolean redact = cli.hasOption(\"r\");\n        if (cli.hasOption(\"all\")) {\n          withSuggestions = true;\n          withDiagnostics = true;\n          withSortedNodes = true;\n          withClusterState = true;\n          withStats = true;\n        }\n        // prepare to redact also host names / IPs in base_url and other properties\n        Set<String> redactNames = new HashSet<>();\n        for (String nodeName : liveNodes) {\n          String urlString = Utils.getBaseUrlForNodeName(nodeName, \"http\");\n          try {\n            URL u = new URL(urlString);\n            // protocol format\n            redactNames.add(u.getHost() + \":\" + u.getPort());\n            // node name format\n            redactNames.add(u.getHost() + \"_\" + u.getPort() + \"_\");\n          } catch (MalformedURLException e) {\n            log.warn(\"Invalid URL for node name \" + nodeName + \", replacing including protocol and path\", e);\n            redactNames.add(urlString);\n            redactNames.add(Utils.getBaseUrlForNodeName(nodeName, \"https\"));\n          }\n        }\n        // redact collection names too\n        Set<String> redactCollections = new HashSet<>();\n        ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n        clusterState.forEachCollection(coll -> redactCollections.add(coll.getName()));\n        if (!withSuggestions && !withDiagnostics) {\n          withSuggestions = true;\n        }\n        Map<String, Object> results = prepareResults(cloudManager, config, withClusterState,\n            withStats, withSuggestions, withSortedNodes, withDiagnostics);\n        if (cli.hasOption(\"simulate\")) {\n          String iterStr = cli.getOptionValue(\"i\", \"10\");\n          int iterations;\n          try {\n            iterations = Integer.parseInt(iterStr);\n          } catch (Exception e) {\n            log.warn(\"Invalid option 'i' value, using default 10:\" + e);\n            iterations = 10;\n          }\n          Map<String, Object> simulationResults = new HashMap<>();\n          simulate(cloudManager, config, simulationResults, withClusterState,\n              withStats, withSuggestions, withSortedNodes, withDiagnostics, iterations);\n          results.put(\"simulation\", simulationResults);\n        }\n        String data = Utils.toJSONString(results);\n        if (redact) {\n          data = RedactionUtils.redactNames(redactCollections, COLL_REDACTION_PREFIX, data);\n          data = RedactionUtils.redactNames(redactNames, NODE_REDACTION_PREFIX, data);\n        }\n        stdout.println(data);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void runCloudTool(CloudSolrClient cloudSolrClient, CommandLine cli) throws Exception {\n      DistributedQueueFactory dummmyFactory = new DistributedQueueFactory() {\n        @Override\n        public DistributedQueue makeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"makeQueue\");\n        }\n\n        @Override\n        public void removeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"removeQueue\");\n        }\n      };\n      try (SolrClientCloudManager clientCloudManager = new SolrClientCloudManager(dummmyFactory, cloudSolrClient)) {\n        AutoScalingConfig config = null;\n        HashSet<String> liveNodes = new HashSet<>();\n        String configFile = cli.getOptionValue(\"a\");\n        if (configFile != null) {\n          log.info(\"- reading autoscaling config from \" + configFile);\n          config = new AutoScalingConfig(IOUtils.toByteArray(new FileInputStream(configFile)));\n        } else {\n          log.info(\"- reading autoscaling config from the cluster.\");\n          config = clientCloudManager.getDistribStateManager().getAutoScalingConfig();\n        }\n        log.info(\"- calculating suggestions...\");\n        long start = TimeSource.NANO_TIME.getTimeNs();\n        // collect live node names for optional redaction\n        liveNodes.addAll(clientCloudManager.getClusterStateProvider().getLiveNodes());\n        List<Suggester.SuggestionInfo> suggestions = PolicyHelper.getSuggestions(config, clientCloudManager);\n        long end = TimeSource.NANO_TIME.getTimeNs();\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n        log.info(\"- calculating diagnostics...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        // update the live nodes\n        liveNodes.addAll(clientCloudManager.getClusterStateProvider().getLiveNodes());\n        Policy.Session session = config.getPolicy().createSession(clientCloudManager);\n        MapWriter mw = PolicyHelper.getDiagnostics(session);\n        Map<String, Object> diagnostics = new LinkedHashMap<>();\n        mw.toMap(diagnostics);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n        boolean withSuggestions = cli.hasOption(\"s\");\n        boolean withDiagnostics = cli.hasOption(\"d\") || cli.hasOption(\"n\");\n        boolean withSortedNodes = cli.hasOption(\"n\");\n        boolean withClusterState = cli.hasOption(\"c\");\n        boolean withStats = cli.hasOption(\"stats\");\n        boolean redact = cli.hasOption(\"r\");\n        if (cli.hasOption(\"all\")) {\n          withSuggestions = true;\n          withDiagnostics = true;\n          withSortedNodes = true;\n          withClusterState = true;\n          withStats = true;\n        }\n        // prepare to redact also host names / IPs in base_url and other properties\n        Set<String> redactNames = new HashSet<>();\n        for (String nodeName : liveNodes) {\n          String urlString = Utils.getBaseUrlForNodeName(nodeName, \"http\");\n          try {\n            URL u = new URL(urlString);\n            // protocol format\n            redactNames.add(u.getHost() + \":\" + u.getPort());\n            // node name format\n            redactNames.add(u.getHost() + \"_\" + u.getPort() + \"_\");\n          } catch (MalformedURLException e) {\n            log.warn(\"Invalid URL for node name \" + nodeName + \", replacing including protocol and path\", e);\n            redactNames.add(urlString);\n            redactNames.add(Utils.getBaseUrlForNodeName(nodeName, \"https\"));\n          }\n        }\n        // redact collection names too\n        Set<String> redactCollections = new HashSet<>();\n        ClusterState clusterState = clientCloudManager.getClusterStateProvider().getClusterState();\n        clusterState.forEachCollection(coll -> redactCollections.add(coll.getName()));\n        if (!withSuggestions && !withDiagnostics) {\n          withSuggestions = true;\n        }\n        Map<String, Object> results = new LinkedHashMap<>();\n        if (withClusterState) {\n          Map<String, Object> map = new LinkedHashMap<>();\n          map.put(\"znodeVersion\", clusterState.getZNodeVersion());\n          map.put(\"liveNodes\", new TreeSet<>(clusterState.getLiveNodes()));\n          map.put(\"collections\", clusterState.getCollectionsMap());\n          results.put(\"CLUSTERSTATE\", map);\n        }\n        if (withStats) {\n          Map<String, Map<String, Number>> collStats = new TreeMap<>();\n          clusterState.forEachCollection(coll -> {\n            Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n            AtomicInteger numCores = new AtomicInteger();\n            HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n            coll.getSlices().forEach(s -> {\n              numCores.addAndGet(s.getReplicas().size());\n              s.getReplicas().forEach(r -> {\n                nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n                    .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n              });\n            });\n            int maxCoresPerNode = 0;\n            int minCoresPerNode = 0;\n            int maxActualShardsPerNode = 0;\n            int minActualShardsPerNode = 0;\n            int maxShardReplicasPerNode = 0;\n            int minShardReplicasPerNode = 0;\n            if (!nodes.isEmpty()) {\n              minCoresPerNode = Integer.MAX_VALUE;\n              minActualShardsPerNode = Integer.MAX_VALUE;\n              minShardReplicasPerNode = Integer.MAX_VALUE;\n              for (Map<String, AtomicInteger> counts : nodes.values()) {\n                int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n                for (AtomicInteger count : counts.values()) {\n                  if (count.get() > maxShardReplicasPerNode) {\n                    maxShardReplicasPerNode = count.get();\n                  }\n                  if (count.get() < minShardReplicasPerNode) {\n                    minShardReplicasPerNode = count.get();\n                  }\n                }\n                if (total > maxCoresPerNode) {\n                  maxCoresPerNode = total;\n                }\n                if (total < minCoresPerNode) {\n                  minCoresPerNode = total;\n                }\n                if (counts.size() > maxActualShardsPerNode) {\n                  maxActualShardsPerNode = counts.size();\n                }\n                if (counts.size() < minActualShardsPerNode) {\n                  minActualShardsPerNode = counts.size();\n                }\n              }\n            }\n            perColl.put(\"activeShards\", coll.getActiveSlices().size());\n            perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n            perColl.put(\"rf\", coll.getReplicationFactor());\n            perColl.put(\"maxShardsPerNode\", coll.getMaxShardsPerNode());\n            perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n            perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n            perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n            perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n            perColl.put(\"numCores\", numCores.get());\n            perColl.put(\"numNodes\", nodes.size());\n            perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n            perColl.put(\"minCoresPerNode\", minCoresPerNode);\n          });\n          Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n          Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n          for (Row row : session.getSortedNodes()) {\n            Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n            nodeStat.put(\"isLive\", row.isLive());\n            nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n            nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n            int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n            nodeStat.put(\"cores\", cores);\n            coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n            Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n            row.forEachReplica(ri -> {\n              Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n                  .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n              perReplica.put(\"INDEX.sizeInGB\", ri.getVariable(\"INDEX.sizeInGB\"));\n              perReplica.put(\"coreNode\", ri.getName());\n              if (ri.getBool(\"leader\", false)) {\n                perReplica.put(\"leader\", true);\n                Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n                    .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n                Number riSize = (Number)ri.getVariable(\"INDEX.sizeInGB\");\n                if (riSize != null) {\n                  totalSize += riSize.doubleValue();\n                  collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n                  Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n                  if (max == null) max = 0.0;\n                  if (riSize.doubleValue() > max) {\n                    collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n                  }\n                  Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n                  if (min == null) min = Double.MAX_VALUE;\n                  if (riSize.doubleValue() < min) {\n                    collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n                  }\n                }\n              }\n              nodeStat.put(\"replicas\", collReplicas);\n            });\n          }\n\n          // calculate average per shard\n          for (Map<String, Number> perColl : collStats.values()) {\n            Double avg = (Double)perColl.get(\"avgShardSize\");\n            if (avg != null) {\n              avg = avg / ((Number)perColl.get(\"activeShards\")).doubleValue();\n              perColl.put(\"avgShardSize\", avg);\n            }\n          }\n          Map<String, Object> stats = new LinkedHashMap<>();\n          results.put(\"STATISTICS\", stats);\n          stats.put(\"coresPerNodes\", coreStats);\n          stats.put(\"nodeStats\", nodeStats);\n          stats.put(\"collectionStats\", collStats);\n        }\n        if (withSuggestions) {\n          results.put(\"SUGGESTIONS\", suggestions);\n        }\n        if (!withSortedNodes) {\n          diagnostics.remove(\"sortedNodes\");\n        }\n        if (withDiagnostics) {\n          results.put(\"DIAGNOSTICS\", diagnostics);\n        }\n        String data = Utils.toJSONString(results);\n        if (redact) {\n          data = RedactionUtils.redactNames(redactCollections, COLL_REDACTION_PREFIX, data);\n          data = RedactionUtils.redactNames(redactNames, NODE_REDACTION_PREFIX, data);\n        }\n        stdout.println(data);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"edf5b262a72d10530eb2f01dc8f19060355b213e","date":1557765866,"type":4,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#runCloudTool(CloudSolrClient,CommandLine).mjava","sourceNew":null,"sourceOld":"    @Override\n    protected void runCloudTool(CloudSolrClient cloudSolrClient, CommandLine cli) throws Exception {\n      DistributedQueueFactory dummmyFactory = new DistributedQueueFactory() {\n        @Override\n        public DistributedQueue makeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"makeQueue\");\n        }\n\n        @Override\n        public void removeQueue(String path) throws IOException {\n          throw new UnsupportedOperationException(\"removeQueue\");\n        }\n      };\n      try (SolrClientCloudManager realCloudManager = new SolrClientCloudManager(dummmyFactory, cloudSolrClient)) {\n        AutoScalingConfig config = null;\n        HashSet<String> liveNodes = new HashSet<>();\n        String configFile = cli.getOptionValue(\"a\");\n        if (configFile != null) {\n          if (verbose) {\n            log.info(\"- reading autoscaling config from \" + configFile);\n          }\n          config = new AutoScalingConfig(IOUtils.toByteArray(new FileInputStream(configFile)));\n        } else {\n          if (verbose) {\n            log.info(\"- reading autoscaling config from the cluster.\");\n          }\n          config = realCloudManager.getDistribStateManager().getAutoScalingConfig();\n        }\n        // freeze the cluster state\n        SimCloudManager cloudManager = SimCloudManager.createCluster(realCloudManager, TimeSource.get(\"simTime:50\"));\n        liveNodes.addAll(cloudManager.getClusterStateProvider().getLiveNodes());\n        boolean withSuggestions = cli.hasOption(\"s\");\n        boolean withDiagnostics = cli.hasOption(\"d\") || cli.hasOption(\"n\");\n        boolean withSortedNodes = cli.hasOption(\"n\");\n        boolean withClusterState = cli.hasOption(\"c\");\n        boolean withStats = cli.hasOption(\"stats\");\n        boolean redact = cli.hasOption(\"r\");\n        if (cli.hasOption(\"all\")) {\n          withSuggestions = true;\n          withDiagnostics = true;\n          withSortedNodes = true;\n          withClusterState = true;\n          withStats = true;\n        }\n        // prepare to redact also host names / IPs in base_url and other properties\n        Set<String> redactNames = new HashSet<>();\n        for (String nodeName : liveNodes) {\n          String urlString = Utils.getBaseUrlForNodeName(nodeName, \"http\");\n          try {\n            URL u = new URL(urlString);\n            // protocol format\n            redactNames.add(u.getHost() + \":\" + u.getPort());\n            // node name format\n            redactNames.add(u.getHost() + \"_\" + u.getPort() + \"_\");\n          } catch (MalformedURLException e) {\n            log.warn(\"Invalid URL for node name \" + nodeName + \", replacing including protocol and path\", e);\n            redactNames.add(urlString);\n            redactNames.add(Utils.getBaseUrlForNodeName(nodeName, \"https\"));\n          }\n        }\n        // redact collection names too\n        Set<String> redactCollections = new HashSet<>();\n        ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n        clusterState.forEachCollection(coll -> redactCollections.add(coll.getName()));\n        if (!withSuggestions && !withDiagnostics) {\n          withSuggestions = true;\n        }\n        Map<String, Object> results = prepareResults(cloudManager, config, withClusterState,\n            withStats, withSuggestions, withSortedNodes, withDiagnostics);\n        if (cli.hasOption(\"simulate\")) {\n          String iterStr = cli.getOptionValue(\"i\", \"10\");\n          int iterations;\n          try {\n            iterations = Integer.parseInt(iterStr);\n          } catch (Exception e) {\n            log.warn(\"Invalid option 'i' value, using default 10:\" + e);\n            iterations = 10;\n          }\n          Map<String, Object> simulationResults = new HashMap<>();\n          simulate(cloudManager, config, simulationResults, withClusterState,\n              withStats, withSuggestions, withSortedNodes, withDiagnostics, iterations);\n          results.put(\"simulation\", simulationResults);\n        }\n        String data = Utils.toJSONString(results);\n        if (redact) {\n          data = RedactionUtils.redactNames(redactCollections, COLL_REDACTION_PREFIX, data);\n          data = RedactionUtils.redactNames(redactNames, NODE_REDACTION_PREFIX, data);\n        }\n        stdout.println(data);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"edbd289ce380f0c9e1b3f3d53a1f096b0fdfd518":["76bb93998d4d4fa60da28429640216a0a249111a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"edf5b262a72d10530eb2f01dc8f19060355b213e":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["edbd289ce380f0c9e1b3f3d53a1f096b0fdfd518"],"76bb93998d4d4fa60da28429640216a0a249111a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["edf5b262a72d10530eb2f01dc8f19060355b213e"]},"commit2Childs":{"edbd289ce380f0c9e1b3f3d53a1f096b0fdfd518":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["76bb93998d4d4fa60da28429640216a0a249111a"],"edf5b262a72d10530eb2f01dc8f19060355b213e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["edf5b262a72d10530eb2f01dc8f19060355b213e"],"76bb93998d4d4fa60da28429640216a0a249111a":["edbd289ce380f0c9e1b3f3d53a1f096b0fdfd518"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}