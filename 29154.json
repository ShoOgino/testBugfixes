{"path":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","commits":[{"id":"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7","date":1255555265,"type":1,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader[])\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader[])\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ef82ff03e4016c705811b2658e81471a645c0e49","date":1255900293,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"ef82ff03e4016c705811b2658e81471a645c0e49":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["ef82ff03e4016c705811b2658e81471a645c0e49"]},"commit2Childs":{"ef82ff03e4016c705811b2658e81471a645c0e49":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["ef82ff03e4016c705811b2658e81471a645c0e49"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}