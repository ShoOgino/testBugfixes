{"path":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(Object,TermsEnum[],float).mjava","commits":[{"id":"9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595","date":1402950824,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(Object,TermsEnum[],float).mjava","pathOld":"/dev/null","sourceNew":"    /** \n     * Creates an ordinal map that allows mapping ords to/from a merged\n     * space from <code>subs</code>.\n     * @param owner a cache key\n     * @param subs TermsEnums that support {@link TermsEnum#ord()}. They need\n     *             not be dense (e.g. can be FilteredTermsEnums}.\n     * @throws IOException if an I/O error occurred.\n     */\n    public OrdinalMap(Object owner, TermsEnum subs[], float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      globalOrdDeltas = new MonotonicAppendingLongBuffer(PackedInts.COMPACT);\n      firstSegments = new AppendingPackedLongBuffer(PackedInts.COMPACT);\n      final MonotonicAppendingLongBuffer[] ordDeltas = new MonotonicAppendingLongBuffer[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = new MonotonicAppendingLongBuffer(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[i], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // for each unique term, just mark the first segment index/delta where it occurs\n          if (i == 0) {\n            firstSegments.add(segmentIndex);\n            globalOrdDeltas.add(delta);\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        globalOrd++;\n      }\n      firstSegments.freeze();\n      globalOrdDeltas.freeze();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        ordDeltas[i].freeze();\n      }\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + globalOrdDeltas.ramBytesUsed() + firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds);\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final MonotonicAppendingLongBuffer deltas = ordDeltas[i];\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final MonotonicAppendingLongBuffer.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["140b7301c4cfdc226d0fc8fcb4b46977fd1b49a6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f6d0aee18c1653f7ee634fa8830abdb001dcfe1b","date":1402998114,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(Object,TermsEnum[],float).mjava","pathOld":"/dev/null","sourceNew":"    /** \n     * Creates an ordinal map that allows mapping ords to/from a merged\n     * space from <code>subs</code>.\n     * @param owner a cache key\n     * @param subs TermsEnums that support {@link TermsEnum#ord()}. They need\n     *             not be dense (e.g. can be FilteredTermsEnums}.\n     * @throws IOException if an I/O error occurred.\n     */\n    public OrdinalMap(Object owner, TermsEnum subs[], float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      globalOrdDeltas = new MonotonicAppendingLongBuffer(PackedInts.COMPACT);\n      firstSegments = new AppendingPackedLongBuffer(PackedInts.COMPACT);\n      final MonotonicAppendingLongBuffer[] ordDeltas = new MonotonicAppendingLongBuffer[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = new MonotonicAppendingLongBuffer(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[i], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // for each unique term, just mark the first segment index/delta where it occurs\n          if (i == 0) {\n            firstSegments.add(segmentIndex);\n            globalOrdDeltas.add(delta);\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        globalOrd++;\n      }\n      firstSegments.freeze();\n      globalOrdDeltas.freeze();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        ordDeltas[i].freeze();\n      }\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + globalOrdDeltas.ramBytesUsed() + firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds);\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final MonotonicAppendingLongBuffer deltas = ordDeltas[i];\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final MonotonicAppendingLongBuffer.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4f5cef0a99b295b8d6da564f186ae815707a86d","date":1403226156,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(Object,TermsEnum[],float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(Object,TermsEnum[],float).mjava","sourceNew":"    /** \n     * Creates an ordinal map that allows mapping ords to/from a merged\n     * space from <code>subs</code>.\n     * @param owner a cache key\n     * @param subs TermsEnums that support {@link TermsEnum#ord()}. They need\n     *             not be dense (e.g. can be FilteredTermsEnums}.\n     * @throws IOException if an I/O error occurred.\n     */\n    public OrdinalMap(Object owner, TermsEnum subs[], float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      globalOrdDeltas = new MonotonicAppendingLongBuffer(PackedInts.COMPACT);\n      firstSegments = new AppendingPackedLongBuffer(PackedInts.COMPACT);\n      final MonotonicAppendingLongBuffer[] ordDeltas = new MonotonicAppendingLongBuffer[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = new MonotonicAppendingLongBuffer(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[i], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // for each unique term, just mark the first segment index/delta where it occurs\n          if (i == 0) {\n            firstSegments.add(segmentIndex);\n            globalOrdDeltas.add(delta);\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        globalOrd++;\n      }\n      firstSegments.freeze();\n      globalOrdDeltas.freeze();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        ordDeltas[i].freeze();\n      }\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + globalOrdDeltas.ramBytesUsed() + firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds);\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final MonotonicAppendingLongBuffer deltas = ordDeltas[i];\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final MonotonicAppendingLongBuffer.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","sourceOld":"    /** \n     * Creates an ordinal map that allows mapping ords to/from a merged\n     * space from <code>subs</code>.\n     * @param owner a cache key\n     * @param subs TermsEnums that support {@link TermsEnum#ord()}. They need\n     *             not be dense (e.g. can be FilteredTermsEnums}.\n     * @throws IOException if an I/O error occurred.\n     */\n    public OrdinalMap(Object owner, TermsEnum subs[], float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      globalOrdDeltas = new MonotonicAppendingLongBuffer(PackedInts.COMPACT);\n      firstSegments = new AppendingPackedLongBuffer(PackedInts.COMPACT);\n      final MonotonicAppendingLongBuffer[] ordDeltas = new MonotonicAppendingLongBuffer[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = new MonotonicAppendingLongBuffer(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[i], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // for each unique term, just mark the first segment index/delta where it occurs\n          if (i == 0) {\n            firstSegments.add(segmentIndex);\n            globalOrdDeltas.add(delta);\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        globalOrd++;\n      }\n      firstSegments.freeze();\n      globalOrdDeltas.freeze();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        ordDeltas[i].freeze();\n      }\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + globalOrdDeltas.ramBytesUsed() + firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds);\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final MonotonicAppendingLongBuffer deltas = ordDeltas[i];\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final MonotonicAppendingLongBuffer.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"140b7301c4cfdc226d0fc8fcb4b46977fd1b49a6","date":1403266951,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(Object,TermsEnum[],float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(Object,TermsEnum[],float).mjava","sourceNew":"    /** \n     * Creates an ordinal map that allows mapping ords to/from a merged\n     * space from <code>subs</code>.\n     * @param owner a cache key\n     * @param subs TermsEnums that support {@link TermsEnum#ord()}. They need\n     *             not be dense (e.g. can be FilteredTermsEnums}.\n     * @throws IOException if an I/O error occurred.\n     */\n    public OrdinalMap(Object owner, TermsEnum subs[], float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      globalOrdDeltas = new MonotonicAppendingLongBuffer(PackedInts.COMPACT);\n      firstSegments = new AppendingPackedLongBuffer(PackedInts.COMPACT);\n      final MonotonicAppendingLongBuffer[] ordDeltas = new MonotonicAppendingLongBuffer[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = new MonotonicAppendingLongBuffer(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[i], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        int firstSegmentIndex = Integer.MAX_VALUE;\n        long globalOrdDelta = Long.MAX_VALUE;\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // We compute the least segment where the term occurs. In case the\n          // first segment contains most (or better all) values, this will\n          // help save significant memory\n          if (segmentIndex < firstSegmentIndex) {\n            firstSegmentIndex = segmentIndex;\n            globalOrdDelta = delta;\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        // for each unique term, just mark the first segment index/delta where it occurs\n        assert firstSegmentIndex < segmentOrds.length;\n        firstSegments.add(firstSegmentIndex);\n        globalOrdDeltas.add(globalOrdDelta);\n        globalOrd++;\n      }\n      firstSegments.freeze();\n      globalOrdDeltas.freeze();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        ordDeltas[i].freeze();\n      }\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + globalOrdDeltas.ramBytesUsed() + firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds);\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final MonotonicAppendingLongBuffer deltas = ordDeltas[i];\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final MonotonicAppendingLongBuffer.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","sourceOld":"    /** \n     * Creates an ordinal map that allows mapping ords to/from a merged\n     * space from <code>subs</code>.\n     * @param owner a cache key\n     * @param subs TermsEnums that support {@link TermsEnum#ord()}. They need\n     *             not be dense (e.g. can be FilteredTermsEnums}.\n     * @throws IOException if an I/O error occurred.\n     */\n    public OrdinalMap(Object owner, TermsEnum subs[], float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      globalOrdDeltas = new MonotonicAppendingLongBuffer(PackedInts.COMPACT);\n      firstSegments = new AppendingPackedLongBuffer(PackedInts.COMPACT);\n      final MonotonicAppendingLongBuffer[] ordDeltas = new MonotonicAppendingLongBuffer[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = new MonotonicAppendingLongBuffer(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[i], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // for each unique term, just mark the first segment index/delta where it occurs\n          if (i == 0) {\n            firstSegments.add(segmentIndex);\n            globalOrdDeltas.add(delta);\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        globalOrd++;\n      }\n      firstSegments.freeze();\n      globalOrdDeltas.freeze();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        ordDeltas[i].freeze();\n      }\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + globalOrdDeltas.ramBytesUsed() + firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds);\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final MonotonicAppendingLongBuffer deltas = ordDeltas[i];\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final MonotonicAppendingLongBuffer.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","bugFix":["9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5bcfd864fb8b916f7d21f2579d2010a31892055d","date":1403359094,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(Object,TermsEnum[],SegmentMap,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(Object,TermsEnum[],float).mjava","sourceNew":"    OrdinalMap(Object owner, TermsEnum subs[], SegmentMap segmentMap, float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      this.segmentMap = segmentMap;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      globalOrdDeltas = new MonotonicAppendingLongBuffer(PackedInts.COMPACT);\n      firstSegments = new AppendingPackedLongBuffer(PackedInts.COMPACT);\n      final MonotonicAppendingLongBuffer[] ordDeltas = new MonotonicAppendingLongBuffer[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = new MonotonicAppendingLongBuffer(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[segmentMap.newToOld(i)], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        int firstSegmentIndex = Integer.MAX_VALUE;\n        long globalOrdDelta = Long.MAX_VALUE;\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // We compute the least segment where the term occurs. In case the\n          // first segment contains most (or better all) values, this will\n          // help save significant memory\n          if (segmentIndex < firstSegmentIndex) {\n            firstSegmentIndex = segmentIndex;\n            globalOrdDelta = delta;\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        // for each unique term, just mark the first segment index/delta where it occurs\n        assert firstSegmentIndex < segmentOrds.length;\n        firstSegments.add(firstSegmentIndex);\n        globalOrdDeltas.add(globalOrdDelta);\n        globalOrd++;\n      }\n      firstSegments.freeze();\n      globalOrdDeltas.freeze();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        ordDeltas[i].freeze();\n      }\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + globalOrdDeltas.ramBytesUsed()\n          + firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds)\n          + segmentMap.ramBytesUsed();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final MonotonicAppendingLongBuffer deltas = ordDeltas[i];\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final MonotonicAppendingLongBuffer.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","sourceOld":"    /** \n     * Creates an ordinal map that allows mapping ords to/from a merged\n     * space from <code>subs</code>.\n     * @param owner a cache key\n     * @param subs TermsEnums that support {@link TermsEnum#ord()}. They need\n     *             not be dense (e.g. can be FilteredTermsEnums}.\n     * @throws IOException if an I/O error occurred.\n     */\n    public OrdinalMap(Object owner, TermsEnum subs[], float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      globalOrdDeltas = new MonotonicAppendingLongBuffer(PackedInts.COMPACT);\n      firstSegments = new AppendingPackedLongBuffer(PackedInts.COMPACT);\n      final MonotonicAppendingLongBuffer[] ordDeltas = new MonotonicAppendingLongBuffer[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = new MonotonicAppendingLongBuffer(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[i], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        int firstSegmentIndex = Integer.MAX_VALUE;\n        long globalOrdDelta = Long.MAX_VALUE;\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // We compute the least segment where the term occurs. In case the\n          // first segment contains most (or better all) values, this will\n          // help save significant memory\n          if (segmentIndex < firstSegmentIndex) {\n            firstSegmentIndex = segmentIndex;\n            globalOrdDelta = delta;\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        // for each unique term, just mark the first segment index/delta where it occurs\n        assert firstSegmentIndex < segmentOrds.length;\n        firstSegments.add(firstSegmentIndex);\n        globalOrdDeltas.add(globalOrdDelta);\n        globalOrd++;\n      }\n      firstSegments.freeze();\n      globalOrdDeltas.freeze();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        ordDeltas[i].freeze();\n      }\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + globalOrdDeltas.ramBytesUsed() + firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds);\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final MonotonicAppendingLongBuffer deltas = ordDeltas[i];\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final MonotonicAppendingLongBuffer.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f6d0aee18c1653f7ee634fa8830abdb001dcfe1b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595"],"140b7301c4cfdc226d0fc8fcb4b46977fd1b49a6":["d4f5cef0a99b295b8d6da564f186ae815707a86d"],"d4f5cef0a99b295b8d6da564f186ae815707a86d":["9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5bcfd864fb8b916f7d21f2579d2010a31892055d":["140b7301c4cfdc226d0fc8fcb4b46977fd1b49a6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5bcfd864fb8b916f7d21f2579d2010a31892055d"]},"commit2Childs":{"f6d0aee18c1653f7ee634fa8830abdb001dcfe1b":[],"140b7301c4cfdc226d0fc8fcb4b46977fd1b49a6":["5bcfd864fb8b916f7d21f2579d2010a31892055d"],"d4f5cef0a99b295b8d6da564f186ae815707a86d":["140b7301c4cfdc226d0fc8fcb4b46977fd1b49a6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f6d0aee18c1653f7ee634fa8830abdb001dcfe1b","9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595"],"9b25d26ad3e8f824d95db88ecc5e5d9d71d3c595":["f6d0aee18c1653f7ee634fa8830abdb001dcfe1b","d4f5cef0a99b295b8d6da564f186ae815707a86d"],"5bcfd864fb8b916f7d21f2579d2010a31892055d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f6d0aee18c1653f7ee634fa8830abdb001dcfe1b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}