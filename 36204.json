{"path":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","commits":[{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":1,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    waitForThingsToLevelOut(30);\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    waitForThingsToLevelOut(30);\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7eb9eeb98092dd50454c2889785b80bfeb3119a","date":1452782401,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForRecoveriesToFinish(false, 45);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    waitForThingsToLevelOut(30);\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0158ced21948b6626f733c1c42c1e18d94449789","date":1462994341,"type":3,"author":"Bartosz Krasiński","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForRecoveriesToFinish(false, 45);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundant deletes?)\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForRecoveriesToFinish(false, 45);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5ebf70dabe6279454c5ff460bdea3f0dc2814a86","date":1463672611,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    \n    // some docs with no expiration\n    UpdateRequest req1 = new UpdateRequest();\n    for (int i = 1; i <= 100; i++) {\n      req1.add(sdoc(\"id\", i));\n    }\n    req1.commit(cluster.getSolrClient(), COLLECTION);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n    // add a doc with a short TTL \n    new UpdateRequest().add(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\")).commit(cluster.getSolrClient(), COLLECTION);\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    DocCollection collectionState = cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION);\n    for (Replica replica : collectionState.getReplicas()) {\n      coresCompared++;\n      String name = replica.getName();\n      String core = replica.getCoreName();\n      Long initVersion = initIndexVersions.get(core);\n      Long finalVersion = finalIndexVersions.get(core);\n      assertNotNull(name + \": no init version for core: \" + core, initVersion);\n      assertNotNull(name + \": no final version for core: \" + core, finalVersion);\n\n      if (!initVersion.equals(finalVersion)) {\n        nodesThatChange.add(core + \"(\"+name+\")\");\n        shardsThatChange.add(name);\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundant deletes?)\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForRecoveriesToFinish(false, 45);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundant deletes?)\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    \n    // some docs with no expiration\n    UpdateRequest req1 = new UpdateRequest();\n    for (int i = 1; i <= 100; i++) {\n      req1.add(sdoc(\"id\", i));\n    }\n    req1.commit(cluster.getSolrClient(), COLLECTION);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n    // add a doc with a short TTL \n    new UpdateRequest().add(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\")).commit(cluster.getSolrClient(), COLLECTION);\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    DocCollection collectionState = cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION);\n    for (Replica replica : collectionState.getReplicas()) {\n      coresCompared++;\n      String name = replica.getName();\n      String core = replica.getCoreName();\n      Long initVersion = initIndexVersions.get(core);\n      Long finalVersion = finalIndexVersions.get(core);\n      assertNotNull(name + \": no init version for core: \" + core, initVersion);\n      assertNotNull(name + \": no final version for core: \" + core, finalVersion);\n\n      if (!initVersion.equals(finalVersion)) {\n        nodesThatChange.add(core + \"(\"+name+\")\");\n        shardsThatChange.add(name);\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundant deletes?)\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForRecoveriesToFinish(false, 45);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    \n    // some docs with no expiration\n    UpdateRequest req1 = new UpdateRequest();\n    for (int i = 1; i <= 100; i++) {\n      req1.add(sdoc(\"id\", i));\n    }\n    req1.commit(cluster.getSolrClient(), COLLECTION);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n    // add a doc with a short TTL \n    new UpdateRequest().add(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\")).commit(cluster.getSolrClient(), COLLECTION);\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    DocCollection collectionState = cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION);\n    for (Replica replica : collectionState.getReplicas()) {\n      coresCompared++;\n      String name = replica.getName();\n      String core = replica.getCoreName();\n      Long initVersion = initIndexVersions.get(core);\n      Long finalVersion = finalIndexVersions.get(core);\n      assertNotNull(name + \": no init version for core: \" + core, initVersion);\n      assertNotNull(name + \": no final version for core: \" + core, finalVersion);\n\n      if (!initVersion.equals(finalVersion)) {\n        nodesThatChange.add(core + \"(\"+name+\")\");\n        shardsThatChange.add(name);\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundant deletes?)\n\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForRecoveriesToFinish(false, 45);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c92ac83d1c2f8811300bb0df797465cca0aa8e92","date":1579710745,"type":4,"author":"Chris Hostetter","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void test() throws Exception {\n    \n    // some docs with no expiration\n    UpdateRequest req1 = new UpdateRequest();\n    for (int i = 1; i <= 100; i++) {\n      req1.add(sdoc(\"id\", i));\n    }\n    req1.commit(cluster.getSolrClient(), COLLECTION);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n    // add a doc with a short TTL \n    new UpdateRequest().add(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\")).commit(cluster.getSolrClient(), COLLECTION);\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    DocCollection collectionState = cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION);\n    for (Replica replica : collectionState.getReplicas()) {\n      coresCompared++;\n      String name = replica.getName();\n      String core = replica.getCoreName();\n      Long initVersion = initIndexVersions.get(core);\n      Long finalVersion = finalIndexVersions.get(core);\n      assertNotNull(name + \": no init version for core: \" + core, initVersion);\n      assertNotNull(name + \": no final version for core: \" + core, finalVersion);\n\n      if (!initVersion.equals(finalVersion)) {\n        nodesThatChange.add(core + \"(\"+name+\")\");\n        shardsThatChange.add(name);\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundant deletes?)\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c92ac83d1c2f8811300bb0df797465cca0aa8e92":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"abb23fcc2461782ab204e61213240feb77d355aa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a7eb9eeb98092dd50454c2889785b80bfeb3119a":["abb23fcc2461782ab204e61213240feb77d355aa"],"5ebf70dabe6279454c5ff460bdea3f0dc2814a86":["0158ced21948b6626f733c1c42c1e18d94449789"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a7eb9eeb98092dd50454c2889785b80bfeb3119a","d470c8182e92b264680e34081b75e70a9f2b3c89"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c92ac83d1c2f8811300bb0df797465cca0aa8e92"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["a7eb9eeb98092dd50454c2889785b80bfeb3119a","5ebf70dabe6279454c5ff460bdea3f0dc2814a86"],"0158ced21948b6626f733c1c42c1e18d94449789":["a7eb9eeb98092dd50454c2889785b80bfeb3119a"]},"commit2Childs":{"c92ac83d1c2f8811300bb0df797465cca0aa8e92":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"abb23fcc2461782ab204e61213240feb77d355aa":["a7eb9eeb98092dd50454c2889785b80bfeb3119a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["abb23fcc2461782ab204e61213240feb77d355aa"],"a7eb9eeb98092dd50454c2889785b80bfeb3119a":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","0158ced21948b6626f733c1c42c1e18d94449789"],"5ebf70dabe6279454c5ff460bdea3f0dc2814a86":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d470c8182e92b264680e34081b75e70a9f2b3c89":["c92ac83d1c2f8811300bb0df797465cca0aa8e92","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"0158ced21948b6626f733c1c42c1e18d94449789":["5ebf70dabe6279454c5ff460bdea3f0dc2814a86"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}