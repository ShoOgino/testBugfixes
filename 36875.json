{"path":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms,int,int,float).mjava","commits":[{"id":"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","date":1475611903,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms,int,int,float).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Constructor.\n   *\n   * @param vector        Terms that contains the data for\n   *                      creating the TokenStream. Must have positions and/or offsets.\n   * @param filteredDocId The docID we will process.\n   * @param offsetLength  Supply the character length of the text being uninverted, or a lower value if you don't want\n   *                      to invert text beyond an offset (in so doing this will act as a filter).  If you don't\n   *                      know the length, pass -1.  In conjunction with {@code loadFactor}, it's used to\n   *                      determine how many buckets to create during uninversion.\n   *                      It's also used to filter out tokens with a start offset exceeding this value.\n   * @param loadFactor    The percent of tokens from the original terms (by position count) that are\n   *                      expected to be inverted.  If they are filtered (e.g.\n   *                      {@link org.apache.lucene.index.FilterLeafReader.FilterTerms})\n   *                      then consider using less than 1.0 to avoid wasting space.\n   *                      1.0 means all, 1/64th would suggest 1/64th of all tokens coming from vector.\n   */\n  TokenStreamFromTermVector(Terms vector, int filteredDocId, int offsetLength, float loadFactor) throws IOException {\n    super();\n    this.filteredDocId = filteredDocId;\n    this.offsetLength = offsetLength == Integer.MAX_VALUE ? -1 : offsetLength;\n    if (loadFactor <= 0f || loadFactor > 1f) {\n      throw new IllegalArgumentException(\"loadFactor should be > 0 and <= 1\");\n    }\n    this.loadFactor = loadFactor;\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms,int,int,float).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Constructor.\n   *\n   * @param vector        Terms that contains the data for\n   *                      creating the TokenStream. Must have positions and/or offsets.\n   * @param filteredDocId The docID we will process.\n   * @param offsetLength  Supply the character length of the text being uninverted, or a lower value if you don't want\n   *                      to invert text beyond an offset (in so doing this will act as a filter).  If you don't\n   *                      know the length, pass -1.  In conjunction with {@code loadFactor}, it's used to\n   *                      determine how many buckets to create during uninversion.\n   *                      It's also used to filter out tokens with a start offset exceeding this value.\n   * @param loadFactor    The percent of tokens from the original terms (by position count) that are\n   *                      expected to be inverted.  If they are filtered (e.g.\n   *                      {@link org.apache.lucene.index.FilterLeafReader.FilterTerms})\n   *                      then consider using less than 1.0 to avoid wasting space.\n   *                      1.0 means all, 1/64th would suggest 1/64th of all tokens coming from vector.\n   */\n  TokenStreamFromTermVector(Terms vector, int filteredDocId, int offsetLength, float loadFactor) throws IOException {\n    super();\n    this.filteredDocId = filteredDocId;\n    this.offsetLength = offsetLength == Integer.MAX_VALUE ? -1 : offsetLength;\n    if (loadFactor <= 0f || loadFactor > 1f) {\n      throw new IllegalArgumentException(\"loadFactor should be > 0 and <= 1\");\n    }\n    this.loadFactor = loadFactor;\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2e9861e4a2b724d9fc51b618714c579491b78d7","date":1479244606,"type":4,"author":"David Smiley","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms,int,int,float).mjava","sourceNew":null,"sourceOld":"  /**\n   * Constructor.\n   *\n   * @param vector        Terms that contains the data for\n   *                      creating the TokenStream. Must have positions and/or offsets.\n   * @param filteredDocId The docID we will process.\n   * @param offsetLength  Supply the character length of the text being uninverted, or a lower value if you don't want\n   *                      to invert text beyond an offset (in so doing this will act as a filter).  If you don't\n   *                      know the length, pass -1.  In conjunction with {@code loadFactor}, it's used to\n   *                      determine how many buckets to create during uninversion.\n   *                      It's also used to filter out tokens with a start offset exceeding this value.\n   * @param loadFactor    The percent of tokens from the original terms (by position count) that are\n   *                      expected to be inverted.  If they are filtered (e.g.\n   *                      {@link org.apache.lucene.index.FilterLeafReader.FilterTerms})\n   *                      then consider using less than 1.0 to avoid wasting space.\n   *                      1.0 means all, 1/64th would suggest 1/64th of all tokens coming from vector.\n   */\n  TokenStreamFromTermVector(Terms vector, int filteredDocId, int offsetLength, float loadFactor) throws IOException {\n    super();\n    this.filteredDocId = filteredDocId;\n    this.offsetLength = offsetLength == Integer.MAX_VALUE ? -1 : offsetLength;\n    if (loadFactor <= 0f || loadFactor > 1f) {\n      throw new IllegalArgumentException(\"loadFactor should be > 0 and <= 1\");\n    }\n    this.loadFactor = loadFactor;\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1ef55e1fff7ff44354432770ad8bc19be1fcc75","date":1479266056,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms,int,int,float).mjava","sourceNew":null,"sourceOld":"  /**\n   * Constructor.\n   *\n   * @param vector        Terms that contains the data for\n   *                      creating the TokenStream. Must have positions and/or offsets.\n   * @param filteredDocId The docID we will process.\n   * @param offsetLength  Supply the character length of the text being uninverted, or a lower value if you don't want\n   *                      to invert text beyond an offset (in so doing this will act as a filter).  If you don't\n   *                      know the length, pass -1.  In conjunction with {@code loadFactor}, it's used to\n   *                      determine how many buckets to create during uninversion.\n   *                      It's also used to filter out tokens with a start offset exceeding this value.\n   * @param loadFactor    The percent of tokens from the original terms (by position count) that are\n   *                      expected to be inverted.  If they are filtered (e.g.\n   *                      {@link org.apache.lucene.index.FilterLeafReader.FilterTerms})\n   *                      then consider using less than 1.0 to avoid wasting space.\n   *                      1.0 means all, 1/64th would suggest 1/64th of all tokens coming from vector.\n   */\n  TokenStreamFromTermVector(Terms vector, int filteredDocId, int offsetLength, float loadFactor) throws IOException {\n    super();\n    this.filteredDocId = filteredDocId;\n    this.offsetLength = offsetLength == Integer.MAX_VALUE ? -1 : offsetLength;\n    if (loadFactor <= 0f || loadFactor > 1f) {\n      throw new IllegalArgumentException(\"loadFactor should be > 0 and <= 1\");\n    }\n    this.loadFactor = loadFactor;\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f2e9861e4a2b724d9fc51b618714c579491b78d7":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","f2e9861e4a2b724d9fc51b618714c579491b78d7"],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f2e9861e4a2b724d9fc51b618714c579491b78d7"]},"commit2Childs":{"f2e9861e4a2b724d9fc51b618714c579491b78d7":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":[],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["f2e9861e4a2b724d9fc51b618714c579491b78d7","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}