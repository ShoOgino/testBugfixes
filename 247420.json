{"path":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","commits":[{"id":"23e44daeaa8b89694d10df5999956c8e14a7dd09","date":1476689300,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","pathOld":"/dev/null","sourceNew":"  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    long numValues = 0;\n    long min = Long.MAX_VALUE;\n    long max = Long.MIN_VALUE;\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - min);\n          }\n        }\n\n        min = Math.min(min, v);\n        max = Math.max(max, v);\n\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n\n        numValues++;\n      }\n\n      numDocsWithValue++;\n    }\n\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {\n      numBitsPerValue = 0;\n      meta.writeInt(-1);\n    } else {\n      if (uniqueValues != null\n          && uniqueValues.size() > 1\n          && DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) < DirectWriter.unsignedBitsRequired((max - min) / gcd)) {\n        numBitsPerValue = DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1);\n        final Long[] sortedUniqueValues = uniqueValues.toArray(new Long[0]);\n        Arrays.sort(sortedUniqueValues);\n        meta.writeInt(sortedUniqueValues.length);\n        for (Long v : sortedUniqueValues) {\n          meta.writeLong(v);\n        }\n        encode = new HashMap<>();\n        for (int i = 0; i < sortedUniqueValues.length; ++i) {\n          encode.put(sortedUniqueValues[i], i);\n        }\n        min = 0;\n        gcd = 1;\n      } else {\n        uniqueValues = null;\n        numBitsPerValue = DirectWriter.unsignedBitsRequired((max - min) / gcd);\n        if (gcd == 1 && min > 0\n            && DirectWriter.unsignedBitsRequired(max) == DirectWriter.unsignedBitsRequired(max - min)) {\n          min = 0;\n        }\n        meta.writeInt(-1);\n      }\n    }\n\n    meta.writeByte((byte) numBitsPerValue);\n    meta.writeLong(min);\n    meta.writeLong(gcd);\n    long startOffset = data.getFilePointer();\n    meta.writeLong(startOffset);\n    if (numBitsPerValue != 0) {\n      values = valuesProducer.getSortedNumeric(field);\n      DirectWriter writer = DirectWriter.getInstance(data, numValues, numBitsPerValue);\n      for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n        for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n          long v = values.nextValue();\n          if (encode == null) {\n            writer.add((v - min) / gcd);\n          } else {\n            writer.add(encode.get(v));\n          }\n        }\n      }\n      writer.finish();\n    }\n    meta.writeLong(data.getFilePointer() - startOffset);\n\n    return new long[] {numDocsWithValue, numValues};\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","pathOld":"/dev/null","sourceNew":"  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    long numValues = 0;\n    long min = Long.MAX_VALUE;\n    long max = Long.MIN_VALUE;\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - min);\n          }\n        }\n\n        min = Math.min(min, v);\n        max = Math.max(max, v);\n\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n\n        numValues++;\n      }\n\n      numDocsWithValue++;\n    }\n\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {\n      numBitsPerValue = 0;\n      meta.writeInt(-1);\n    } else {\n      if (uniqueValues != null\n          && uniqueValues.size() > 1\n          && DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) < DirectWriter.unsignedBitsRequired((max - min) / gcd)) {\n        numBitsPerValue = DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1);\n        final Long[] sortedUniqueValues = uniqueValues.toArray(new Long[0]);\n        Arrays.sort(sortedUniqueValues);\n        meta.writeInt(sortedUniqueValues.length);\n        for (Long v : sortedUniqueValues) {\n          meta.writeLong(v);\n        }\n        encode = new HashMap<>();\n        for (int i = 0; i < sortedUniqueValues.length; ++i) {\n          encode.put(sortedUniqueValues[i], i);\n        }\n        min = 0;\n        gcd = 1;\n      } else {\n        uniqueValues = null;\n        numBitsPerValue = DirectWriter.unsignedBitsRequired((max - min) / gcd);\n        if (gcd == 1 && min > 0\n            && DirectWriter.unsignedBitsRequired(max) == DirectWriter.unsignedBitsRequired(max - min)) {\n          min = 0;\n        }\n        meta.writeInt(-1);\n      }\n    }\n\n    meta.writeByte((byte) numBitsPerValue);\n    meta.writeLong(min);\n    meta.writeLong(gcd);\n    long startOffset = data.getFilePointer();\n    meta.writeLong(startOffset);\n    if (numBitsPerValue != 0) {\n      values = valuesProducer.getSortedNumeric(field);\n      DirectWriter writer = DirectWriter.getInstance(data, numValues, numBitsPerValue);\n      for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n        for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n          long v = values.nextValue();\n          if (encode == null) {\n            writer.add((v - min) / gcd);\n          } else {\n            writer.add(encode.get(v));\n          }\n        }\n      }\n      writer.finish();\n    }\n    meta.writeLong(data.getFilePointer() - startOffset);\n\n    return new long[] {numDocsWithValue, numValues};\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"740ee098a9980924f1de3e712c61e1bdd3869b9c","date":1481818674,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","sourceNew":"  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    MinMaxTracker minMax = new MinMaxTracker();\n    MinMaxTracker blockMinMax = new MinMaxTracker();\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (minMax.numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minMax.min);\n          }\n        }\n\n        minMax.update(v);\n        blockMinMax.update(v);\n        if (blockMinMax.numValues == NUMERIC_BLOCK_SIZE) {\n          blockMinMax.nextBlock();\n        }\n\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n      }\n\n      numDocsWithValue++;\n    }\n\n    minMax.finish();\n    blockMinMax.finish();\n\n    final long numValues = minMax.numValues;\n    long min = minMax.min;\n    final long max = minMax.max;\n    assert blockMinMax.spaceInBits <= minMax.spaceInBits;\n\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    boolean doBlocks = false;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {\n      numBitsPerValue = 0;\n      meta.writeInt(-1);\n    } else {\n      if (uniqueValues != null\n          && uniqueValues.size() > 1\n          && DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) < DirectWriter.unsignedBitsRequired((max - min) / gcd)) {\n        numBitsPerValue = DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1);\n        final Long[] sortedUniqueValues = uniqueValues.toArray(new Long[0]);\n        Arrays.sort(sortedUniqueValues);\n        meta.writeInt(sortedUniqueValues.length);\n        for (Long v : sortedUniqueValues) {\n          meta.writeLong(v);\n        }\n        encode = new HashMap<>();\n        for (int i = 0; i < sortedUniqueValues.length; ++i) {\n          encode.put(sortedUniqueValues[i], i);\n        }\n        min = 0;\n        gcd = 1;\n      } else {\n        uniqueValues = null;\n        // we do blocks if that appears to save 10+% storage\n        doBlocks = minMax.spaceInBits > 0 && (double) blockMinMax.spaceInBits / minMax.spaceInBits <= 0.9;\n        if (doBlocks) {\n          numBitsPerValue = 0xFF;\n          meta.writeInt(-2 - NUMERIC_BLOCK_SHIFT);\n        } else {\n          numBitsPerValue = DirectWriter.unsignedBitsRequired((max - min) / gcd);\n          if (gcd == 1 && min > 0\n              && DirectWriter.unsignedBitsRequired(max) == DirectWriter.unsignedBitsRequired(max - min)) {\n            min = 0;\n          }\n          meta.writeInt(-1);\n        }\n      }\n    }\n\n    meta.writeByte((byte) numBitsPerValue);\n    meta.writeLong(min);\n    meta.writeLong(gcd);\n    long startOffset = data.getFilePointer();\n    meta.writeLong(startOffset);\n    if (doBlocks) {\n      writeValuesMultipleBlocks(valuesProducer.getSortedNumeric(field), gcd);\n    } else if (numBitsPerValue != 0) {\n      writeValuesSingleBlock(valuesProducer.getSortedNumeric(field), numValues, numBitsPerValue, min, gcd, encode);\n    }\n    meta.writeLong(data.getFilePointer() - startOffset);\n\n    return new long[] {numDocsWithValue, numValues};\n  }\n\n","sourceOld":"  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    long numValues = 0;\n    long min = Long.MAX_VALUE;\n    long max = Long.MIN_VALUE;\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - min);\n          }\n        }\n\n        min = Math.min(min, v);\n        max = Math.max(max, v);\n\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n\n        numValues++;\n      }\n\n      numDocsWithValue++;\n    }\n\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {\n      numBitsPerValue = 0;\n      meta.writeInt(-1);\n    } else {\n      if (uniqueValues != null\n          && uniqueValues.size() > 1\n          && DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) < DirectWriter.unsignedBitsRequired((max - min) / gcd)) {\n        numBitsPerValue = DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1);\n        final Long[] sortedUniqueValues = uniqueValues.toArray(new Long[0]);\n        Arrays.sort(sortedUniqueValues);\n        meta.writeInt(sortedUniqueValues.length);\n        for (Long v : sortedUniqueValues) {\n          meta.writeLong(v);\n        }\n        encode = new HashMap<>();\n        for (int i = 0; i < sortedUniqueValues.length; ++i) {\n          encode.put(sortedUniqueValues[i], i);\n        }\n        min = 0;\n        gcd = 1;\n      } else {\n        uniqueValues = null;\n        numBitsPerValue = DirectWriter.unsignedBitsRequired((max - min) / gcd);\n        if (gcd == 1 && min > 0\n            && DirectWriter.unsignedBitsRequired(max) == DirectWriter.unsignedBitsRequired(max - min)) {\n          min = 0;\n        }\n        meta.writeInt(-1);\n      }\n    }\n\n    meta.writeByte((byte) numBitsPerValue);\n    meta.writeLong(min);\n    meta.writeLong(gcd);\n    long startOffset = data.getFilePointer();\n    meta.writeLong(startOffset);\n    if (numBitsPerValue != 0) {\n      values = valuesProducer.getSortedNumeric(field);\n      DirectWriter writer = DirectWriter.getInstance(data, numValues, numBitsPerValue);\n      for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n        for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n          long v = values.nextValue();\n          if (encode == null) {\n            writer.add((v - min) / gcd);\n          } else {\n            writer.add(encode.get(v));\n          }\n        }\n      }\n      writer.finish();\n    }\n    meta.writeLong(data.getFilePointer() - startOffset);\n\n    return new long[] {numDocsWithValue, numValues};\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","sourceNew":"  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    MinMaxTracker minMax = new MinMaxTracker();\n    MinMaxTracker blockMinMax = new MinMaxTracker();\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (minMax.numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minMax.min);\n          }\n        }\n\n        minMax.update(v);\n        blockMinMax.update(v);\n        if (blockMinMax.numValues == NUMERIC_BLOCK_SIZE) {\n          blockMinMax.nextBlock();\n        }\n\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n      }\n\n      numDocsWithValue++;\n    }\n\n    minMax.finish();\n    blockMinMax.finish();\n\n    final long numValues = minMax.numValues;\n    long min = minMax.min;\n    final long max = minMax.max;\n    assert blockMinMax.spaceInBits <= minMax.spaceInBits;\n\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    boolean doBlocks = false;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {\n      numBitsPerValue = 0;\n      meta.writeInt(-1);\n    } else {\n      if (uniqueValues != null\n          && uniqueValues.size() > 1\n          && DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) < DirectWriter.unsignedBitsRequired((max - min) / gcd)) {\n        numBitsPerValue = DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1);\n        final Long[] sortedUniqueValues = uniqueValues.toArray(new Long[0]);\n        Arrays.sort(sortedUniqueValues);\n        meta.writeInt(sortedUniqueValues.length);\n        for (Long v : sortedUniqueValues) {\n          meta.writeLong(v);\n        }\n        encode = new HashMap<>();\n        for (int i = 0; i < sortedUniqueValues.length; ++i) {\n          encode.put(sortedUniqueValues[i], i);\n        }\n        min = 0;\n        gcd = 1;\n      } else {\n        uniqueValues = null;\n        // we do blocks if that appears to save 10+% storage\n        doBlocks = minMax.spaceInBits > 0 && (double) blockMinMax.spaceInBits / minMax.spaceInBits <= 0.9;\n        if (doBlocks) {\n          numBitsPerValue = 0xFF;\n          meta.writeInt(-2 - NUMERIC_BLOCK_SHIFT);\n        } else {\n          numBitsPerValue = DirectWriter.unsignedBitsRequired((max - min) / gcd);\n          if (gcd == 1 && min > 0\n              && DirectWriter.unsignedBitsRequired(max) == DirectWriter.unsignedBitsRequired(max - min)) {\n            min = 0;\n          }\n          meta.writeInt(-1);\n        }\n      }\n    }\n\n    meta.writeByte((byte) numBitsPerValue);\n    meta.writeLong(min);\n    meta.writeLong(gcd);\n    long startOffset = data.getFilePointer();\n    meta.writeLong(startOffset);\n    if (doBlocks) {\n      writeValuesMultipleBlocks(valuesProducer.getSortedNumeric(field), gcd);\n    } else if (numBitsPerValue != 0) {\n      writeValuesSingleBlock(valuesProducer.getSortedNumeric(field), numValues, numBitsPerValue, min, gcd, encode);\n    }\n    meta.writeLong(data.getFilePointer() - startOffset);\n\n    return new long[] {numDocsWithValue, numValues};\n  }\n\n","sourceOld":"  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    long numValues = 0;\n    long min = Long.MAX_VALUE;\n    long max = Long.MIN_VALUE;\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - min);\n          }\n        }\n\n        min = Math.min(min, v);\n        max = Math.max(max, v);\n\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n\n        numValues++;\n      }\n\n      numDocsWithValue++;\n    }\n\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {\n      numBitsPerValue = 0;\n      meta.writeInt(-1);\n    } else {\n      if (uniqueValues != null\n          && uniqueValues.size() > 1\n          && DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) < DirectWriter.unsignedBitsRequired((max - min) / gcd)) {\n        numBitsPerValue = DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1);\n        final Long[] sortedUniqueValues = uniqueValues.toArray(new Long[0]);\n        Arrays.sort(sortedUniqueValues);\n        meta.writeInt(sortedUniqueValues.length);\n        for (Long v : sortedUniqueValues) {\n          meta.writeLong(v);\n        }\n        encode = new HashMap<>();\n        for (int i = 0; i < sortedUniqueValues.length; ++i) {\n          encode.put(sortedUniqueValues[i], i);\n        }\n        min = 0;\n        gcd = 1;\n      } else {\n        uniqueValues = null;\n        numBitsPerValue = DirectWriter.unsignedBitsRequired((max - min) / gcd);\n        if (gcd == 1 && min > 0\n            && DirectWriter.unsignedBitsRequired(max) == DirectWriter.unsignedBitsRequired(max - min)) {\n          min = 0;\n        }\n        meta.writeInt(-1);\n      }\n    }\n\n    meta.writeByte((byte) numBitsPerValue);\n    meta.writeLong(min);\n    meta.writeLong(gcd);\n    long startOffset = data.getFilePointer();\n    meta.writeLong(startOffset);\n    if (numBitsPerValue != 0) {\n      values = valuesProducer.getSortedNumeric(field);\n      DirectWriter writer = DirectWriter.getInstance(data, numValues, numBitsPerValue);\n      for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n        for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n          long v = values.nextValue();\n          if (encode == null) {\n            writer.add((v - min) / gcd);\n          } else {\n            writer.add(encode.get(v));\n          }\n        }\n      }\n      writer.finish();\n    }\n    meta.writeLong(data.getFilePointer() - startOffset);\n\n    return new long[] {numDocsWithValue, numValues};\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"03e17b020972a0d6e8d6823f545571a66646a167","date":1547847724,"type":5,"author":"Toke Eskildsen","isMerge":false,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","sourceNew":"  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    MinMaxTracker minMax = new MinMaxTracker();\n    MinMaxTracker blockMinMax = new MinMaxTracker();\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (minMax.numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minMax.min);\n          }\n        }\n\n        minMax.update(v);\n        blockMinMax.update(v);\n        if (blockMinMax.numValues == NUMERIC_BLOCK_SIZE) {\n          blockMinMax.nextBlock();\n        }\n\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n      }\n\n      numDocsWithValue++;\n    }\n\n    minMax.finish();\n    blockMinMax.finish();\n\n    final long numValues = minMax.numValues;\n    long min = minMax.min;\n    final long max = minMax.max;\n    assert blockMinMax.spaceInBits <= minMax.spaceInBits;\n\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    boolean doBlocks = false;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {\n      numBitsPerValue = 0;\n      meta.writeInt(-1);\n    } else {\n      if (uniqueValues != null\n          && uniqueValues.size() > 1\n          && DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) < DirectWriter.unsignedBitsRequired((max - min) / gcd)) {\n        numBitsPerValue = DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1);\n        final Long[] sortedUniqueValues = uniqueValues.toArray(new Long[0]);\n        Arrays.sort(sortedUniqueValues);\n        meta.writeInt(sortedUniqueValues.length);\n        for (Long v : sortedUniqueValues) {\n          meta.writeLong(v);\n        }\n        encode = new HashMap<>();\n        for (int i = 0; i < sortedUniqueValues.length; ++i) {\n          encode.put(sortedUniqueValues[i], i);\n        }\n        min = 0;\n        gcd = 1;\n      } else {\n        uniqueValues = null;\n        // we do blocks if that appears to save 10+% storage\n        doBlocks = minMax.spaceInBits > 0 && (double) blockMinMax.spaceInBits / minMax.spaceInBits <= 0.9;\n        if (doBlocks) {\n          numBitsPerValue = 0xFF;\n          meta.writeInt(-2 - NUMERIC_BLOCK_SHIFT);\n        } else {\n          numBitsPerValue = DirectWriter.unsignedBitsRequired((max - min) / gcd);\n          if (gcd == 1 && min > 0\n              && DirectWriter.unsignedBitsRequired(max) == DirectWriter.unsignedBitsRequired(max - min)) {\n            min = 0;\n          }\n          meta.writeInt(-1);\n        }\n      }\n    }\n\n    meta.writeByte((byte) numBitsPerValue);\n    meta.writeLong(min);\n    meta.writeLong(gcd);\n    long startOffset = data.getFilePointer();\n    meta.writeLong(startOffset);\n    if (doBlocks) {\n      writeValuesMultipleBlocks(valuesProducer.getSortedNumeric(field), gcd);\n    } else if (numBitsPerValue != 0) {\n      writeValuesSingleBlock(valuesProducer.getSortedNumeric(field), numValues, numBitsPerValue, min, gcd, encode);\n    }\n    meta.writeLong(data.getFilePointer() - startOffset);\n\n    return new long[] {numDocsWithValue, numValues};\n  }\n\n","sourceOld":"  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    MinMaxTracker minMax = new MinMaxTracker();\n    MinMaxTracker blockMinMax = new MinMaxTracker();\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (minMax.numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minMax.min);\n          }\n        }\n\n        minMax.update(v);\n        blockMinMax.update(v);\n        if (blockMinMax.numValues == NUMERIC_BLOCK_SIZE) {\n          blockMinMax.nextBlock();\n        }\n\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n      }\n\n      numDocsWithValue++;\n    }\n\n    minMax.finish();\n    blockMinMax.finish();\n\n    final long numValues = minMax.numValues;\n    long min = minMax.min;\n    final long max = minMax.max;\n    assert blockMinMax.spaceInBits <= minMax.spaceInBits;\n\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    boolean doBlocks = false;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {\n      numBitsPerValue = 0;\n      meta.writeInt(-1);\n    } else {\n      if (uniqueValues != null\n          && uniqueValues.size() > 1\n          && DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) < DirectWriter.unsignedBitsRequired((max - min) / gcd)) {\n        numBitsPerValue = DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1);\n        final Long[] sortedUniqueValues = uniqueValues.toArray(new Long[0]);\n        Arrays.sort(sortedUniqueValues);\n        meta.writeInt(sortedUniqueValues.length);\n        for (Long v : sortedUniqueValues) {\n          meta.writeLong(v);\n        }\n        encode = new HashMap<>();\n        for (int i = 0; i < sortedUniqueValues.length; ++i) {\n          encode.put(sortedUniqueValues[i], i);\n        }\n        min = 0;\n        gcd = 1;\n      } else {\n        uniqueValues = null;\n        // we do blocks if that appears to save 10+% storage\n        doBlocks = minMax.spaceInBits > 0 && (double) blockMinMax.spaceInBits / minMax.spaceInBits <= 0.9;\n        if (doBlocks) {\n          numBitsPerValue = 0xFF;\n          meta.writeInt(-2 - NUMERIC_BLOCK_SHIFT);\n        } else {\n          numBitsPerValue = DirectWriter.unsignedBitsRequired((max - min) / gcd);\n          if (gcd == 1 && min > 0\n              && DirectWriter.unsignedBitsRequired(max) == DirectWriter.unsignedBitsRequired(max - min)) {\n            min = 0;\n          }\n          meta.writeInt(-1);\n        }\n      }\n    }\n\n    meta.writeByte((byte) numBitsPerValue);\n    meta.writeLong(min);\n    meta.writeLong(gcd);\n    long startOffset = data.getFilePointer();\n    meta.writeLong(startOffset);\n    if (doBlocks) {\n      writeValuesMultipleBlocks(valuesProducer.getSortedNumeric(field), gcd);\n    } else if (numBitsPerValue != 0) {\n      writeValuesSingleBlock(valuesProducer.getSortedNumeric(field), numValues, numBitsPerValue, min, gcd, encode);\n    }\n    meta.writeLong(data.getFilePointer() - startOffset);\n\n    return new long[] {numDocsWithValue, numValues};\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c89f1ef80a9432f4eabaeda9a1e135cd72e60836","date":1547972642,"type":5,"author":"Tommaso Teofili","isMerge":true,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesConsumer#writeValues(FieldInfo,DocValuesProducer).mjava","sourceNew":"  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    MinMaxTracker minMax = new MinMaxTracker();\n    MinMaxTracker blockMinMax = new MinMaxTracker();\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (minMax.numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minMax.min);\n          }\n        }\n\n        minMax.update(v);\n        blockMinMax.update(v);\n        if (blockMinMax.numValues == NUMERIC_BLOCK_SIZE) {\n          blockMinMax.nextBlock();\n        }\n\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n      }\n\n      numDocsWithValue++;\n    }\n\n    minMax.finish();\n    blockMinMax.finish();\n\n    final long numValues = minMax.numValues;\n    long min = minMax.min;\n    final long max = minMax.max;\n    assert blockMinMax.spaceInBits <= minMax.spaceInBits;\n\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    boolean doBlocks = false;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {\n      numBitsPerValue = 0;\n      meta.writeInt(-1);\n    } else {\n      if (uniqueValues != null\n          && uniqueValues.size() > 1\n          && DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) < DirectWriter.unsignedBitsRequired((max - min) / gcd)) {\n        numBitsPerValue = DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1);\n        final Long[] sortedUniqueValues = uniqueValues.toArray(new Long[0]);\n        Arrays.sort(sortedUniqueValues);\n        meta.writeInt(sortedUniqueValues.length);\n        for (Long v : sortedUniqueValues) {\n          meta.writeLong(v);\n        }\n        encode = new HashMap<>();\n        for (int i = 0; i < sortedUniqueValues.length; ++i) {\n          encode.put(sortedUniqueValues[i], i);\n        }\n        min = 0;\n        gcd = 1;\n      } else {\n        uniqueValues = null;\n        // we do blocks if that appears to save 10+% storage\n        doBlocks = minMax.spaceInBits > 0 && (double) blockMinMax.spaceInBits / minMax.spaceInBits <= 0.9;\n        if (doBlocks) {\n          numBitsPerValue = 0xFF;\n          meta.writeInt(-2 - NUMERIC_BLOCK_SHIFT);\n        } else {\n          numBitsPerValue = DirectWriter.unsignedBitsRequired((max - min) / gcd);\n          if (gcd == 1 && min > 0\n              && DirectWriter.unsignedBitsRequired(max) == DirectWriter.unsignedBitsRequired(max - min)) {\n            min = 0;\n          }\n          meta.writeInt(-1);\n        }\n      }\n    }\n\n    meta.writeByte((byte) numBitsPerValue);\n    meta.writeLong(min);\n    meta.writeLong(gcd);\n    long startOffset = data.getFilePointer();\n    meta.writeLong(startOffset);\n    if (doBlocks) {\n      writeValuesMultipleBlocks(valuesProducer.getSortedNumeric(field), gcd);\n    } else if (numBitsPerValue != 0) {\n      writeValuesSingleBlock(valuesProducer.getSortedNumeric(field), numValues, numBitsPerValue, min, gcd, encode);\n    }\n    meta.writeLong(data.getFilePointer() - startOffset);\n\n    return new long[] {numDocsWithValue, numValues};\n  }\n\n","sourceOld":"  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    MinMaxTracker minMax = new MinMaxTracker();\n    MinMaxTracker blockMinMax = new MinMaxTracker();\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (minMax.numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minMax.min);\n          }\n        }\n\n        minMax.update(v);\n        blockMinMax.update(v);\n        if (blockMinMax.numValues == NUMERIC_BLOCK_SIZE) {\n          blockMinMax.nextBlock();\n        }\n\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n      }\n\n      numDocsWithValue++;\n    }\n\n    minMax.finish();\n    blockMinMax.finish();\n\n    final long numValues = minMax.numValues;\n    long min = minMax.min;\n    final long max = minMax.max;\n    assert blockMinMax.spaceInBits <= minMax.spaceInBits;\n\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    boolean doBlocks = false;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {\n      numBitsPerValue = 0;\n      meta.writeInt(-1);\n    } else {\n      if (uniqueValues != null\n          && uniqueValues.size() > 1\n          && DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1) < DirectWriter.unsignedBitsRequired((max - min) / gcd)) {\n        numBitsPerValue = DirectWriter.unsignedBitsRequired(uniqueValues.size() - 1);\n        final Long[] sortedUniqueValues = uniqueValues.toArray(new Long[0]);\n        Arrays.sort(sortedUniqueValues);\n        meta.writeInt(sortedUniqueValues.length);\n        for (Long v : sortedUniqueValues) {\n          meta.writeLong(v);\n        }\n        encode = new HashMap<>();\n        for (int i = 0; i < sortedUniqueValues.length; ++i) {\n          encode.put(sortedUniqueValues[i], i);\n        }\n        min = 0;\n        gcd = 1;\n      } else {\n        uniqueValues = null;\n        // we do blocks if that appears to save 10+% storage\n        doBlocks = minMax.spaceInBits > 0 && (double) blockMinMax.spaceInBits / minMax.spaceInBits <= 0.9;\n        if (doBlocks) {\n          numBitsPerValue = 0xFF;\n          meta.writeInt(-2 - NUMERIC_BLOCK_SHIFT);\n        } else {\n          numBitsPerValue = DirectWriter.unsignedBitsRequired((max - min) / gcd);\n          if (gcd == 1 && min > 0\n              && DirectWriter.unsignedBitsRequired(max) == DirectWriter.unsignedBitsRequired(max - min)) {\n            min = 0;\n          }\n          meta.writeInt(-1);\n        }\n      }\n    }\n\n    meta.writeByte((byte) numBitsPerValue);\n    meta.writeLong(min);\n    meta.writeLong(gcd);\n    long startOffset = data.getFilePointer();\n    meta.writeLong(startOffset);\n    if (doBlocks) {\n      writeValuesMultipleBlocks(valuesProducer.getSortedNumeric(field), gcd);\n    } else if (numBitsPerValue != 0) {\n      writeValuesSingleBlock(valuesProducer.getSortedNumeric(field), numValues, numBitsPerValue, min, gcd, encode);\n    }\n    meta.writeLong(data.getFilePointer() - startOffset);\n\n    return new long[] {numDocsWithValue, numValues};\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"03e17b020972a0d6e8d6823f545571a66646a167":["740ee098a9980924f1de3e712c61e1bdd3869b9c"],"23e44daeaa8b89694d10df5999956c8e14a7dd09":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c89f1ef80a9432f4eabaeda9a1e135cd72e60836":["740ee098a9980924f1de3e712c61e1bdd3869b9c","03e17b020972a0d6e8d6823f545571a66646a167"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9856095f7afb5a607bf5e65077615ed91273508c":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","740ee098a9980924f1de3e712c61e1bdd3869b9c"],"740ee098a9980924f1de3e712c61e1bdd3869b9c":["23e44daeaa8b89694d10df5999956c8e14a7dd09"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","23e44daeaa8b89694d10df5999956c8e14a7dd09"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["03e17b020972a0d6e8d6823f545571a66646a167"]},"commit2Childs":{"03e17b020972a0d6e8d6823f545571a66646a167":["c89f1ef80a9432f4eabaeda9a1e135cd72e60836","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"23e44daeaa8b89694d10df5999956c8e14a7dd09":["740ee098a9980924f1de3e712c61e1bdd3869b9c","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"c89f1ef80a9432f4eabaeda9a1e135cd72e60836":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["23e44daeaa8b89694d10df5999956c8e14a7dd09","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"9856095f7afb5a607bf5e65077615ed91273508c":[],"740ee098a9980924f1de3e712c61e1bdd3869b9c":["03e17b020972a0d6e8d6823f545571a66646a167","c89f1ef80a9432f4eabaeda9a1e135cd72e60836","9856095f7afb5a607bf5e65077615ed91273508c"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9856095f7afb5a607bf5e65077615ed91273508c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c89f1ef80a9432f4eabaeda9a1e135cd72e60836","9856095f7afb5a607bf5e65077615ed91273508c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}