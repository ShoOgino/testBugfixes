{"path":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","commits":[{"id":"8476949555f799dff381770c01cfad051a264487","date":1570505073,"type":0,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    tmpFiles.put(entry.getPath(), info);\n    List<String> nodes = shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = myNode();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + entry.getPath() + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for blob notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      new Thread(() -> {\n        try {\n          // keep the jar in memory for 10 secs , so that\n          //every node can download it from memory without the file system\n          Thread.sleep(10 * 1000);\n        } catch (Exception e) {\n          //don't care\n        } finally {\n          tmpFiles.remove(entry.getPath());\n        }\n      }).start();\n\n\n    }\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d218decf811b7a0a4d86218c54c79c74a962374b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"849dbf8570155b8e5fd03d8c2274a0a60491051c","date":1570859954,"type":3,"author":"noble","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","pathOld":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","sourceNew":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    tmpFiles.put(entry.getPath(), info);\n    List<String> nodes =  coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + entry.getPath() + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for blob notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      new Thread(() -> {\n        try {\n          // keep the jar in memory for 10 secs , so that\n          //every node can download it from memory without the file system\n          Thread.sleep(10 * 1000);\n        } catch (Exception e) {\n          //don't care\n        } finally {\n          tmpFiles.remove(entry.getPath());\n        }\n      }).start();\n\n\n    }\n\n  }\n\n","sourceOld":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    tmpFiles.put(entry.getPath(), info);\n    List<String> nodes = shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = myNode();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + entry.getPath() + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for blob notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      new Thread(() -> {\n        try {\n          // keep the jar in memory for 10 secs , so that\n          //every node can download it from memory without the file system\n          Thread.sleep(10 * 1000);\n        } catch (Exception e) {\n          //don't care\n        } finally {\n          tmpFiles.remove(entry.getPath());\n        }\n      }).start();\n\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":["d218decf811b7a0a4d86218c54c79c74a962374b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0b597c65628ca9e73913a07e81691f8229bae35","date":1571224353,"type":0,"author":"jimczi","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    tmpFiles.put(entry.getPath(), info);\n    List<String> nodes =  coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + entry.getPath() + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for blob notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      new Thread(() -> {\n        try {\n          // keep the jar in memory for 10 secs , so that\n          //every node can download it from memory without the file system\n          Thread.sleep(10 * 1000);\n        } catch (Exception e) {\n          //don't care\n        } finally {\n          tmpFiles.remove(entry.getPath());\n        }\n      }).start();\n\n\n    }\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"954ae83b7dfacaa33d48ea056448ae11f7745a93","date":1571867711,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","pathOld":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","sourceNew":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    tmpFiles.put(entry.getPath(), info);\n    List<String> nodes =  coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + entry.getPath() + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for file fetch notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(entry.getPath());\n        }\n        return null;\n      });\n    }\n\n  }\n\n","sourceOld":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    tmpFiles.put(entry.getPath(), info);\n    List<String> nodes =  coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + entry.getPath() + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for blob notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      new Thread(() -> {\n        try {\n          // keep the jar in memory for 10 secs , so that\n          //every node can download it from memory without the file system\n          Thread.sleep(10 * 1000);\n        } catch (Exception e) {\n          //don't care\n        } finally {\n          tmpFiles.remove(entry.getPath());\n        }\n      }).start();\n\n\n    }\n\n  }\n\n","bugFix":null,"bugIntro":["d218decf811b7a0a4d86218c54c79c74a962374b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d1a6448412ce640b28861f4c00f899484a9adac1","date":1573627357,"type":3,"author":"noble","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","pathOld":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","sourceNew":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    tmpFiles.put(entry.getPath(), info);\n    List<String> nodes = coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + entry.getPath() + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for file fetch notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(entry.getPath());\n        }\n        return null;\n      });\n    }\n\n  }\n\n","sourceOld":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    tmpFiles.put(entry.getPath(), info);\n    List<String> nodes =  coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + entry.getPath() + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for file fetch notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(entry.getPath());\n        }\n        return null;\n      });\n    }\n\n  }\n\n","bugFix":null,"bugIntro":["d218decf811b7a0a4d86218c54c79c74a962374b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d218decf811b7a0a4d86218c54c79c74a962374b","date":1578632144,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","pathOld":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","sourceNew":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    distribute(info);\n  }\n\n","sourceOld":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    tmpFiles.put(entry.getPath(), info);\n    List<String> nodes = coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + entry.getPath() + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for file fetch notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(entry.getPath());\n        }\n        return null;\n      });\n    }\n\n  }\n\n","bugFix":["d1a6448412ce640b28861f4c00f899484a9adac1","8476949555f799dff381770c01cfad051a264487","954ae83b7dfacaa33d48ea056448ae11f7745a93","849dbf8570155b8e5fd03d8c2274a0a60491051c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b8f0a7504661c8e51be5c63e87f9d79a36d9116c","date":1578657638,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","pathOld":"solr/core/src/java/org/apache/solr/filestore/DistribPackageStore#put(FileEntry).mjava","sourceNew":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    distribute(info);\n  }\n\n","sourceOld":"  @Override\n  public void put(FileEntry entry) throws IOException {\n    FileInfo info = new FileInfo(entry.path);\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    Utils.writeJson(entry.getMetaData(), baos, true);\n    byte[] bytes = baos.toByteArray();\n    info.persistToFile(entry.buf, ByteBuffer.wrap(bytes, 0, bytes.length));\n    tmpFiles.put(entry.getPath(), info);\n    List<String> nodes = coreContainer.getPackageStoreAPI().shuffledNodes();\n    int i = 0;\n    int FETCHFROM_SRC = 50;\n    String myNodeName = coreContainer.getZkController().getNodeName();\n    try {\n      for (String node : nodes) {\n        String baseUrl = coreContainer.getZkController().getZkStateReader().getBaseUrlForNodeName(node);\n        String url = baseUrl.replace(\"/solr\", \"/api\") + \"/node/files\" + entry.getPath() + \"?getFrom=\";\n        if (i < FETCHFROM_SRC) {\n          // this is to protect very large clusters from overwhelming a single node\n          // the first FETCHFROM_SRC nodes will be asked to fetch from this node.\n          // it's there in  the memory now. So , it must be served fast\n          url += myNodeName;\n        } else {\n          if (i == FETCHFROM_SRC) {\n            // This is just an optimization\n            // at this point a bunch of nodes are already downloading from me\n            // I'll wait for them to finish before asking other nodes to download from each other\n            try {\n              Thread.sleep(2 * 1000);\n            } catch (Exception e) {\n            }\n          }\n          // trying to avoid the thundering herd problem when there are a very large no:of nodes\n          // others should try to fetch it from any node where it is available. By now,\n          // almost FETCHFROM_SRC other nodes may have it\n          url += \"*\";\n        }\n        try {\n          //fire and forget\n          Utils.executeGET(coreContainer.getUpdateShardHandler().getDefaultHttpClient(), url, null);\n        } catch (Exception e) {\n          log.info(\"Node: \" + node +\n              \" failed to respond for file fetch notification\", e);\n          //ignore the exception\n          // some nodes may be down or not responding\n        }\n        i++;\n      }\n    } finally {\n      coreContainer.getUpdateShardHandler().getUpdateExecutor().submit(() -> {\n        try {\n          Thread.sleep(10 * 1000);\n        } finally {\n          tmpFiles.remove(entry.getPath());\n        }\n        return null;\n      });\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b8f0a7504661c8e51be5c63e87f9d79a36d9116c":["d1a6448412ce640b28861f4c00f899484a9adac1","d218decf811b7a0a4d86218c54c79c74a962374b"],"954ae83b7dfacaa33d48ea056448ae11f7745a93":["849dbf8570155b8e5fd03d8c2274a0a60491051c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d218decf811b7a0a4d86218c54c79c74a962374b":["d1a6448412ce640b28861f4c00f899484a9adac1"],"d1a6448412ce640b28861f4c00f899484a9adac1":["954ae83b7dfacaa33d48ea056448ae11f7745a93"],"8476949555f799dff381770c01cfad051a264487":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"849dbf8570155b8e5fd03d8c2274a0a60491051c":["8476949555f799dff381770c01cfad051a264487"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d218decf811b7a0a4d86218c54c79c74a962374b"],"b0b597c65628ca9e73913a07e81691f8229bae35":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","849dbf8570155b8e5fd03d8c2274a0a60491051c"]},"commit2Childs":{"b8f0a7504661c8e51be5c63e87f9d79a36d9116c":[],"954ae83b7dfacaa33d48ea056448ae11f7745a93":["d1a6448412ce640b28861f4c00f899484a9adac1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8476949555f799dff381770c01cfad051a264487","b0b597c65628ca9e73913a07e81691f8229bae35"],"d218decf811b7a0a4d86218c54c79c74a962374b":["b8f0a7504661c8e51be5c63e87f9d79a36d9116c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d1a6448412ce640b28861f4c00f899484a9adac1":["b8f0a7504661c8e51be5c63e87f9d79a36d9116c","d218decf811b7a0a4d86218c54c79c74a962374b"],"8476949555f799dff381770c01cfad051a264487":["849dbf8570155b8e5fd03d8c2274a0a60491051c"],"849dbf8570155b8e5fd03d8c2274a0a60491051c":["954ae83b7dfacaa33d48ea056448ae11f7745a93","b0b597c65628ca9e73913a07e81691f8229bae35"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"b0b597c65628ca9e73913a07e81691f8229bae35":[]},"heads":["b8f0a7504661c8e51be5c63e87f9d79a36d9116c","cd5edd1f2b162a5cfa08efd17851a07373a96817","b0b597c65628ca9e73913a07e81691f8229bae35"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}