{"path":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","commits":[{"id":"e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0","date":1433439403,"type":0,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    int[] maxDocList = new int[] {300, 700, 1200};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    indexThread = new StoppableIndexingThread(controlClient, cloudClient, \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {200, 2000, 3000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = chaosMonkey.getShard(\"shard1\", 1);\n    SolrDispatchFilter filter = (SolrDispatchFilter)jetty.getDispatchFilter().getFilter();\n    CoreContainer coreContainer = filter.getCores();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(\"collection1\")) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    JettySolrRunner replica = chaosMonkey.stopShard(\"shard1\", 1).jetty;\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n    \n    // bring shard replica up\n    replica.start();\n    \n    // make sure replication can start\n    Thread.sleep(3000);\n    ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n    \n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    Thread.sleep(1000);\n  \n    waitForThingsToLevelOut(120);\n    waitForRecoveriesToFinish(DEFAULT_COLLECTION, zkStateReader, false, true);\n\n    // test that leader and replica have same doc count\n    \n    String fail = checkShardConsistency(\"shard1\", false, false);\n    if (fail != null)\n      fail(fail);\n\n    SolrQuery query = new SolrQuery(\"*:*\");\n    query.setParam(\"distrib\", \"false\");\n    long client1Docs = shardToJetty.get(\"shard1\").get(0).client.solrClient.query(query).getResults().getNumFound();\n    long client2Docs = shardToJetty.get(\"shard1\").get(1).client.solrClient.query(query).getResults().getNumFound();\n    \n    assertTrue(client1Docs > 0);\n    assertEquals(client1Docs, client2Docs);\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1088b72b3b4cc45316b7595bd09023c859cd2327","date":1447150009,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    int[] maxDocList = new int[] {300, 700, 1200};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    indexThread = new StoppableIndexingThread(controlClient, cloudClient, \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {200, 2000, 3000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = chaosMonkey.getShard(\"shard1\", 1);\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(\"collection1\")) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    JettySolrRunner replica = chaosMonkey.stopShard(\"shard1\", 1).jetty;\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n    \n    // bring shard replica up\n    replica.start();\n    \n    // make sure replication can start\n    Thread.sleep(3000);\n    ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n    \n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    Thread.sleep(1000);\n  \n    waitForThingsToLevelOut(120);\n    waitForRecoveriesToFinish(DEFAULT_COLLECTION, zkStateReader, false, true);\n\n    // test that leader and replica have same doc count\n    \n    String fail = checkShardConsistency(\"shard1\", false, false);\n    if (fail != null)\n      fail(fail);\n\n    SolrQuery query = new SolrQuery(\"*:*\");\n    query.setParam(\"distrib\", \"false\");\n    long client1Docs = shardToJetty.get(\"shard1\").get(0).client.solrClient.query(query).getResults().getNumFound();\n    long client2Docs = shardToJetty.get(\"shard1\").get(1).client.solrClient.query(query).getResults().getNumFound();\n    \n    assertTrue(client1Docs > 0);\n    assertEquals(client1Docs, client2Docs);\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    int[] maxDocList = new int[] {300, 700, 1200};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    indexThread = new StoppableIndexingThread(controlClient, cloudClient, \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {200, 2000, 3000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = chaosMonkey.getShard(\"shard1\", 1);\n    SolrDispatchFilter filter = (SolrDispatchFilter)jetty.getDispatchFilter().getFilter();\n    CoreContainer coreContainer = filter.getCores();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(\"collection1\")) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    JettySolrRunner replica = chaosMonkey.stopShard(\"shard1\", 1).jetty;\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n    \n    // bring shard replica up\n    replica.start();\n    \n    // make sure replication can start\n    Thread.sleep(3000);\n    ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n    \n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    Thread.sleep(1000);\n  \n    waitForThingsToLevelOut(120);\n    waitForRecoveriesToFinish(DEFAULT_COLLECTION, zkStateReader, false, true);\n\n    // test that leader and replica have same doc count\n    \n    String fail = checkShardConsistency(\"shard1\", false, false);\n    if (fail != null)\n      fail(fail);\n\n    SolrQuery query = new SolrQuery(\"*:*\");\n    query.setParam(\"distrib\", \"false\");\n    long client1Docs = shardToJetty.get(\"shard1\").get(0).client.solrClient.query(query).getResults().getNumFound();\n    long client2Docs = shardToJetty.get(\"shard1\").get(1).client.solrClient.query(query).getResults().getNumFound();\n    \n    assertTrue(client1Docs > 0);\n    assertEquals(client1Docs, client2Docs);\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5ebf70dabe6279454c5ff460bdea3f0dc2814a86","date":1463672611,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    CollectionAdminRequest.createCollection(COLLECTION, \"conf1\", 1, 2)\n        .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    cluster.getSolrClient().setDefaultCollection(COLLECTION); // TODO make this configurable on StoppableIndexingThread\n\n    int[] maxDocList = new int[] {300, 700, 1200};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    StoppableIndexingThread indexThread = new StoppableIndexingThread(null, cluster.getSolrClient(), \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {200, 2000, 3000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = cluster.getRandomJetty(random());\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(coreContainer.getCoreDescriptors().get(0).getName())) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    jetty.stop();\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // bring shard replica up\n    jetty.start();\n\n    // make sure replication can start\n    Thread.sleep(3000);\n\n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    cluster.getSolrClient().waitForState(COLLECTION, DEFAULT_TIMEOUT, TimeUnit.SECONDS,\n        (n, c) -> DocCollection.isFullyActive(n, c, 1, 2));\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    int[] maxDocList = new int[] {300, 700, 1200};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    indexThread = new StoppableIndexingThread(controlClient, cloudClient, \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {200, 2000, 3000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = chaosMonkey.getShard(\"shard1\", 1);\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(\"collection1\")) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    JettySolrRunner replica = chaosMonkey.stopShard(\"shard1\", 1).jetty;\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n    \n    // bring shard replica up\n    replica.start();\n    \n    // make sure replication can start\n    Thread.sleep(3000);\n    ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n    \n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    Thread.sleep(1000);\n  \n    waitForThingsToLevelOut(120);\n    waitForRecoveriesToFinish(DEFAULT_COLLECTION, zkStateReader, false, true);\n\n    // test that leader and replica have same doc count\n    \n    String fail = checkShardConsistency(\"shard1\", false, false);\n    if (fail != null)\n      fail(fail);\n\n    SolrQuery query = new SolrQuery(\"*:*\");\n    query.setParam(\"distrib\", \"false\");\n    long client1Docs = shardToJetty.get(\"shard1\").get(0).client.solrClient.query(query).getResults().getNumFound();\n    long client2Docs = shardToJetty.get(\"shard1\").get(1).client.solrClient.query(query).getResults().getNumFound();\n    \n    assertTrue(client1Docs > 0);\n    assertEquals(client1Docs, client2Docs);\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    CollectionAdminRequest.createCollection(COLLECTION, \"conf1\", 1, 2)\n        .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    cluster.getSolrClient().setDefaultCollection(COLLECTION); // TODO make this configurable on StoppableIndexingThread\n\n    int[] maxDocList = new int[] {300, 700, 1200};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    StoppableIndexingThread indexThread = new StoppableIndexingThread(null, cluster.getSolrClient(), \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {200, 2000, 3000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = cluster.getRandomJetty(random());\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(coreContainer.getCoreDescriptors().get(0).getName())) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    jetty.stop();\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // bring shard replica up\n    jetty.start();\n\n    // make sure replication can start\n    Thread.sleep(3000);\n\n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    cluster.getSolrClient().waitForState(COLLECTION, DEFAULT_TIMEOUT, TimeUnit.SECONDS,\n        (n, c) -> DocCollection.isFullyActive(n, c, 1, 2));\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    int[] maxDocList = new int[] {300, 700, 1200};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    indexThread = new StoppableIndexingThread(controlClient, cloudClient, \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {200, 2000, 3000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = chaosMonkey.getShard(\"shard1\", 1);\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(\"collection1\")) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    JettySolrRunner replica = chaosMonkey.stopShard(\"shard1\", 1).jetty;\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n    \n    // bring shard replica up\n    replica.start();\n    \n    // make sure replication can start\n    Thread.sleep(3000);\n    ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n    \n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    Thread.sleep(1000);\n  \n    waitForThingsToLevelOut(120);\n    waitForRecoveriesToFinish(DEFAULT_COLLECTION, zkStateReader, false, true);\n\n    // test that leader and replica have same doc count\n    \n    String fail = checkShardConsistency(\"shard1\", false, false);\n    if (fail != null)\n      fail(fail);\n\n    SolrQuery query = new SolrQuery(\"*:*\");\n    query.setParam(\"distrib\", \"false\");\n    long client1Docs = shardToJetty.get(\"shard1\").get(0).client.solrClient.query(query).getResults().getNumFound();\n    long client2Docs = shardToJetty.get(\"shard1\").get(1).client.solrClient.query(query).getResults().getNumFound();\n    \n    assertTrue(client1Docs > 0);\n    assertEquals(client1Docs, client2Docs);\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    CollectionAdminRequest.createCollection(COLLECTION, \"conf1\", 1, 2)\n        .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    cluster.getSolrClient().setDefaultCollection(COLLECTION); // TODO make this configurable on StoppableIndexingThread\n\n    int[] maxDocList = new int[] {300, 700, 1200};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    StoppableIndexingThread indexThread = new StoppableIndexingThread(null, cluster.getSolrClient(), \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {200, 2000, 3000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = cluster.getRandomJetty(random());\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(coreContainer.getCoreDescriptors().get(0).getName())) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    jetty.stop();\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // bring shard replica up\n    jetty.start();\n\n    // make sure replication can start\n    Thread.sleep(3000);\n\n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    cluster.getSolrClient().waitForState(COLLECTION, DEFAULT_TIMEOUT, TimeUnit.SECONDS,\n        (n, c) -> DocCollection.isFullyActive(n, c, 1, 2));\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    int[] maxDocList = new int[] {300, 700, 1200};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    indexThread = new StoppableIndexingThread(controlClient, cloudClient, \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {200, 2000, 3000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = chaosMonkey.getShard(\"shard1\", 1);\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(\"collection1\")) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    JettySolrRunner replica = chaosMonkey.stopShard(\"shard1\", 1).jetty;\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n    \n    // bring shard replica up\n    replica.start();\n    \n    // make sure replication can start\n    Thread.sleep(3000);\n    ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n    \n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    Thread.sleep(1000);\n  \n    waitForThingsToLevelOut(120);\n    waitForRecoveriesToFinish(DEFAULT_COLLECTION, zkStateReader, false, true);\n\n    // test that leader and replica have same doc count\n    \n    String fail = checkShardConsistency(\"shard1\", false, false);\n    if (fail != null)\n      fail(fail);\n\n    SolrQuery query = new SolrQuery(\"*:*\");\n    query.setParam(\"distrib\", \"false\");\n    long client1Docs = shardToJetty.get(\"shard1\").get(0).client.solrClient.query(query).getResults().getNumFound();\n    long client2Docs = shardToJetty.get(\"shard1\").get(1).client.solrClient.query(query).getResults().getNumFound();\n    \n    assertTrue(client1Docs > 0);\n    assertEquals(client1Docs, client2Docs);\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1ba5d9d26b1a7a34c517472d53464ff7f0404b91","date":1488256427,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    CollectionAdminRequest.createCollection(COLLECTION, \"conf1\", 1, 2)\n        .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    cluster.getSolrClient().setDefaultCollection(COLLECTION); // TODO make this configurable on StoppableIndexingThread\n\n    int[] maxDocList = new int[] {300, 500, 700};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    StoppableIndexingThread indexThread = new StoppableIndexingThread(null, cluster.getSolrClient(), \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {3000, 4000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = cluster.getRandomJetty(random());\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(coreContainer.getCoreDescriptors().get(0).getName())) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    ChaosMonkey.stop(jetty);\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // bring shard replica up\n    ChaosMonkey.start(jetty);\n\n    // make sure replication can start\n    Thread.sleep(3000);\n\n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    cluster.getSolrClient().waitForState(COLLECTION, DEFAULT_TIMEOUT, TimeUnit.SECONDS,\n        (n, c) -> DocCollection.isFullyActive(n, c, 1, 2));\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    CollectionAdminRequest.createCollection(COLLECTION, \"conf1\", 1, 2)\n        .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    cluster.getSolrClient().setDefaultCollection(COLLECTION); // TODO make this configurable on StoppableIndexingThread\n\n    int[] maxDocList = new int[] {300, 700, 1200};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    StoppableIndexingThread indexThread = new StoppableIndexingThread(null, cluster.getSolrClient(), \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {200, 2000, 3000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = cluster.getRandomJetty(random());\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(coreContainer.getCoreDescriptors().get(0).getName())) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    jetty.stop();\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // bring shard replica up\n    jetty.start();\n\n    // make sure replication can start\n    Thread.sleep(3000);\n\n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    cluster.getSolrClient().waitForState(COLLECTION, DEFAULT_TIMEOUT, TimeUnit.SECONDS,\n        (n, c) -> DocCollection.isFullyActive(n, c, 1, 2));\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/CleanupOldIndexTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    CollectionAdminRequest.createCollection(COLLECTION, \"conf1\", 1, 2)\n        .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    cluster.getSolrClient().setDefaultCollection(COLLECTION); // TODO make this configurable on StoppableIndexingThread\n\n    int[] maxDocList = new int[] {300, 500, 700};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    StoppableIndexingThread indexThread = new StoppableIndexingThread(null, cluster.getSolrClient(), \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {3000, 4000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = cluster.getRandomJetty(random());\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(coreContainer.getCoreDescriptors().get(0).getName())) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    jetty.stop();\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // bring shard replica up\n    jetty.start();\n\n    // make sure replication can start\n    Thread.sleep(3000);\n\n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    cluster.getSolrClient().waitForState(COLLECTION, DEFAULT_TIMEOUT, TimeUnit.SECONDS,\n        (n, c) -> DocCollection.isFullyActive(n, c, 1, 2));\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    CollectionAdminRequest.createCollection(COLLECTION, \"conf1\", 1, 2)\n        .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    cluster.getSolrClient().setDefaultCollection(COLLECTION); // TODO make this configurable on StoppableIndexingThread\n\n    int[] maxDocList = new int[] {300, 500, 700};\n    int maxDoc = maxDocList[random().nextInt(maxDocList.length - 1)];\n\n    StoppableIndexingThread indexThread = new StoppableIndexingThread(null, cluster.getSolrClient(), \"1\", true, maxDoc, 1, true);\n    indexThread.start();\n\n    // give some time to index...\n    int[] waitTimes = new int[] {3000, 4000};\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // create some \"old\" index directories\n    JettySolrRunner jetty = cluster.getRandomJetty(random());\n    CoreContainer coreContainer = jetty.getCoreContainer();\n    File dataDir = null;\n    try (SolrCore solrCore = coreContainer.getCore(coreContainer.getCoreDescriptors().get(0).getName())) {\n      dataDir = new File(solrCore.getDataDir());\n    }\n    assertTrue(dataDir.isDirectory());\n\n    long msInDay = 60*60*24L;\n    String timestamp1 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(1*msInDay));\n    String timestamp2 = new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date(2*msInDay));\n    File oldIndexDir1 = new File(dataDir, \"index.\"+timestamp1);\n    FileUtils.forceMkdir(oldIndexDir1);\n    File oldIndexDir2 = new File(dataDir, \"index.\"+timestamp2);\n    FileUtils.forceMkdir(oldIndexDir2);\n\n    // verify the \"old\" index directories exist\n    assertTrue(oldIndexDir1.isDirectory());\n    assertTrue(oldIndexDir2.isDirectory());\n\n    // bring shard replica down\n    ChaosMonkey.stop(jetty);\n\n    // wait a moment - lets allow some docs to be indexed so replication time is non 0\n    Thread.sleep(waitTimes[random().nextInt(waitTimes.length - 1)]);\n\n    // bring shard replica up\n    ChaosMonkey.start(jetty);\n\n    // make sure replication can start\n    Thread.sleep(3000);\n\n    // stop indexing threads\n    indexThread.safeStop();\n    indexThread.join();\n\n    cluster.getSolrClient().waitForState(COLLECTION, DEFAULT_TIMEOUT, TimeUnit.SECONDS,\n        (n, c) -> DocCollection.isFullyActive(n, c, 1, 2));\n\n    assertTrue(!oldIndexDir1.isDirectory());\n    assertTrue(!oldIndexDir2.isDirectory());\n  }\n\n","bugFix":["1ba5d9d26b1a7a34c517472d53464ff7f0404b91"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["1ba5d9d26b1a7a34c517472d53464ff7f0404b91"],"1088b72b3b4cc45316b7595bd09023c859cd2327":["e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5ebf70dabe6279454c5ff460bdea3f0dc2814a86":["1088b72b3b4cc45316b7595bd09023c859cd2327"],"1ba5d9d26b1a7a34c517472d53464ff7f0404b91":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["1088b72b3b4cc45316b7595bd09023c859cd2327","d470c8182e92b264680e34081b75e70a9f2b3c89"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["1088b72b3b4cc45316b7595bd09023c859cd2327","5ebf70dabe6279454c5ff460bdea3f0dc2814a86"]},"commit2Childs":{"e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0":["1088b72b3b4cc45316b7595bd09023c859cd2327"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1088b72b3b4cc45316b7595bd09023c859cd2327":["5ebf70dabe6279454c5ff460bdea3f0dc2814a86","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e0bbfc7e96e0cf26b51dcefe3b2e4c93fe34e8c0"],"5ebf70dabe6279454c5ff460bdea3f0dc2814a86":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"1ba5d9d26b1a7a34c517472d53464ff7f0404b91":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d470c8182e92b264680e34081b75e70a9f2b3c89":["1ba5d9d26b1a7a34c517472d53464ff7f0404b91","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}