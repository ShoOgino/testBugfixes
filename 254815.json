{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","commits":[{"id":"0932eb10135843758b2ca508d5aa2b4798aa07f9","date":1426947197,"type":1,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,boolean,String).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n          throws KeeperException, InterruptedException \n  {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN, leaderCoreNodeName, retryOnConnLoss);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState, String leaderCoreNodeName)\n          throws KeeperException, InterruptedException \n  {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN, leaderCoreNodeName);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbcfc050b9f253136eaa5950b57248b2109eac11","date":1427308993,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN, leaderCoreNodeName, retryOnConnLoss);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();\n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN,\n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(),\n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \" + forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n          throws KeeperException, InterruptedException \n  {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN, leaderCoreNodeName, retryOnConnLoss);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN, leaderCoreNodeName, retryOnConnLoss);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();\n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN,\n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(),\n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \" + forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a219f1dcad1700e84807666bdbd2b573e8de7021","date":1428130940,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, Replica.State.DOWN, leaderCoreNodeName);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();\n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n          ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(),\n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \" + forcePublishState,\n          replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN, leaderCoreNodeName, retryOnConnLoss);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();\n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN,\n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(),\n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \" + forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    }\n\n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b693a83132c9e45afcd564fd65a25b60ed80388b","date":1436882146,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, Replica.State.DOWN, leaderCoreNodeName);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();\n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n          ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(),\n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \" + forcePublishState,\n          replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n      overseerJobQueue.offer(Utils.toJSON(m));\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, Replica.State.DOWN, leaderCoreNodeName);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();\n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n          ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(),\n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \" + forcePublishState,\n          replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));\n    }\n\n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22859cb40e09867e7da8de84a31956c07259f82f","date":1441822065,"type":5,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,String,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCoreNodeName); // core node name of current leader\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, Replica.State.DOWN, leaderCoreNodeName);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();\n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n          ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(),\n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \" + forcePublishState,\n          replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n      overseerJobQueue.offer(Utils.toJSON(m));\n    }\n\n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a219f1dcad1700e84807666bdbd2b573e8de7021":["fbcfc050b9f253136eaa5950b57248b2109eac11"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","fbcfc050b9f253136eaa5950b57248b2109eac11"],"22859cb40e09867e7da8de84a31956c07259f82f":["b693a83132c9e45afcd564fd65a25b60ed80388b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b693a83132c9e45afcd564fd65a25b60ed80388b":["a219f1dcad1700e84807666bdbd2b573e8de7021"],"0932eb10135843758b2ca508d5aa2b4798aa07f9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["22859cb40e09867e7da8de84a31956c07259f82f"],"fbcfc050b9f253136eaa5950b57248b2109eac11":["0932eb10135843758b2ca508d5aa2b4798aa07f9"]},"commit2Childs":{"a219f1dcad1700e84807666bdbd2b573e8de7021":["b693a83132c9e45afcd564fd65a25b60ed80388b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"22859cb40e09867e7da8de84a31956c07259f82f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","0932eb10135843758b2ca508d5aa2b4798aa07f9"],"b693a83132c9e45afcd564fd65a25b60ed80388b":["22859cb40e09867e7da8de84a31956c07259f82f"],"0932eb10135843758b2ca508d5aa2b4798aa07f9":["fbcfc050b9f253136eaa5950b57248b2109eac11"],"fbcfc050b9f253136eaa5950b57248b2109eac11":["a219f1dcad1700e84807666bdbd2b573e8de7021","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}