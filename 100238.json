{"path":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#createOffsetsEnumsForSpans(LeafReader,int,List[OffsetsEnum]).mjava","commits":[{"id":"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571","date":1515642580,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#createOffsetsEnumsForSpans(LeafReader,int,List[OffsetsEnum]).mjava","pathOld":"/dev/null","sourceNew":"  /** Given the internal SpanQueries, produce a number of OffsetsEnum into the {@code results} param. */\n  public void createOffsetsEnumsForSpans(LeafReader leafReader, int docId, List<OffsetsEnum> results) throws IOException {\n    leafReader = new SingleFieldWithOffsetsFilterLeafReader(leafReader, fieldName);\n    //TODO avoid searcher and do what it does to rewrite & get weight?\n    IndexSearcher searcher = new IndexSearcher(leafReader);\n    searcher.setQueryCache(null);\n\n    // for each SpanQuery, grab it's Spans and put it into a PriorityQueue\n    PriorityQueue<Spans> spansPriorityQueue = new PriorityQueue<Spans>(spanQueries.size()) {\n      @Override\n      protected boolean lessThan(Spans a, Spans b) {\n        return a.startPosition() <= b.startPosition();\n      }\n    };\n    for (Query query : spanQueries) {\n      Weight weight = searcher.createNormalizedWeight(query, ScoreMode.COMPLETE_NO_SCORES);\n      Scorer scorer = weight.scorer(leafReader.getContext());\n      if (scorer == null) {\n        continue;\n      }\n      TwoPhaseIterator twoPhaseIterator = scorer.twoPhaseIterator();\n      if (twoPhaseIterator != null) {\n        if (twoPhaseIterator.approximation().advance(docId) != docId || !twoPhaseIterator.matches()) {\n          continue;\n        }\n      } else if (scorer.iterator().advance(docId) != docId) { // preposition, and return doing nothing if find none\n        continue;\n      }\n\n      Spans spans = ((SpanScorer) scorer).getSpans();\n      assert spans.docID() == docId;\n      if (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.add(spans);\n      }\n    }\n\n    // Iterate the Spans in the PriorityQueue, collecting as we go.  By using a PriorityQueue ordered by position,\n    //   the underlying offsets in our collector will be mostly appended to the end of arrays (efficient).\n    // note: alternatively it'd interesting if we produced one OffsetsEnum that internally advanced\n    //   this PriorityQueue when nextPosition is called; it would cap what we have to cache for large docs and\n    //   exiting early (due to maxLen) is easy.\n    //   But at least we have an accurate \"freq\" and it shouldn't be too much data to collect.  Even SpanScorer\n    //   navigates the spans fully to compute a good freq (and thus score)!\n    OffsetSpanCollector spanCollector = new OffsetSpanCollector();\n    while (spansPriorityQueue.size() > 0) {\n      Spans spans = spansPriorityQueue.top();\n      //TODO limit to a capped endOffset length somehow so we can break this loop early\n      spans.collect(spanCollector);\n\n      if (spans.nextStartPosition() == Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.pop();\n      } else {\n        spansPriorityQueue.updateTop();\n      }\n    }\n    results.addAll(spanCollector.termToOffsetsEnums.values());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#createOffsetsEnumsForSpans(LeafReader,int,List[OffsetsEnum]).mjava","pathOld":"/dev/null","sourceNew":"  /** Given the internal SpanQueries, produce a number of OffsetsEnum into the {@code results} param. */\n  public void createOffsetsEnumsForSpans(LeafReader leafReader, int docId, List<OffsetsEnum> results) throws IOException {\n    leafReader = new SingleFieldWithOffsetsFilterLeafReader(leafReader, fieldName);\n    //TODO avoid searcher and do what it does to rewrite & get weight?\n    IndexSearcher searcher = new IndexSearcher(leafReader);\n    searcher.setQueryCache(null);\n\n    // for each SpanQuery, grab it's Spans and put it into a PriorityQueue\n    PriorityQueue<Spans> spansPriorityQueue = new PriorityQueue<Spans>(spanQueries.size()) {\n      @Override\n      protected boolean lessThan(Spans a, Spans b) {\n        return a.startPosition() <= b.startPosition();\n      }\n    };\n    for (Query query : spanQueries) {\n      Weight weight = searcher.createNormalizedWeight(query, ScoreMode.COMPLETE_NO_SCORES);\n      Scorer scorer = weight.scorer(leafReader.getContext());\n      if (scorer == null) {\n        continue;\n      }\n      TwoPhaseIterator twoPhaseIterator = scorer.twoPhaseIterator();\n      if (twoPhaseIterator != null) {\n        if (twoPhaseIterator.approximation().advance(docId) != docId || !twoPhaseIterator.matches()) {\n          continue;\n        }\n      } else if (scorer.iterator().advance(docId) != docId) { // preposition, and return doing nothing if find none\n        continue;\n      }\n\n      Spans spans = ((SpanScorer) scorer).getSpans();\n      assert spans.docID() == docId;\n      if (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.add(spans);\n      }\n    }\n\n    // Iterate the Spans in the PriorityQueue, collecting as we go.  By using a PriorityQueue ordered by position,\n    //   the underlying offsets in our collector will be mostly appended to the end of arrays (efficient).\n    // note: alternatively it'd interesting if we produced one OffsetsEnum that internally advanced\n    //   this PriorityQueue when nextPosition is called; it would cap what we have to cache for large docs and\n    //   exiting early (due to maxLen) is easy.\n    //   But at least we have an accurate \"freq\" and it shouldn't be too much data to collect.  Even SpanScorer\n    //   navigates the spans fully to compute a good freq (and thus score)!\n    OffsetSpanCollector spanCollector = new OffsetSpanCollector();\n    while (spansPriorityQueue.size() > 0) {\n      Spans spans = spansPriorityQueue.top();\n      //TODO limit to a capped endOffset length somehow so we can break this loop early\n      spans.collect(spanCollector);\n\n      if (spans.nextStartPosition() == Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.pop();\n      } else {\n        spansPriorityQueue.updateTop();\n      }\n    }\n    results.addAll(spanCollector.termToOffsetsEnums.values());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"475584d5e08a22ad3fc7babefe006d77bc744567","date":1523282824,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#createOffsetsEnumsForSpans(LeafReader,int,List[OffsetsEnum]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#createOffsetsEnumsForSpans(LeafReader,int,List[OffsetsEnum]).mjava","sourceNew":"  /** Given the internal SpanQueries, produce a number of OffsetsEnum into the {@code results} param. */\n  public void createOffsetsEnumsForSpans(LeafReader leafReader, int docId, List<OffsetsEnum> results) throws IOException {\n    leafReader = new SingleFieldWithOffsetsFilterLeafReader(leafReader, fieldName);\n    //TODO avoid searcher and do what it does to rewrite & get weight?\n    IndexSearcher searcher = new IndexSearcher(leafReader);\n    searcher.setQueryCache(null);\n\n    // for each SpanQuery, grab it's Spans and put it into a PriorityQueue\n    PriorityQueue<Spans> spansPriorityQueue = new PriorityQueue<Spans>(spanQueries.size()) {\n      @Override\n      protected boolean lessThan(Spans a, Spans b) {\n        return a.startPosition() <= b.startPosition();\n      }\n    };\n    for (Query query : spanQueries) {\n      Weight weight = searcher.createWeight(searcher.rewrite(query), ScoreMode.COMPLETE_NO_SCORES, 1);\n      Scorer scorer = weight.scorer(leafReader.getContext());\n      if (scorer == null) {\n        continue;\n      }\n      TwoPhaseIterator twoPhaseIterator = scorer.twoPhaseIterator();\n      if (twoPhaseIterator != null) {\n        if (twoPhaseIterator.approximation().advance(docId) != docId || !twoPhaseIterator.matches()) {\n          continue;\n        }\n      } else if (scorer.iterator().advance(docId) != docId) { // preposition, and return doing nothing if find none\n        continue;\n      }\n\n      Spans spans = ((SpanScorer) scorer).getSpans();\n      assert spans.docID() == docId;\n      if (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.add(spans);\n      }\n    }\n\n    // Iterate the Spans in the PriorityQueue, collecting as we go.  By using a PriorityQueue ordered by position,\n    //   the underlying offsets in our collector will be mostly appended to the end of arrays (efficient).\n    // note: alternatively it'd interesting if we produced one OffsetsEnum that internally advanced\n    //   this PriorityQueue when nextPosition is called; it would cap what we have to cache for large docs and\n    //   exiting early (due to maxLen) is easy.\n    //   But at least we have an accurate \"freq\" and it shouldn't be too much data to collect.  Even SpanScorer\n    //   navigates the spans fully to compute a good freq (and thus score)!\n    OffsetSpanCollector spanCollector = new OffsetSpanCollector();\n    while (spansPriorityQueue.size() > 0) {\n      Spans spans = spansPriorityQueue.top();\n      //TODO limit to a capped endOffset length somehow so we can break this loop early\n      spans.collect(spanCollector);\n\n      if (spans.nextStartPosition() == Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.pop();\n      } else {\n        spansPriorityQueue.updateTop();\n      }\n    }\n    results.addAll(spanCollector.termToOffsetsEnums.values());\n  }\n\n","sourceOld":"  /** Given the internal SpanQueries, produce a number of OffsetsEnum into the {@code results} param. */\n  public void createOffsetsEnumsForSpans(LeafReader leafReader, int docId, List<OffsetsEnum> results) throws IOException {\n    leafReader = new SingleFieldWithOffsetsFilterLeafReader(leafReader, fieldName);\n    //TODO avoid searcher and do what it does to rewrite & get weight?\n    IndexSearcher searcher = new IndexSearcher(leafReader);\n    searcher.setQueryCache(null);\n\n    // for each SpanQuery, grab it's Spans and put it into a PriorityQueue\n    PriorityQueue<Spans> spansPriorityQueue = new PriorityQueue<Spans>(spanQueries.size()) {\n      @Override\n      protected boolean lessThan(Spans a, Spans b) {\n        return a.startPosition() <= b.startPosition();\n      }\n    };\n    for (Query query : spanQueries) {\n      Weight weight = searcher.createNormalizedWeight(query, ScoreMode.COMPLETE_NO_SCORES);\n      Scorer scorer = weight.scorer(leafReader.getContext());\n      if (scorer == null) {\n        continue;\n      }\n      TwoPhaseIterator twoPhaseIterator = scorer.twoPhaseIterator();\n      if (twoPhaseIterator != null) {\n        if (twoPhaseIterator.approximation().advance(docId) != docId || !twoPhaseIterator.matches()) {\n          continue;\n        }\n      } else if (scorer.iterator().advance(docId) != docId) { // preposition, and return doing nothing if find none\n        continue;\n      }\n\n      Spans spans = ((SpanScorer) scorer).getSpans();\n      assert spans.docID() == docId;\n      if (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.add(spans);\n      }\n    }\n\n    // Iterate the Spans in the PriorityQueue, collecting as we go.  By using a PriorityQueue ordered by position,\n    //   the underlying offsets in our collector will be mostly appended to the end of arrays (efficient).\n    // note: alternatively it'd interesting if we produced one OffsetsEnum that internally advanced\n    //   this PriorityQueue when nextPosition is called; it would cap what we have to cache for large docs and\n    //   exiting early (due to maxLen) is easy.\n    //   But at least we have an accurate \"freq\" and it shouldn't be too much data to collect.  Even SpanScorer\n    //   navigates the spans fully to compute a good freq (and thus score)!\n    OffsetSpanCollector spanCollector = new OffsetSpanCollector();\n    while (spansPriorityQueue.size() > 0) {\n      Spans spans = spansPriorityQueue.top();\n      //TODO limit to a capped endOffset length somehow so we can break this loop early\n      spans.collect(spanCollector);\n\n      if (spans.nextStartPosition() == Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.pop();\n      } else {\n        spansPriorityQueue.updateTop();\n      }\n    }\n    results.addAll(spanCollector.termToOffsetsEnums.values());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d58e44159788900f4a2113b84463dc3fbbf80f20","date":1523319203,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#createOffsetsEnumsForSpans(LeafReader,int,List[OffsetsEnum]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/PhraseHelper#createOffsetsEnumsForSpans(LeafReader,int,List[OffsetsEnum]).mjava","sourceNew":"  /** Given the internal SpanQueries, produce a number of OffsetsEnum into the {@code results} param. */\n  public void createOffsetsEnumsForSpans(LeafReader leafReader, int docId, List<OffsetsEnum> results) throws IOException {\n    leafReader = new SingleFieldWithOffsetsFilterLeafReader(leafReader, fieldName);\n    //TODO avoid searcher and do what it does to rewrite & get weight?\n    IndexSearcher searcher = new IndexSearcher(leafReader);\n    searcher.setQueryCache(null);\n\n    // for each SpanQuery, grab it's Spans and put it into a PriorityQueue\n    PriorityQueue<Spans> spansPriorityQueue = new PriorityQueue<Spans>(spanQueries.size()) {\n      @Override\n      protected boolean lessThan(Spans a, Spans b) {\n        return a.startPosition() <= b.startPosition();\n      }\n    };\n    for (Query query : spanQueries) {\n      Weight weight = searcher.createWeight(searcher.rewrite(query), ScoreMode.COMPLETE_NO_SCORES, 1);\n      Scorer scorer = weight.scorer(leafReader.getContext());\n      if (scorer == null) {\n        continue;\n      }\n      TwoPhaseIterator twoPhaseIterator = scorer.twoPhaseIterator();\n      if (twoPhaseIterator != null) {\n        if (twoPhaseIterator.approximation().advance(docId) != docId || !twoPhaseIterator.matches()) {\n          continue;\n        }\n      } else if (scorer.iterator().advance(docId) != docId) { // preposition, and return doing nothing if find none\n        continue;\n      }\n\n      Spans spans = ((SpanScorer) scorer).getSpans();\n      assert spans.docID() == docId;\n      if (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.add(spans);\n      }\n    }\n\n    // Iterate the Spans in the PriorityQueue, collecting as we go.  By using a PriorityQueue ordered by position,\n    //   the underlying offsets in our collector will be mostly appended to the end of arrays (efficient).\n    // note: alternatively it'd interesting if we produced one OffsetsEnum that internally advanced\n    //   this PriorityQueue when nextPosition is called; it would cap what we have to cache for large docs and\n    //   exiting early (due to maxLen) is easy.\n    //   But at least we have an accurate \"freq\" and it shouldn't be too much data to collect.  Even SpanScorer\n    //   navigates the spans fully to compute a good freq (and thus score)!\n    OffsetSpanCollector spanCollector = new OffsetSpanCollector();\n    while (spansPriorityQueue.size() > 0) {\n      Spans spans = spansPriorityQueue.top();\n      //TODO limit to a capped endOffset length somehow so we can break this loop early\n      spans.collect(spanCollector);\n\n      if (spans.nextStartPosition() == Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.pop();\n      } else {\n        spansPriorityQueue.updateTop();\n      }\n    }\n    results.addAll(spanCollector.termToOffsetsEnums.values());\n  }\n\n","sourceOld":"  /** Given the internal SpanQueries, produce a number of OffsetsEnum into the {@code results} param. */\n  public void createOffsetsEnumsForSpans(LeafReader leafReader, int docId, List<OffsetsEnum> results) throws IOException {\n    leafReader = new SingleFieldWithOffsetsFilterLeafReader(leafReader, fieldName);\n    //TODO avoid searcher and do what it does to rewrite & get weight?\n    IndexSearcher searcher = new IndexSearcher(leafReader);\n    searcher.setQueryCache(null);\n\n    // for each SpanQuery, grab it's Spans and put it into a PriorityQueue\n    PriorityQueue<Spans> spansPriorityQueue = new PriorityQueue<Spans>(spanQueries.size()) {\n      @Override\n      protected boolean lessThan(Spans a, Spans b) {\n        return a.startPosition() <= b.startPosition();\n      }\n    };\n    for (Query query : spanQueries) {\n      Weight weight = searcher.createNormalizedWeight(query, ScoreMode.COMPLETE_NO_SCORES);\n      Scorer scorer = weight.scorer(leafReader.getContext());\n      if (scorer == null) {\n        continue;\n      }\n      TwoPhaseIterator twoPhaseIterator = scorer.twoPhaseIterator();\n      if (twoPhaseIterator != null) {\n        if (twoPhaseIterator.approximation().advance(docId) != docId || !twoPhaseIterator.matches()) {\n          continue;\n        }\n      } else if (scorer.iterator().advance(docId) != docId) { // preposition, and return doing nothing if find none\n        continue;\n      }\n\n      Spans spans = ((SpanScorer) scorer).getSpans();\n      assert spans.docID() == docId;\n      if (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.add(spans);\n      }\n    }\n\n    // Iterate the Spans in the PriorityQueue, collecting as we go.  By using a PriorityQueue ordered by position,\n    //   the underlying offsets in our collector will be mostly appended to the end of arrays (efficient).\n    // note: alternatively it'd interesting if we produced one OffsetsEnum that internally advanced\n    //   this PriorityQueue when nextPosition is called; it would cap what we have to cache for large docs and\n    //   exiting early (due to maxLen) is easy.\n    //   But at least we have an accurate \"freq\" and it shouldn't be too much data to collect.  Even SpanScorer\n    //   navigates the spans fully to compute a good freq (and thus score)!\n    OffsetSpanCollector spanCollector = new OffsetSpanCollector();\n    while (spansPriorityQueue.size() > 0) {\n      Spans spans = spansPriorityQueue.top();\n      //TODO limit to a capped endOffset length somehow so we can break this loop early\n      spans.collect(spanCollector);\n\n      if (spans.nextStartPosition() == Spans.NO_MORE_POSITIONS) {\n        spansPriorityQueue.pop();\n      } else {\n        spansPriorityQueue.updateTop();\n      }\n    }\n    results.addAll(spanCollector.termToOffsetsEnums.values());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b94236357aaa22b76c10629851fe4e376e0cea82":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571"],"d58e44159788900f4a2113b84463dc3fbbf80f20":["b94236357aaa22b76c10629851fe4e376e0cea82","475584d5e08a22ad3fc7babefe006d77bc744567"],"475584d5e08a22ad3fc7babefe006d77bc744567":["b94236357aaa22b76c10629851fe4e376e0cea82"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d58e44159788900f4a2113b84463dc3fbbf80f20"]},"commit2Childs":{"b94236357aaa22b76c10629851fe4e376e0cea82":["d58e44159788900f4a2113b84463dc3fbbf80f20","475584d5e08a22ad3fc7babefe006d77bc744567"],"d58e44159788900f4a2113b84463dc3fbbf80f20":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"475584d5e08a22ad3fc7babefe006d77bc744567":["d58e44159788900f4a2113b84463dc3fbbf80f20"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b94236357aaa22b76c10629851fe4e376e0cea82","eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571"],"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571":["b94236357aaa22b76c10629851fe4e376e0cea82"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}