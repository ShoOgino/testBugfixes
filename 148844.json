{"path":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f08557cdb6c60ac7b88a9342c983a20cd236e74f","date":1330954480,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bdb5e42b0cecd8dfb27767a02ada71899bf17917","date":1334100099,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a238fc456663f685a9db1ed8d680e348bb45171","date":1334173266,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a470c93b2b0f8f51241f52705fc110a01f27ad2","date":1337969379,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, 0);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, 0);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, 0);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, false);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d56999cfc1772fd594a2a43a40007a11a188bd96","date":1345554776,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, 0);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, 0);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"001b25b42373b22a52f399dbf072f1224632e8e6","date":1345889167,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, 0);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.size();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, 0);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4d374b2bebd0d52acaa61038fbf23068620fba7","date":1353240004,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":null,"sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, 0);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15250ca94ba8ab3bcdd476daf6bf3f3febb92640","date":1355200097,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, 0);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,Entry,boolean).mjava","sourceNew":null,"sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      Terms terms = reader.terms(entryKey.field);\n\n      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // size (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"001b25b42373b22a52f399dbf072f1224632e8e6":["d6f074e73200c07d54f242d3880a8da5a35ff97b","d56999cfc1772fd594a2a43a40007a11a188bd96"],"bdb5e42b0cecd8dfb27767a02ada71899bf17917":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"5a238fc456663f685a9db1ed8d680e348bb45171":["f08557cdb6c60ac7b88a9342c983a20cd236e74f","bdb5e42b0cecd8dfb27767a02ada71899bf17917"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a4d374b2bebd0d52acaa61038fbf23068620fba7":["d56999cfc1772fd594a2a43a40007a11a188bd96"],"4a470c93b2b0f8f51241f52705fc110a01f27ad2":["bdb5e42b0cecd8dfb27767a02ada71899bf17917"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640","a4d374b2bebd0d52acaa61038fbf23068620fba7"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["d56999cfc1772fd594a2a43a40007a11a188bd96"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["4a470c93b2b0f8f51241f52705fc110a01f27ad2","02331260bb246364779cb6f04919ca47900d01bb"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["4a470c93b2b0f8f51241f52705fc110a01f27ad2","02331260bb246364779cb6f04919ca47900d01bb"],"d56999cfc1772fd594a2a43a40007a11a188bd96":["02331260bb246364779cb6f04919ca47900d01bb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"02331260bb246364779cb6f04919ca47900d01bb":["4a470c93b2b0f8f51241f52705fc110a01f27ad2"]},"commit2Childs":{"001b25b42373b22a52f399dbf072f1224632e8e6":[],"bdb5e42b0cecd8dfb27767a02ada71899bf17917":["5a238fc456663f685a9db1ed8d680e348bb45171","4a470c93b2b0f8f51241f52705fc110a01f27ad2"],"5a238fc456663f685a9db1ed8d680e348bb45171":[],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["bdb5e42b0cecd8dfb27767a02ada71899bf17917","5a238fc456663f685a9db1ed8d680e348bb45171","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"a4d374b2bebd0d52acaa61038fbf23068620fba7":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"4a470c93b2b0f8f51241f52705fc110a01f27ad2":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","02331260bb246364779cb6f04919ca47900d01bb"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["001b25b42373b22a52f399dbf072f1224632e8e6"],"d56999cfc1772fd594a2a43a40007a11a188bd96":["001b25b42373b22a52f399dbf072f1224632e8e6","a4d374b2bebd0d52acaa61038fbf23068620fba7","15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"02331260bb246364779cb6f04919ca47900d01bb":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","d56999cfc1772fd594a2a43a40007a11a188bd96"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["001b25b42373b22a52f399dbf072f1224632e8e6","5a238fc456663f685a9db1ed8d680e348bb45171","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}