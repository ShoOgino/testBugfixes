{"path":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","commits":[{"id":"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7","date":1524496660,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap).mjava","sourceNew":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link MutableBits}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        IndexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context, flushNotifications::deleteUnusedFiles);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final MutableBits bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link MutableBits}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        indexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final MutableBits bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb8a3dfca2d8b39c20bdfc87eb7171c06ea9400d","date":1525873214,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","sourceNew":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link FixedBitSet}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        IndexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context, flushNotifications::deleteUnusedFiles);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final FixedBitSet bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link MutableBits}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        IndexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context, flushNotifications::deleteUnusedFiles);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final MutableBits bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f97270426d92300e08ac1bd1a4ef499ae02e88b7","date":1592503330,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","sourceNew":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link FixedBitSet}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (getIndexWriterConfig().getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        IndexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context, flushNotifications::deleteUnusedFiles);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final FixedBitSet bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link FixedBitSet}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        IndexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context, flushNotifications::deleteUnusedFiles);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final FixedBitSet bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"49f1924bd448393fbdfef8b5ebed799f938169d3","date":1600069616,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","sourceNew":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link FixedBitSet}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        IndexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context, flushNotifications::deleteUnusedFiles);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final FixedBitSet bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link FixedBitSet}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (getIndexWriterConfig().getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        IndexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context, flushNotifications::deleteUnusedFiles);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final FixedBitSet bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0dcf8f79417865e5028d753e669fae06457e8369","date":1600073240,"type":3,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#sealFlushedSegment(FlushedSegment,Sorter.DocMap,DocumentsWriter.FlushNotifications).mjava","sourceNew":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link FixedBitSet}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (indexWriterConfig.getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        IndexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context, flushNotifications::deleteUnusedFiles);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final FixedBitSet bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Seals the {@link SegmentInfo} for the new flushed segment and persists\n   * the deleted documents {@link FixedBitSet}.\n   */\n  void sealFlushedSegment(FlushedSegment flushedSegment, Sorter.DocMap sortMap, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {\n    assert flushedSegment != null;\n    SegmentCommitInfo newSegment = flushedSegment.segmentInfo;\n\n    IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);\n    \n    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));\n\n    boolean success = false;\n    try {\n      \n      if (getIndexWriterConfig().getUseCompoundFile()) {\n        Set<String> originalFiles = newSegment.info.files();\n        // TODO: like addIndexes, we are relying on createCompoundFile to successfully cleanup...\n        IndexWriter.createCompoundFile(infoStream, new TrackingDirectoryWrapper(directory), newSegment.info, context, flushNotifications::deleteUnusedFiles);\n        filesToDelete.addAll(originalFiles);\n        newSegment.info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(directory, newSegment.info, context);\n\n      // TODO: ideally we would freeze newSegment here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushedSegment.liveDocs != null) {\n        final int delCount = flushedSegment.delCount;\n        assert delCount > 0;\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\", \"flush: write \" + delCount + \" deletes gen=\" + flushedSegment.segmentInfo.getDelGen());\n        }\n\n        // TODO: we should prune the segment if it's 100%\n        // deleted... but merge will also catch it.\n\n        // TODO: in the NRT case it'd be better to hand\n        // this del vector over to the\n        // shortly-to-be-opened SegmentReader and let it\n        // carry the changes; there's no reason to use\n        // filesystem as intermediary here.\n          \n        SegmentCommitInfo info = flushedSegment.segmentInfo;\n        Codec codec = info.info.getCodec();\n        final FixedBitSet bits;\n        if (sortMap == null) {\n          bits = flushedSegment.liveDocs;\n        } else {\n          bits = sortLiveDocs(flushedSegment.liveDocs, sortMap);\n        }\n        codec.liveDocsFormat().writeLiveDocs(bits, directory, info, delCount, context);\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n      }\n\n      success = true;\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"DWPT\")) {\n          infoStream.message(\"DWPT\",\n                             \"hit exception creating compound file for newly flushed segment \" + newSegment.info.name);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0dcf8f79417865e5028d753e669fae06457e8369":["f97270426d92300e08ac1bd1a4ef499ae02e88b7","49f1924bd448393fbdfef8b5ebed799f938169d3"],"fb8a3dfca2d8b39c20bdfc87eb7171c06ea9400d":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["fb8a3dfca2d8b39c20bdfc87eb7171c06ea9400d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0dcf8f79417865e5028d753e669fae06457e8369"]},"commit2Childs":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["0dcf8f79417865e5028d753e669fae06457e8369"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["fb8a3dfca2d8b39c20bdfc87eb7171c06ea9400d"],"0dcf8f79417865e5028d753e669fae06457e8369":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"fb8a3dfca2d8b39c20bdfc87eb7171c06ea9400d":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["49f1924bd448393fbdfef8b5ebed799f938169d3","0dcf8f79417865e5028d753e669fae06457e8369"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}