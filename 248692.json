{"path":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter#copyFieldsWithDeletions(MergeState,AtomicReader,Lucene40StoredFieldsReader,int[]).mjava","commits":[{"id":"c95a819869502635864dac0a788f874787e3395b","date":1341394787,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter#copyFieldsWithDeletions(MergeState,AtomicReader,Lucene40StoredFieldsReader,int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter#copyFieldsWithDeletions(MergeState,MergeState.IndexReaderAndLiveDocs,Lucene40StoredFieldsReader,int[]).mjava","sourceNew":"  private int copyFieldsWithDeletions(MergeState mergeState, final AtomicReader reader,\n                                      final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])\n    throws IOException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // TODO: this could be more efficient using\n        // FieldVisitor instead of loading/writing entire\n        // doc; ie we just have to renumber the field number\n        // on the fly?\n        // NOTE: it's very important to first assign to doc then pass it to\n        // fieldsWriter.addDocument; see LUCENE-1282\n        Document doc = reader.document(j);\n        addDocument(doc, mergeState.fieldInfos);\n        docCount++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(MergeState mergeState, final MergeState.IndexReaderAndLiveDocs reader,\n                                      final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])\n    throws IOException {\n    int docCount = 0;\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // TODO: this could be more efficient using\n        // FieldVisitor instead of loading/writing entire\n        // doc; ie we just have to renumber the field number\n        // on the fly?\n        // NOTE: it's very important to first assign to doc then pass it to\n        // fieldsWriter.addDocument; see LUCENE-1282\n        Document doc = reader.reader.document(j);\n        addDocument(doc, mergeState.fieldInfos);\n        docCount++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a","date":1341524239,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter#copyFieldsWithDeletions(MergeState,AtomicReader,Lucene40StoredFieldsReader,int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter#copyFieldsWithDeletions(MergeState,AtomicReader,Lucene40StoredFieldsReader,int[]).mjava","sourceNew":"  private int copyFieldsWithDeletions(MergeState mergeState, final AtomicReader reader,\n                                      final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])\n    throws IOException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // TODO: this could be more efficient using\n        // FieldVisitor instead of loading/writing entire\n        // doc; ie we just have to renumber the field number\n        // on the fly?\n        // NOTE: it's very important to first assign to doc then pass it to\n        // fieldsWriter.addDocument; see LUCENE-1282\n        StoredDocument doc = reader.document(j);\n        addDocument(doc, mergeState.fieldInfos);\n        docCount++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(MergeState mergeState, final AtomicReader reader,\n                                      final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])\n    throws IOException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // TODO: this could be more efficient using\n        // FieldVisitor instead of loading/writing entire\n        // doc; ie we just have to renumber the field number\n        // on the fly?\n        // NOTE: it's very important to first assign to doc then pass it to\n        // fieldsWriter.addDocument; see LUCENE-1282\n        Document doc = reader.document(j);\n        addDocument(doc, mergeState.fieldInfos);\n        docCount++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter#copyFieldsWithDeletions(MergeState,AtomicReader,Lucene40StoredFieldsReader,int[]).mjava","pathOld":"/dev/null","sourceNew":"  private int copyFieldsWithDeletions(MergeState mergeState, final AtomicReader reader,\n                                      final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])\n    throws IOException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // TODO: this could be more efficient using\n        // FieldVisitor instead of loading/writing entire\n        // doc; ie we just have to renumber the field number\n        // on the fly?\n        // NOTE: it's very important to first assign to doc then pass it to\n        // fieldsWriter.addDocument; see LUCENE-1282\n        Document doc = reader.document(j);\n        addDocument(doc, mergeState.fieldInfos);\n        docCount++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter#copyFieldsWithDeletions(MergeState,AtomicReader,Lucene40StoredFieldsReader,int[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter#copyFieldsWithDeletions(MergeState,AtomicReader,Lucene40StoredFieldsReader,int[]).mjava","sourceNew":"  private int copyFieldsWithDeletions(MergeState mergeState, final AtomicReader reader,\n                                      final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])\n    throws IOException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // TODO: this could be more efficient using\n        // FieldVisitor instead of loading/writing entire\n        // doc; ie we just have to renumber the field number\n        // on the fly?\n        // NOTE: it's very important to first assign to doc then pass it to\n        // fieldsWriter.addDocument; see LUCENE-1282\n        StoredDocument doc = reader.document(j);\n        addDocument(doc, mergeState.fieldInfos);\n        docCount++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(MergeState mergeState, final AtomicReader reader,\n                                      final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])\n    throws IOException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // TODO: this could be more efficient using\n        // FieldVisitor instead of loading/writing entire\n        // doc; ie we just have to renumber the field number\n        // on the fly?\n        // NOTE: it's very important to first assign to doc then pass it to\n        // fieldsWriter.addDocument; see LUCENE-1282\n        Document doc = reader.document(j);\n        addDocument(doc, mergeState.fieldInfos);\n        docCount++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22a2e66dfda83847e80095b8693c660742ab3e9c","date":1408628796,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter#copyFieldsWithDeletions(MergeState,AtomicReader,Lucene40StoredFieldsReader,int[]).mjava","sourceNew":null,"sourceOld":"  private int copyFieldsWithDeletions(MergeState mergeState, final AtomicReader reader,\n                                      final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])\n    throws IOException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        mergeState.checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // TODO: this could be more efficient using\n        // FieldVisitor instead of loading/writing entire\n        // doc; ie we just have to renumber the field number\n        // on the fly?\n        // NOTE: it's very important to first assign to doc then pass it to\n        // fieldsWriter.addDocument; see LUCENE-1282\n        StoredDocument doc = reader.document(j);\n        addDocument(doc, mergeState.fieldInfos);\n        docCount++;\n        mergeState.checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c95a819869502635864dac0a788f874787e3395b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1d028314cced5858683a1bb4741423d0f934257b":["c95a819869502635864dac0a788f874787e3395b","33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a"],"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a":["c95a819869502635864dac0a788f874787e3395b"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c95a819869502635864dac0a788f874787e3395b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"22a2e66dfda83847e80095b8693c660742ab3e9c":["1d028314cced5858683a1bb4741423d0f934257b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["22a2e66dfda83847e80095b8693c660742ab3e9c"]},"commit2Childs":{"c95a819869502635864dac0a788f874787e3395b":["1d028314cced5858683a1bb4741423d0f934257b","33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"1d028314cced5858683a1bb4741423d0f934257b":["22a2e66dfda83847e80095b8693c660742ab3e9c"],"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a":["1d028314cced5858683a1bb4741423d0f934257b"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c95a819869502635864dac0a788f874787e3395b","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"22a2e66dfda83847e80095b8693c660742ab3e9c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}