{"path":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","commits":[{"id":"172cf08877d0e6738a51edd238c4dc5ffc088345","date":1342794823,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"/dev/null","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum, hasOffsets);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum, hasFreq);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"/dev/null","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum, hasOffsets);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum, hasFreq);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"/dev/null","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum, hasOffsets);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum, hasFreq);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum, hasFreq);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum, hasOffsets);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum, hasFreq);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum, hasFreq);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":["172cf08877d0e6738a51edd238c4dc5ffc088345"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum, hasOffsets);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum, hasFreq);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum, hasOffsets);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum, hasFreq);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","date":1344797146,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload;\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      payload = docsAndPositionsEnum.getPayload();\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    if (docsAndPositionsEnum.hasPayload()) {\n                      BytesRef payload = docsAndPositionsEnum.getPayload();\n                      assert payload != null;\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e45d45bc3730ddd1341f4eb6025f33b8482e6e2","date":1346834651,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3e45d45bc3730ddd1341f4eb6025f33b8482e6e2":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"aba371508186796cc6151d8223a5b4e16d02e26e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","172cf08877d0e6738a51edd238c4dc5ffc088345"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["02331260bb246364779cb6f04919ca47900d01bb"],"172cf08877d0e6738a51edd238c4dc5ffc088345":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","02331260bb246364779cb6f04919ca47900d01bb"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["aba371508186796cc6151d8223a5b4e16d02e26e","02331260bb246364779cb6f04919ca47900d01bb"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","172cf08877d0e6738a51edd238c4dc5ffc088345"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["172cf08877d0e6738a51edd238c4dc5ffc088345"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["d6f074e73200c07d54f242d3880a8da5a35ff97b","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3e45d45bc3730ddd1341f4eb6025f33b8482e6e2"],"02331260bb246364779cb6f04919ca47900d01bb":["322360ac5185a8446d3e0b530b2068bef67cd3d5"]},"commit2Childs":{"3e45d45bc3730ddd1341f4eb6025f33b8482e6e2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"aba371508186796cc6151d8223a5b4e16d02e26e":["d6f074e73200c07d54f242d3880a8da5a35ff97b"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["3e45d45bc3730ddd1341f4eb6025f33b8482e6e2","c7869f64c874ebf7f317d22c00baf2b6857797a6","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["aba371508186796cc6151d8223a5b4e16d02e26e","172cf08877d0e6738a51edd238c4dc5ffc088345","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"172cf08877d0e6738a51edd238c4dc5ffc088345":["aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["02331260bb246364779cb6f04919ca47900d01bb"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":[],"02331260bb246364779cb6f04919ca47900d01bb":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c7869f64c874ebf7f317d22c00baf2b6857797a6","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}