{"path":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","commits":[{"id":"92304613d4e239eb290f8879c862a0656cc41638","date":1486746489,"type":0,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  @AwaitsFix(bugUrl = \"https://issues.apache.org/jira/browse/SOLR-10121\")\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    /***\n    final int blocksInTest = 256;\n    final int blockSize = 1024;\n    final int slabSize = blockSize * 128;\n    final long totalMemory = 2 * slabSize;\n    ***/\n\n    final int blocksInTest = 16384;  // pick something that won't fit in memory, but is small enough to cause a medium hit rate.  16MB of blocks is double the total memory size of the cache.\n    final int blockSize = 1024;\n    final int slabSize = blockSize * 4096;\n    final long totalMemory = 2 * slabSize;  // should give us 2 slabs (8MB)\n\n    final int nThreads=2;\n    final int nReads=1000000;\n    final int readsPerThread=nReads/nThreads;\n    final int readLastBlockOdds=10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey = new BlockCacheKey();\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i=0; i<iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0) block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                failed.set(true);\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n\n    assertFalse( failed.get() );\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["750b9dd5a93ea77a4c6e90577efc464ec3353b9c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"46386805f467fa40cb9d5a3cab791713306548c2","date":1487170610,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","pathOld":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","sourceNew":"  @Test\n  @AwaitsFix(bugUrl = \"https://issues.apache.org/jira/browse/SOLR-10121\")\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    final int blocksInTest = 400;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 64;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n\n    /***\n     final int blocksInTest = 16384;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n     final int blockSize = 1024;\n     final int slabSize = blocksInTest * blockSize / 4;\n     final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n     ***/\n\n    final int nThreads=64;\n    final int nReads=1000000;\n    final int readsPerThread=nReads/nThreads;\n    final int readLastBlockOdds=10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n    final int showErrors=50; // show first 50 validation failures\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n    final AtomicLong validateFails = new AtomicLong(0);\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey;\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i=0; i<iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0) block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                failed.set(true);\n                if (validateFails.incrementAndGet() <= showErrors) System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                break;\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n    System.out.println(\"Blocks with Errors = \" + validateFails.get());\n\n    assertFalse( failed.get() );\n  }\n\n","sourceOld":"  @Test\n  @AwaitsFix(bugUrl = \"https://issues.apache.org/jira/browse/SOLR-10121\")\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    /***\n    final int blocksInTest = 256;\n    final int blockSize = 1024;\n    final int slabSize = blockSize * 128;\n    final long totalMemory = 2 * slabSize;\n    ***/\n\n    final int blocksInTest = 16384;  // pick something that won't fit in memory, but is small enough to cause a medium hit rate.  16MB of blocks is double the total memory size of the cache.\n    final int blockSize = 1024;\n    final int slabSize = blockSize * 4096;\n    final long totalMemory = 2 * slabSize;  // should give us 2 slabs (8MB)\n\n    final int nThreads=2;\n    final int nReads=1000000;\n    final int readsPerThread=nReads/nThreads;\n    final int readLastBlockOdds=10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey = new BlockCacheKey();\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i=0; i<iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0) block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                failed.set(true);\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n\n    assertFalse( failed.get() );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8702182bc92026984124976ef1fa3393bdcbbc05","date":1487384465,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","pathOld":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","sourceNew":"  @Test\n  @AwaitsFix(bugUrl = \"https://issues.apache.org/jira/browse/SOLR-10121\")\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    final int blocksInTest = 400;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 64;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n\n    /***\n    final int blocksInTest = 16384;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 1024;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n    ***/\n\n    final int nThreads=64;\n    final int nReads=1000000;\n    final int readsPerThread=nReads/nThreads;\n    final int readLastBlockOdds=10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n    final int showErrors=50; // show first 50 validation failures\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n    final AtomicLong validateFails = new AtomicLong(0);\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey;\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i=0; i<iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0) block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                failed.set(true);\n                if (validateFails.incrementAndGet() <= showErrors) System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                break;\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n    System.out.println(\"Blocks with Errors = \" + validateFails.get());\n\n    assertFalse( failed.get() );\n  }\n\n","sourceOld":"  @Test\n  @AwaitsFix(bugUrl = \"https://issues.apache.org/jira/browse/SOLR-10121\")\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    final int blocksInTest = 400;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 64;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n\n    /***\n     final int blocksInTest = 16384;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n     final int blockSize = 1024;\n     final int slabSize = blocksInTest * blockSize / 4;\n     final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n     ***/\n\n    final int nThreads=64;\n    final int nReads=1000000;\n    final int readsPerThread=nReads/nThreads;\n    final int readLastBlockOdds=10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n    final int showErrors=50; // show first 50 validation failures\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n    final AtomicLong validateFails = new AtomicLong(0);\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey;\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i=0; i<iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0) block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                failed.set(true);\n                if (validateFails.incrementAndGet() <= showErrors) System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                break;\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n    System.out.println(\"Blocks with Errors = \" + validateFails.get());\n\n    assertFalse( failed.get() );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"750b9dd5a93ea77a4c6e90577efc464ec3353b9c","date":1487721399,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","pathOld":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","sourceNew":"  @Test\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    final int blocksInTest = 400;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 64;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n\n    /***\n    final int blocksInTest = 16384;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 1024;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n    ***/\n\n    final int nThreads=64;\n    final int nReads=1000000;\n    final int readsPerThread=nReads/nThreads;\n    final int readLastBlockOdds=10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n    final int showErrors=50; // show first 50 validation failures\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n    final AtomicLong validateFails = new AtomicLong(0);\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey;\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i=0; i<iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0) block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                failed.set(true);\n                if (validateFails.incrementAndGet() <= showErrors) System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                break;\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n    System.out.println(\"Blocks with Errors = \" + validateFails.get());\n\n    assertFalse( failed.get() );\n  }\n\n","sourceOld":"  @Test\n  @AwaitsFix(bugUrl = \"https://issues.apache.org/jira/browse/SOLR-10121\")\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    final int blocksInTest = 400;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 64;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n\n    /***\n    final int blocksInTest = 16384;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 1024;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n    ***/\n\n    final int nThreads=64;\n    final int nReads=1000000;\n    final int readsPerThread=nReads/nThreads;\n    final int readLastBlockOdds=10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n    final int showErrors=50; // show first 50 validation failures\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n    final AtomicLong validateFails = new AtomicLong(0);\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey;\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i=0; i<iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0) block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                failed.set(true);\n                if (validateFails.incrementAndGet() <= showErrors) System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                break;\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n    System.out.println(\"Blocks with Errors = \" + validateFails.get());\n\n    assertFalse( failed.get() );\n  }\n\n","bugFix":["92304613d4e239eb290f8879c862a0656cc41638"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"87f0484c38f986062889ed50f3bf3bd462848c26","date":1570108628,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","pathOld":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","sourceNew":"  @Test\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    final int blocksInTest = 400;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 64;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n\n    /***\n     final int blocksInTest = 16384;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n     final int blockSize = 1024;\n     final int slabSize = blocksInTest * blockSize / 4;\n     final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n     ***/\n\n    final int nThreads = 64;\n    final int nReads = 1000000;\n    final int readsPerThread = nReads / nThreads;\n    final int readLastBlockOdds = 10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n    final int showErrors = 50; // show first 50 validation failures\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n    final AtomicLong validateFails = new AtomicLong(0);\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i = 0; i < threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey;\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i = 0; i < iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0)\n            block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                failed.set(true);\n                if (validateFails.incrementAndGet() <= showErrors)\n                  System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                break;\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n    System.out.println(\"Blocks with Errors = \" + validateFails.get());\n\n    assertFalse(\"cached bytes differ from expected\", failed.get());\n  }\n\n","sourceOld":"  @Test\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    final int blocksInTest = 400;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 64;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n\n    /***\n    final int blocksInTest = 16384;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 1024;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n    ***/\n\n    final int nThreads=64;\n    final int nReads=1000000;\n    final int readsPerThread=nReads/nThreads;\n    final int readLastBlockOdds=10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n    final int showErrors=50; // show first 50 validation failures\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n    final AtomicLong validateFails = new AtomicLong(0);\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey;\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i=0; i<iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0) block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                failed.set(true);\n                if (validateFails.incrementAndGet() <= showErrors) System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                break;\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n    System.out.println(\"Blocks with Errors = \" + validateFails.get());\n\n    assertFalse( failed.get() );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0b597c65628ca9e73913a07e81691f8229bae35","date":1571224353,"type":3,"author":"jimczi","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","pathOld":"solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest#testBlockCacheConcurrent().mjava","sourceNew":"  @Test\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    final int blocksInTest = 400;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 64;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n\n    /***\n     final int blocksInTest = 16384;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n     final int blockSize = 1024;\n     final int slabSize = blocksInTest * blockSize / 4;\n     final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n     ***/\n\n    final int nThreads = 64;\n    final int nReads = 1000000;\n    final int readsPerThread = nReads / nThreads;\n    final int readLastBlockOdds = 10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n    final int showErrors = 50; // show first 50 validation failures\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n    final AtomicLong validateFails = new AtomicLong(0);\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i = 0; i < threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey;\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i = 0; i < iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0)\n            block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                failed.set(true);\n                if (validateFails.incrementAndGet() <= showErrors)\n                  System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                break;\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n    System.out.println(\"Blocks with Errors = \" + validateFails.get());\n\n    assertFalse(\"cached bytes differ from expected\", failed.get());\n  }\n\n","sourceOld":"  @Test\n  public void testBlockCacheConcurrent() throws Exception {\n    Random rnd = random();\n\n    final int blocksInTest = 400;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 64;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n\n    /***\n    final int blocksInTest = 16384;  // pick something bigger than 256, since that would lead to a slab size of 64 blocks and the bitset locks would consist of a single word.\n    final int blockSize = 1024;\n    final int slabSize = blocksInTest * blockSize / 4;\n    final long totalMemory = 2 * slabSize;  // 2 slabs of memory, so only half of what is needed for all blocks\n    ***/\n\n    final int nThreads=64;\n    final int nReads=1000000;\n    final int readsPerThread=nReads/nThreads;\n    final int readLastBlockOdds=10; // odds (1 in N) of the next block operation being on the same block as the previous operation... helps flush concurrency issues\n    final int showErrors=50; // show first 50 validation failures\n\n    final BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);\n\n    final AtomicBoolean failed = new AtomicBoolean(false);\n    final AtomicLong hitsInCache = new AtomicLong();\n    final AtomicLong missesInCache = new AtomicLong();\n    final AtomicLong storeFails = new AtomicLong();\n    final AtomicLong lastBlock = new AtomicLong();\n    final AtomicLong validateFails = new AtomicLong(0);\n\n    final int file = 0;\n\n\n    Thread[] threads = new Thread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      final int threadnum = i;\n      final long seed = rnd.nextLong();\n\n      threads[i] = new Thread() {\n        Random r;\n        BlockCacheKey blockCacheKey;\n        byte[] buffer = new byte[blockSize];\n\n        @Override\n        public void run() {\n          try {\n            r = new Random(seed);\n            blockCacheKey = new BlockCacheKey();\n            blockCacheKey.setFile(file);\n            blockCacheKey.setPath(\"/foo.txt\");\n\n            test(readsPerThread);\n\n          } catch (Throwable e) {\n            failed.set(true);\n            e.printStackTrace();\n          }\n        }\n\n        public void test(int iter) {\n          for (int i=0; i<iter; i++) {\n            test();\n          }\n        }\n\n        public void test() {\n          long block = r.nextInt(blocksInTest);\n          if (r.nextInt(readLastBlockOdds) == 0) block = lastBlock.get();  // some percent of the time, try to read the last block another thread was just reading/writing\n          lastBlock.set(block);\n\n\n          int blockOffset = r.nextInt(blockSize);\n          long globalOffset = block * blockSize + blockOffset;\n          int len = r.nextInt(blockSize - blockOffset) + 1;  // TODO: bias toward smaller reads?\n\n          blockCacheKey.setBlock(block);\n\n          if (blockCache.fetch(blockCacheKey, buffer, blockOffset, 0, len)) {\n            hitsInCache.incrementAndGet();\n            // validate returned bytes\n            for (int i = 0; i < len; i++) {\n              long globalPos = globalOffset + i;\n              if (buffer[i] != getByte(globalPos)) {\n                failed.set(true);\n                if (validateFails.incrementAndGet() <= showErrors) System.out.println(\"ERROR: read was \" + \"block=\" + block + \" blockOffset=\" + blockOffset + \" len=\" + len + \" globalPos=\" + globalPos + \" localReadOffset=\" + i + \" got=\" + buffer[i] + \" expected=\" + getByte(globalPos));\n                break;\n              }\n            }\n          } else {\n            missesInCache.incrementAndGet();\n\n            // OK, we should \"get\" the data and then cache the block\n            for (int i = 0; i < blockSize; i++) {\n              buffer[i] = getByte(block * blockSize + i);\n            }\n            boolean cached = blockCache.store(blockCacheKey, 0, buffer, 0, blockSize);\n            if (!cached) {\n              storeFails.incrementAndGet();\n            }\n          }\n\n        }\n\n      };\n    }\n\n\n    for (Thread thread : threads) {\n      thread.start();\n    }\n\n    for (Thread thread : threads) {\n      thread.join();\n    }\n\n    System.out.println(\"# of Elements = \" + blockCache.getSize());\n    System.out.println(\"Cache Hits = \" + hitsInCache.get());\n    System.out.println(\"Cache Misses = \" + missesInCache.get());\n    System.out.println(\"Cache Store Fails = \" + storeFails.get());\n    System.out.println(\"Blocks with Errors = \" + validateFails.get());\n\n    assertFalse( failed.get() );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"92304613d4e239eb290f8879c862a0656cc41638":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8702182bc92026984124976ef1fa3393bdcbbc05":["46386805f467fa40cb9d5a3cab791713306548c2"],"87f0484c38f986062889ed50f3bf3bd462848c26":["750b9dd5a93ea77a4c6e90577efc464ec3353b9c"],"46386805f467fa40cb9d5a3cab791713306548c2":["92304613d4e239eb290f8879c862a0656cc41638"],"750b9dd5a93ea77a4c6e90577efc464ec3353b9c":["8702182bc92026984124976ef1fa3393bdcbbc05"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["87f0484c38f986062889ed50f3bf3bd462848c26"],"b0b597c65628ca9e73913a07e81691f8229bae35":["750b9dd5a93ea77a4c6e90577efc464ec3353b9c","87f0484c38f986062889ed50f3bf3bd462848c26"]},"commit2Childs":{"92304613d4e239eb290f8879c862a0656cc41638":["46386805f467fa40cb9d5a3cab791713306548c2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["92304613d4e239eb290f8879c862a0656cc41638"],"8702182bc92026984124976ef1fa3393bdcbbc05":["750b9dd5a93ea77a4c6e90577efc464ec3353b9c"],"87f0484c38f986062889ed50f3bf3bd462848c26":["cd5edd1f2b162a5cfa08efd17851a07373a96817","b0b597c65628ca9e73913a07e81691f8229bae35"],"46386805f467fa40cb9d5a3cab791713306548c2":["8702182bc92026984124976ef1fa3393bdcbbc05"],"750b9dd5a93ea77a4c6e90577efc464ec3353b9c":["87f0484c38f986062889ed50f3bf3bd462848c26","b0b597c65628ca9e73913a07e81691f8229bae35"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"b0b597c65628ca9e73913a07e81691f8229bae35":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","b0b597c65628ca9e73913a07e81691f8229bae35"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}