{"path":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","commits":[{"id":"581320e68d1383a2350b6a7e52adb01c63ab8407","date":1303577715,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"/dev/null","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24f1664166601a0f7376d051dda5dd63c068c313","date":1303641250,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"/dev/null","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"/dev/null","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","date":1305207152,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"/dev/null","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"46a47f84cd37ce2aeaade285844ee750552c2d37","date":1309895846,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      List<String> tokens = new ArrayList<String>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        // TODO: we could collect offsets etc here for better checking that reset() really works.\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty())\n        assertAnalyzesToReuse(a, text, tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c","date":1310389132,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n      \n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"69e043c521d4e8db770cc140c63f5ef51f03426a","date":1317187614,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.tokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.reusableTokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.tokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.tokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"581320e68d1383a2350b6a7e52adb01c63ab8407":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"24f1664166601a0f7376d051dda5dd63c068c313":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","581320e68d1383a2350b6a7e52adb01c63ab8407"],"46a47f84cd37ce2aeaade285844ee750552c2d37":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","581320e68d1383a2350b6a7e52adb01c63ab8407"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","46a47f84cd37ce2aeaade285844ee750552c2d37"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a3776dccca01c11e7046323cfad46a3b4a471233","46a47f84cd37ce2aeaade285844ee750552c2d37"],"7b91922b55d15444d554721b352861d028eb8278":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["581320e68d1383a2350b6a7e52adb01c63ab8407"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c":["46a47f84cd37ce2aeaade285844ee750552c2d37"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["44d6f0ab53c1962856b9f48dedb7a2a6cc18905c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7b91922b55d15444d554721b352861d028eb8278"]},"commit2Childs":{"581320e68d1383a2350b6a7e52adb01c63ab8407":["24f1664166601a0f7376d051dda5dd63c068c313","135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"24f1664166601a0f7376d051dda5dd63c068c313":[],"46a47f84cd37ce2aeaade285844ee750552c2d37":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","44d6f0ab53c1962856b9f48dedb7a2a6cc18905c"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"7b91922b55d15444d554721b352861d028eb8278":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["46a47f84cd37ce2aeaade285844ee750552c2d37","d083e83f225b11e5fdd900e83d26ddb385b6955c","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"a3776dccca01c11e7046323cfad46a3b4a471233":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["581320e68d1383a2350b6a7e52adb01c63ab8407","24f1664166601a0f7376d051dda5dd63c068c313","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["7b91922b55d15444d554721b352861d028eb8278"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["24f1664166601a0f7376d051dda5dd63c068c313","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}