{"path":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","commits":[{"id":"7e4c214a1f904dde76f5611b56d4081533055b3b","date":1421938451,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","pathOld":"/dev/null","sourceNew":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    while (true) {\n\n      boolean newField;\n\n      newField = iter.next();\n\n      if (newField) {\n        field = iter.field();\n        if (field == null) {\n          // No more terms:\n          break;\n        }\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator(state.termsEnum);\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      // Get next term to delete\n      BytesRef term = iter.term();\n      assert checkDeleteTerm(term);\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          state.docsEnum = state.termsEnum.docs(state.rld.getLiveDocs(), state.docsEnum, DocsEnum.FLAG_NONE);\n\n          assert state.docsEnum != null;\n\n          while (true) {\n            final int docID = state.docsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","sourceNew":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    while (true) {\n\n      boolean newField;\n\n      newField = iter.next();\n\n      if (newField) {\n        field = iter.field();\n        if (field == null) {\n          // No more terms:\n          break;\n        }\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator(state.termsEnum);\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      // Get next term to delete\n      BytesRef term = iter.term();\n      assert checkDeleteTerm(term);\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          state.postingsEnum = state.termsEnum.postings(state.rld.getLiveDocs(), state.postingsEnum, PostingsEnum.FLAG_NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","sourceOld":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    while (true) {\n\n      boolean newField;\n\n      newField = iter.next();\n\n      if (newField) {\n        field = iter.field();\n        if (field == null) {\n          // No more terms:\n          break;\n        }\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator(state.termsEnum);\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      // Get next term to delete\n      BytesRef term = iter.term();\n      assert checkDeleteTerm(term);\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          state.docsEnum = state.termsEnum.docs(state.rld.getLiveDocs(), state.docsEnum, DocsEnum.FLAG_NONE);\n\n          assert state.docsEnum != null;\n\n          while (true) {\n            final int docID = state.docsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","sourceNew":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    while (true) {\n\n      boolean newField;\n\n      newField = iter.next();\n\n      if (newField) {\n        field = iter.field();\n        if (field == null) {\n          // No more terms:\n          break;\n        }\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator(state.termsEnum);\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      // Get next term to delete\n      BytesRef term = iter.term();\n      assert checkDeleteTerm(term);\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          state.postingsEnum = state.termsEnum.postings(state.rld.getLiveDocs(), state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","sourceOld":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    while (true) {\n\n      boolean newField;\n\n      newField = iter.next();\n\n      if (newField) {\n        field = iter.field();\n        if (field == null) {\n          // No more terms:\n          break;\n        }\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator(state.termsEnum);\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      // Get next term to delete\n      BytesRef term = iter.term();\n      assert checkDeleteTerm(term);\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          state.postingsEnum = state.termsEnum.postings(state.rld.getLiveDocs(), state.postingsEnum, PostingsEnum.FLAG_NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","sourceNew":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    while (true) {\n\n      boolean newField;\n\n      newField = iter.next();\n\n      if (newField) {\n        field = iter.field();\n        if (field == null) {\n          // No more terms:\n          break;\n        }\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      // Get next term to delete\n      BytesRef term = iter.term();\n      assert checkDeleteTerm(term);\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          state.postingsEnum = state.termsEnum.postings(state.rld.getLiveDocs(), state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","sourceOld":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    while (true) {\n\n      boolean newField;\n\n      newField = iter.next();\n\n      if (newField) {\n        field = iter.field();\n        if (field == null) {\n          // No more terms:\n          break;\n        }\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator(state.termsEnum);\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      // Get next term to delete\n      BytesRef term = iter.term();\n      assert checkDeleteTerm(term);\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          state.postingsEnum = state.termsEnum.postings(state.rld.getLiveDocs(), state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"258f227b48a4dbfc180f6ec70f172469d6a2bef8","date":1428687213,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","sourceNew":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    BytesRef term;\n\n    while ((term = iter.next()) != null) {\n\n      if (iter.field() != field) {\n        // field changed\n        field = iter.field();\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      assert checkDeleteTerm(term);\n\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          state.postingsEnum = state.termsEnum.postings(state.rld.getLiveDocs(), state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","sourceOld":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    while (true) {\n\n      boolean newField;\n\n      newField = iter.next();\n\n      if (newField) {\n        field = iter.field();\n        if (field == null) {\n          // No more terms:\n          break;\n        }\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      // Get next term to delete\n      BytesRef term = iter.term();\n      assert checkDeleteTerm(term);\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          state.postingsEnum = state.termsEnum.postings(state.rld.getLiveDocs(), state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","sourceNew":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    BytesRef term;\n\n    while ((term = iter.next()) != null) {\n\n      if (iter.field() != field) {\n        // field changed\n        field = iter.field();\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      assert checkDeleteTerm(term);\n\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          final Bits acceptDocs = state.rld.getLiveDocs();\n          state.postingsEnum = state.termsEnum.postings(state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (acceptDocs != null && acceptDocs.get(docID) == false) {\n              continue;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","sourceOld":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    BytesRef term;\n\n    while ((term = iter.next()) != null) {\n\n      if (iter.field() != field) {\n        // field changed\n        field = iter.field();\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      assert checkDeleteTerm(term);\n\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          state.postingsEnum = state.termsEnum.postings(state.rld.getLiveDocs(), state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":4,"author":"Mike McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","sourceNew":null,"sourceOld":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    BytesRef term;\n\n    while ((term = iter.next()) != null) {\n\n      if (iter.field() != field) {\n        // field changed\n        field = iter.field();\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for (SegmentState state : segStates) {\n          Terms terms = state.reader.terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      assert checkDeleteTerm(term);\n\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          final Bits acceptDocs = state.rld.getLiveDocs();\n          state.postingsEnum = state.termsEnum.postings(state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (acceptDocs != null && acceptDocs.get(docID) == false) {\n              continue;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","date":1497408244,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","sourceNew":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    BytesRef term;\n\n    while ((term = iter.next()) != null) {\n\n      if (iter.field() != field) {\n        // field changed\n        field = iter.field();\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for (SegmentState state : segStates) {\n          Terms terms = state.reader.terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      assert checkDeleteTerm(term);\n\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          final Bits acceptDocs = state.rld.getLiveDocs();\n          state.postingsEnum = state.termsEnum.postings(state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (acceptDocs != null && acceptDocs.get(docID) == false) {\n              continue;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","sourceOld":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    BytesRef term;\n\n    while ((term = iter.next()) != null) {\n\n      if (iter.field() != field) {\n        // field changed\n        field = iter.field();\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      assert checkDeleteTerm(term);\n\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          final Bits acceptDocs = state.rld.getLiveDocs();\n          state.postingsEnum = state.termsEnum.postings(state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (acceptDocs != null && acceptDocs.get(docID) == false) {\n              continue;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","sourceNew":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    BytesRef term;\n\n    while ((term = iter.next()) != null) {\n\n      if (iter.field() != field) {\n        // field changed\n        field = iter.field();\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for (SegmentState state : segStates) {\n          Terms terms = state.reader.terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      assert checkDeleteTerm(term);\n\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          final Bits acceptDocs = state.rld.getLiveDocs();\n          state.postingsEnum = state.termsEnum.postings(state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (acceptDocs != null && acceptDocs.get(docID) == false) {\n              continue;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","sourceOld":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    BytesRef term;\n\n    while ((term = iter.next()) != null) {\n\n      if (iter.field() != field) {\n        // field changed\n        field = iter.field();\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      assert checkDeleteTerm(term);\n\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          final Bits acceptDocs = state.rld.getLiveDocs();\n          state.postingsEnum = state.termsEnum.postings(state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (acceptDocs != null && acceptDocs.get(docID) == false) {\n              continue;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":4,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","sourceNew":null,"sourceOld":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    BytesRef term;\n\n    while ((term = iter.next()) != null) {\n\n      if (iter.field() != field) {\n        // field changed\n        field = iter.field();\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for (SegmentState state : segStates) {\n          Terms terms = state.reader.terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      assert checkDeleteTerm(term);\n\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          final Bits acceptDocs = state.rld.getLiveDocs();\n          state.postingsEnum = state.termsEnum.postings(state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (acceptDocs != null && acceptDocs.get(docID) == false) {\n              continue;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream#applyTermDeletes(CoalescedUpdates,SegmentState[]).mjava","sourceNew":null,"sourceOld":"  /** Merge sorts the deleted terms and all segments to resolve terms to docIDs for deletion. */\n  private synchronized long applyTermDeletes(CoalescedUpdates updates, SegmentState[] segStates) throws IOException {\n\n    long startNS = System.nanoTime();\n\n    int numReaders = segStates.length;\n\n    long delTermVisitedCount = 0;\n    long segTermVisitedCount = 0;\n\n    FieldTermIterator iter = updates.termIterator();\n\n    String field = null;\n    SegmentQueue queue = null;\n\n    BytesRef term;\n\n    while ((term = iter.next()) != null) {\n\n      if (iter.field() != field) {\n        // field changed\n        field = iter.field();\n\n        queue = new SegmentQueue(numReaders);\n\n        long segTermCount = 0;\n        for(int i=0;i<numReaders;i++) {\n          SegmentState state = segStates[i];\n          Terms terms = state.reader.fields().terms(field);\n          if (terms != null) {\n            segTermCount += terms.size();\n            state.termsEnum = terms.iterator();\n            state.term = state.termsEnum.next();\n            if (state.term != null) {\n              queue.add(state);\n            }\n          }\n        }\n\n        assert checkDeleteTerm(null);\n      }\n\n      assert checkDeleteTerm(term);\n\n      delTermVisitedCount++;\n\n      long delGen = iter.delGen();\n\n      while (queue.size() != 0) {\n\n        // Get next term merged across all segments\n        SegmentState state = queue.top();\n        segTermVisitedCount++;\n\n        int cmp = term.compareTo(state.term);\n\n        if (cmp < 0) {\n          break;\n        } else if (cmp == 0) {\n          // fall through\n        } else {\n          TermsEnum.SeekStatus status = state.termsEnum.seekCeil(term);\n          if (status == TermsEnum.SeekStatus.FOUND) {\n            // fallthrough\n          } else {\n            if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              state.term = state.termsEnum.term();\n              queue.updateTop();\n            } else {\n              // No more terms in this segment\n              queue.pop();\n            }\n\n            continue;\n          }\n        }\n\n        assert state.delGen != delGen;\n\n        if (state.delGen < delGen) {\n\n          // we don't need term frequencies for this\n          final Bits acceptDocs = state.rld.getLiveDocs();\n          state.postingsEnum = state.termsEnum.postings(state.postingsEnum, PostingsEnum.NONE);\n\n          assert state.postingsEnum != null;\n\n          while (true) {\n            final int docID = state.postingsEnum.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            if (acceptDocs != null && acceptDocs.get(docID) == false) {\n              continue;\n            }\n            if (!state.any) {\n              state.rld.initWritableLiveDocs();\n              state.any = true;\n            }\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            state.rld.delete(docID);\n          }\n        }\n\n        state.term = state.termsEnum.next();\n        if (state.term == null) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.1f msec for %d segments and %d packets; %d del terms visited; %d seg terms visited\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       numReaders,\n                                       updates.terms.size(),\n                                       delTermVisitedCount, segTermVisitedCount));\n    }\n\n    return delTermVisitedCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"51f5280f31484820499077f41fcdfe92d527d9dc":["7e4c214a1f904dde76f5611b56d4081533055b3b"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["0f4464508ee83288c8c4585b533f9faaa93aa314","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["258f227b48a4dbfc180f6ec70f172469d6a2bef8"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"258f227b48a4dbfc180f6ec70f172469d6a2bef8":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7e4c214a1f904dde76f5611b56d4081533055b3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["0f4464508ee83288c8c4585b533f9faaa93aa314","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["258f227b48a4dbfc180f6ec70f172469d6a2bef8"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"258f227b48a4dbfc180f6ec70f172469d6a2bef8":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7e4c214a1f904dde76f5611b56d4081533055b3b"],"7e4c214a1f904dde76f5611b56d4081533055b3b":["51f5280f31484820499077f41fcdfe92d527d9dc"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}