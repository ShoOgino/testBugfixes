{"path":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","commits":[{"id":"8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf","date":1522951207,"type":1,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelExecutorStream().mjava","sourceNew":"  @Test\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"workQueue1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"mainCorpus1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destination1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n\n    for (int i = 0; i < 500; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == 500);\n    for(int i=0; i<500; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"workQueue1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"mainCorpus1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destination1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n\n    for (int i = 0; i < 500; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == 500);\n    for(int i=0; i<500; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108","date":1533256859,"type":3,"author":"Erick","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","sourceNew":"  @Test\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"workQueue1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"mainCorpus1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destination1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n\n    for (int i = 0; i < 500; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == 500);\n    for(int i=0; i<500; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"workQueue1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"mainCorpus1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destination1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n\n    for (int i = 0; i < 500; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == 500);\n    for(int i=0; i<500; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b73cc5cc718a5ccdc940b7e3ecbf17e6c145d233","date":1543335722,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","sourceNew":"  @Test\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"workQueue1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"mainCorpus1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destination1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n\n    for (int i = 0; i < 500; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\", qt=\\\"/export\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == 500);\n    for(int i=0; i<500; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"workQueue1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"mainCorpus1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destination1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n\n    for (int i = 0; i < 500; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == 500);\n    for(int i=0; i<500; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","sourceNew":"  @Test\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n\n    cluster.waitForActiveCollection(\"workQueue1\", 2, 2);\n    cluster.waitForActiveCollection(\"mainCorpus1\", 2, 2);\n    cluster.waitForActiveCollection(\"destination1\", 2, 2);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n    int cnt = TEST_NIGHTLY ? 500 : 100;\n    for (int i = 0; i < cnt; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\", qt=\\\"/export\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == cnt);\n    for(int i=0; i<cnt; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"workQueue1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"mainCorpus1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"destination1\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n\n    for (int i = 0; i < 500; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\", qt=\\\"/export\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == 500);\n    for(int i=0; i<500; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","bugFix":["5c9480bf8e52feb02af9b7fa13bf50929da7900b","a9645f101eefedb85283f98a5db88d10e2572746"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b5c929d2716fa79d443b93a82adb1da5b578ebd8","date":1550428858,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","sourceNew":"  @Test\n  // commented out on: 17-Feb-2019   @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n\n    cluster.waitForActiveCollection(\"workQueue1\", 2, 2);\n    cluster.waitForActiveCollection(\"mainCorpus1\", 2, 2);\n    cluster.waitForActiveCollection(\"destination1\", 2, 2);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n    int cnt = TEST_NIGHTLY ? 500 : 100;\n    for (int i = 0; i < cnt; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\", qt=\\\"/export\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == cnt);\n    for(int i=0; i<cnt; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n\n    cluster.waitForActiveCollection(\"workQueue1\", 2, 2);\n    cluster.waitForActiveCollection(\"mainCorpus1\", 2, 2);\n    cluster.waitForActiveCollection(\"destination1\", 2, 2);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n    int cnt = TEST_NIGHTLY ? 500 : 100;\n    for (int i = 0; i < cnt; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\", qt=\\\"/export\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == cnt);\n    for(int i=0; i<cnt; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a1cae9aea470e88146567017129e8280d21ca76","date":1563504024,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testParallelExecutorStream().mjava","sourceNew":"  @Test\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n\n    cluster.waitForActiveCollection(\"workQueue1\", 2, 2);\n    cluster.waitForActiveCollection(\"mainCorpus1\", 2, 2);\n    cluster.waitForActiveCollection(\"destination1\", 2, 2);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n    int cnt = TEST_NIGHTLY ? 500 : 100;\n    for (int i = 0; i < cnt; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\", qt=\\\"/export\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == cnt);\n    for(int i=0; i<cnt; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  // commented out on: 17-Feb-2019   @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelExecutorStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"workQueue1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(),DEFAULT_TIMEOUT);\n\n    CollectionAdminRequest.createCollection(\"mainCorpus1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n\n    CollectionAdminRequest.createCollection(\"destination1\", \"conf\", 2, 1).processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT);\n\n    cluster.waitForActiveCollection(\"workQueue1\", 2, 2);\n    cluster.waitForActiveCollection(\"mainCorpus1\", 2, 2);\n    cluster.waitForActiveCollection(\"destination1\", 2, 2);\n\n    UpdateRequest workRequest = new UpdateRequest();\n    UpdateRequest dataRequest = new UpdateRequest();\n\n    int cnt = TEST_NIGHTLY ? 500 : 100;\n    for (int i = 0; i < cnt; i++) {\n      workRequest.add(id, String.valueOf(i), \"expr_s\", \"update(destination1, batchSize=50, search(mainCorpus1, q=id:\"+i+\", rows=1, sort=\\\"id asc\\\", fl=\\\"id, body_t, field_i\\\"))\");\n      dataRequest.add(id, String.valueOf(i), \"body_t\", \"hello world \"+i, \"field_i\", Integer.toString(i));\n    }\n\n    workRequest.commit(cluster.getSolrClient(), \"workQueue1\");\n    dataRequest.commit(cluster.getSolrClient(), \"mainCorpus1\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/destination1\";\n    TupleStream executorStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"workQueue1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"mainCorpus1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"destination1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"executor\", ExecutorStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    String executorExpression = \"parallel(workQueue1, workers=2, sort=\\\"EOF asc\\\", executor(threads=3, queueSize=100, search(workQueue1, q=\\\"*:*\\\", fl=\\\"id, expr_s\\\", rows=1000, partitionKeys=id, sort=\\\"id desc\\\", qt=\\\"/export\\\")))\";\n    executorStream = factory.constructStream(executorExpression);\n\n    StreamContext context = new StreamContext();\n    SolrClientCache clientCache = new SolrClientCache();\n    context.setSolrClientCache(clientCache);\n    executorStream.setStreamContext(context);\n    getTuples(executorStream);\n    //Destination collection should now contain all the records in the main corpus.\n    cluster.getSolrClient().commit(\"destination1\");\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", \"search(destination1, q=\\\"*:*\\\", fl=\\\"id, body_t, field_i\\\", rows=1000, sort=\\\"field_i asc\\\")\");\n    paramsLoc.set(\"qt\", \"/stream\");\n\n    SolrStream solrStream = new SolrStream(url, paramsLoc);\n    List<Tuple> tuples = getTuples(solrStream);\n    assertTrue(tuples.size() == cnt);\n    for(int i=0; i<cnt; i++) {\n      Tuple tuple = tuples.get(i);\n      long ivalue = tuple.getLong(\"field_i\");\n      String body = tuple.getString(\"body_t\");\n      assertTrue(ivalue == i);\n      assertTrue(body.equals(\"hello world \" + i));\n    }\n\n    solrStream.close();\n    clientCache.close();\n    CollectionAdminRequest.deleteCollection(\"workQueue1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"mainCorpus1\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"destination1\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["b73cc5cc718a5ccdc940b7e3ecbf17e6c145d233"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b5c929d2716fa79d443b93a82adb1da5b578ebd8":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"8a1cae9aea470e88146567017129e8280d21ca76":["b5c929d2716fa79d443b93a82adb1da5b578ebd8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8a1cae9aea470e88146567017129e8280d21ca76"],"b73cc5cc718a5ccdc940b7e3ecbf17e6c145d233":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108"],"8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["b73cc5cc718a5ccdc940b7e3ecbf17e6c145d233"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["b5c929d2716fa79d443b93a82adb1da5b578ebd8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf"],"b5c929d2716fa79d443b93a82adb1da5b578ebd8":["8a1cae9aea470e88146567017129e8280d21ca76"],"8a1cae9aea470e88146567017129e8280d21ca76":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108"],"b73cc5cc718a5ccdc940b7e3ecbf17e6c145d233":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}