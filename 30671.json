{"path":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","commits":[{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":0,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["8c146731a64debc22c115bbf11ee1a060aa7ea02"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(null, docs, PostingsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(null, docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(null, docs, PostingsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(null, docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(null, docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(null, docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8c146731a64debc22c115bbf11ee1a060aa7ea02","date":1457616596,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        }, null);\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a076c3c721f685b7559308fdc2cd72d91bba67e5","date":1464168992,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        }, null);\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        }, null);\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e121d43b5a10f2df530f406f935102656e9c4e8","date":1464198131,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        }, null);\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        }, null);\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83870855d82aba6819217abeff5a40779dbb28b4","date":1464291012,"type":5,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        }, null);\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        }, null);\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(LeafReader,CacheKey,boolean).mjava","sourceNew":null,"sourceOld":"    @Override\n    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator();\n        PostingsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        }, null);\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"a076c3c721f685b7559308fdc2cd72d91bba67e5":["8c146731a64debc22c115bbf11ee1a060aa7ea02"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["8c146731a64debc22c115bbf11ee1a060aa7ea02","a076c3c721f685b7559308fdc2cd72d91bba67e5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"83870855d82aba6819217abeff5a40779dbb28b4":["8c146731a64debc22c115bbf11ee1a060aa7ea02","0e121d43b5a10f2df530f406f935102656e9c4e8"],"8c146731a64debc22c115bbf11ee1a060aa7ea02":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"51f5280f31484820499077f41fcdfe92d527d9dc":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["8c146731a64debc22c115bbf11ee1a060aa7ea02","0e121d43b5a10f2df530f406f935102656e9c4e8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0e121d43b5a10f2df530f406f935102656e9c4e8"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["8c146731a64debc22c115bbf11ee1a060aa7ea02"],"a076c3c721f685b7559308fdc2cd72d91bba67e5":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"83870855d82aba6819217abeff5a40779dbb28b4":[],"8c146731a64debc22c115bbf11ee1a060aa7ea02":["a076c3c721f685b7559308fdc2cd72d91bba67e5","0e121d43b5a10f2df530f406f935102656e9c4e8","83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["51f5280f31484820499077f41fcdfe92d527d9dc"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}