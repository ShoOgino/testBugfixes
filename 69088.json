{"path":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","sourceNew":"  /** Set the starting and ending offset.\n    @see #startOffset() and #endOffset()*/\n  public void setOffset(int startOffset, int endOffset) {\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","sourceOld":"  /** Set the starting and ending offset.\n    @see #startOffset() and #endOffset()*/\n  public void setOffset(int startOffset, int endOffset) {\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6795c6bc2f5a6b2a2230cb20ff4744003faf7802","date":1333839972,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","sourceNew":"  /** Set the starting and ending offset.\n    @see #startOffset() and #endOffset()*/\n  public void setOffset(int startOffset, int endOffset) {\n    // TODO: check that these are valid!  IE, each should be\n    // >= 0, and endOffset should be >= startOffset.\n    // Problem is this could \"break\" existing\n    // tokenizers/filters.\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","sourceOld":"  /** Set the starting and ending offset.\n    @see #startOffset() and #endOffset()*/\n  public void setOffset(int startOffset, int endOffset) {\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","bugFix":null,"bugIntro":["cd65a3c65e7917a381c935b0b663d8e783bd9a1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2d87cda278a2b66b08f00f816c73adc6f799851a","date":1333917898,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","sourceNew":"  /** Set the starting and ending offset.\n    @see #startOffset() and #endOffset()*/\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    // TODO: check that these are valid!  IE, each should be\n    // >= 0, and endOffset should be >= startOffset.\n    // Problem is this could \"break\" existing\n    // tokenizers/filters.\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","sourceOld":"  /** Set the starting and ending offset.\n    @see #startOffset() and #endOffset()*/\n  public void setOffset(int startOffset, int endOffset) {\n    // TODO: check that these are valid!  IE, each should be\n    // >= 0, and endOffset should be >= startOffset.\n    // Problem is this could \"break\" existing\n    // tokenizers/filters.\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cd65a3c65e7917a381c935b0b663d8e783bd9a1e","date":1339372221,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","sourceNew":"  /** Set the starting and ending offset.\n    @see #startOffset() and #endOffset()*/\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","sourceOld":"  /** Set the starting and ending offset.\n    @see #startOffset() and #endOffset()*/\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    // TODO: check that these are valid!  IE, each should be\n    // >= 0, and endOffset should be >= startOffset.\n    // Problem is this could \"break\" existing\n    // tokenizers/filters.\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","bugFix":["6795c6bc2f5a6b2a2230cb20ff4744003faf7802"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"27f7a67b528a360bdc01ea05af57e6459fe42ac0","date":1346299172,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","sourceNew":"  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","sourceOld":"  /** Set the starting and ending offset.\n    @see #startOffset() and #endOffset()*/\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","sourceNew":"  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","sourceOld":"  /** Set the starting and ending offset.\n    @see #startOffset() and #endOffset()*/\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7530de27b87b961b51f01bd1299b7004d46e8823","date":1355236261,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","sourceNew":"  @Override\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","sourceOld":"  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","sourceNew":"  @Override\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","sourceOld":"  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"098528909bb70948871fd7ed865fafb87ed73964","date":1484667487,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","sourceNew":"  @Override\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset; got \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","sourceOld":"  @Override\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"302d34f2c66e8d489ee13078305c330cbf67b226","date":1484754357,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/OffsetAttributeImpl#setOffset(int,int).mjava","sourceNew":"  @Override\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset; got \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","sourceOld":"  @Override\n  public void setOffset(int startOffset, int endOffset) {\n\n    // TODO: we could assert that this is set-once, ie,\n    // current values are -1?  Very few token filters should\n    // change offsets once set by the tokenizer... and\n    // tokenizer should call clearAtts before re-using\n    // OffsetAtt\n\n    if (startOffset < 0 || endOffset < startOffset) {\n      throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, \"\n          + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset);\n    }\n\n    this.startOffset = startOffset;\n    this.endOffset = endOffset;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["27f7a67b528a360bdc01ea05af57e6459fe42ac0","7530de27b87b961b51f01bd1299b7004d46e8823"],"27f7a67b528a360bdc01ea05af57e6459fe42ac0":["cd65a3c65e7917a381c935b0b663d8e783bd9a1e"],"cd65a3c65e7917a381c935b0b663d8e783bd9a1e":["2d87cda278a2b66b08f00f816c73adc6f799851a"],"2d87cda278a2b66b08f00f816c73adc6f799851a":["6795c6bc2f5a6b2a2230cb20ff4744003faf7802"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"098528909bb70948871fd7ed865fafb87ed73964":["7530de27b87b961b51f01bd1299b7004d46e8823"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"05a14b2611ead08655a2b2bdc61632eb31316e57":["cd65a3c65e7917a381c935b0b663d8e783bd9a1e","27f7a67b528a360bdc01ea05af57e6459fe42ac0"],"7530de27b87b961b51f01bd1299b7004d46e8823":["27f7a67b528a360bdc01ea05af57e6459fe42ac0"],"6795c6bc2f5a6b2a2230cb20ff4744003faf7802":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["098528909bb70948871fd7ed865fafb87ed73964"],"302d34f2c66e8d489ee13078305c330cbf67b226":["7530de27b87b961b51f01bd1299b7004d46e8823","098528909bb70948871fd7ed865fafb87ed73964"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"27f7a67b528a360bdc01ea05af57e6459fe42ac0":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","05a14b2611ead08655a2b2bdc61632eb31316e57","7530de27b87b961b51f01bd1299b7004d46e8823"],"cd65a3c65e7917a381c935b0b663d8e783bd9a1e":["27f7a67b528a360bdc01ea05af57e6459fe42ac0","05a14b2611ead08655a2b2bdc61632eb31316e57"],"2d87cda278a2b66b08f00f816c73adc6f799851a":["cd65a3c65e7917a381c935b0b663d8e783bd9a1e"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["6795c6bc2f5a6b2a2230cb20ff4744003faf7802"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"098528909bb70948871fd7ed865fafb87ed73964":["cd5edd1f2b162a5cfa08efd17851a07373a96817","302d34f2c66e8d489ee13078305c330cbf67b226"],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"7530de27b87b961b51f01bd1299b7004d46e8823":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","098528909bb70948871fd7ed865fafb87ed73964","302d34f2c66e8d489ee13078305c330cbf67b226"],"6795c6bc2f5a6b2a2230cb20ff4744003faf7802":["2d87cda278a2b66b08f00f816c73adc6f799851a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"302d34f2c66e8d489ee13078305c330cbf67b226":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","05a14b2611ead08655a2b2bdc61632eb31316e57","cd5edd1f2b162a5cfa08efd17851a07373a96817","302d34f2c66e8d489ee13078305c330cbf67b226"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}