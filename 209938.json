{"path":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(LuceneTestCase.TEMP_DIR, dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(LuceneTestCase.TEMP_DIR, dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(LuceneTestCase.TEMP_DIR, dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(LuceneTestCase.TEMP_DIR, dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a3b444707abab6c7f63c331b3f44971c53b0f07","date":1339533739,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/4x\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(LuceneTestCase.TEMP_DIR, dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7b103c0ed2ba7edf422d1ccb5489815dc6acb84","date":1345973500,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/4x\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/4x\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/4x\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/4x\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7ce4b8d6049478dd27c2263487467c9b1c9de295","date":1358222178,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/4x\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4015cd39dff8d4dec562d909f9766debac53aa6","date":1358548736,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/4x\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"088a7ef694fd43d5d9a4d200c4005865f773d1e7","date":1371136274,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setUseCompoundFile(doCFS);\n    mp.setNoCFSRatio(1.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setUseCompoundFile(doCFS);\n      mp.setNoCFSRatio(1.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":["cbc3688252d4a8045d69a164236b2cf87b721f17","cbc3688252d4a8045d69a164236b2cf87b721f17"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a0f5bb79c600763ffe7b8141df59a3169d31e48","date":1396689440,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.shutdown();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.shutdown();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.shutdown();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e2fb55c0777755badd3b46d8140f3d4301febed","date":1398881584,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.shutdown();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.shutdown();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.shutdown();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.shutdown();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.shutdown();\n\n      writer = new IndexWriter(dir,\n        conf.setMergePolicy(doCFS ? NoMergePolicy.COMPOUND_FILES : NoMergePolicy.NO_COMPOUND_FILES)\n      );\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.shutdown();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.shutdown();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.shutdown();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.shutdown();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cc45c615dbb82bf79d5f9550286098367874fbf","date":1409571423,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"402ad3ddc9da7b70da1b167667a60ece6a1381fb","date":1409656478,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(\"/tmp/idx\", dirName);\n    TestUtil.rm(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();\n    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);\n    // TODO: remove randomness\n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setMaxBufferedDocs(10).setMergePolicy(mp);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (fullyMerged) {\n      writer.forceMerge(1);\n    }\n    writer.close();\n\n    if (!fullyMerged) {\n      // open fresh writer so we get no prx file in the added segment\n      mp = new LogByteSizeMergePolicy();\n      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);\n      // TODO: remove randomness\n      conf = new IndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(10).setMergePolicy(mp);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));\n      Term searchTerm = new Term(\"id\", \"7\");\n      writer.deleteDocuments(searchTerm);\n      writer.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2fb55c0777755badd3b46d8140f3d4301febed":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"5eb2511ababf862ea11e10761c70ee560cd84510":["6613659748fe4411a7dcf85266e55db1f95f7315","6613659748fe4411a7dcf85266e55db1f95f7315"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["6613659748fe4411a7dcf85266e55db1f95f7315","5eb2511ababf862ea11e10761c70ee560cd84510"],"c4015cd39dff8d4dec562d909f9766debac53aa6":["e7b103c0ed2ba7edf422d1ccb5489815dc6acb84","7ce4b8d6049478dd27c2263487467c9b1c9de295"],"6613659748fe4411a7dcf85266e55db1f95f7315":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7ce4b8d6049478dd27c2263487467c9b1c9de295":["e7b103c0ed2ba7edf422d1ccb5489815dc6acb84"],"05a14b2611ead08655a2b2bdc61632eb31316e57":["5a3b444707abab6c7f63c331b3f44971c53b0f07","e7b103c0ed2ba7edf422d1ccb5489815dc6acb84"],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["d0ef034a4f10871667ae75181537775ddcf8ade4","4cc45c615dbb82bf79d5f9550286098367874fbf"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["7ce4b8d6049478dd27c2263487467c9b1c9de295"],"e7b103c0ed2ba7edf422d1ccb5489815dc6acb84":["5a3b444707abab6c7f63c331b3f44971c53b0f07"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4cc45c615dbb82bf79d5f9550286098367874fbf":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"5a3b444707abab6c7f63c331b3f44971c53b0f07":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"]},"commit2Childs":{"7e2fb55c0777755badd3b46d8140f3d4301febed":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"5eb2511ababf862ea11e10761c70ee560cd84510":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"c4015cd39dff8d4dec562d909f9766debac53aa6":[],"6613659748fe4411a7dcf85266e55db1f95f7315":["5eb2511ababf862ea11e10761c70ee560cd84510","2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"7ce4b8d6049478dd27c2263487467c9b1c9de295":["c4015cd39dff8d4dec562d909f9766debac53aa6","088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7b103c0ed2ba7edf422d1ccb5489815dc6acb84":["c4015cd39dff8d4dec562d909f9766debac53aa6","7ce4b8d6049478dd27c2263487467c9b1c9de295","05a14b2611ead08655a2b2bdc61632eb31316e57"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["6613659748fe4411a7dcf85266e55db1f95f7315"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4cc45c615dbb82bf79d5f9550286098367874fbf":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"5a3b444707abab6c7f63c331b3f44971c53b0f07":["05a14b2611ead08655a2b2bdc61632eb31316e57","e7b103c0ed2ba7edf422d1ccb5489815dc6acb84"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["402ad3ddc9da7b70da1b167667a60ece6a1381fb","4cc45c615dbb82bf79d5f9550286098367874fbf"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["5a3b444707abab6c7f63c331b3f44971c53b0f07"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c4015cd39dff8d4dec562d909f9766debac53aa6","05a14b2611ead08655a2b2bdc61632eb31316e57","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}