{"path":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter.MergePartitionsTask#call().mjava","commits":[{"id":"7dcb0432bcb41451b41e9aaaabe99f5d208258fe","date":1493203108,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter.MergePartitionsTask#call().mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    public Partition call() throws IOException, InterruptedException, ExecutionException {\n      long totalCount = 0;\n      for (Future<Partition> segment : segmentsToMerge) {\n        totalCount += segment.get().count;\n      }\n\n      PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n          @Override\n          protected boolean lessThan(FileAndTop a, FileAndTop b) {\n            return comparator.compare(a.current, b.current) < 0;\n          }\n        };\n\n      ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n      String newSegmentName = null;\n\n      long startMS = System.currentTimeMillis();\n      try (ByteSequencesWriter writer = getWriter(dir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT), totalCount)) {\n\n        newSegmentName = writer.out.getName();\n      \n        // Open streams and read the top for each file\n        for (int i = 0; i < segmentsToMerge.size(); i++) {\n          Partition segment = segmentsToMerge.get(i).get();\n          streams[i] = getReader(dir.openChecksumInput(segment.fileName, IOContext.READONCE), segment.fileName);\n              \n          BytesRef item = null;\n          try {\n            item = streams[i].next();\n          } catch (Throwable t) {\n            verifyChecksum(t, streams[i]);\n          }\n          assert item != null;\n          queue.insertWithOverflow(new FileAndTop(i, item));\n        }\n  \n        // Unix utility sort() uses ordered array of files to pick the next line from, updating\n        // it as it reads new lines. The PQ used here is a more elegant solution and has \n        // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n        // so it shouldn't make much of a difference (didn't check).\n        FileAndTop top;\n        while ((top = queue.top()) != null) {\n          writer.write(top.current);\n          try {\n            top.current = streams[top.fd].next();\n          } catch (Throwable t) {\n            verifyChecksum(t, streams[top.fd]);\n          }\n\n          if (top.current != null) {\n            queue.updateTop();\n          } else {\n            queue.pop();\n          }\n        }\n\n        CodecUtil.writeFooter(writer.out);\n\n        for(ByteSequencesReader reader : streams) {\n          CodecUtil.checkFooter(reader.in);\n        }\n\n        sortInfo.mergeTimeMS.addAndGet(System.currentTimeMillis() - startMS);\n      } finally {\n        IOUtils.close(streams);\n      }\n      List<String> toDelete = new ArrayList<>();\n      for (Future<Partition> segment : segmentsToMerge) {\n        toDelete.add(segment.get().fileName);\n      }\n      IOUtils.deleteFiles(dir, toDelete);\n\n      return new Partition(newSegmentName, totalCount);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"36a23c6fa37610e466602f47f4ddf1e7a8e702e5","date":1493213774,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter.MergePartitionsTask#call().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter.MergePartitionsTask#call().mjava","sourceNew":"    @Override\n    public Partition call() throws IOException {\n      long totalCount = 0;\n      for (Future<Partition> segment : segmentsToMerge) {\n        totalCount += getPartition(segment).count;\n      }\n\n      PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n          @Override\n          protected boolean lessThan(FileAndTop a, FileAndTop b) {\n            return comparator.compare(a.current, b.current) < 0;\n          }\n        };\n\n      ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n      String newSegmentName = null;\n\n      long startMS = System.currentTimeMillis();\n      try (ByteSequencesWriter writer = getWriter(dir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT), totalCount)) {\n\n        newSegmentName = writer.out.getName();\n      \n        // Open streams and read the top for each file\n        for (int i = 0; i < segmentsToMerge.size(); i++) {\n          Partition segment = getPartition(segmentsToMerge.get(i));\n          streams[i] = getReader(dir.openChecksumInput(segment.fileName, IOContext.READONCE), segment.fileName);\n              \n          BytesRef item = null;\n          try {\n            item = streams[i].next();\n          } catch (Throwable t) {\n            verifyChecksum(t, streams[i]);\n          }\n          assert item != null;\n          queue.insertWithOverflow(new FileAndTop(i, item));\n        }\n  \n        // Unix utility sort() uses ordered array of files to pick the next line from, updating\n        // it as it reads new lines. The PQ used here is a more elegant solution and has \n        // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n        // so it shouldn't make much of a difference (didn't check).\n        FileAndTop top;\n        while ((top = queue.top()) != null) {\n          writer.write(top.current);\n          try {\n            top.current = streams[top.fd].next();\n          } catch (Throwable t) {\n            verifyChecksum(t, streams[top.fd]);\n          }\n\n          if (top.current != null) {\n            queue.updateTop();\n          } else {\n            queue.pop();\n          }\n        }\n\n        CodecUtil.writeFooter(writer.out);\n\n        for(ByteSequencesReader reader : streams) {\n          CodecUtil.checkFooter(reader.in);\n        }\n\n        sortInfo.mergeTimeMS.addAndGet(System.currentTimeMillis() - startMS);\n      } finally {\n        IOUtils.close(streams);\n      }\n      List<String> toDelete = new ArrayList<>();\n      for (Future<Partition> segment : segmentsToMerge) {\n        toDelete.add(getPartition(segment).fileName);\n      }\n      IOUtils.deleteFiles(dir, toDelete);\n\n      return new Partition(newSegmentName, totalCount);\n    }\n\n","sourceOld":"    @Override\n    public Partition call() throws IOException, InterruptedException, ExecutionException {\n      long totalCount = 0;\n      for (Future<Partition> segment : segmentsToMerge) {\n        totalCount += segment.get().count;\n      }\n\n      PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n          @Override\n          protected boolean lessThan(FileAndTop a, FileAndTop b) {\n            return comparator.compare(a.current, b.current) < 0;\n          }\n        };\n\n      ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n      String newSegmentName = null;\n\n      long startMS = System.currentTimeMillis();\n      try (ByteSequencesWriter writer = getWriter(dir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT), totalCount)) {\n\n        newSegmentName = writer.out.getName();\n      \n        // Open streams and read the top for each file\n        for (int i = 0; i < segmentsToMerge.size(); i++) {\n          Partition segment = segmentsToMerge.get(i).get();\n          streams[i] = getReader(dir.openChecksumInput(segment.fileName, IOContext.READONCE), segment.fileName);\n              \n          BytesRef item = null;\n          try {\n            item = streams[i].next();\n          } catch (Throwable t) {\n            verifyChecksum(t, streams[i]);\n          }\n          assert item != null;\n          queue.insertWithOverflow(new FileAndTop(i, item));\n        }\n  \n        // Unix utility sort() uses ordered array of files to pick the next line from, updating\n        // it as it reads new lines. The PQ used here is a more elegant solution and has \n        // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n        // so it shouldn't make much of a difference (didn't check).\n        FileAndTop top;\n        while ((top = queue.top()) != null) {\n          writer.write(top.current);\n          try {\n            top.current = streams[top.fd].next();\n          } catch (Throwable t) {\n            verifyChecksum(t, streams[top.fd]);\n          }\n\n          if (top.current != null) {\n            queue.updateTop();\n          } else {\n            queue.pop();\n          }\n        }\n\n        CodecUtil.writeFooter(writer.out);\n\n        for(ByteSequencesReader reader : streams) {\n          CodecUtil.checkFooter(reader.in);\n        }\n\n        sortInfo.mergeTimeMS.addAndGet(System.currentTimeMillis() - startMS);\n      } finally {\n        IOUtils.close(streams);\n      }\n      List<String> toDelete = new ArrayList<>();\n      for (Future<Partition> segment : segmentsToMerge) {\n        toDelete.add(segment.get().fileName);\n      }\n      IOUtils.deleteFiles(dir, toDelete);\n\n      return new Partition(newSegmentName, totalCount);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter.MergePartitionsTask#call().mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    public Partition call() throws IOException {\n      long totalCount = 0;\n      for (Future<Partition> segment : segmentsToMerge) {\n        totalCount += getPartition(segment).count;\n      }\n\n      PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n          @Override\n          protected boolean lessThan(FileAndTop a, FileAndTop b) {\n            return comparator.compare(a.current, b.current) < 0;\n          }\n        };\n\n      ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n      String newSegmentName = null;\n\n      long startMS = System.currentTimeMillis();\n      try (ByteSequencesWriter writer = getWriter(dir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT), totalCount)) {\n\n        newSegmentName = writer.out.getName();\n      \n        // Open streams and read the top for each file\n        for (int i = 0; i < segmentsToMerge.size(); i++) {\n          Partition segment = getPartition(segmentsToMerge.get(i));\n          streams[i] = getReader(dir.openChecksumInput(segment.fileName, IOContext.READONCE), segment.fileName);\n              \n          BytesRef item = null;\n          try {\n            item = streams[i].next();\n          } catch (Throwable t) {\n            verifyChecksum(t, streams[i]);\n          }\n          assert item != null;\n          queue.insertWithOverflow(new FileAndTop(i, item));\n        }\n  \n        // Unix utility sort() uses ordered array of files to pick the next line from, updating\n        // it as it reads new lines. The PQ used here is a more elegant solution and has \n        // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n        // so it shouldn't make much of a difference (didn't check).\n        FileAndTop top;\n        while ((top = queue.top()) != null) {\n          writer.write(top.current);\n          try {\n            top.current = streams[top.fd].next();\n          } catch (Throwable t) {\n            verifyChecksum(t, streams[top.fd]);\n          }\n\n          if (top.current != null) {\n            queue.updateTop();\n          } else {\n            queue.pop();\n          }\n        }\n\n        CodecUtil.writeFooter(writer.out);\n\n        for(ByteSequencesReader reader : streams) {\n          CodecUtil.checkFooter(reader.in);\n        }\n\n        sortInfo.mergeTimeMS.addAndGet(System.currentTimeMillis() - startMS);\n      } finally {\n        IOUtils.close(streams);\n      }\n      List<String> toDelete = new ArrayList<>();\n      for (Future<Partition> segment : segmentsToMerge) {\n        toDelete.add(getPartition(segment).fileName);\n      }\n      IOUtils.deleteFiles(dir, toDelete);\n\n      return new Partition(newSegmentName, totalCount);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e9017cf144952056066919f1ebc7897ff9bd71b1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","36a23c6fa37610e466602f47f4ddf1e7a8e702e5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"36a23c6fa37610e466602f47f4ddf1e7a8e702e5":["7dcb0432bcb41451b41e9aaaabe99f5d208258fe"],"7dcb0432bcb41451b41e9aaaabe99f5d208258fe":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["36a23c6fa37610e466602f47f4ddf1e7a8e702e5"]},"commit2Childs":{"e9017cf144952056066919f1ebc7897ff9bd71b1":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e9017cf144952056066919f1ebc7897ff9bd71b1","7dcb0432bcb41451b41e9aaaabe99f5d208258fe"],"36a23c6fa37610e466602f47f4ddf1e7a8e702e5":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7dcb0432bcb41451b41e9aaaabe99f5d208258fe":["36a23c6fa37610e466602f47f4ddf1e7a8e702e5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e9017cf144952056066919f1ebc7897ff9bd71b1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}