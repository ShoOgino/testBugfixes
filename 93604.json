{"path":"lucene/backwards/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(FreqProxTermsWriterPerField[],FormatPostingsFieldsConsumer).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(FreqProxTermsWriterPerField[],FormatPostingsFieldsConsumer).mjava","pathOld":"backwards/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(FreqProxTermsWriterPerField[],FormatPostingsFieldsConsumer).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(FreqProxTermsWriterPerField[] fields,\n                      FormatPostingsFieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i]);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final FormatPostingsTermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      for(int i=1;i<numFields;i++) {\n        final char[] text = mergeStates[i].text;\n        final int textOffset = mergeStates[i].textOffset;\n        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);\n\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0)\n          termStates[numToMerge++] = mergeStates[i];\n      }\n\n      final FormatPostingsDocsConsumer docConsumer = termsConsumer.addTerm(termStates[0].text, termStates[0].textOffset);\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++)\n          if (termStates[i].docID < minState.docID)\n            minState = termStates[i];\n\n        final int termDocFreq = minState.termFreq;\n\n        final FormatPostingsPositionsConsumer posConsumer = docConsumer.addDoc(minState.docID, termDocFreq);\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength)\n                payloadBuffer = new byte[payloadLength];\n\n              prox.readBytes(payloadBuffer, 0, payloadLength);\n\n            } else\n              payloadLength = 0;\n\n            posConsumer.addPosition(position, payloadBuffer, 0, payloadLength);\n          } //End for\n\n          posConsumer.finish();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          for(int i=0;i<numToMerge;i++)\n            if (termStates[i] != minState)\n              termStates[upto++] = termStates[i];\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      docConsumer.finish();\n    }\n\n    termsConsumer.finish();\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(FreqProxTermsWriterPerField[] fields,\n                      FormatPostingsFieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i]);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final FormatPostingsTermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      for(int i=1;i<numFields;i++) {\n        final char[] text = mergeStates[i].text;\n        final int textOffset = mergeStates[i].textOffset;\n        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);\n\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0)\n          termStates[numToMerge++] = mergeStates[i];\n      }\n\n      final FormatPostingsDocsConsumer docConsumer = termsConsumer.addTerm(termStates[0].text, termStates[0].textOffset);\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++)\n          if (termStates[i].docID < minState.docID)\n            minState = termStates[i];\n\n        final int termDocFreq = minState.termFreq;\n\n        final FormatPostingsPositionsConsumer posConsumer = docConsumer.addDoc(minState.docID, termDocFreq);\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength)\n                payloadBuffer = new byte[payloadLength];\n\n              prox.readBytes(payloadBuffer, 0, payloadLength);\n\n            } else\n              payloadLength = 0;\n\n            posConsumer.addPosition(position, payloadBuffer, 0, payloadLength);\n          } //End for\n\n          posConsumer.finish();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          for(int i=0;i<numToMerge;i++)\n            if (termStates[i] != minState)\n              termStates[upto++] = termStates[i];\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      docConsumer.finish();\n    }\n\n    termsConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(FreqProxTermsWriterPerField[],FormatPostingsFieldsConsumer).mjava","sourceNew":null,"sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(FreqProxTermsWriterPerField[] fields,\n                      FormatPostingsFieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i]);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final FormatPostingsTermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      for(int i=1;i<numFields;i++) {\n        final char[] text = mergeStates[i].text;\n        final int textOffset = mergeStates[i].textOffset;\n        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);\n\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0)\n          termStates[numToMerge++] = mergeStates[i];\n      }\n\n      final FormatPostingsDocsConsumer docConsumer = termsConsumer.addTerm(termStates[0].text, termStates[0].textOffset);\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++)\n          if (termStates[i].docID < minState.docID)\n            minState = termStates[i];\n\n        final int termDocFreq = minState.termFreq;\n\n        final FormatPostingsPositionsConsumer posConsumer = docConsumer.addDoc(minState.docID, termDocFreq);\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength)\n                payloadBuffer = new byte[payloadLength];\n\n              prox.readBytes(payloadBuffer, 0, payloadLength);\n\n            } else\n              payloadLength = 0;\n\n            posConsumer.addPosition(position, payloadBuffer, 0, payloadLength);\n          } //End for\n\n          posConsumer.finish();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          for(int i=0;i<numToMerge;i++)\n            if (termStates[i] != minState)\n              termStates[upto++] = termStates[i];\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      docConsumer.finish();\n    }\n\n    termsConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}