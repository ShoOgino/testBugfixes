{"path":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","commits":[{"id":"4e2d06d853b2be7aee1c9d69a6b36d26410459a9","date":1460361562,"type":1,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       names of the field used as the label for classification\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // try to get the exact no. of existing classes\n    Terms terms = originalIndex.terms(classFieldName);\n    long noOfClasses = -1;\n    if (terms != null) {\n      noOfClasses = terms.size();\n\n    }\n    if (noOfClasses == -1) {\n      noOfClasses = 10000; // fallback\n    }\n\n    HashMap<String, UninvertingReader.Type> mapping = new HashMap<>();\n    mapping.put(classFieldName, UninvertingReader.Type.SORTED);\n    UninvertingReader uninvertingReader = new UninvertingReader(originalIndex, mapping);\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(uninvertingReader);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, (int) noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      uninvertingReader.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    try {\n      int size = originalIndex.maxDoc();\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(true);\n      ft.setStoreTermVectorPositions(true);\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (ScoreDoc scoreDoc : topDocs.scoreDocs) {\n\n        // create a new document for indexing\n        Document doc = new Document();\n        Document document = originalIndex.document(scoreDoc.doc);\n        if (fieldNames != null && fieldNames.length > 0) {\n          for (String fieldName : fieldNames) {\n            IndexableField field = document.getField(fieldName);\n            if (field != null) {\n              doc.add(new Field(fieldName, field.stringValue(), ft));\n            }\n          }\n        } else {\n          for (IndexableField field : document.getFields()) {\n            if (field.readerValue() != null) {\n              doc.add(new Field(field.name(), field.readerValue(), ft));\n            } else if (field.binaryValue() != null) {\n              doc.add(new Field(field.name(), field.binaryValue(), ft));\n            } else if (field.stringValue() != null) {\n              doc.add(new Field(field.name(), field.stringValue(), ft));\n            } else if (field.numericValue() != null) {\n              doc.add(new Field(field.name(), field.numericValue().toString(), ft));\n            }\n          }\n        }\n\n        // add it to one of the IDXs\n        if (b % 2 == 0 && testWriter.maxDoc() < size * testRatio) {\n          testWriter.addDocument(doc);\n        } else if (cvWriter.maxDoc() < size * crossValidationRatio) {\n          cvWriter.addDocument(doc);\n        } else {\n          trainingWriter.addDocument(doc);\n        }\n        b++;\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8","date":1462567286,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    SortedDocValues classValues = originalIndex.getSortedDocValues(classFieldName);\n    if (classValues == null) {\n      throw new IllegalStateException(\"the classFieldName \\\"\" + classFieldName + \"\\\" must index sorted doc values\");\n    }\n\n    int noOfClasses = classValues.getValueCount();\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       names of the field used as the label for classification\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // try to get the exact no. of existing classes\n    Terms terms = originalIndex.terms(classFieldName);\n    long noOfClasses = -1;\n    if (terms != null) {\n      noOfClasses = terms.size();\n\n    }\n    if (noOfClasses == -1) {\n      noOfClasses = 10000; // fallback\n    }\n\n    HashMap<String, UninvertingReader.Type> mapping = new HashMap<>();\n    mapping.put(classFieldName, UninvertingReader.Type.SORTED);\n    UninvertingReader uninvertingReader = new UninvertingReader(originalIndex, mapping);\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(uninvertingReader);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, (int) noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      uninvertingReader.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    SortedDocValues classValues = originalIndex.getSortedDocValues(classFieldName);\n    if (classValues == null) {\n      throw new IllegalStateException(\"the classFieldName \\\"\" + classFieldName + \"\\\" must index sorted doc values\");\n    }\n\n    int noOfClasses = classValues.getValueCount();\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       names of the field used as the label for classification\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // try to get the exact no. of existing classes\n    Terms terms = originalIndex.terms(classFieldName);\n    long noOfClasses = -1;\n    if (terms != null) {\n      noOfClasses = terms.size();\n\n    }\n    if (noOfClasses == -1) {\n      noOfClasses = 10000; // fallback\n    }\n\n    HashMap<String, UninvertingReader.Type> mapping = new HashMap<>();\n    mapping.put(classFieldName, UninvertingReader.Type.SORTED);\n    UninvertingReader uninvertingReader = new UninvertingReader(originalIndex, mapping);\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(uninvertingReader);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, (int) noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      uninvertingReader.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    SortedDocValues classValues = originalIndex.getSortedDocValues(classFieldName);\n    if (classValues == null) {\n      throw new IllegalStateException(\"the classFieldName \\\"\" + classFieldName + \"\\\" must index sorted doc values\");\n    }\n\n    int noOfClasses = classValues.getValueCount();\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       names of the field used as the label for classification\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // try to get the exact no. of existing classes\n    Terms terms = originalIndex.terms(classFieldName);\n    long noOfClasses = -1;\n    if (terms != null) {\n      noOfClasses = terms.size();\n\n    }\n    if (noOfClasses == -1) {\n      noOfClasses = 10000; // fallback\n    }\n\n    HashMap<String, UninvertingReader.Type> mapping = new HashMap<>();\n    mapping.put(classFieldName, UninvertingReader.Type.SORTED);\n    UninvertingReader uninvertingReader = new UninvertingReader(originalIndex, mapping);\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(uninvertingReader);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, (int) noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      uninvertingReader.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    SortedDocValues classValues = originalIndex.getSortedDocValues(classFieldName);\n    if (classValues == null) {\n      throw new IllegalStateException(\"the classFieldName \\\"\" + classFieldName + \"\\\" must index sorted doc values\");\n    }\n\n    int noOfClasses = classValues.getValueCount();\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       names of the field used as the label for classification\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // try to get the exact no. of existing classes\n    Terms terms = originalIndex.terms(classFieldName);\n    long noOfClasses = -1;\n    if (terms != null) {\n      noOfClasses = terms.size();\n\n    }\n    if (noOfClasses == -1) {\n      noOfClasses = 10000; // fallback\n    }\n\n    HashMap<String, UninvertingReader.Type> mapping = new HashMap<>();\n    mapping.put(classFieldName, UninvertingReader.Type.SORTED);\n    UninvertingReader uninvertingReader = new UninvertingReader(originalIndex, mapping);\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(uninvertingReader);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, (int) noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      uninvertingReader.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"444d4b906d0e3398f87d6a5c4967c508f11a7f0b","date":1466507434,"type":5,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(IndexReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(IndexReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    int noOfClasses = 0;\n    for (LeafReaderContext leave : originalIndex.leaves()) {\n      SortedDocValues classValues = leave.reader().getSortedDocValues(classFieldName);\n      if (classValues == null) {\n        throw new IllegalStateException(\"the classFieldName \\\"\" + classFieldName + \"\\\" must index sorted doc values\");\n      }\n      noOfClasses += classValues.getValueCount();\n    }\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       name of the field used as the label for classification; this must be indexed with sorted doc values\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // get the exact no. of existing classes\n    SortedDocValues classValues = originalIndex.getSortedDocValues(classFieldName);\n    if (classValues == null) {\n      throw new IllegalStateException(\"the classFieldName \\\"\" + classFieldName + \"\\\" must index sorted doc values\");\n    }\n\n    int noOfClasses = classValues.getValueCount();\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(originalIndex);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      originalIndex.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter#split(LeafReader,Directory,Directory,Directory,Analyzer,boolean,String,String...).mjava","sourceNew":null,"sourceOld":"  /**\n   * Split a given index into 3 indexes for training, test and cross validation tasks respectively\n   *\n   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index\n   * @param trainingIndex        a {@link Directory} used to write the training index\n   * @param testIndex            a {@link Directory} used to write the test index\n   * @param crossValidationIndex a {@link Directory} used to write the cross validation index\n   * @param analyzer             {@link Analyzer} used to create the new docs\n   * @param termVectors          {@code true} if term vectors should be kept\n   * @param classFieldName       names of the field used as the label for classification\n   * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used\n   * @throws IOException if any writing operation fails on any of the indexes\n   */\n  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,\n                    Analyzer analyzer, boolean termVectors, String classFieldName, String... fieldNames) throws IOException {\n\n    // create IWs for train / test / cv IDXs\n    IndexWriter testWriter = new IndexWriter(testIndex, new IndexWriterConfig(analyzer));\n    IndexWriter cvWriter = new IndexWriter(crossValidationIndex, new IndexWriterConfig(analyzer));\n    IndexWriter trainingWriter = new IndexWriter(trainingIndex, new IndexWriterConfig(analyzer));\n\n    // try to get the exact no. of existing classes\n    Terms terms = originalIndex.terms(classFieldName);\n    long noOfClasses = -1;\n    if (terms != null) {\n      noOfClasses = terms.size();\n\n    }\n    if (noOfClasses == -1) {\n      noOfClasses = 10000; // fallback\n    }\n\n    HashMap<String, UninvertingReader.Type> mapping = new HashMap<>();\n    mapping.put(classFieldName, UninvertingReader.Type.SORTED);\n    UninvertingReader uninvertingReader = new UninvertingReader(originalIndex, mapping);\n\n    try {\n\n      IndexSearcher indexSearcher = new IndexSearcher(uninvertingReader);\n      GroupingSearch gs = new GroupingSearch(classFieldName);\n      gs.setGroupSort(Sort.INDEXORDER);\n      gs.setSortWithinGroup(Sort.INDEXORDER);\n      gs.setAllGroups(true);\n      gs.setGroupDocsLimit(originalIndex.maxDoc());\n      TopGroups<Object> topGroups = gs.search(indexSearcher, new MatchAllDocsQuery(), 0, (int) noOfClasses);\n\n      // set the type to be indexed, stored, with term vectors\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      if (termVectors) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      }\n\n      int b = 0;\n\n      // iterate over existing documents\n      for (GroupDocs group : topGroups.groups) {\n        int totalHits = group.totalHits;\n        double testSize = totalHits * testRatio;\n        int tc = 0;\n        double cvSize = totalHits * crossValidationRatio;\n        int cvc = 0;\n        for (ScoreDoc scoreDoc : group.scoreDocs) {\n\n          // create a new document for indexing\n          Document doc = createNewDoc(originalIndex, ft, scoreDoc, fieldNames);\n\n          // add it to one of the IDXs\n          if (b % 2 == 0 && tc < testSize) {\n            testWriter.addDocument(doc);\n            tc++;\n          } else if (cvc < cvSize) {\n            cvWriter.addDocument(doc);\n            cvc++;\n          } else {\n            trainingWriter.addDocument(doc);\n          }\n          b++;\n        }\n      }\n      // commit\n      testWriter.commit();\n      cvWriter.commit();\n      trainingWriter.commit();\n\n      // merge\n      testWriter.forceMerge(3);\n      cvWriter.forceMerge(3);\n      trainingWriter.forceMerge(3);\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      // close IWs\n      testWriter.close();\n      cvWriter.close();\n      trainingWriter.close();\n      uninvertingReader.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"444d4b906d0e3398f87d6a5c4967c508f11a7f0b":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0ad30c6a479e764150a3316e57263319775f1df2":["4e2d06d853b2be7aee1c9d69a6b36d26410459a9","3d33e731a93d4b57e662ff094f64f94a745422d4"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["4e2d06d853b2be7aee1c9d69a6b36d26410459a9","fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8"],"4e2d06d853b2be7aee1c9d69a6b36d26410459a9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8":["4e2d06d853b2be7aee1c9d69a6b36d26410459a9"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["4e2d06d853b2be7aee1c9d69a6b36d26410459a9","444d4b906d0e3398f87d6a5c4967c508f11a7f0b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["4e2d06d853b2be7aee1c9d69a6b36d26410459a9","0ad30c6a479e764150a3316e57263319775f1df2"]},"commit2Childs":{"444d4b906d0e3398f87d6a5c4967c508f11a7f0b":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4e2d06d853b2be7aee1c9d69a6b36d26410459a9"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"4e2d06d853b2be7aee1c9d69a6b36d26410459a9":["0ad30c6a479e764150a3316e57263319775f1df2","3d33e731a93d4b57e662ff094f64f94a745422d4","fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89"],"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d470c8182e92b264680e34081b75e70a9f2b3c89":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}