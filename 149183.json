{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9","date":1270985469,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10","date":1270996866,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4","date":1271167458,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5ef87af8c7bd0f8429622b83aa74202383f2e757","date":1280262785,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":["5350389bf83287111f7760b9e3db3af8e3648474"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig(newRandom(), TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    Random random = newRandom();\n    MockRAMDirectory dir = newDirectory(random);\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig(newRandom(), TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a05409176bd65129d67a785ee70e881e238a9aef","date":1282582843,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    Random random = newRandom();\n    MockDirectoryWrapper dir = newDirectory(random);\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    Random random = newRandom();\n    MockRAMDirectory dir = newDirectory(random);\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    Random random = newRandom();\n    MockDirectoryWrapper dir = newDirectory(random);\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"44fcbde6fb2ac44ee3b45e013e54a42911e689ff","date":1292065621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n\n    MockRAMDirectory dir = new MockRAMDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.2));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE)\n      writer.setInfoStream(System.out);\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","date":1305207152,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null)\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff6fd241dc6610f7f81b62e3ba4cedf105939623","date":1307331653,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"79c2cb24929f2649a8875fb629086171f914d5ce","date":1307332717,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c5b026d03cbbb03ca4c0b97d14e9839682281dc","date":1323049298,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir, true);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testRandomExceptionsThreads().mjava","sourceNew":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomExceptionsThreads() throws Throwable {\n    MockDirectoryWrapper dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random);\n    analyzer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n    MockIndexWriter writer  = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n        .setRAMBufferSizeMB(0.2).setMergeScheduler(new ConcurrentMergeScheduler()));\n    ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).setSuppressExceptions();\n    //writer.setMaxBufferedDocs(10);\n    writer.commit();\n\n    final int NUM_THREADS = 4;\n\n    final IndexerThread[] threads = new IndexerThread[NUM_THREADS];\n    for(int i=0;i<NUM_THREADS;i++) {\n      threads[i] = new IndexerThread(i, writer);\n      threads[i].start();\n    }\n\n    for(int i=0;i<NUM_THREADS;i++)\n      threads[i].join();\n\n    for(int i=0;i<NUM_THREADS;i++)\n      if (threads[i].failure != null) {\n        fail(\"thread \" + threads[i].getName() + \": hit unexpected failure\");\n      }\n\n    writer.commit();\n\n    try {\n      writer.close();\n    } catch (Throwable t) {\n      System.out.println(\"exception during close:\");\n      t.printStackTrace(System.out);\n      writer.rollback();\n    }\n\n    // Confirm that when doc hits exception partway through tokenization, it's deleted:\n    IndexReader r2 = IndexReader.open(dir);\n    final int count = r2.docFreq(new Term(\"content4\", \"aaa\"));\n    final int count2 = r2.docFreq(new Term(\"content4\", \"ddd\"));\n    assertEquals(count, count2);\n    r2.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["06584e6e98d592b34e1329b384182f368d2025e8","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["06584e6e98d592b34e1329b384182f368d2025e8"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5ef87af8c7bd0f8429622b83aa74202383f2e757","44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4":["d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a3776dccca01c11e7046323cfad46a3b4a471233","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["a05409176bd65129d67a785ee70e881e238a9aef"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a05409176bd65129d67a785ee70e881e238a9aef":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"5ef87af8c7bd0f8429622b83aa74202383f2e757":["d572389229127c297dd1fa5ce4758e1cec41e799"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["d572389229127c297dd1fa5ce4758e1cec41e799"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"06584e6e98d592b34e1329b384182f368d2025e8":["ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"d572389229127c297dd1fa5ce4758e1cec41e799":["69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["f2c5f0cb44df114db4228c8f77861714b5cabaea","962d04139994fce5193143ef35615499a9a96d78"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["1f653cfcf159baeaafe5d01682a911e95bba4012","44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"962d04139994fce5193143ef35615499a9a96d78":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10":["b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["06584e6e98d592b34e1329b384182f368d2025e8","1c5b026d03cbbb03ca4c0b97d14e9839682281dc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f2c5f0cb44df114db4228c8f77861714b5cabaea","ab5cb6a74aefb78aa0569857970b9151dfe2e787","a3776dccca01c11e7046323cfad46a3b4a471233"],"b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9":["d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["962d04139994fce5193143ef35615499a9a96d78"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","962d04139994fce5193143ef35615499a9a96d78"],"69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4":["d572389229127c297dd1fa5ce4758e1cec41e799"],"79c2cb24929f2649a8875fb629086171f914d5ce":[],"1f653cfcf159baeaafe5d01682a911e95bba4012":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a05409176bd65129d67a785ee70e881e238a9aef":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"5ef87af8c7bd0f8429622b83aa74202383f2e757":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9"],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["79c2cb24929f2649a8875fb629086171f914d5ce","06584e6e98d592b34e1329b384182f368d2025e8","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["a05409176bd65129d67a785ee70e881e238a9aef"],"06584e6e98d592b34e1329b384182f368d2025e8":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","1c5b026d03cbbb03ca4c0b97d14e9839682281dc","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"d572389229127c297dd1fa5ce4758e1cec41e799":["5ef87af8c7bd0f8429622b83aa74202383f2e757","b21422ff1d1d56499dec481f193b402e5e8def5b"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"962d04139994fce5193143ef35615499a9a96d78":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10":["69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","ff6fd241dc6610f7f81b62e3ba4cedf105939623","a3776dccca01c11e7046323cfad46a3b4a471233"],"a3776dccca01c11e7046323cfad46a3b4a471233":["79c2cb24929f2649a8875fb629086171f914d5ce"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","79c2cb24929f2649a8875fb629086171f914d5ce","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}