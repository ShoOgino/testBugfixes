{"path":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n        int offset = 0;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              public int doc() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":null,"sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n        int offset = 0;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              public int doc() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n        int offset = 0;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              public int doc() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n        int offset = 0;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              public int doc() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n        int offset = 0;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              public int doc() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"321f2a5084d8bb0f0d5acbb637671346f695879b","date":1274540153,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n        int offset = 0;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n        int offset = 0;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              public int doc() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2dadf0f3286a34a0fee6e788ffce88624bf2984e","date":1294260428,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(ReaderContext context) throws IOException {\n        int offset = 0;\n        IndexReader reader = context.reader;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n        int offset = 0;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"34743077dbbe20cd7a27d5c6c2511e62b99a0e36","date":1294410669,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(ReaderContext contextX) throws IOException {\n        AtomicReaderContext context = (AtomicReaderContext)contextX;  // TODO: remove after lucene migration\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(ReaderContext context) throws IOException {\n        int offset = 0;\n        IndexReader reader = context.reader;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a10b98ef1ef4bf9e38d2e07a9e425a916afa8705","date":1294747166,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext contextX) throws IOException {\n        AtomicReaderContext context = (AtomicReaderContext)contextX;  // TODO: remove after lucene migration\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(ReaderContext contextX) throws IOException {\n        AtomicReaderContext context = (AtomicReaderContext)contextX;  // TODO: remove after lucene migration\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext contextX) throws IOException {\n        AtomicReaderContext context = (AtomicReaderContext)contextX;  // TODO: remove after lucene migration\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n        int offset = 0;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01","date":1296400215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext contextX) throws IOException {\n        AtomicReaderContext context = (AtomicReaderContext)contextX;  // TODO: remove after lucene migration\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext contextX) throws IOException {\n        AtomicReaderContext context = (AtomicReaderContext)contextX;  // TODO: remove after lucene migration\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d335bac0769ee9d21f92daa2576f28926fee3163","date":1296663771,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext contextX) throws IOException {\n        AtomicReaderContext context = (AtomicReaderContext)contextX;  // TODO: remove after lucene migration\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n        int offset = 0;\n        SolrIndexReader r = (SolrIndexReader)reader;\n        while (r.getParent() != null) {\n          offset += r.getBase();\n          r = r.getParent();\n        }\n        final int base = offset;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext contextX) throws IOException {\n        AtomicReaderContext context = (AtomicReaderContext)contextX;  // TODO: remove after lucene migration\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","pathOld":"solr/src/java/org/apache/solr/search/SortedIntDocSet#getTopFilter().mjava","sourceNew":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Filter getTopFilter() {\n    return new Filter() {\n      int lastEndIdx = 0;\n\n      @Override\n      public DocIdSet getDocIdSet(AtomicReaderContext context) throws IOException {\n        IndexReader reader = context.reader;\n\n        final int base = context.docBase;\n        final int maxDoc = reader.maxDoc();\n        final int max = base + maxDoc;   // one past the max doc in this segment.\n        int sidx = Math.max(0,lastEndIdx);\n\n        if (sidx > 0 && docs[sidx-1] >= base) {\n          // oops, the lastEndIdx isn't correct... we must have been used\n          // in a multi-threaded context, or the indexreaders are being\n          // used out-of-order.  start at 0.\n          sidx = 0;\n        }\n        if (sidx < docs.length && docs[sidx] < base) {\n          // if docs[sidx] is < base, we need to seek to find the real start.\n          sidx = findIndex(docs, base, sidx, docs.length-1);\n        }\n\n        final int startIdx = sidx;\n\n        // Largest possible end index is limited to the start index\n        // plus the number of docs contained in the segment.  Subtract 1 since\n        // the end index is inclusive.\n        int eidx = Math.min(docs.length, startIdx + maxDoc) - 1;\n\n        // find the real end\n        eidx = findIndex(docs, max, startIdx, eidx) - 1;\n\n        final int endIdx = eidx;\n        lastEndIdx = endIdx;\n\n\n        return new DocIdSet() {\n          @Override\n          public DocIdSetIterator iterator() throws IOException {\n            return new DocIdSetIterator() {\n              int idx = startIdx;\n              int adjustedDoc = -1;\n\n              @Override\n              public int docID() {\n                return adjustedDoc;\n              }\n\n              @Override\n              public int nextDoc() throws IOException {\n                return adjustedDoc = (idx > endIdx) ? NO_MORE_DOCS : (docs[idx++] - base);\n              }\n\n              @Override\n              public int advance(int target) throws IOException {\n                if (idx > endIdx || target==NO_MORE_DOCS) return adjustedDoc=NO_MORE_DOCS;\n                target += base;\n\n                // probe next\n                int rawDoc = docs[idx++];\n                if (rawDoc >= target) return adjustedDoc=rawDoc-base;\n\n                int high = endIdx;\n\n                // TODO: probe more before resorting to binary search?\n\n                // binary search\n                while (idx <= high) {\n                  int mid = (idx+high) >>> 1;\n                  rawDoc = docs[mid];\n\n                  if (rawDoc < target) {\n                    idx = mid+1;\n                  }\n                  else if (rawDoc > target) {\n                    high = mid-1;\n                  }\n                  else {\n                    idx=mid+1;\n                    return adjustedDoc=rawDoc - base;\n                  }\n                }\n\n                // low is on the insertion point...\n                if (idx <= endIdx) {\n                  return adjustedDoc = docs[idx++] - base;\n                } else {\n                  return adjustedDoc=NO_MORE_DOCS;\n                }\n              }\n\n            };\n          }\n\n          @Override\n          public boolean isCacheable() {\n            return true;\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01":["a10b98ef1ef4bf9e38d2e07a9e425a916afa8705"],"d335bac0769ee9d21f92daa2576f28926fee3163":["70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"c26f00b574427b55127e869b935845554afde1fa":["d335bac0769ee9d21f92daa2576f28926fee3163","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"34743077dbbe20cd7a27d5c6c2511e62b99a0e36":["2dadf0f3286a34a0fee6e788ffce88624bf2984e"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["d335bac0769ee9d21f92daa2576f28926fee3163"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["321f2a5084d8bb0f0d5acbb637671346f695879b","d335bac0769ee9d21f92daa2576f28926fee3163"],"2dadf0f3286a34a0fee6e788ffce88624bf2984e":["321f2a5084d8bb0f0d5acbb637671346f695879b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":["d335bac0769ee9d21f92daa2576f28926fee3163"],"a10b98ef1ef4bf9e38d2e07a9e425a916afa8705":["34743077dbbe20cd7a27d5c6c2511e62b99a0e36"],"321f2a5084d8bb0f0d5acbb637671346f695879b":["1da8d55113b689b06716246649de6f62430f15c0"],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["321f2a5084d8bb0f0d5acbb637671346f695879b","a10b98ef1ef4bf9e38d2e07a9e425a916afa8705"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["868da859b43505d9d2a023bfeae6dd0c795f5295","d335bac0769ee9d21f92daa2576f28926fee3163"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01":["d335bac0769ee9d21f92daa2576f28926fee3163"],"d335bac0769ee9d21f92daa2576f28926fee3163":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","29ef99d61cda9641b6250bf9567329a6e65f901d","a258fbb26824fd104ed795e5d9033d2d040049ee","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"34743077dbbe20cd7a27d5c6c2511e62b99a0e36":["a10b98ef1ef4bf9e38d2e07a9e425a916afa8705"],"1da8d55113b689b06716246649de6f62430f15c0":["321f2a5084d8bb0f0d5acbb637671346f695879b"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"29ef99d61cda9641b6250bf9567329a6e65f901d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"2dadf0f3286a34a0fee6e788ffce88624bf2984e":["34743077dbbe20cd7a27d5c6c2511e62b99a0e36"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"a10b98ef1ef4bf9e38d2e07a9e425a916afa8705":["70e12dd4a648dadc5999dde1f0fb3a71a6ae4b01","868da859b43505d9d2a023bfeae6dd0c795f5295"],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"321f2a5084d8bb0f0d5acbb637671346f695879b":["29ef99d61cda9641b6250bf9567329a6e65f901d","2dadf0f3286a34a0fee6e788ffce88624bf2984e","868da859b43505d9d2a023bfeae6dd0c795f5295"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["29ef99d61cda9641b6250bf9567329a6e65f901d","a258fbb26824fd104ed795e5d9033d2d040049ee","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}