{"path":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","sourceNew":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = IndexReader.open(directory);\n    IndexSearcher theSearcher = new IndexSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, null, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.next()) {\n      count++;\n    }\n    reader.close();\n  }\n\n","sourceOld":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = IndexReader.open(directory);\n    IndexSearcher theSearcher = new IndexSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, null, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.next()) {\n      count++;\n    }\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","sourceNew":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = DirectoryReader.open(directory);\n    IndexSearcher theSearcher = new IndexSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, null, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.next()) {\n      count++;\n    }\n    reader.close();\n  }\n\n","sourceOld":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = IndexReader.open(directory);\n    IndexSearcher theSearcher = new IndexSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, null, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.next()) {\n      count++;\n    }\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173","date":1365631993,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","sourceNew":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = DirectoryReader.open(directory);\n    IndexSearcher theSearcher = newSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, null, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.next()) {\n      count++;\n    }\n    reader.close();\n  }\n\n","sourceOld":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = DirectoryReader.open(directory);\n    IndexSearcher theSearcher = new IndexSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, null, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.next()) {\n      count++;\n    }\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc","date":1424799790,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","sourceNew":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = DirectoryReader.open(directory);\n    IndexSearcher theSearcher = newSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.next()) {\n      count++;\n    }\n    reader.close();\n  }\n\n","sourceOld":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = DirectoryReader.open(directory);\n    IndexSearcher theSearcher = newSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, null, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.next()) {\n      count++;\n    }\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30de45e50bdc1a79a6797f34dca6271c8866cb6e","date":1427790465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","sourceNew":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = DirectoryReader.open(directory);\n    IndexSearcher theSearcher = newSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getIndexReader(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.nextDoc() != Spans.NO_MORE_DOCS) {\n      while (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) {\n        count++;\n      }\n    }\n    reader.close();\n  }\n\n","sourceOld":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = DirectoryReader.open(directory);\n    IndexSearcher theSearcher = newSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.next()) {\n      count++;\n    }\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fab172655716b96f7e42376116235017a922de3a","date":1427850611,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","sourceNew":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = DirectoryReader.open(directory);\n    IndexSearcher theSearcher = newSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getIndexReader(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.nextDoc() != Spans.NO_MORE_DOCS) {\n      while (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) {\n        count++;\n      }\n    }\n    reader.close();\n  }\n\n","sourceOld":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = DirectoryReader.open(directory);\n    IndexSearcher theSearcher = newSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.next()) {\n      count++;\n    }\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"73561ba4c64c1aea2ba4b3873f80b9b2a2946816","date":1438618499,"type":4,"author":"Alan Woodward","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery#testIgnoreSpanScorer().mjava","sourceNew":null,"sourceOld":"  //Set includeSpanScore to false, in which case just the payload score comes through.\n  public void testIgnoreSpanScorer() throws Exception {\n    PayloadTermQuery query = new PayloadTermQuery(new Term(PayloadHelper.MULTI_FIELD, \"seventy\"),\n            new MaxPayloadFunction(), false);\n\n    IndexReader reader = DirectoryReader.open(directory);\n    IndexSearcher theSearcher = newSearcher(reader);\n    theSearcher.setSimilarity(new FullSimilarity());\n    TopDocs hits = searcher.search(query, 100);\n    assertTrue(\"hits is null and it shouldn't be\", hits != null);\n    assertTrue(\"hits Size: \" + hits.totalHits + \" is not: \" + 100, hits.totalHits == 100);\n\n    //they should all have the exact same score, because they all contain seventy once, and we set\n    //all the other similarity factors to be 1\n\n    //System.out.println(\"Hash: \" + seventyHash + \" Twice Hash: \" + 2*seventyHash);\n    assertTrue(hits.getMaxScore() + \" does not equal: \" + 4.0, hits.getMaxScore() == 4.0);\n    //there should be exactly 10 items that score a 4, all the rest should score a 2\n    //The 10 items are: 70 + i*100 where i in [0-9]\n    int numTens = 0;\n    for (int i = 0; i < hits.scoreDocs.length; i++) {\n      ScoreDoc doc = hits.scoreDocs[i];\n      if (doc.doc % 10 == 0) {\n        numTens++;\n        assertTrue(doc.score + \" does not equal: \" + 4.0, doc.score == 4.0);\n      } else {\n        assertTrue(doc.score + \" does not equal: \" + 2, doc.score == 2);\n      }\n    }\n    assertTrue(numTens + \" does not equal: \" + 10, numTens == 10);\n    CheckHits.checkExplanations(query, \"field\", searcher, true);\n    Spans spans = MultiSpansWrapper.wrap(searcher.getIndexReader(), query);\n    assertTrue(\"spans is null and it shouldn't be\", spans != null);\n    //should be two matches per document\n    int count = 0;\n    //100 hits times 2 matches per hit, we should have 200 in count\n    while (spans.nextDoc() != Spans.NO_MORE_DOCS) {\n      while (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) {\n        count++;\n      }\n    }\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"73561ba4c64c1aea2ba4b3873f80b9b2a2946816":["30de45e50bdc1a79a6797f34dca6271c8866cb6e"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc":["3b0e8c39ca08b5a02de6edcd33d6f3b90b865173"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"30de45e50bdc1a79a6797f34dca6271c8866cb6e":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc"],"fab172655716b96f7e42376116235017a922de3a":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc","30de45e50bdc1a79a6797f34dca6271c8866cb6e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["73561ba4c64c1aea2ba4b3873f80b9b2a2946816"]},"commit2Childs":{"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["3b0e8c39ca08b5a02de6edcd33d6f3b90b865173"],"73561ba4c64c1aea2ba4b3873f80b9b2a2946816":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc":["30de45e50bdc1a79a6797f34dca6271c8866cb6e","fab172655716b96f7e42376116235017a922de3a"],"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc"],"30de45e50bdc1a79a6797f34dca6271c8866cb6e":["73561ba4c64c1aea2ba4b3873f80b9b2a2946816","fab172655716b96f7e42376116235017a922de3a"],"fab172655716b96f7e42376116235017a922de3a":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["fab172655716b96f7e42376116235017a922de3a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}