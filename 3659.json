{"path":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","commits":[{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom(System.nanoTime());\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new KeywordAnalyzer(),\n        IndexWriter.MaxFieldLength.UNLIMITED);\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.YES, Field.Index.ANALYZED);\n    doc.add(field);\n    \n    for (int i = 0; i < 1000; i++) {\n      field.setValue(randomString());\n      writer.addDocument(doc);\n    }\n    \n    writer.optimize();\n    writer.close();\n    searcher = new IndexSearcher(dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acb0ee59cc50caf85402e92d148fdb2af61bc19","date":1272929037,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new KeywordAnalyzer(),\n        IndexWriter.MaxFieldLength.UNLIMITED);\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    \n    for (int i = 0; i < 2000; i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    \n    writer.optimize();\n    writer.close();\n    searcher = new IndexSearcher(dir);\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom(System.nanoTime());\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new KeywordAnalyzer(),\n        IndexWriter.MaxFieldLength.UNLIMITED);\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.YES, Field.Index.ANALYZED);\n    doc.add(field);\n    \n    for (int i = 0; i < 1000; i++) {\n      field.setValue(randomString());\n      writer.addDocument(doc);\n    }\n    \n    writer.optimize();\n    writer.close();\n    searcher = new IndexSearcher(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2aafd88401639311b0404e67c94e829e123a0e45","date":1273477632,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new KeywordAnalyzer(),\n        IndexWriter.MaxFieldLength.UNLIMITED);\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    \n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    \n    writer.optimize();\n    writer.close();\n    searcher = new IndexSearcher(dir);\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new KeywordAnalyzer(),\n        IndexWriter.MaxFieldLength.UNLIMITED);\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    \n    for (int i = 0; i < 2000; i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    \n    writer.optimize();\n    writer.close();\n    searcher = new IndexSearcher(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790c3f61c9b891d66d919c5d10db9fa5216eb0f1","date":1274818604,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new MockAnalyzer(MockTokenizer.KEYWORD, false),\n        IndexWriter.MaxFieldLength.UNLIMITED);\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    \n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    \n    writer.optimize();\n    writer.close();\n    searcher = new IndexSearcher(dir);\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new KeywordAnalyzer(),\n        IndexWriter.MaxFieldLength.UNLIMITED);\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    \n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    \n    writer.optimize();\n    writer.close();\n    searcher = new IndexSearcher(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c084e47df29de3330311d69dabf515ceaa989512","date":1279030906,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                                        new MockAnalyzer(MockTokenizer.KEYWORD, false)));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new MockAnalyzer(MockTokenizer.KEYWORD, false),\n        IndexWriter.MaxFieldLength.UNLIMITED);\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    \n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    \n    writer.optimize();\n    writer.close();\n    searcher = new IndexSearcher(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55ae302c442697def14ecceba1b3fd3c53575b1b","date":1279211006,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                                        new MockAnalyzer(MockTokenizer.KEYWORD, false)));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                                        new MockAnalyzer(MockTokenizer.KEYWORD, false)));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                                        new MockAnalyzer(MockTokenizer.KEYWORD, false)));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new MockAnalyzer(MockTokenizer.KEYWORD, false),\n        IndexWriter.MaxFieldLength.UNLIMITED);\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    \n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    \n    writer.optimize();\n    writer.close();\n    searcher = new IndexSearcher(dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a09096fda3618d6f041c36896a70ce7f8715b09c","date":1279899153,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    // nocommit seed\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                                        new MockAnalyzer(MockTokenizer.KEYWORD, false)));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                                        new MockAnalyzer(MockTokenizer.KEYWORD, false)));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"15bbd254c1506df5299c4df8c148262c7bd6301e","date":1279913113,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    // nocommit seed\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    // nocommit seed\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                                        new MockAnalyzer(MockTokenizer.KEYWORD, false)));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9f6bb0c3d7b1c1dd8aeab43a34a89f050069e97","date":1280158618,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    // nocommit seed\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b103252dee6afa1b6d7a622c773d178788eb85a","date":1280180143,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                                        new MockAnalyzer(MockTokenizer.KEYWORD, false)));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0e45742e10e8e3b98e854babe6dbb07a4197b71","date":1280230285,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3242a09f703274d3b9283f2064a1a33064b53a1b","date":1280263474,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new IndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                                        new MockAnalyzer(MockTokenizer.KEYWORD, false)));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n\n    for (int i = 0; i < 2000*_TestUtil.getRandomMultiplier(); i++) {\n      field.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90527e04096200f63999268b5d84d5530b191cf6","date":1281075048,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = newDirectory(random);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = newDirectory(random);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"150488c1317972164a9a824be05b1ba2ba0fc68c","date":1284316090,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    \n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"43b04c27924fe393e38e9f0986e32c634f261859","date":1284399440,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    \n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    \n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"13452165d8bf3d45a72f572aaed3c679735d3af2","date":1290101307,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    \n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bce89597a7c3a4535b5b7f8100c2078e520f6e57","date":1290106041,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    \n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"eeebcf026b55d8ce3ac8165210782b26cc4efe30","date":1290108396,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    \n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    \n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    random = newRandom();\n    dir = new MockRAMDirectory();\n    // TODO: fix mocktokenizer to not extend chartokenizer, so you can have an 'empty' keyword.\n    // currently, this means 'empty tokens' arent created/tested in the enumeration:\n    // <mikemccand> it's like having a big hairy scary monster in the basement but being upset that it doesn't have fangs\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));\n    \n    Document doc = new Document();\n    Field field = new Field(\"field\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790e1fde4caa765b3faaad3fbcd25c6973450336","date":1296689245,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = new IndexSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1bb50752d43a65ef1b623eabdb8e865983d3cd6","date":1304257984,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0762b640e0d0d12b6edb96db68986e13145c3484","date":1307575932,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = 2000 * RANDOM_MULTIPLIER;\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":["c5145e7af5e55022ec6ba083c0aef480e49e0d3b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c5145e7af5e55022ec6ba083c0aef480e49e0d3b","date":1319475599,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    fieldName = random.nextBoolean() ? \"field\" : \"\"; // sometimes use an empty string as field name\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(fieldName, \"\", StringField.TYPE_UNSTORED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestRegexpRandom2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    fieldName = random.nextBoolean() ? \"field\" : \"\"; // sometimes use an empty string as field name\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(fieldName, \"\", StringField.TYPE_UNSTORED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    dir = newDirectory();\n    fieldName = random.nextBoolean() ? \"field\" : \"\"; // sometimes use an empty string as field name\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(fieldName, \"\", StringField.TYPE_UNSTORED);\n    doc.add(field);\n    List<String> terms = new ArrayList<String>();\n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(s);\n      writer.addDocument(doc);\n    }\n\n    if (VERBOSE) {\n      // utf16 order\n      Collections.sort(terms);\n      System.out.println(\"UTF16 order:\");\n      for(String s : terms) {\n        System.out.println(\"  \" + UnicodeUtil.toHexString(s));\n      }\n    }\n    \n    reader = writer.getReader();\n    searcher1 = newSearcher(reader);\n    searcher2 = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e45742e10e8e3b98e854babe6dbb07a4197b71":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"0762b640e0d0d12b6edb96db68986e13145c3484":["c1bb50752d43a65ef1b623eabdb8e865983d3cd6"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["c5145e7af5e55022ec6ba083c0aef480e49e0d3b"],"5acb0ee59cc50caf85402e92d148fdb2af61bc19":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"15bbd254c1506df5299c4df8c148262c7bd6301e":["a09096fda3618d6f041c36896a70ce7f8715b09c"],"55ae302c442697def14ecceba1b3fd3c53575b1b":["c084e47df29de3330311d69dabf515ceaa989512"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["3242a09f703274d3b9283f2064a1a33064b53a1b","eeebcf026b55d8ce3ac8165210782b26cc4efe30"],"c9f6bb0c3d7b1c1dd8aeab43a34a89f050069e97":["15bbd254c1506df5299c4df8c148262c7bd6301e"],"43b04c27924fe393e38e9f0986e32c634f261859":["150488c1317972164a9a824be05b1ba2ba0fc68c"],"a09096fda3618d6f041c36896a70ce7f8715b09c":["55ae302c442697def14ecceba1b3fd3c53575b1b"],"c084e47df29de3330311d69dabf515ceaa989512":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["3bb13258feba31ab676502787ab2e1779f129b7a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"eeebcf026b55d8ce3ac8165210782b26cc4efe30":["bce89597a7c3a4535b5b7f8100c2078e520f6e57"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["0762b640e0d0d12b6edb96db68986e13145c3484"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"c5145e7af5e55022ec6ba083c0aef480e49e0d3b":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","0762b640e0d0d12b6edb96db68986e13145c3484"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["5f4e87790277826a2aea119328600dfb07761f32","a0e45742e10e8e3b98e854babe6dbb07a4197b71"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["55ae302c442697def14ecceba1b3fd3c53575b1b","c9f6bb0c3d7b1c1dd8aeab43a34a89f050069e97"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["90527e04096200f63999268b5d84d5530b191cf6"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","c1bb50752d43a65ef1b623eabdb8e865983d3cd6"],"90527e04096200f63999268b5d84d5530b191cf6":["a0e45742e10e8e3b98e854babe6dbb07a4197b71"],"2aafd88401639311b0404e67c94e829e123a0e45":["5acb0ee59cc50caf85402e92d148fdb2af61bc19"],"5f4e87790277826a2aea119328600dfb07761f32":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1","55ae302c442697def14ecceba1b3fd3c53575b1b"],"962d04139994fce5193143ef35615499a9a96d78":["bde51b089eb7f86171eb3406e38a274743f9b7ac","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"c1bb50752d43a65ef1b623eabdb8e865983d3cd6":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["eeebcf026b55d8ce3ac8165210782b26cc4efe30"],"bce89597a7c3a4535b5b7f8100c2078e520f6e57":["13452165d8bf3d45a72f572aaed3c679735d3af2"],"13452165d8bf3d45a72f572aaed3c679735d3af2":["43b04c27924fe393e38e9f0986e32c634f261859"],"a3776dccca01c11e7046323cfad46a3b4a471233":["790e1fde4caa765b3faaad3fbcd25c6973450336","c1bb50752d43a65ef1b623eabdb8e865983d3cd6"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["135621f3a0670a9394eb563224a3b76cc4dddc0f","0762b640e0d0d12b6edb96db68986e13145c3484"],"150488c1317972164a9a824be05b1ba2ba0fc68c":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"3bb13258feba31ab676502787ab2e1779f129b7a":["43b04c27924fe393e38e9f0986e32c634f261859","eeebcf026b55d8ce3ac8165210782b26cc4efe30"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"790c3f61c9b891d66d919c5d10db9fa5216eb0f1":["2aafd88401639311b0404e67c94e829e123a0e45"]},"commit2Childs":{"a0e45742e10e8e3b98e854babe6dbb07a4197b71":["3242a09f703274d3b9283f2064a1a33064b53a1b","90527e04096200f63999268b5d84d5530b191cf6"],"0762b640e0d0d12b6edb96db68986e13145c3484":["1509f151d7692d84fae414b2b799ac06ba60fcb4","a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5acb0ee59cc50caf85402e92d148fdb2af61bc19":["2aafd88401639311b0404e67c94e829e123a0e45"],"15bbd254c1506df5299c4df8c148262c7bd6301e":["c9f6bb0c3d7b1c1dd8aeab43a34a89f050069e97"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["5acb0ee59cc50caf85402e92d148fdb2af61bc19"],"55ae302c442697def14ecceba1b3fd3c53575b1b":["a09096fda3618d6f041c36896a70ce7f8715b09c","4b103252dee6afa1b6d7a622c773d178788eb85a","5f4e87790277826a2aea119328600dfb07761f32"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["150488c1317972164a9a824be05b1ba2ba0fc68c"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c9f6bb0c3d7b1c1dd8aeab43a34a89f050069e97":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"43b04c27924fe393e38e9f0986e32c634f261859":["13452165d8bf3d45a72f572aaed3c679735d3af2","3bb13258feba31ab676502787ab2e1779f129b7a"],"a09096fda3618d6f041c36896a70ce7f8715b09c":["15bbd254c1506df5299c4df8c148262c7bd6301e"],"c084e47df29de3330311d69dabf515ceaa989512":["55ae302c442697def14ecceba1b3fd3c53575b1b"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["962d04139994fce5193143ef35615499a9a96d78","c1bb50752d43a65ef1b623eabdb8e865983d3cd6"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"eeebcf026b55d8ce3ac8165210782b26cc4efe30":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","790e1fde4caa765b3faaad3fbcd25c6973450336","3bb13258feba31ab676502787ab2e1779f129b7a"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["c5145e7af5e55022ec6ba083c0aef480e49e0d3b"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["962d04139994fce5193143ef35615499a9a96d78"],"c5145e7af5e55022ec6ba083c0aef480e49e0d3b":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":[],"3242a09f703274d3b9283f2064a1a33064b53a1b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["a0e45742e10e8e3b98e854babe6dbb07a4197b71"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"90527e04096200f63999268b5d84d5530b191cf6":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"5f4e87790277826a2aea119328600dfb07761f32":["3242a09f703274d3b9283f2064a1a33064b53a1b"],"2aafd88401639311b0404e67c94e829e123a0e45":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1"],"962d04139994fce5193143ef35615499a9a96d78":[],"c1bb50752d43a65ef1b623eabdb8e865983d3cd6":["0762b640e0d0d12b6edb96db68986e13145c3484","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["f2c5f0cb44df114db4228c8f77861714b5cabaea","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"bce89597a7c3a4535b5b7f8100c2078e520f6e57":["eeebcf026b55d8ce3ac8165210782b26cc4efe30"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"13452165d8bf3d45a72f572aaed3c679735d3af2":["bce89597a7c3a4535b5b7f8100c2078e520f6e57"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"150488c1317972164a9a824be05b1ba2ba0fc68c":["43b04c27924fe393e38e9f0986e32c634f261859"],"3bb13258feba31ab676502787ab2e1779f129b7a":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"790c3f61c9b891d66d919c5d10db9fa5216eb0f1":["c084e47df29de3330311d69dabf515ceaa989512","5f4e87790277826a2aea119328600dfb07761f32"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","962d04139994fce5193143ef35615499a9a96d78","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}