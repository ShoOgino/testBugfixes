{"path":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","commits":[{"id":"0c3e228bf650e96f3002a8fb73dd0c13d55af077","date":1138253849,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"/dev/null","sourceNew":"  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    SetHitCollector hc = new SetHitCollector(filter, maxDoc());\n    searcher.search(query, null, hc);\n    return hc.getDocSet();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"21400778a0bf704d187a4848279049f5d90276c8","date":1149955512,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      searcher.search(query,null,hc);\n      return hc.getDocSet();\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      final DocSet filt = filter;\n      searcher.search(query, null, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt.exists(doc)) hc.collect(doc,score);\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","sourceOld":"  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    SetHitCollector hc = new SetHitCollector(filter, maxDoc());\n    searcher.search(query, null, hc);\n    return hc.getDocSet();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d6820cd5ba42858fda6cbe6bcfd7b290fdb9418","date":1158180946,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        TermDocs tdocs = null;\n        try {\n          tdocs = reader.termDocs(t);\n          while (tdocs.next()) hc.collect(tdocs.doc(),0.0f);\n        } finally {\n          if (tdocs!=null) tdocs.close();\n        }\n      } else {\n        searcher.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      final DocSet filt = filter;\n      searcher.search(query, null, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt.exists(doc)) hc.collect(doc,score);\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","sourceOld":"  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      searcher.search(query,null,hc);\n      return hc.getDocSet();\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      final DocSet filt = filter;\n      searcher.search(query, null, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt.exists(doc)) hc.collect(doc,score);\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":["7c236c213a9b74e1ab39bf9e289391bf6c749c17","7c236c213a9b74e1ab39bf9e289391bf6c749c17","7c236c213a9b74e1ab39bf9e289391bf6c749c17"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1940b60224897131cf61bb615e02af1b26558c8","date":1169501002,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        TermDocs tdocs = null;\n        try {\n          tdocs = reader.termDocs(t);\n          while (tdocs.next()) hc.collect(tdocs.doc(),0.0f);\n        } finally {\n          if (tdocs!=null) tdocs.close();\n        }\n      } else {\n        searcher.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      final DocSet filt = filter;\n      searcher.search(query, null, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt.exists(doc)) hc.collect(doc,score);\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","sourceOld":"  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        TermDocs tdocs = null;\n        try {\n          tdocs = reader.termDocs(t);\n          while (tdocs.next()) hc.collect(tdocs.doc(),0.0f);\n        } finally {\n          if (tdocs!=null) tdocs.close();\n        }\n      } else {\n        searcher.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      final DocSet filt = filter;\n      searcher.search(query, null, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt.exists(doc)) hc.collect(doc,score);\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6d6338c87060be5f66757a94945975f3bbd377a9","date":1189278234,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetHitCollector hc = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        TermDocs tdocs = null;\n        try {\n          tdocs = reader.termDocs(t);\n          while (tdocs.next()) hc.collect(tdocs.doc(),0.0f);\n        } finally {\n          if (tdocs!=null) tdocs.close();\n        }\n      } else {\n        searcher.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetHitCollector hc = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      final DocSet filt = filter;\n      searcher.search(query, null, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt.exists(doc)) hc.collect(doc,score);\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        TermDocs tdocs = null;\n        try {\n          tdocs = reader.termDocs(t);\n          while (tdocs.next()) hc.collect(tdocs.doc(),0.0f);\n        } finally {\n          if (tdocs!=null) tdocs.close();\n        }\n      } else {\n        searcher.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetHitCollector hc = new DocSetHitCollector(maxDoc());\n      final DocSet filt = filter;\n      searcher.search(query, null, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt.exists(doc)) hc.collect(doc,score);\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7ceb8ef10f044820fbd058f02d5a8e26539d255c","date":1242149378,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetHitCollector hc = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        TermDocs tdocs = null;\n        try {\n          tdocs = reader.termDocs(t);\n          while (tdocs.next()) hc.collect(tdocs.doc(),0.0f);\n        } finally {\n          if (tdocs!=null) tdocs.close();\n        }\n      } else {\n        super.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetHitCollector hc = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt.exists(doc)) hc.collect(doc,score);\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetHitCollector hc = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        TermDocs tdocs = null;\n        try {\n          tdocs = reader.termDocs(t);\n          while (tdocs.next()) hc.collect(tdocs.doc(),0.0f);\n        } finally {\n          if (tdocs!=null) tdocs.close();\n        }\n      } else {\n        searcher.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetHitCollector hc = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      final DocSet filt = filter;\n      searcher.search(query, null, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt.exists(doc)) hc.collect(doc,score);\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"af88ddf3d03b8f9d83ad08cafaa7438a1206e405","date":1242338740,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetCollector hc = new DocSetCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          hc.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            while (--num>=0) {\n              hc.collect(arr[num]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetCollector hc = new DocSetCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new Collector() {\n        int base = 0;\n        public void collect(int doc) {\n          doc += base;\n          if (filt.exists(doc)) hc.collect(doc);\n        }\n\n        public void setNextReader(IndexReader reader, int docBase) throws IOException {\n          this.base = docBase;  \n        }\n\n        public void setScorer(Scorer scorer) throws IOException {\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetHitCollector hc = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        TermDocs tdocs = null;\n        try {\n          tdocs = reader.termDocs(t);\n          while (tdocs.next()) hc.collect(tdocs.doc(),0.0f);\n        } finally {\n          if (tdocs!=null) tdocs.close();\n        }\n      } else {\n        super.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetHitCollector hc = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new HitCollector() {\n        public void collect(int doc, float score) {\n          if (filt.exists(doc)) hc.collect(doc,score);\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":["7c236c213a9b74e1ab39bf9e289391bf6c749c17","7c236c213a9b74e1ab39bf9e289391bf6c749c17","7c236c213a9b74e1ab39bf9e289391bf6c749c17"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"30391436869a41b74d4ba7098c40d955b686a10c","date":1242835871,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          hc.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            while (--num>=0) {\n              hc.collect(arr[num]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new Collector() {\n        int base = 0;\n        public void collect(int doc) {\n          doc += base;\n          if (filt.exists(doc)) hc.collect(doc);\n        }\n\n        public void setNextReader(IndexReader reader, int docBase) throws IOException {\n          this.base = docBase;  \n        }\n\n        public void setScorer(Scorer scorer) throws IOException {\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetCollector hc = new DocSetCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          hc.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            while (--num>=0) {\n              hc.collect(arr[num]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetCollector hc = new DocSetCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new Collector() {\n        int base = 0;\n        public void collect(int doc) {\n          doc += base;\n          if (filt.exists(doc)) hc.collect(doc);\n        }\n\n        public void setNextReader(IndexReader reader, int docBase) throws IOException {\n          this.base = docBase;  \n        }\n\n        public void setScorer(Scorer scorer) throws IOException {\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"029ffe7502a7a8ff1f425020bc204311ade99687","date":1243301392,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          hc.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            while (--num>=0) {\n              hc.collect(arr[num]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        searcher.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new Collector() {\n        int base = 0;\n        public void collect(int doc) {\n          doc += base;\n          if (filt.exists(doc)) hc.collect(doc);\n        }\n\n        public void setNextReader(IndexReader reader, int docBase) throws IOException {\n          this.base = docBase;  \n        }\n\n        public void setScorer(Scorer scorer) throws IOException {\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          hc.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            while (--num>=0) {\n              hc.collect(arr[num]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new Collector() {\n        int base = 0;\n        public void collect(int doc) {\n          doc += base;\n          if (filt.exists(doc)) hc.collect(doc);\n        }\n\n        public void setNextReader(IndexReader reader, int docBase) throws IOException {\n          this.base = docBase;  \n        }\n\n        public void setScorer(Scorer scorer) throws IOException {\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ac3e6437547a34cce2b5405ce0cf9e3af578401e","date":1243373693,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          hc.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            while (--num>=0) {\n              hc.collect(arr[num]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new Collector() {\n        int base = 0;\n        public void collect(int doc) throws IOException {\n          doc += base;\n          if (filt.exists(doc)) hc.collect(doc);\n        }\n\n        public void setNextReader(IndexReader reader, int docBase) throws IOException {\n          this.base = docBase;  \n        }\n\n        public void setScorer(Scorer scorer) throws IOException {\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          hc.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            while (--num>=0) {\n              hc.collect(arr[num]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        searcher.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new Collector() {\n        int base = 0;\n        public void collect(int doc) {\n          doc += base;\n          if (filt.exists(doc)) hc.collect(doc);\n        }\n\n        public void setNextReader(IndexReader reader, int docBase) throws IOException {\n          this.base = docBase;  \n        }\n\n        public void setScorer(Scorer scorer) throws IOException {\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3c008338f79bd89b144d1fc8a57a39060df14b67","date":1243431296,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          hc.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              hc.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new Collector() {\n        int base = 0;\n        public void collect(int doc) throws IOException {\n          doc += base;\n          if (filt.exists(doc)) hc.collect(doc);\n        }\n\n        public void setNextReader(IndexReader reader, int docBase) throws IOException {\n          this.base = docBase;  \n        }\n\n        public void setScorer(Scorer scorer) throws IOException {\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          hc.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            while (--num>=0) {\n              hc.collect(arr[num]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new Collector() {\n        int base = 0;\n        public void collect(int doc) throws IOException {\n          doc += base;\n          if (filt.exists(doc)) hc.collect(doc);\n        }\n\n        public void setNextReader(IndexReader reader, int docBase) throws IOException {\n          this.base = docBase;  \n        }\n\n        public void setScorer(Scorer scorer) throws IOException {\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"24dca205cbeb46e5e9f594f1f87e4a2c9788dd81","date":1243477385,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {    \n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    if (filter==null) {\n      DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          hc.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              hc.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,hc);\n      }\n      return hc.getDocSet();\n\n    } else {\n      // FUTURE: if the filter is sorted by docid, could use skipTo (SkipQueryFilter)\n      final DocSetCollector hc = new DocSetCollector(maxDoc()>>6, maxDoc());\n      final DocSet filt = filter;\n      super.search(query, null, new Collector() {\n        int base = 0;\n        public void collect(int doc) throws IOException {\n          doc += base;\n          if (filt.exists(doc)) hc.collect(doc);\n        }\n\n        public void setNextReader(IndexReader reader, int docBase) throws IOException {\n          this.base = docBase;  \n        }\n\n        public void setScorer(Scorer scorer) throws IOException {\n        }\n      }\n      );\n      return hc.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":["7c236c213a9b74e1ab39bf9e289391bf6c749c17","7c236c213a9b74e1ab39bf9e289391bf6c749c17","7c236c213a9b74e1ab39bf9e289391bf6c749c17"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"65dec11943a7948910323662eb0428486ea7d167","date":1243477865,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {    \n    query = QueryUtils.simplifyQuery(query);\n\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {    \n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2404ffa4f4549bef011c766c08dd404e0ab45109","date":1243478511,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {    \n    query = QueryUtils.simplifyQuery(query);\n\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d1475fdbacd73c160adc96a10aadc123d489b583","date":1246052926,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    query = QueryUtils.simplifyQuery(query);\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e40ef862aa0c9f1335f630dcab5994d3f602cc58","date":1246101444,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    query = QueryUtils.simplifyQuery(query);\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocSetNC(Query,DocSet).mjava","sourceNew":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","sourceOld":"  // query must be positive\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n\n    if (filter==null) {\n      if (query instanceof TermQuery) {\n        Term t = ((TermQuery)query).getTerm();\n        SolrIndexReader[] readers = reader.getLeafReaders();\n        int[] offsets = reader.getLeafOffsets();\n        int[] arr = new int[256];\n        int[] freq = new int[256];\n        for (int i=0; i<readers.length; i++) {\n          SolrIndexReader sir = readers[i];\n          int offset = offsets[i];\n          collector.setNextReader(sir, offset);\n          TermDocs tdocs = sir.termDocs(t);\n          for(;;) {\n            int num = tdocs.read(arr, freq);\n            if (num==0) break;\n            for (int j=0; j<num; j++) {\n              collector.collect(arr[j]);\n            }\n          }\n          tdocs.close();\n        }\n      } else {\n        super.search(query,null,collector);\n      }\n      return collector.getDocSet();\n\n    } else {\n      Filter luceneFilter = filter.getTopFilter();\n      super.search(query, luceneFilter, collector);\n      return collector.getDocSet();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"ac3e6437547a34cce2b5405ce0cf9e3af578401e":["029ffe7502a7a8ff1f425020bc204311ade99687"],"6d6338c87060be5f66757a94945975f3bbd377a9":["b1940b60224897131cf61bb615e02af1b26558c8"],"e40ef862aa0c9f1335f630dcab5994d3f602cc58":["d1475fdbacd73c160adc96a10aadc123d489b583"],"3c008338f79bd89b144d1fc8a57a39060df14b67":["ac3e6437547a34cce2b5405ce0cf9e3af578401e"],"21400778a0bf704d187a4848279049f5d90276c8":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"65dec11943a7948910323662eb0428486ea7d167":["24dca205cbeb46e5e9f594f1f87e4a2c9788dd81"],"029ffe7502a7a8ff1f425020bc204311ade99687":["30391436869a41b74d4ba7098c40d955b686a10c"],"b1940b60224897131cf61bb615e02af1b26558c8":["5d6820cd5ba42858fda6cbe6bcfd7b290fdb9418"],"5d6820cd5ba42858fda6cbe6bcfd7b290fdb9418":["21400778a0bf704d187a4848279049f5d90276c8"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"30391436869a41b74d4ba7098c40d955b686a10c":["af88ddf3d03b8f9d83ad08cafaa7438a1206e405"],"d1475fdbacd73c160adc96a10aadc123d489b583":["2404ffa4f4549bef011c766c08dd404e0ab45109"],"7ceb8ef10f044820fbd058f02d5a8e26539d255c":["6d6338c87060be5f66757a94945975f3bbd377a9"],"0c3e228bf650e96f3002a8fb73dd0c13d55af077":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"2404ffa4f4549bef011c766c08dd404e0ab45109":["65dec11943a7948910323662eb0428486ea7d167"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"24dca205cbeb46e5e9f594f1f87e4a2c9788dd81":["3c008338f79bd89b144d1fc8a57a39060df14b67"],"ad94625fb8d088209f46650c8097196fec67f00c":["e40ef862aa0c9f1335f630dcab5994d3f602cc58"],"af88ddf3d03b8f9d83ad08cafaa7438a1206e405":["7ceb8ef10f044820fbd058f02d5a8e26539d255c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"ac3e6437547a34cce2b5405ce0cf9e3af578401e":["3c008338f79bd89b144d1fc8a57a39060df14b67"],"6d6338c87060be5f66757a94945975f3bbd377a9":["7ceb8ef10f044820fbd058f02d5a8e26539d255c"],"e40ef862aa0c9f1335f630dcab5994d3f602cc58":["ad94625fb8d088209f46650c8097196fec67f00c"],"3c008338f79bd89b144d1fc8a57a39060df14b67":["24dca205cbeb46e5e9f594f1f87e4a2c9788dd81"],"21400778a0bf704d187a4848279049f5d90276c8":["5d6820cd5ba42858fda6cbe6bcfd7b290fdb9418"],"65dec11943a7948910323662eb0428486ea7d167":["2404ffa4f4549bef011c766c08dd404e0ab45109"],"029ffe7502a7a8ff1f425020bc204311ade99687":["ac3e6437547a34cce2b5405ce0cf9e3af578401e"],"b1940b60224897131cf61bb615e02af1b26558c8":["6d6338c87060be5f66757a94945975f3bbd377a9"],"5d6820cd5ba42858fda6cbe6bcfd7b290fdb9418":["b1940b60224897131cf61bb615e02af1b26558c8"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"30391436869a41b74d4ba7098c40d955b686a10c":["029ffe7502a7a8ff1f425020bc204311ade99687"],"d1475fdbacd73c160adc96a10aadc123d489b583":["e40ef862aa0c9f1335f630dcab5994d3f602cc58"],"7ceb8ef10f044820fbd058f02d5a8e26539d255c":["af88ddf3d03b8f9d83ad08cafaa7438a1206e405"],"0c3e228bf650e96f3002a8fb73dd0c13d55af077":["21400778a0bf704d187a4848279049f5d90276c8"],"2404ffa4f4549bef011c766c08dd404e0ab45109":["d1475fdbacd73c160adc96a10aadc123d489b583"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"24dca205cbeb46e5e9f594f1f87e4a2c9788dd81":["65dec11943a7948910323662eb0428486ea7d167"],"af88ddf3d03b8f9d83ad08cafaa7438a1206e405":["30391436869a41b74d4ba7098c40d955b686a10c"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}