{"path":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d2dee33619431ada2a7a07f5fe2dbd94bac6a460","date":1337274029,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0));\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0));\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1));\n        FieldInfos fis3 = _TestUtil.getFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d153abcf92dc5329d98571a8c3035df9bd80648","date":1337702630,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);\n        FieldInfos fis3 = _TestUtil.getFieldInfos(sis.info(2).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0));\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0));\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1));\n        FieldInfos fis3 = _TestUtil.getFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);\n        FieldInfos fis3 = _TestUtil.getFieldInfos(sis.info(2).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);\n        FieldInfos fis3 = _TestUtil.getFieldInfos(sis.info(2).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);\n        FieldInfos fis3 = _TestUtil.getFieldInfos(sis.info(2).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8435160e9702b19398118ddf76b61c846612b6a4","date":1380349140,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);\n        FieldInfos fis3 = _TestUtil.getFieldInfos(sis.info(2).info);\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.shutdown();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2fb55c0777755badd3b46d8140f3d4301febed","date":1398881584,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.shutdown();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            random().nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.shutdown();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.shutdown();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n            NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.shutdown();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.shutdown();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.shutdown();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.shutdown();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"256a0e54e76f18e115a43e7fe793b54d4e9a3005","date":1412426514,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = IndexWriter.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = IndexWriter.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3384e6013a93e4d11b7d75388693f8d0388602bf","date":1413951663,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = IndexWriter.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = IndexWriter.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = IndexWriter.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d1 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d1 second field\", Field.Store.YES));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d2 first field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        Document d = new Document();\n        d.add(new TextField(\"f1\", \"d3 first field\", Field.Store.YES));\n        d.add(new TextField(\"f2\", \"d3 second field\", Field.Store.YES));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));\n        FieldInfos fis3 = IndexWriter.readFieldInfos(sis.info(2));\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                    .setMergePolicy(NoMergePolicy.INSTANCE));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                  .setMergePolicy(new LogByteSizeMergePolicy())\n                                                  .setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2fb55c0777755badd3b46d8140f3d4301febed":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"256a0e54e76f18e115a43e7fe793b54d4e9a3005":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["d2dee33619431ada2a7a07f5fe2dbd94bac6a460"],"8435160e9702b19398118ddf76b61c846612b6a4":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["256a0e54e76f18e115a43e7fe793b54d4e9a3005"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"d2dee33619431ada2a7a07f5fe2dbd94bac6a460":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","9d153abcf92dc5329d98571a8c3035df9bd80648"],"9bb9a29a5e71a90295f175df8919802993142c9a":["d0ef034a4f10871667ae75181537775ddcf8ade4","256a0e54e76f18e115a43e7fe793b54d4e9a3005"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["8435160e9702b19398118ddf76b61c846612b6a4"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["9bb9a29a5e71a90295f175df8919802993142c9a","3384e6013a93e4d11b7d75388693f8d0388602bf"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"]},"commit2Childs":{"7e2fb55c0777755badd3b46d8140f3d4301febed":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"256a0e54e76f18e115a43e7fe793b54d4e9a3005":["3384e6013a93e4d11b7d75388693f8d0388602bf","9bb9a29a5e71a90295f175df8919802993142c9a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"8435160e9702b19398118ddf76b61c846612b6a4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"d2dee33619431ada2a7a07f5fe2dbd94bac6a460":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["8435160e9702b19398118ddf76b61c846612b6a4"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"9bb9a29a5e71a90295f175df8919802993142c9a":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["256a0e54e76f18e115a43e7fe793b54d4e9a3005","9bb9a29a5e71a90295f175df8919802993142c9a"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["7e2fb55c0777755badd3b46d8140f3d4301febed"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["d2dee33619431ada2a7a07f5fe2dbd94bac6a460","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}