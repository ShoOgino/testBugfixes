{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymGraphFilter#randomNonEmptyString().mjava","commits":[{"id":"24a98f5fdd23e04f85819dbc63b47a12f7c44311","date":1482439157,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymGraphFilter#randomNonEmptyString().mjava","pathOld":"/dev/null","sourceNew":"  /** If we expand synonyms at search time, the results are correct. */\n  // Needs TermAutomatonQuery, which is in sandbox still:\n  /*\n  public void testAccurateGraphQuery2() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"say wtf happened\", Field.Store.NO));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    IndexSearcher s = newSearcher(r);\n\n    SynonymMap.Builder b = new SynonymMap.Builder();\n    add(b, \"what the fudge\", \"wtf\", true);\n\n    SynonymMap map = b.build();\n\n    TokenStream in = new CannedTokenStream(0, 26, new Token[] {\n        token(\"say\", 1, 1, 0, 3),\n        token(\"what\", 1, 1, 3, 7),\n        token(\"the\", 1, 1, 8, 11),\n        token(\"fudge\", 1, 1, 12, 17),\n        token(\"happened\", 1, 1, 18, 26),\n      });\n\n    TokenStreamToTermAutomatonQuery ts2q = new TokenStreamToTermAutomatonQuery();\n\n    assertEquals(1, s.count(ts2q.toQuery(\"field\", new SynonymGraphFilter(in, map, true))));\n\n    // \"what happened\" should NOT match:\n    in = new CannedTokenStream(0, 13, new Token[] {\n        token(\"what\", 1, 1, 0, 4),\n        token(\"happened\", 1, 1, 5, 13),\n      });\n    assertEquals(0, s.count(ts2q.toQuery(\"field\", new SynonymGraphFilter(in, map, true))));\n\n    IOUtils.close(r, dir);\n  }\n  */\n\n  // Needs TermAutomatonQuery, which is in sandbox still:\n  /*\n  public void testAccurateGraphQuery3() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"say what the fudge happened\", Field.Store.NO));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    IndexSearcher s = newSearcher(r);\n\n    SynonymMap.Builder b = new SynonymMap.Builder();\n    add(b, \"wtf\", \"what the fudge\", true);\n\n    SynonymMap map = b.build();\n\n    TokenStream in = new CannedTokenStream(0, 15, new Token[] {\n        token(\"say\", 1, 1, 0, 3),\n        token(\"wtf\", 1, 1, 3, 6),\n        token(\"happened\", 1, 1, 7, 15),\n      });\n\n    TokenStreamToTermAutomatonQuery ts2q = new TokenStreamToTermAutomatonQuery();\n\n    assertEquals(1, s.count(ts2q.toQuery(\"field\", new SynonymGraphFilter(in, map, true))));\n\n    // \"what happened\" should NOT match:\n    in = new CannedTokenStream(0, 13, new Token[] {\n        token(\"what\", 1, 1, 0, 4),\n        token(\"happened\", 1, 1, 5, 13),\n      });\n    assertEquals(0, s.count(ts2q.toQuery(\"field\", new SynonymGraphFilter(in, map, true))));\n\n    IOUtils.close(r, dir);\n  }\n\n  private static Token token(String term, int posInc, int posLength, int startOffset, int endOffset) {\n    final Token t = new Token(term, startOffset, endOffset);\n    t.setPositionIncrement(posInc);\n    t.setPositionLength(posLength);\n    return t;\n  }\n  */\n\n  private String randomNonEmptyString() {\n    while(true) {\n      String s = TestUtil.randomUnicodeString(random()).trim();\n      //String s = TestUtil.randomSimpleString(random()).trim();\n      if (s.length() != 0 && s.indexOf('\\u0000') == -1) {\n        return s;\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymGraphFilter#randomNonEmptyString().mjava","pathOld":"/dev/null","sourceNew":"  /** If we expand synonyms at search time, the results are correct. */\n  // Needs TermAutomatonQuery, which is in sandbox still:\n  /*\n  public void testAccurateGraphQuery2() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"say wtf happened\", Field.Store.NO));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    IndexSearcher s = newSearcher(r);\n\n    SynonymMap.Builder b = new SynonymMap.Builder();\n    add(b, \"what the fudge\", \"wtf\", true);\n\n    SynonymMap map = b.build();\n\n    TokenStream in = new CannedTokenStream(0, 26, new Token[] {\n        token(\"say\", 1, 1, 0, 3),\n        token(\"what\", 1, 1, 3, 7),\n        token(\"the\", 1, 1, 8, 11),\n        token(\"fudge\", 1, 1, 12, 17),\n        token(\"happened\", 1, 1, 18, 26),\n      });\n\n    TokenStreamToTermAutomatonQuery ts2q = new TokenStreamToTermAutomatonQuery();\n\n    assertEquals(1, s.count(ts2q.toQuery(\"field\", new SynonymGraphFilter(in, map, true))));\n\n    // \"what happened\" should NOT match:\n    in = new CannedTokenStream(0, 13, new Token[] {\n        token(\"what\", 1, 1, 0, 4),\n        token(\"happened\", 1, 1, 5, 13),\n      });\n    assertEquals(0, s.count(ts2q.toQuery(\"field\", new SynonymGraphFilter(in, map, true))));\n\n    IOUtils.close(r, dir);\n  }\n  */\n\n  // Needs TermAutomatonQuery, which is in sandbox still:\n  /*\n  public void testAccurateGraphQuery3() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"say what the fudge happened\", Field.Store.NO));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    IndexSearcher s = newSearcher(r);\n\n    SynonymMap.Builder b = new SynonymMap.Builder();\n    add(b, \"wtf\", \"what the fudge\", true);\n\n    SynonymMap map = b.build();\n\n    TokenStream in = new CannedTokenStream(0, 15, new Token[] {\n        token(\"say\", 1, 1, 0, 3),\n        token(\"wtf\", 1, 1, 3, 6),\n        token(\"happened\", 1, 1, 7, 15),\n      });\n\n    TokenStreamToTermAutomatonQuery ts2q = new TokenStreamToTermAutomatonQuery();\n\n    assertEquals(1, s.count(ts2q.toQuery(\"field\", new SynonymGraphFilter(in, map, true))));\n\n    // \"what happened\" should NOT match:\n    in = new CannedTokenStream(0, 13, new Token[] {\n        token(\"what\", 1, 1, 0, 4),\n        token(\"happened\", 1, 1, 5, 13),\n      });\n    assertEquals(0, s.count(ts2q.toQuery(\"field\", new SynonymGraphFilter(in, map, true))));\n\n    IOUtils.close(r, dir);\n  }\n\n  private static Token token(String term, int posInc, int posLength, int startOffset, int endOffset) {\n    final Token t = new Token(term, startOffset, endOffset);\n    t.setPositionIncrement(posInc);\n    t.setPositionLength(posLength);\n    return t;\n  }\n  */\n\n  private String randomNonEmptyString() {\n    while(true) {\n      String s = TestUtil.randomUnicodeString(random()).trim();\n      //String s = TestUtil.randomSimpleString(random()).trim();\n      if (s.length() != 0 && s.indexOf('\\u0000') == -1) {\n        return s;\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["24a98f5fdd23e04f85819dbc63b47a12f7c44311"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","24a98f5fdd23e04f85819dbc63b47a12f7c44311"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["24a98f5fdd23e04f85819dbc63b47a12f7c44311","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["cd5edd1f2b162a5cfa08efd17851a07373a96817","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}