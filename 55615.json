{"path":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","commits":[{"id":"78e689a3b60e84c75dc6dd7b181a71fc19ef8482","date":1591689554,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            Runnable finalizer = writer.merge(dataOut, dataOut, dataOut, docMaps, bkdReaders);\n            if (finalizer != null) {\n              indexFPs.put(fieldInfo.name, dataOut.getFilePointer());\n              finalizer.run();\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb94bf667d51f9c390c99d97afb36b7caab6b6e9","date":1599548621,"type":3,"author":"Ignacio Vera","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          BKDConfig config = new BKDConfig(fieldInfo.getPointDimensionCount(),\n                                           fieldInfo.getPointIndexDimensionCount(),\n                                           fieldInfo.getPointNumBytes(),\n                                           maxPointsInLeafNode);\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                config,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            Runnable finalizer = writer.merge(dataOut, dataOut, dataOut, docMaps, bkdReaders);\n            if (finalizer != null) {\n              indexFPs.put(fieldInfo.name, dataOut.getFilePointer());\n              finalizer.run();\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            Runnable finalizer = writer.merge(dataOut, dataOut, dataOut, docMaps, bkdReaders);\n            if (finalizer != null) {\n              indexFPs.put(fieldInfo.name, dataOut.getFilePointer());\n              finalizer.run();\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bb94bf667d51f9c390c99d97afb36b7caab6b6e9":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bb94bf667d51f9c390c99d97afb36b7caab6b6e9"]},"commit2Childs":{"bb94bf667d51f9c390c99d97afb36b7caab6b6e9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["bb94bf667d51f9c390c99d97afb36b7caab6b6e9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}