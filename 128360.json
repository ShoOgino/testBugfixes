{"path":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","commits":[{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(StoredDocument,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(StoredDocument doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a0c04b71951333291abc7f317109a6a5957bd28","date":1457097827,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, new Comparator<TextFragment>() {//TODO make TextFragment Comparable\n          @Override\n          public int compare(TextFragment arg0, TextFragment arg1) {\n            return Float.compare(arg1.getScore(), arg0.getScore());\n          }\n        });\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1807cf7ff48453a48bc28608f557e16b6a1f7fa8","date":1464474872,"type":3,"author":"Jan Høydahl","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da8a02bef7458089240404614139b53c9f875ec7","date":1464597207,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5b8ee93140fd0efef7e101786e3ed5160a700b5f","date":1464820111,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        req.getParams().getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (req.getParams().getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2a846001bbec14e653bf0dda99d628b86ee8fd8","date":1486061499,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS, DEFAULT_MAX_CHARS);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS,\n        Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f15af35d55d70c34451f9df5edeaeff6b31f8cbe","date":1519625627,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS, DEFAULT_MAX_CHARS);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    try (OffsetWindowTokenFilter tvWindowStream = (tvStream != null && fieldValues.size() > 1) ? new OffsetWindowTokenFilter(tvStream) : null) {\n\n      for (String thisText : fieldValues) {\n        if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n          break;\n        }\n\n        TokenStream tstream;\n        if (tvWindowStream != null) {\n          // if we have a multi-valued field with term vectors, then get the next offset window\n          tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n        } else if (tvStream != null) {\n          tstream = tvStream; // single-valued with term vectors\n        } else {\n          // fall back to analyzer\n          tstream = createAnalyzerTStream(schemaField, thisText);\n        }\n\n        Highlighter highlighter;\n        if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n          // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n          // needs to implement reset() efficiently.\n\n          //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n          //  It should be okay if OffsetLimit won't get applied in this case.\n          final TokenStream tempTokenStream;\n          if (tstream != tvStream) {\n            if (maxCharsToAnalyze >= thisText.length()) {\n              tempTokenStream = new CachingTokenFilter(tstream);\n            } else {\n              tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n            }\n          } else {\n            tempTokenStream = tstream;\n          }\n\n          // get highlighter\n          highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n          // if the CachingTokenFilter was consumed then use it going forward.\n          if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n            tstream = tempTokenStream;\n          }\n          //tstream.reset(); not needed; getBestTextFragments will reset it.\n        } else {\n          // use \"the old way\"\n          highlighter = getHighlighter(query, fieldName, req);\n        }\n\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n        maxCharsToAnalyze -= thisText.length();\n\n        // Highlight!\n        try {\n          TextFragment[] bestTextFragments =\n              highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n          for (TextFragment bestTextFragment : bestTextFragments) {\n            if (bestTextFragment == null)//can happen via mergeContiguousFragments\n              continue;\n            // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n            if (bestTextFragment.getScore() > 0 || preserveMulti) {\n              frags.add(bestTextFragment);\n              if (bestTextFragment.getScore() > 0)\n                --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n            }\n          }\n        } catch (InvalidTokenOffsetsException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n        }\n      }//end field value loop\n    }\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS, DEFAULT_MAX_CHARS);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    final OffsetWindowTokenFilter tvWindowStream;\n    if (tvStream != null && fieldValues.size() > 1) {\n      tvWindowStream = new OffsetWindowTokenFilter(tvStream);\n    } else {\n      tvWindowStream = null;\n    }\n\n    for (String thisText : fieldValues) {\n      if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n        break;\n      }\n\n      TokenStream tstream;\n      if (tvWindowStream != null) {\n        // if we have a multi-valued field with term vectors, then get the next offset window\n        tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n      } else if (tvStream != null) {\n        tstream = tvStream; // single-valued with term vectors\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schemaField, thisText);\n      }\n\n      Highlighter highlighter;\n      if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n        // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n        // needs to implement reset() efficiently.\n\n        //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n        //  It should be okay if OffsetLimit won't get applied in this case.\n        final TokenStream tempTokenStream;\n        if (tstream != tvStream) {\n          if (maxCharsToAnalyze >= thisText.length()) {\n            tempTokenStream = new CachingTokenFilter(tstream);\n          } else {\n            tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n          }\n        } else {\n          tempTokenStream = tstream;\n        }\n\n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n        // if the CachingTokenFilter was consumed then use it going forward.\n        if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n          tstream = tempTokenStream;\n        }\n        //tstream.reset(); not needed; getBestTextFragments will reset it.\n      } else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n\n      highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      maxCharsToAnalyze -= thisText.length();\n\n      // Highlight!\n      try {\n        TextFragment[] bestTextFragments =\n            highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (TextFragment bestTextFragment : bestTextFragments) {\n          if (bestTextFragment == null)//can happen via mergeContiguousFragments\n            continue;\n          // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n          if (bestTextFragment.getScore() > 0 || preserveMulti) {\n            frags.add(bestTextFragment);\n            if (bestTextFragment.getScore() > 0)\n              --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }//end field value loop\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a9aeb4a98b03660f065aa31f6b3f2251a12b613","date":1581405488,"type":5,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(SolrDocument,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Document,int,SchemaField,Query,IndexReader,SolrQueryRequest).mjava","sourceNew":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(SolrDocument doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS, DEFAULT_MAX_CHARS);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    try (OffsetWindowTokenFilter tvWindowStream = (tvStream != null && fieldValues.size() > 1) ? new OffsetWindowTokenFilter(tvStream) : null) {\n\n      for (String thisText : fieldValues) {\n        if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n          break;\n        }\n\n        TokenStream tstream;\n        if (tvWindowStream != null) {\n          // if we have a multi-valued field with term vectors, then get the next offset window\n          tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n        } else if (tvStream != null) {\n          tstream = tvStream; // single-valued with term vectors\n        } else {\n          // fall back to analyzer\n          tstream = createAnalyzerTStream(schemaField, thisText);\n        }\n\n        Highlighter highlighter;\n        if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n          // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n          // needs to implement reset() efficiently.\n\n          //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n          //  It should be okay if OffsetLimit won't get applied in this case.\n          final TokenStream tempTokenStream;\n          if (tstream != tvStream) {\n            if (maxCharsToAnalyze >= thisText.length()) {\n              tempTokenStream = new CachingTokenFilter(tstream);\n            } else {\n              tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n            }\n          } else {\n            tempTokenStream = tstream;\n          }\n\n          // get highlighter\n          highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n          // if the CachingTokenFilter was consumed then use it going forward.\n          if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n            tstream = tempTokenStream;\n          }\n          //tstream.reset(); not needed; getBestTextFragments will reset it.\n        } else {\n          // use \"the old way\"\n          highlighter = getHighlighter(query, fieldName, req);\n        }\n\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n        maxCharsToAnalyze -= thisText.length();\n\n        // Highlight!\n        try {\n          TextFragment[] bestTextFragments =\n              highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n          for (TextFragment bestTextFragment : bestTextFragments) {\n            if (bestTextFragment == null)//can happen via mergeContiguousFragments\n              continue;\n            // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n            if (bestTextFragment.getScore() > 0 || preserveMulti) {\n              frags.add(bestTextFragment);\n              if (bestTextFragment.getScore() > 0)\n                --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n            }\n          }\n        } catch (InvalidTokenOffsetsException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n        }\n      }//end field value loop\n    }\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","sourceOld":"  /** Highlights and returns the highlight object for this field -- a String[] by default. Null if none. */\n  @SuppressWarnings(\"unchecked\")\n  protected Object doHighlightingByHighlighter(Document doc, int docId, SchemaField schemaField, Query query,\n                                               IndexReader reader, SolrQueryRequest req) throws IOException {\n    final SolrParams params = req.getParams();\n    final String fieldName = schemaField.getName();\n\n    final int mvToExamine =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n            (schemaField.multiValued()) ? Integer.MAX_VALUE : 1);\n\n    // Technically this is the max *fragments* (snippets), not max values:\n    int mvToMatch =\n        params.getFieldInt(fieldName, HighlightParams.MAX_MULTIVALUED_TO_MATCH, Integer.MAX_VALUE);\n    if (mvToExamine <= 0 || mvToMatch <= 0) {\n      return null;\n    }\n\n    int maxCharsToAnalyze = params.getFieldInt(fieldName,\n        HighlightParams.MAX_CHARS, DEFAULT_MAX_CHARS);\n    if (maxCharsToAnalyze < 0) {//e.g. -1\n      maxCharsToAnalyze = Integer.MAX_VALUE;\n    }\n\n    List<String> fieldValues = getFieldValues(doc, fieldName, mvToExamine, maxCharsToAnalyze, req);\n    if (fieldValues.isEmpty()) {\n      return null;\n    }\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    List<TextFragment> frags = new ArrayList<>();\n\n    //Try term vectors, which is faster\n    //  note: offsets are minimally sufficient for this HL.\n    final Fields tvFields = schemaField.storeTermOffsets() ? reader.getTermVectors(docId) : null;\n    final TokenStream tvStream =\n        TokenSources.getTermVectorTokenStreamOrNull(fieldName, tvFields, maxCharsToAnalyze - 1);\n    //  We need to wrap in OffsetWindowTokenFilter if multi-valued\n    try (OffsetWindowTokenFilter tvWindowStream = (tvStream != null && fieldValues.size() > 1) ? new OffsetWindowTokenFilter(tvStream) : null) {\n\n      for (String thisText : fieldValues) {\n        if (mvToMatch <= 0 || maxCharsToAnalyze <= 0) {\n          break;\n        }\n\n        TokenStream tstream;\n        if (tvWindowStream != null) {\n          // if we have a multi-valued field with term vectors, then get the next offset window\n          tstream = tvWindowStream.advanceToNextWindowOfLength(thisText.length());\n        } else if (tvStream != null) {\n          tstream = tvStream; // single-valued with term vectors\n        } else {\n          // fall back to analyzer\n          tstream = createAnalyzerTStream(schemaField, thisText);\n        }\n\n        Highlighter highlighter;\n        if (params.getFieldBool(fieldName, HighlightParams.USE_PHRASE_HIGHLIGHTER, true)) {\n          // We're going to call getPhraseHighlighter and it might consume the tokenStream. If it does, the tokenStream\n          // needs to implement reset() efficiently.\n\n          //If the tokenStream is right from the term vectors, then CachingTokenFilter is unnecessary.\n          //  It should be okay if OffsetLimit won't get applied in this case.\n          final TokenStream tempTokenStream;\n          if (tstream != tvStream) {\n            if (maxCharsToAnalyze >= thisText.length()) {\n              tempTokenStream = new CachingTokenFilter(tstream);\n            } else {\n              tempTokenStream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n            }\n          } else {\n            tempTokenStream = tstream;\n          }\n\n          // get highlighter\n          highlighter = getPhraseHighlighter(query, fieldName, req, tempTokenStream);\n\n          // if the CachingTokenFilter was consumed then use it going forward.\n          if (tempTokenStream instanceof CachingTokenFilter && ((CachingTokenFilter) tempTokenStream).isCached()) {\n            tstream = tempTokenStream;\n          }\n          //tstream.reset(); not needed; getBestTextFragments will reset it.\n        } else {\n          // use \"the old way\"\n          highlighter = getHighlighter(query, fieldName, req);\n        }\n\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n        maxCharsToAnalyze -= thisText.length();\n\n        // Highlight!\n        try {\n          TextFragment[] bestTextFragments =\n              highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n          for (TextFragment bestTextFragment : bestTextFragments) {\n            if (bestTextFragment == null)//can happen via mergeContiguousFragments\n              continue;\n            // normally we want a score (must be highlighted), but if preserveMulti then we return a snippet regardless.\n            if (bestTextFragment.getScore() > 0 || preserveMulti) {\n              frags.add(bestTextFragment);\n              if (bestTextFragment.getScore() > 0)\n                --mvToMatch; // note: limits fragments (for multi-valued fields), not quite the number of values\n            }\n          }\n        } catch (InvalidTokenOffsetsException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n        }\n      }//end field value loop\n    }\n\n    // Put the fragments onto the Solr response (docSummaries)\n    if (frags.size() > 0) {\n      // sort such that the fragments with the highest score come first\n      if (!preserveMulti) {\n        Collections.sort(frags, (arg0, arg1) -> Float.compare(arg1.getScore(), arg0.getScore()));\n      }\n\n      // Truncate list to hl.snippets, but not when hl.preserveMulti\n      if (frags.size() > numFragments && !preserveMulti) {\n        frags = frags.subList(0, numFragments);\n      }\n      return getResponseForFragments(frags, req);\n    }\n    return null;//no highlights for this field\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"da8a02bef7458089240404614139b53c9f875ec7":["3a0c04b71951333291abc7f317109a6a5957bd28","1807cf7ff48453a48bc28608f557e16b6a1f7fa8"],"f2a846001bbec14e653bf0dda99d628b86ee8fd8":["1807cf7ff48453a48bc28608f557e16b6a1f7fa8"],"1807cf7ff48453a48bc28608f557e16b6a1f7fa8":["3a0c04b71951333291abc7f317109a6a5957bd28"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f15af35d55d70c34451f9df5edeaeff6b31f8cbe":["f2a846001bbec14e653bf0dda99d628b86ee8fd8"],"5b8ee93140fd0efef7e101786e3ed5160a700b5f":["3a0c04b71951333291abc7f317109a6a5957bd28","1807cf7ff48453a48bc28608f557e16b6a1f7fa8"],"1a9aeb4a98b03660f065aa31f6b3f2251a12b613":["f15af35d55d70c34451f9df5edeaeff6b31f8cbe"],"3a0c04b71951333291abc7f317109a6a5957bd28":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["3a0c04b71951333291abc7f317109a6a5957bd28","1807cf7ff48453a48bc28608f557e16b6a1f7fa8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1a9aeb4a98b03660f065aa31f6b3f2251a12b613"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"da8a02bef7458089240404614139b53c9f875ec7":[],"1807cf7ff48453a48bc28608f557e16b6a1f7fa8":["da8a02bef7458089240404614139b53c9f875ec7","f2a846001bbec14e653bf0dda99d628b86ee8fd8","5b8ee93140fd0efef7e101786e3ed5160a700b5f","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"f2a846001bbec14e653bf0dda99d628b86ee8fd8":["f15af35d55d70c34451f9df5edeaeff6b31f8cbe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"f15af35d55d70c34451f9df5edeaeff6b31f8cbe":["1a9aeb4a98b03660f065aa31f6b3f2251a12b613"],"5b8ee93140fd0efef7e101786e3ed5160a700b5f":[],"3a0c04b71951333291abc7f317109a6a5957bd28":["da8a02bef7458089240404614139b53c9f875ec7","1807cf7ff48453a48bc28608f557e16b6a1f7fa8","5b8ee93140fd0efef7e101786e3ed5160a700b5f","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"1a9aeb4a98b03660f065aa31f6b3f2251a12b613":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["3a0c04b71951333291abc7f317109a6a5957bd28"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["da8a02bef7458089240404614139b53c9f875ec7","5b8ee93140fd0efef7e101786e3ed5160a700b5f","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}