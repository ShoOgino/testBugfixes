{"path":"lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy#testForcedMergesRespectSegSize().mjava","commits":[{"id":"56fb5e4e4b239474721e13b4cd9542ea2d215451","date":1529091182,"type":0,"author":"Erick","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy#testForcedMergesRespectSegSize().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-7976 makes findForceMergeDeletes and findForcedDeletes respect max segment size by default,\n  // so insure that this works.\n  public void testForcedMergesRespectSegSize() throws Exception {\n    final Directory dir = newDirectory();\n    final IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n\n    // Empirically, 100 docs the size below give us segments of 3,330 bytes. It's not all that reliable in terms\n    // of how big a segment _can_ get, so set it to prevent merges on commit.\n    double mbSize = 0.004;\n    long maxSegBytes = (long) ((1024.0 * 1024.0)); // fudge it up, we're trying to catch egregious errors and segbytes don't really reflect the number for original merges.\n    tmp.setMaxMergedSegmentMB(mbSize);\n    conf.setMaxBufferedDocs(100);\n    conf.setMergePolicy(tmp);\n\n    final IndexWriter w = new IndexWriter(dir, conf);\n\n    final int numDocs = atLeast(2400);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit();\n\n    // These should be no-ops on an index with no deletions and segments are pretty big.\n    List<String> segNamesBefore = getSegmentNames(w);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);  // There should have been no merges.\n\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentsInExpectations(w, segNamesBefore, true);\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n\n\n    // Delete 12-17% of each segment and expungeDeletes. This should result in:\n    // > the same number of segments as before.\n    // > no segments larger than maxSegmentSize.\n    // > no deleted docs left.\n    int remainingDocs = numDocs - deletePctDocsFromEachSeg(w, random().nextInt(5) + 12, true);\n    w.forceMergeDeletes();\n    w.commit();\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n    assertFalse(\"There should be no deleted docs in the index.\", w.hasDeletions());\n\n    // Check that deleting _fewer_ than 10% doesn't merge inappropriately. Nothing should be merged since no segment\n    // has had more than 10% of its docs deleted.\n    segNamesBefore = getSegmentNames(w);\n    int deletedThisPass = deletePctDocsFromEachSeg(w, random().nextInt(4) + 3, false);\n    w.forceMergeDeletes();\n    remainingDocs -= deletedThisPass;\n    checkSegmentsInExpectations(w, segNamesBefore, false); // There should have been no merges\n    assertEquals(\"NumDocs should reflect removed documents \", remainingDocs, w.numDocs());\n    assertTrue(\"Should still be deleted docs in the index\", w.numDocs() < w.maxDoc());\n\n    // This time, forceMerge. By default this should respect max segment size.\n    // Will change for LUCENE-8236\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n\n    // Now forceMerge down to one segment, there should be exactly remainingDocs in exactly one segment.\n    w.forceMerge(1);\n    assertEquals(\"There should be exaclty one segment now\", 1, w.getSegmentCount());\n    assertEquals(\"maxDoc and numDocs should be identical\", w.numDocs(), w.maxDoc());\n    assertEquals(\"There should be an exact number of documents in that one segment\", remainingDocs, w.numDocs());\n\n    // Delete 5% and expunge, should be no change.\n    segNamesBefore = getSegmentNames(w);\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 1, false);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertTrue(\"The segment should have deleted documents\", w.numDocs() < w.maxDoc());\n\n    w.forceMerge(1); // back to one segment so deletePctDocsFromEachSeg still works\n\n    // Test singleton merge for expungeDeletes\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 20, true);\n    w.forceMergeDeletes();\n\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertEquals(\"The segment should have no deleted documents\", w.numDocs(), w.maxDoc());\n\n\n    // sanity check, at this point we should have an over`-large segment, we know we have exactly one.\n    assertTrue(\"Our single segment should have quite a few docs\", w.numDocs() > 1_000);\n\n    // Delete 60% of the documents and then add a few more docs and commit. This should \"singleton merge\" the large segment\n    // created above. 60% leaves some wriggle room, LUCENE-8263 will change this assumption and should be tested\n    // when we deal with that JIRA.\n\n    deletedThisPass = deletePctDocsFromEachSeg(w, (w.numDocs() * 60) / 100, true);\n    remainingDocs -= deletedThisPass;\n\n    for (int i = 0; i < 50; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i + numDocs, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit(); // want to trigger merge no matter what.\n\n    assertEquals(\"There should be exactly one very large and one small segment\", w.segmentInfos.size(), 2);\n    SegmentCommitInfo info0 = w.segmentInfos.info(0);\n    SegmentCommitInfo info1 = w.segmentInfos.info(1);\n    int largeSegDocCount = Math.max(info0.info.maxDoc(), info1.info.maxDoc());\n    int smallSegDocCount = Math.min(info0.info.maxDoc(), info1.info.maxDoc());\n    assertEquals(\"The large segment should have a bunch of docs\", largeSegDocCount, remainingDocs);\n    assertEquals(\"Small segment shold have fewer docs\", smallSegDocCount, 50);\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["631ea3d1607299c59f33edef140ffc19a81f07a0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":0,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy#testForcedMergesRespectSegSize().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-7976 makes findForceMergeDeletes and findForcedDeletes respect max segment size by default,\n  // so insure that this works.\n  public void testForcedMergesRespectSegSize() throws Exception {\n    final Directory dir = newDirectory();\n    final IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n\n    // Empirically, 100 docs the size below give us segments of 3,330 bytes. It's not all that reliable in terms\n    // of how big a segment _can_ get, so set it to prevent merges on commit.\n    double mbSize = 0.004;\n    long maxSegBytes = (long) ((1024.0 * 1024.0)); // fudge it up, we're trying to catch egregious errors and segbytes don't really reflect the number for original merges.\n    tmp.setMaxMergedSegmentMB(mbSize);\n    conf.setMaxBufferedDocs(100);\n    conf.setMergePolicy(tmp);\n\n    final IndexWriter w = new IndexWriter(dir, conf);\n\n    final int numDocs = atLeast(2400);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit();\n\n    // These should be no-ops on an index with no deletions and segments are pretty big.\n    List<String> segNamesBefore = getSegmentNames(w);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);  // There should have been no merges.\n\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentsInExpectations(w, segNamesBefore, true);\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n\n\n    // Delete 12-17% of each segment and expungeDeletes. This should result in:\n    // > the same number of segments as before.\n    // > no segments larger than maxSegmentSize.\n    // > no deleted docs left.\n    int remainingDocs = numDocs - deletePctDocsFromEachSeg(w, random().nextInt(5) + 12, true);\n    w.forceMergeDeletes();\n    w.commit();\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n    assertFalse(\"There should be no deleted docs in the index.\", w.hasDeletions());\n\n    // Check that deleting _fewer_ than 10% doesn't merge inappropriately. Nothing should be merged since no segment\n    // has had more than 10% of its docs deleted.\n    segNamesBefore = getSegmentNames(w);\n    int deletedThisPass = deletePctDocsFromEachSeg(w, random().nextInt(4) + 3, false);\n    w.forceMergeDeletes();\n    remainingDocs -= deletedThisPass;\n    checkSegmentsInExpectations(w, segNamesBefore, false); // There should have been no merges\n    assertEquals(\"NumDocs should reflect removed documents \", remainingDocs, w.numDocs());\n    assertTrue(\"Should still be deleted docs in the index\", w.numDocs() < w.maxDoc());\n\n    // This time, forceMerge. By default this should respect max segment size.\n    // Will change for LUCENE-8236\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n\n    // Now forceMerge down to one segment, there should be exactly remainingDocs in exactly one segment.\n    w.forceMerge(1);\n    assertEquals(\"There should be exaclty one segment now\", 1, w.getSegmentCount());\n    assertEquals(\"maxDoc and numDocs should be identical\", w.numDocs(), w.maxDoc());\n    assertEquals(\"There should be an exact number of documents in that one segment\", remainingDocs, w.numDocs());\n\n    // Delete 5% and expunge, should be no change.\n    segNamesBefore = getSegmentNames(w);\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 1, false);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertTrue(\"The segment should have deleted documents\", w.numDocs() < w.maxDoc());\n\n    w.forceMerge(1); // back to one segment so deletePctDocsFromEachSeg still works\n\n    // Test singleton merge for expungeDeletes\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 20, true);\n    w.forceMergeDeletes();\n\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertEquals(\"The segment should have no deleted documents\", w.numDocs(), w.maxDoc());\n\n\n    // sanity check, at this point we should have an over`-large segment, we know we have exactly one.\n    assertTrue(\"Our single segment should have quite a few docs\", w.numDocs() > 1_000);\n\n    // Delete 60% of the documents and then add a few more docs and commit. This should \"singleton merge\" the large segment\n    // created above. 60% leaves some wriggle room, LUCENE-8263 will change this assumption and should be tested\n    // when we deal with that JIRA.\n\n    deletedThisPass = deletePctDocsFromEachSeg(w, (w.numDocs() * 60) / 100, true);\n    remainingDocs -= deletedThisPass;\n\n    for (int i = 0; i < 50; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i + numDocs, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit(); // want to trigger merge no matter what.\n\n    assertEquals(\"There should be exactly one very large and one small segment\", w.segmentInfos.size(), 2);\n    SegmentCommitInfo info0 = w.segmentInfos.info(0);\n    SegmentCommitInfo info1 = w.segmentInfos.info(1);\n    int largeSegDocCount = Math.max(info0.info.maxDoc(), info1.info.maxDoc());\n    int smallSegDocCount = Math.min(info0.info.maxDoc(), info1.info.maxDoc());\n    assertEquals(\"The large segment should have a bunch of docs\", largeSegDocCount, remainingDocs);\n    assertEquals(\"Small segment shold have fewer docs\", smallSegDocCount, 50);\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":0,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy#testForcedMergesRespectSegSize().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-7976 makes findForceMergeDeletes and findForcedDeletes respect max segment size by default,\n  // so insure that this works.\n  public void testForcedMergesRespectSegSize() throws Exception {\n    final Directory dir = newDirectory();\n    final IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n\n    // Empirically, 100 docs the size below give us segments of 3,330 bytes. It's not all that reliable in terms\n    // of how big a segment _can_ get, so set it to prevent merges on commit.\n    double mbSize = 0.004;\n    long maxSegBytes = (long) ((1024.0 * 1024.0)); // fudge it up, we're trying to catch egregious errors and segbytes don't really reflect the number for original merges.\n    tmp.setMaxMergedSegmentMB(mbSize);\n    conf.setMaxBufferedDocs(100);\n    conf.setMergePolicy(tmp);\n\n    final IndexWriter w = new IndexWriter(dir, conf);\n\n    final int numDocs = atLeast(2400);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit();\n\n    // These should be no-ops on an index with no deletions and segments are pretty big.\n    List<String> segNamesBefore = getSegmentNames(w);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);  // There should have been no merges.\n\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentsInExpectations(w, segNamesBefore, true);\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n\n\n    // Delete 12-17% of each segment and expungeDeletes. This should result in:\n    // > the same number of segments as before.\n    // > no segments larger than maxSegmentSize.\n    // > no deleted docs left.\n    int remainingDocs = numDocs - deletePctDocsFromEachSeg(w, random().nextInt(5) + 12, true);\n    w.forceMergeDeletes();\n    w.commit();\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n    assertFalse(\"There should be no deleted docs in the index.\", w.hasDeletions());\n\n    // Check that deleting _fewer_ than 10% doesn't merge inappropriately. Nothing should be merged since no segment\n    // has had more than 10% of its docs deleted.\n    segNamesBefore = getSegmentNames(w);\n    int deletedThisPass = deletePctDocsFromEachSeg(w, random().nextInt(4) + 3, false);\n    w.forceMergeDeletes();\n    remainingDocs -= deletedThisPass;\n    checkSegmentsInExpectations(w, segNamesBefore, false); // There should have been no merges\n    assertEquals(\"NumDocs should reflect removed documents \", remainingDocs, w.numDocs());\n    assertTrue(\"Should still be deleted docs in the index\", w.numDocs() < w.maxDoc());\n\n    // This time, forceMerge. By default this should respect max segment size.\n    // Will change for LUCENE-8236\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n\n    // Now forceMerge down to one segment, there should be exactly remainingDocs in exactly one segment.\n    w.forceMerge(1);\n    assertEquals(\"There should be exaclty one segment now\", 1, w.getSegmentCount());\n    assertEquals(\"maxDoc and numDocs should be identical\", w.numDocs(), w.maxDoc());\n    assertEquals(\"There should be an exact number of documents in that one segment\", remainingDocs, w.numDocs());\n\n    // Delete 5% and expunge, should be no change.\n    segNamesBefore = getSegmentNames(w);\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 1, false);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertTrue(\"The segment should have deleted documents\", w.numDocs() < w.maxDoc());\n\n    w.forceMerge(1); // back to one segment so deletePctDocsFromEachSeg still works\n\n    // Test singleton merge for expungeDeletes\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 20, true);\n    w.forceMergeDeletes();\n\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertEquals(\"The segment should have no deleted documents\", w.numDocs(), w.maxDoc());\n\n\n    // sanity check, at this point we should have an over`-large segment, we know we have exactly one.\n    assertTrue(\"Our single segment should have quite a few docs\", w.numDocs() > 1_000);\n\n    // Delete 60% of the documents and then add a few more docs and commit. This should \"singleton merge\" the large segment\n    // created above. 60% leaves some wriggle room, LUCENE-8263 will change this assumption and should be tested\n    // when we deal with that JIRA.\n\n    deletedThisPass = deletePctDocsFromEachSeg(w, (w.numDocs() * 60) / 100, true);\n    remainingDocs -= deletedThisPass;\n\n    for (int i = 0; i < 50; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i + numDocs, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit(); // want to trigger merge no matter what.\n\n    assertEquals(\"There should be exactly one very large and one small segment\", w.segmentInfos.size(), 2);\n    SegmentCommitInfo info0 = w.segmentInfos.info(0);\n    SegmentCommitInfo info1 = w.segmentInfos.info(1);\n    int largeSegDocCount = Math.max(info0.info.maxDoc(), info1.info.maxDoc());\n    int smallSegDocCount = Math.min(info0.info.maxDoc(), info1.info.maxDoc());\n    assertEquals(\"The large segment should have a bunch of docs\", largeSegDocCount, remainingDocs);\n    assertEquals(\"Small segment shold have fewer docs\", smallSegDocCount, 50);\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"631ea3d1607299c59f33edef140ffc19a81f07a0","date":1532450367,"type":3,"author":"Nhat Nguyen","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy#testForcedMergesRespectSegSize().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy#testForcedMergesRespectSegSize().mjava","sourceNew":"  // LUCENE-7976 makes findForceMergeDeletes and findForcedDeletes respect max segment size by default,\n  // so insure that this works.\n  public void testForcedMergesRespectSegSize() throws Exception {\n    final Directory dir = newDirectory();\n    final IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n\n    // Empirically, 100 docs the size below give us segments of 3,330 bytes. It's not all that reliable in terms\n    // of how big a segment _can_ get, so set it to prevent merges on commit.\n    double mbSize = 0.004;\n    long maxSegBytes = (long) ((1024.0 * 1024.0)); // fudge it up, we're trying to catch egregious errors and segbytes don't really reflect the number for original merges.\n    tmp.setMaxMergedSegmentMB(mbSize);\n    conf.setMaxBufferedDocs(100);\n    conf.setMergePolicy(tmp);\n\n    final IndexWriter w = new IndexWriter(dir, conf);\n\n    final int numDocs = atLeast(2400);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit();\n\n    // These should be no-ops on an index with no deletions and segments are pretty big.\n    List<String> segNamesBefore = getSegmentNames(w);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);  // There should have been no merges.\n\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentsInExpectations(w, segNamesBefore, true);\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n\n\n    // Delete 12-17% of each segment and expungeDeletes. This should result in:\n    // > the same number of segments as before.\n    // > no segments larger than maxSegmentSize.\n    // > no deleted docs left.\n    int remainingDocs = numDocs - deletePctDocsFromEachSeg(w, random().nextInt(5) + 12, true);\n    w.forceMergeDeletes();\n    w.commit();\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n    assertFalse(\"There should be no deleted docs in the index.\", w.hasDeletions());\n\n    // Check that deleting _fewer_ than 10% doesn't merge inappropriately. Nothing should be merged since no segment\n    // has had more than 10% of its docs deleted.\n    segNamesBefore = getSegmentNames(w);\n    int deletedThisPass = deletePctDocsFromEachSeg(w, random().nextInt(4) + 3, false);\n    w.forceMergeDeletes();\n    remainingDocs -= deletedThisPass;\n    checkSegmentsInExpectations(w, segNamesBefore, false); // There should have been no merges\n    assertEquals(\"NumDocs should reflect removed documents \", remainingDocs, w.numDocs());\n    assertTrue(\"Should still be deleted docs in the index\", w.numDocs() < w.maxDoc());\n\n    // This time, forceMerge. By default this should respect max segment size.\n    // Will change for LUCENE-8236\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n\n    // Now forceMerge down to one segment, there should be exactly remainingDocs in exactly one segment.\n    w.forceMerge(1);\n    assertEquals(\"There should be exaclty one segment now\", 1, w.getSegmentCount());\n    assertEquals(\"maxDoc and numDocs should be identical\", w.numDocs(), w.maxDoc());\n    assertEquals(\"There should be an exact number of documents in that one segment\", remainingDocs, w.numDocs());\n\n    // Delete 5% and expunge, should be no change.\n    segNamesBefore = getSegmentNames(w);\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 1, false);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertTrue(\"The segment should have deleted documents\", w.numDocs() < w.maxDoc());\n\n    w.forceMerge(1); // back to one segment so deletePctDocsFromEachSeg still works\n\n    // Test singleton merge for expungeDeletes\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 20, true);\n    w.forceMergeDeletes();\n\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertEquals(\"The segment should have no deleted documents\", w.numDocs(), w.maxDoc());\n\n\n    // sanity check, at this point we should have an over`-large segment, we know we have exactly one.\n    assertTrue(\"Our single segment should have quite a few docs\", w.numDocs() > 1_000);\n\n    // Delete 60% of the documents and then add a few more docs and commit. This should \"singleton merge\" the large segment\n    // created above. 60% leaves some wriggle room, LUCENE-8263 will change this assumption and should be tested\n    // when we deal with that JIRA.\n\n    deletedThisPass = deletePctDocsFromEachSeg(w, (w.numDocs() * 60) / 100, true);\n    remainingDocs -= deletedThisPass;\n\n    for (int i = 0; i < 50; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i + numDocs, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit(); // want to trigger merge no matter what.\n\n    assertEquals(\"There should be exactly one very large and one small segment\", 2, w.listOfSegmentCommitInfos().size());\n    SegmentCommitInfo info0 = w.listOfSegmentCommitInfos().get(0);\n    SegmentCommitInfo info1 = w.listOfSegmentCommitInfos().get(1);\n    int largeSegDocCount = Math.max(info0.info.maxDoc(), info1.info.maxDoc());\n    int smallSegDocCount = Math.min(info0.info.maxDoc(), info1.info.maxDoc());\n    assertEquals(\"The large segment should have a bunch of docs\", largeSegDocCount, remainingDocs);\n    assertEquals(\"Small segment shold have fewer docs\", smallSegDocCount, 50);\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-7976 makes findForceMergeDeletes and findForcedDeletes respect max segment size by default,\n  // so insure that this works.\n  public void testForcedMergesRespectSegSize() throws Exception {\n    final Directory dir = newDirectory();\n    final IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n\n    // Empirically, 100 docs the size below give us segments of 3,330 bytes. It's not all that reliable in terms\n    // of how big a segment _can_ get, so set it to prevent merges on commit.\n    double mbSize = 0.004;\n    long maxSegBytes = (long) ((1024.0 * 1024.0)); // fudge it up, we're trying to catch egregious errors and segbytes don't really reflect the number for original merges.\n    tmp.setMaxMergedSegmentMB(mbSize);\n    conf.setMaxBufferedDocs(100);\n    conf.setMergePolicy(tmp);\n\n    final IndexWriter w = new IndexWriter(dir, conf);\n\n    final int numDocs = atLeast(2400);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit();\n\n    // These should be no-ops on an index with no deletions and segments are pretty big.\n    List<String> segNamesBefore = getSegmentNames(w);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);  // There should have been no merges.\n\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentsInExpectations(w, segNamesBefore, true);\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n\n\n    // Delete 12-17% of each segment and expungeDeletes. This should result in:\n    // > the same number of segments as before.\n    // > no segments larger than maxSegmentSize.\n    // > no deleted docs left.\n    int remainingDocs = numDocs - deletePctDocsFromEachSeg(w, random().nextInt(5) + 12, true);\n    w.forceMergeDeletes();\n    w.commit();\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n    assertFalse(\"There should be no deleted docs in the index.\", w.hasDeletions());\n\n    // Check that deleting _fewer_ than 10% doesn't merge inappropriately. Nothing should be merged since no segment\n    // has had more than 10% of its docs deleted.\n    segNamesBefore = getSegmentNames(w);\n    int deletedThisPass = deletePctDocsFromEachSeg(w, random().nextInt(4) + 3, false);\n    w.forceMergeDeletes();\n    remainingDocs -= deletedThisPass;\n    checkSegmentsInExpectations(w, segNamesBefore, false); // There should have been no merges\n    assertEquals(\"NumDocs should reflect removed documents \", remainingDocs, w.numDocs());\n    assertTrue(\"Should still be deleted docs in the index\", w.numDocs() < w.maxDoc());\n\n    // This time, forceMerge. By default this should respect max segment size.\n    // Will change for LUCENE-8236\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentSizeNotExceeded(w.segmentInfos, maxSegBytes);\n\n    // Now forceMerge down to one segment, there should be exactly remainingDocs in exactly one segment.\n    w.forceMerge(1);\n    assertEquals(\"There should be exaclty one segment now\", 1, w.getSegmentCount());\n    assertEquals(\"maxDoc and numDocs should be identical\", w.numDocs(), w.maxDoc());\n    assertEquals(\"There should be an exact number of documents in that one segment\", remainingDocs, w.numDocs());\n\n    // Delete 5% and expunge, should be no change.\n    segNamesBefore = getSegmentNames(w);\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 1, false);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertTrue(\"The segment should have deleted documents\", w.numDocs() < w.maxDoc());\n\n    w.forceMerge(1); // back to one segment so deletePctDocsFromEachSeg still works\n\n    // Test singleton merge for expungeDeletes\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 20, true);\n    w.forceMergeDeletes();\n\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertEquals(\"The segment should have no deleted documents\", w.numDocs(), w.maxDoc());\n\n\n    // sanity check, at this point we should have an over`-large segment, we know we have exactly one.\n    assertTrue(\"Our single segment should have quite a few docs\", w.numDocs() > 1_000);\n\n    // Delete 60% of the documents and then add a few more docs and commit. This should \"singleton merge\" the large segment\n    // created above. 60% leaves some wriggle room, LUCENE-8263 will change this assumption and should be tested\n    // when we deal with that JIRA.\n\n    deletedThisPass = deletePctDocsFromEachSeg(w, (w.numDocs() * 60) / 100, true);\n    remainingDocs -= deletedThisPass;\n\n    for (int i = 0; i < 50; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i + numDocs, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit(); // want to trigger merge no matter what.\n\n    assertEquals(\"There should be exactly one very large and one small segment\", w.segmentInfos.size(), 2);\n    SegmentCommitInfo info0 = w.segmentInfos.info(0);\n    SegmentCommitInfo info1 = w.segmentInfos.info(1);\n    int largeSegDocCount = Math.max(info0.info.maxDoc(), info1.info.maxDoc());\n    int smallSegDocCount = Math.min(info0.info.maxDoc(), info1.info.maxDoc());\n    assertEquals(\"The large segment should have a bunch of docs\", largeSegDocCount, remainingDocs);\n    assertEquals(\"Small segment shold have fewer docs\", smallSegDocCount, 50);\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":["56fb5e4e4b239474721e13b4cd9542ea2d215451"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"feb4029567b43f074ed7b6eb8fb126d355075dfd","date":1544812585,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy#testForcedMergesRespectSegSize().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy#testForcedMergesRespectSegSize().mjava","sourceNew":"  // LUCENE-7976 makes findForceMergeDeletes and findForcedDeletes respect max segment size by default,\n  // so insure that this works.\n  public void testForcedMergesRespectSegSize() throws Exception {\n    final Directory dir = newDirectory();\n    final IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n\n    // Empirically, 100 docs the size below give us segments of 3,330 bytes. It's not all that reliable in terms\n    // of how big a segment _can_ get, so set it to prevent merges on commit.\n    double mbSize = 0.004;\n    long maxSegBytes = (long) ((1024.0 * 1024.0)); // fudge it up, we're trying to catch egregious errors and segbytes don't really reflect the number for original merges.\n    tmp.setMaxMergedSegmentMB(mbSize);\n    conf.setMaxBufferedDocs(100);\n    conf.setMergePolicy(tmp);\n\n    final IndexWriter w = new IndexWriter(dir, conf);\n\n    final int numDocs = atLeast(2400);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit();\n\n    // These should be no-ops on an index with no deletions and segments are pretty big.\n    List<String> segNamesBefore = getSegmentNames(w);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);  // There should have been no merges.\n\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentsInExpectations(w, segNamesBefore, true);\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n\n\n    // Delete 12-17% of each segment and expungeDeletes. This should result in:\n    // > the same number of segments as before.\n    // > no segments larger than maxSegmentSize.\n    // > no deleted docs left.\n    int remainingDocs = numDocs - deletePctDocsFromEachSeg(w, random().nextInt(5) + 12, true);\n    w.forceMergeDeletes();\n    w.commit();\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n    assertFalse(\"There should be no deleted docs in the index.\", w.hasDeletions());\n\n    // Check that deleting _fewer_ than 10% doesn't merge inappropriately. Nothing should be merged since no segment\n    // has had more than 10% of its docs deleted.\n    segNamesBefore = getSegmentNames(w);\n    int deletedThisPass = deletePctDocsFromEachSeg(w, random().nextInt(4) + 3, false);\n    w.forceMergeDeletes();\n    remainingDocs -= deletedThisPass;\n    checkSegmentsInExpectations(w, segNamesBefore, false); // There should have been no merges\n    assertEquals(\"NumDocs should reflect removed documents \", remainingDocs, w.getDocStats().numDocs);\n    assertTrue(\"Should still be deleted docs in the index\", w.getDocStats().numDocs < w.getDocStats().maxDoc);\n\n    // This time, forceMerge. By default this should respect max segment size.\n    // Will change for LUCENE-8236\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n\n    // Now forceMerge down to one segment, there should be exactly remainingDocs in exactly one segment.\n    w.forceMerge(1);\n    assertEquals(\"There should be exaclty one segment now\", 1, w.getSegmentCount());\n    assertEquals(\"maxDoc and numDocs should be identical\", w.getDocStats().numDocs, w.getDocStats().maxDoc);\n    assertEquals(\"There should be an exact number of documents in that one segment\", remainingDocs, w.getDocStats().numDocs);\n\n    // Delete 5% and expunge, should be no change.\n    segNamesBefore = getSegmentNames(w);\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 1, false);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertTrue(\"The segment should have deleted documents\", w.getDocStats().numDocs < w.getDocStats().maxDoc);\n\n    w.forceMerge(1); // back to one segment so deletePctDocsFromEachSeg still works\n\n    // Test singleton merge for expungeDeletes\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 20, true);\n    w.forceMergeDeletes();\n\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertEquals(\"The segment should have no deleted documents\", w.getDocStats().numDocs, w.getDocStats().maxDoc);\n\n\n    // sanity check, at this point we should have an over`-large segment, we know we have exactly one.\n    assertTrue(\"Our single segment should have quite a few docs\", w.getDocStats().numDocs > 1_000);\n\n    // Delete 60% of the documents and then add a few more docs and commit. This should \"singleton merge\" the large segment\n    // created above. 60% leaves some wriggle room, LUCENE-8263 will change this assumption and should be tested\n    // when we deal with that JIRA.\n\n    deletedThisPass = deletePctDocsFromEachSeg(w, (w.getDocStats().numDocs * 60) / 100, true);\n    remainingDocs -= deletedThisPass;\n\n    for (int i = 0; i < 50; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i + numDocs, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit(); // want to trigger merge no matter what.\n\n    assertEquals(\"There should be exactly one very large and one small segment\", 2, w.listOfSegmentCommitInfos().size());\n    SegmentCommitInfo info0 = w.listOfSegmentCommitInfos().get(0);\n    SegmentCommitInfo info1 = w.listOfSegmentCommitInfos().get(1);\n    int largeSegDocCount = Math.max(info0.info.maxDoc(), info1.info.maxDoc());\n    int smallSegDocCount = Math.min(info0.info.maxDoc(), info1.info.maxDoc());\n    assertEquals(\"The large segment should have a bunch of docs\", largeSegDocCount, remainingDocs);\n    assertEquals(\"Small segment shold have fewer docs\", smallSegDocCount, 50);\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-7976 makes findForceMergeDeletes and findForcedDeletes respect max segment size by default,\n  // so insure that this works.\n  public void testForcedMergesRespectSegSize() throws Exception {\n    final Directory dir = newDirectory();\n    final IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n\n    // Empirically, 100 docs the size below give us segments of 3,330 bytes. It's not all that reliable in terms\n    // of how big a segment _can_ get, so set it to prevent merges on commit.\n    double mbSize = 0.004;\n    long maxSegBytes = (long) ((1024.0 * 1024.0)); // fudge it up, we're trying to catch egregious errors and segbytes don't really reflect the number for original merges.\n    tmp.setMaxMergedSegmentMB(mbSize);\n    conf.setMaxBufferedDocs(100);\n    conf.setMergePolicy(tmp);\n\n    final IndexWriter w = new IndexWriter(dir, conf);\n\n    final int numDocs = atLeast(2400);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit();\n\n    // These should be no-ops on an index with no deletions and segments are pretty big.\n    List<String> segNamesBefore = getSegmentNames(w);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);  // There should have been no merges.\n\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentsInExpectations(w, segNamesBefore, true);\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n\n\n    // Delete 12-17% of each segment and expungeDeletes. This should result in:\n    // > the same number of segments as before.\n    // > no segments larger than maxSegmentSize.\n    // > no deleted docs left.\n    int remainingDocs = numDocs - deletePctDocsFromEachSeg(w, random().nextInt(5) + 12, true);\n    w.forceMergeDeletes();\n    w.commit();\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n    assertFalse(\"There should be no deleted docs in the index.\", w.hasDeletions());\n\n    // Check that deleting _fewer_ than 10% doesn't merge inappropriately. Nothing should be merged since no segment\n    // has had more than 10% of its docs deleted.\n    segNamesBefore = getSegmentNames(w);\n    int deletedThisPass = deletePctDocsFromEachSeg(w, random().nextInt(4) + 3, false);\n    w.forceMergeDeletes();\n    remainingDocs -= deletedThisPass;\n    checkSegmentsInExpectations(w, segNamesBefore, false); // There should have been no merges\n    assertEquals(\"NumDocs should reflect removed documents \", remainingDocs, w.numDocs());\n    assertTrue(\"Should still be deleted docs in the index\", w.numDocs() < w.maxDoc());\n\n    // This time, forceMerge. By default this should respect max segment size.\n    // Will change for LUCENE-8236\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n\n    // Now forceMerge down to one segment, there should be exactly remainingDocs in exactly one segment.\n    w.forceMerge(1);\n    assertEquals(\"There should be exaclty one segment now\", 1, w.getSegmentCount());\n    assertEquals(\"maxDoc and numDocs should be identical\", w.numDocs(), w.maxDoc());\n    assertEquals(\"There should be an exact number of documents in that one segment\", remainingDocs, w.numDocs());\n\n    // Delete 5% and expunge, should be no change.\n    segNamesBefore = getSegmentNames(w);\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 1, false);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertTrue(\"The segment should have deleted documents\", w.numDocs() < w.maxDoc());\n\n    w.forceMerge(1); // back to one segment so deletePctDocsFromEachSeg still works\n\n    // Test singleton merge for expungeDeletes\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 20, true);\n    w.forceMergeDeletes();\n\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertEquals(\"The segment should have no deleted documents\", w.numDocs(), w.maxDoc());\n\n\n    // sanity check, at this point we should have an over`-large segment, we know we have exactly one.\n    assertTrue(\"Our single segment should have quite a few docs\", w.numDocs() > 1_000);\n\n    // Delete 60% of the documents and then add a few more docs and commit. This should \"singleton merge\" the large segment\n    // created above. 60% leaves some wriggle room, LUCENE-8263 will change this assumption and should be tested\n    // when we deal with that JIRA.\n\n    deletedThisPass = deletePctDocsFromEachSeg(w, (w.numDocs() * 60) / 100, true);\n    remainingDocs -= deletedThisPass;\n\n    for (int i = 0; i < 50; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i + numDocs, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit(); // want to trigger merge no matter what.\n\n    assertEquals(\"There should be exactly one very large and one small segment\", 2, w.listOfSegmentCommitInfos().size());\n    SegmentCommitInfo info0 = w.listOfSegmentCommitInfos().get(0);\n    SegmentCommitInfo info1 = w.listOfSegmentCommitInfos().get(1);\n    int largeSegDocCount = Math.max(info0.info.maxDoc(), info1.info.maxDoc());\n    int smallSegDocCount = Math.min(info0.info.maxDoc(), info1.info.maxDoc());\n    assertEquals(\"The large segment should have a bunch of docs\", largeSegDocCount, remainingDocs);\n    assertEquals(\"Small segment shold have fewer docs\", smallSegDocCount, 50);\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2","date":1588002560,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy#testForcedMergesRespectSegSize().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy#testForcedMergesRespectSegSize().mjava","sourceNew":"  // LUCENE-7976 makes findForceMergeDeletes and findForcedDeletes respect max segment size by default,\n  // so insure that this works.\n  public void testForcedMergesRespectSegSize() throws Exception {\n    final Directory dir = newDirectory();\n    final IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n\n    // Empirically, 100 docs the size below give us segments of 3,330 bytes. It's not all that reliable in terms\n    // of how big a segment _can_ get, so set it to prevent merges on commit.\n    double mbSize = 0.004;\n    long maxSegBytes = (long) ((1024.0 * 1024.0)); // fudge it up, we're trying to catch egregious errors and segbytes don't really reflect the number for original merges.\n    tmp.setMaxMergedSegmentMB(mbSize);\n    conf.setMaxBufferedDocs(100);\n    conf.setMergePolicy(tmp);\n\n    final IndexWriter w = new IndexWriter(dir, conf);\n\n    final int numDocs = atLeast(2400);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit();\n\n    // These should be no-ops on an index with no deletions and segments are pretty big.\n    List<String> segNamesBefore = getSegmentNames(w);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);  // There should have been no merges.\n\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentsInExpectations(w, segNamesBefore, true);\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n\n\n    // Delete 12-17% of each segment and expungeDeletes. This should result in:\n    // > the same number of segments as before.\n    // > no segments larger than maxSegmentSize.\n    // > no deleted docs left.\n    int remainingDocs = numDocs - deletePctDocsFromEachSeg(w, random().nextInt(5) + 12, true);\n    w.forceMergeDeletes();\n    w.commit();\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n    assertFalse(\"There should be no deleted docs in the index.\", w.hasDeletions());\n\n    // Check that deleting _fewer_ than 10% doesn't merge inappropriately. Nothing should be merged since no segment\n    // has had more than 10% of its docs deleted.\n    segNamesBefore = getSegmentNames(w);\n    int deletedThisPass = deletePctDocsFromEachSeg(w, random().nextInt(4) + 3, false);\n    w.forceMergeDeletes();\n    remainingDocs -= deletedThisPass;\n    checkSegmentsInExpectations(w, segNamesBefore, false); // There should have been no merges\n    assertEquals(\"NumDocs should reflect removed documents \", remainingDocs, w.getDocStats().numDocs);\n    assertTrue(\"Should still be deleted docs in the index\", w.getDocStats().numDocs < w.getDocStats().maxDoc);\n\n    // This time, forceMerge. By default this should respect max segment size.\n    // Will change for LUCENE-8236\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n\n    // Now forceMerge down to one segment, there should be exactly remainingDocs in exactly one segment.\n    w.forceMerge(1);\n    assertEquals(\"There should be exaclty one segment now\", 1, w.getSegmentCount());\n    assertEquals(\"maxDoc and numDocs should be identical\", w.getDocStats().numDocs, w.getDocStats().maxDoc);\n    assertEquals(\"There should be an exact number of documents in that one segment\", remainingDocs, w.getDocStats().numDocs);\n\n    // Delete 5% and expunge, should be no change.\n    segNamesBefore = getSegmentNames(w);\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 1, false);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertTrue(\"The segment should have deleted documents\", w.getDocStats().numDocs < w.getDocStats().maxDoc);\n\n    w.forceMerge(1); // back to one segment so deletePctDocsFromEachSeg still works\n\n    // Test singleton merge for expungeDeletes\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 20, true);\n    w.forceMergeDeletes();\n\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertEquals(\"The segment should have no deleted documents\", w.getDocStats().numDocs, w.getDocStats().maxDoc);\n\n\n    // sanity check, at this point we should have an over`-large segment, we know we have exactly one.\n    assertTrue(\"Our single segment should have quite a few docs\", w.getDocStats().numDocs > 1_000);\n\n    // Delete 60% of the documents and then add a few more docs and commit. This should \"singleton merge\" the large segment\n    // created above. 60% leaves some wriggle room, LUCENE-8263 will change this assumption and should be tested\n    // when we deal with that JIRA.\n\n    deletedThisPass = deletePctDocsFromEachSeg(w, (w.getDocStats().numDocs * 60) / 100, true);\n    remainingDocs -= deletedThisPass;\n\n    for (int i = 0; i < 50; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i + numDocs, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit(); // want to trigger merge no matter what.\n\n    assertEquals(\"There should be exactly one very large and one small segment\", 2, w.cloneSegmentInfos().size());\n    SegmentCommitInfo info0 = w.cloneSegmentInfos().info(0);\n    SegmentCommitInfo info1 = w.cloneSegmentInfos().info(1);\n    int largeSegDocCount = Math.max(info0.info.maxDoc(), info1.info.maxDoc());\n    int smallSegDocCount = Math.min(info0.info.maxDoc(), info1.info.maxDoc());\n    assertEquals(\"The large segment should have a bunch of docs\", largeSegDocCount, remainingDocs);\n    assertEquals(\"Small segment shold have fewer docs\", smallSegDocCount, 50);\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-7976 makes findForceMergeDeletes and findForcedDeletes respect max segment size by default,\n  // so insure that this works.\n  public void testForcedMergesRespectSegSize() throws Exception {\n    final Directory dir = newDirectory();\n    final IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n\n    // Empirically, 100 docs the size below give us segments of 3,330 bytes. It's not all that reliable in terms\n    // of how big a segment _can_ get, so set it to prevent merges on commit.\n    double mbSize = 0.004;\n    long maxSegBytes = (long) ((1024.0 * 1024.0)); // fudge it up, we're trying to catch egregious errors and segbytes don't really reflect the number for original merges.\n    tmp.setMaxMergedSegmentMB(mbSize);\n    conf.setMaxBufferedDocs(100);\n    conf.setMergePolicy(tmp);\n\n    final IndexWriter w = new IndexWriter(dir, conf);\n\n    final int numDocs = atLeast(2400);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit();\n\n    // These should be no-ops on an index with no deletions and segments are pretty big.\n    List<String> segNamesBefore = getSegmentNames(w);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);  // There should have been no merges.\n\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentsInExpectations(w, segNamesBefore, true);\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n\n\n    // Delete 12-17% of each segment and expungeDeletes. This should result in:\n    // > the same number of segments as before.\n    // > no segments larger than maxSegmentSize.\n    // > no deleted docs left.\n    int remainingDocs = numDocs - deletePctDocsFromEachSeg(w, random().nextInt(5) + 12, true);\n    w.forceMergeDeletes();\n    w.commit();\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n    assertFalse(\"There should be no deleted docs in the index.\", w.hasDeletions());\n\n    // Check that deleting _fewer_ than 10% doesn't merge inappropriately. Nothing should be merged since no segment\n    // has had more than 10% of its docs deleted.\n    segNamesBefore = getSegmentNames(w);\n    int deletedThisPass = deletePctDocsFromEachSeg(w, random().nextInt(4) + 3, false);\n    w.forceMergeDeletes();\n    remainingDocs -= deletedThisPass;\n    checkSegmentsInExpectations(w, segNamesBefore, false); // There should have been no merges\n    assertEquals(\"NumDocs should reflect removed documents \", remainingDocs, w.getDocStats().numDocs);\n    assertTrue(\"Should still be deleted docs in the index\", w.getDocStats().numDocs < w.getDocStats().maxDoc);\n\n    // This time, forceMerge. By default this should respect max segment size.\n    // Will change for LUCENE-8236\n    w.forceMerge(Integer.MAX_VALUE);\n    checkSegmentSizeNotExceeded(w.cloneSegmentInfos(), maxSegBytes);\n\n    // Now forceMerge down to one segment, there should be exactly remainingDocs in exactly one segment.\n    w.forceMerge(1);\n    assertEquals(\"There should be exaclty one segment now\", 1, w.getSegmentCount());\n    assertEquals(\"maxDoc and numDocs should be identical\", w.getDocStats().numDocs, w.getDocStats().maxDoc);\n    assertEquals(\"There should be an exact number of documents in that one segment\", remainingDocs, w.getDocStats().numDocs);\n\n    // Delete 5% and expunge, should be no change.\n    segNamesBefore = getSegmentNames(w);\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 1, false);\n    w.forceMergeDeletes();\n    checkSegmentsInExpectations(w, segNamesBefore, false);\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertTrue(\"The segment should have deleted documents\", w.getDocStats().numDocs < w.getDocStats().maxDoc);\n\n    w.forceMerge(1); // back to one segment so deletePctDocsFromEachSeg still works\n\n    // Test singleton merge for expungeDeletes\n    remainingDocs -= deletePctDocsFromEachSeg(w, random().nextInt(5) + 20, true);\n    w.forceMergeDeletes();\n\n    assertEquals(\"There should still be only one segment. \", 1, w.getSegmentCount());\n    assertEquals(\"The segment should have no deleted documents\", w.getDocStats().numDocs, w.getDocStats().maxDoc);\n\n\n    // sanity check, at this point we should have an over`-large segment, we know we have exactly one.\n    assertTrue(\"Our single segment should have quite a few docs\", w.getDocStats().numDocs > 1_000);\n\n    // Delete 60% of the documents and then add a few more docs and commit. This should \"singleton merge\" the large segment\n    // created above. 60% leaves some wriggle room, LUCENE-8263 will change this assumption and should be tested\n    // when we deal with that JIRA.\n\n    deletedThisPass = deletePctDocsFromEachSeg(w, (w.getDocStats().numDocs * 60) / 100, true);\n    remainingDocs -= deletedThisPass;\n\n    for (int i = 0; i < 50; i++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\" + i + numDocs, Field.Store.NO));\n      doc.add(newTextField(\"content\", \"aaa \" + i, Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    w.commit(); // want to trigger merge no matter what.\n\n    assertEquals(\"There should be exactly one very large and one small segment\", 2, w.listOfSegmentCommitInfos().size());\n    SegmentCommitInfo info0 = w.listOfSegmentCommitInfos().get(0);\n    SegmentCommitInfo info1 = w.listOfSegmentCommitInfos().get(1);\n    int largeSegDocCount = Math.max(info0.info.maxDoc(), info1.info.maxDoc());\n    int smallSegDocCount = Math.min(info0.info.maxDoc(), info1.info.maxDoc());\n    assertEquals(\"The large segment should have a bunch of docs\", largeSegDocCount, remainingDocs);\n    assertEquals(\"Small segment shold have fewer docs\", smallSegDocCount, 50);\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","56fb5e4e4b239474721e13b4cd9542ea2d215451"],"56fb5e4e4b239474721e13b4cd9542ea2d215451":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"631ea3d1607299c59f33edef140ffc19a81f07a0":["56fb5e4e4b239474721e13b4cd9542ea2d215451"],"feb4029567b43f074ed7b6eb8fb126d355075dfd":["631ea3d1607299c59f33edef140ffc19a81f07a0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["feb4029567b43f074ed7b6eb8fb126d355075dfd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","56fb5e4e4b239474721e13b4cd9542ea2d215451"]},"commit2Childs":{"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"56fb5e4e4b239474721e13b4cd9542ea2d215451":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","631ea3d1607299c59f33edef140ffc19a81f07a0","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"631ea3d1607299c59f33edef140ffc19a81f07a0":["feb4029567b43f074ed7b6eb8fb126d355075dfd"],"feb4029567b43f074ed7b6eb8fb126d355075dfd":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","56fb5e4e4b239474721e13b4cd9542ea2d215451","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}