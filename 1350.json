{"path":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","commits":[{"id":"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1","date":1513252583,"type":0,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // mark the old slice as inactive\n    sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>())\n        .put(ZkStateReader.SHARD_STATE_PROP, Slice.State.INACTIVE.toString());\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.ACTIVE.toString());\n    }\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    cloudManager.submit(new LeaderElection(Collections.singleton(collectionName), true));\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["685af99397b6da31116a2cac747ed255d217d080"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a4422b331d00607258b0ed3e43934306e67764aa","date":1513943901,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // mark the old slice as inactive\n    sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>())\n        .put(ZkStateReader.SHARD_STATE_PROP, Slice.State.INACTIVE.toString());\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.ACTIVE.toString());\n    }\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // mark the old slice as inactive\n    sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>())\n        .put(ZkStateReader.SHARD_STATE_PROP, Slice.State.INACTIVE.toString());\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.ACTIVE.toString());\n    }\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    cloudManager.submit(new LeaderElection(Collections.singleton(collectionName), true));\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9c6c0dad4932399aec99b4818086cb1772773916","date":1520515900,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n    props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n    props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTime()));\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTime()));\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // mark the old slice as inactive\n    sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>())\n        .put(ZkStateReader.SHARD_STATE_PROP, Slice.State.INACTIVE.toString());\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.SHARD_STATE_PROP, Slice.State.ACTIVE.toString());\n    }\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4412883c12067d8a4e2a354aa8adc58c32be1d6","date":1521129281,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n    props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n    props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n    props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n    props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTime()));\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTime()));\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1","date":1523453934,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainder = numDocs % subSlices.size();\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainder;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n    props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n    props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"43345f1452f9510f8aaadae6156fe0c834e7d957","date":1523483670,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainder = numDocs % subSlices.size();\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainder;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n    props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n    props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9bf5e4e82a5eeecf4d057625b9698db14d6f093","date":1524651892,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainder = numDocs % subSlices.size();\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainder;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainder = numDocs % subSlices.size();\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainder;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6b55cd711a129fb7fc4c3c4672d652149c9a4faa","date":1528813320,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Suggestion.ConditionType.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Suggestion.coreidxsize, Suggestion.ConditionType.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainder = numDocs % subSlices.size();\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainder;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"685af99397b6da31116a2cac747ed255d217d080","date":1530038134,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Suggestion.ConditionType.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Suggestion.coreidxsize, Suggestion.ConditionType.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Suggestion.ConditionType.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Suggestion.coreidxsize, Suggestion.ConditionType.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":["1d4bf9d5308dfef350829c28f2b3b2648df1e9b1"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Suggestion.ConditionType.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Suggestion.coreidxsize, Suggestion.ConditionType.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainder = numDocs % subSlices.size();\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainder;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Suggestion.ConditionType.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Suggestion.coreidxsize, Suggestion.ConditionType.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainder = numDocs % subSlices.size();\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainder;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"427edb17549d4bb82462a16eec4ee0533d12d5b7","date":1533006754,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Variable.coreidxsize, Type.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Suggestion.ConditionType.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Suggestion.coreidxsize, Suggestion.ConditionType.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"042b92cf48996255bedb0c3c4bf772d7e06e4dea","date":1534272102,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Variable.coreidxsize, Type.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Variable.coreidxsize, Type.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9f71a1c6c905e9489b4d25c83c8d628d978a8ea","date":1538045138,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Variable.coreidxsize, Type.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    List<ReplicaPosition> replicaPositions = Assign.identifyNodes(cloudManager,\n        clusterState,\n        new ArrayList<>(clusterState.getLiveNodes()),\n        collectionName,\n        new ZkNodeProps(collection.getProperties()),\n        // reproduce the bug\n        subSlices, repFactor, 0, 0);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Variable.coreidxsize, Type.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc18bc8ea2e2c1e308757ff50671c774438e9f3e","date":1538052583,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Shard \" + collectionName +\n          \" /  \" + sliceName.get() + \" has no leader and can't be split\");\n    }\n    // start counting buffered updates\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), ss -> new ConcurrentHashMap<>());\n    if (props.containsKey(BUFFERED_UPDATES)) {\n      log.debug(\"--- SOLR-12729: Overlapping splitShard commands for {} / {}\", collectionName, sliceName.get());\n      return;\n    }\n    props.put(BUFFERED_UPDATES, new AtomicLong());\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    String numDocsStr = String.valueOf(getReplicaInfo(leader).getVariable(\"SEARCHER.searcher.numDocs\", \"0\"));\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.CONSTRUCTION.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    // add replicas\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n//      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica_n\" + (replicaPosition.index);\n      String solrCoreName = Assign.buildSolrCoreName(collectionName, subSliceName, replicaPosition.type, Assign.incAndGetId(stateManager, collectionName, 0));\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", new AtomicLong(0));\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, new AtomicLong(replicasIndexSize));\n      replicaProps.put(Variable.coreidxsize, new AtomicDouble((Double)Type.CORE_IDX.convertVal(replicasIndexSize)));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n\n    // delay it once again to better simulate replica recoveries\n    //opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    CloudTestUtils.waitForState(cloudManager, collectionName, 30, TimeUnit.SECONDS, (liveNodes, state) -> {\n      for (String subSlice : subSlices) {\n        Slice s = state.getSlice(subSlice);\n        if (s.getLeader() == null) {\n          log.debug(\"** no leader in {} / {}\", collectionName, s);\n          return false;\n        }\n        if (s.getReplicas().size() < repFactor) {\n          log.debug(\"** expected {} repFactor but there are {} replicas\", repFactor, s.getReplicas().size());\n          return false;\n        }\n      }\n      return true;\n    });\n    // mark the new slices as active and the old slice as inactive\n    log.trace(\"-- switching slice states after split shard: collection={}, parent={}, subSlices={}\", collectionName,\n        sliceName.get(), subSlices);\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      sProps.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      sProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      AtomicLong bufferedUpdates = (AtomicLong)sProps.remove(BUFFERED_UPDATES);\n      if (bufferedUpdates.get() > 0) {\n        // apply buffered updates\n        long perShard = bufferedUpdates.get() / subSlices.size();\n        long remainder = bufferedUpdates.get() % subSlices.size();\n        log.debug(\"-- applying {} buffered docs from {} / {}, perShard={}, remainder={}\", bufferedUpdates.get(),\n            collectionName, parentSlice.getName(), perShard, remainder);\n        for (int i = 0; i < subSlices.size(); i++) {\n          String sub = subSlices.get(i);\n          long numUpdates = perShard;\n          if (i == 0) {\n            numUpdates += remainder;\n          }\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.numDocs\", numUpdates, true, false);\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.maxDoc\", numUpdates, true, false);\n        }\n      }\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      for (String s : subSlices) {\n        Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(s, ss -> new ConcurrentHashMap<>());\n        sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n        sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      }\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    // always invalidate cached collection states to get up-to-date metrics\n    collectionsStatesRef.set(null);\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      leader = parentSlice.getReplicas().iterator().next();\n    }\n    String numDocsStr = leader.getStr(\"SEARCHER.searcher.numDocs\", \"0\");\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica\" + (replicaPosition.index);\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", replicasNumDocs);\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", 0);\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, replicasIndexSize);\n      replicaProps.put(Variable.coreidxsize, Type.CORE_IDX.convertVal(replicasIndexSize));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    // mark the old slice as inactive\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      props.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      props.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    collectionsStatesRef.set(null);\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae70f2df00762dfce0455c0e39381848762662e5","date":1539113410,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Shard \" + collectionName +\n          \" /  \" + sliceName.get() + \" has no leader and can't be split\");\n    }\n    SplitShardCmd.lockForSplit(cloudManager, collectionName, sliceName.get());\n    // start counting buffered updates\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), ss -> new ConcurrentHashMap<>());\n    if (props.containsKey(BUFFERED_UPDATES)) {\n      throw new Exception(\"--- SOLR-12729: Overlapping splitShard commands for \" + collectionName + \"/\" + sliceName.get());\n    }\n    props.put(BUFFERED_UPDATES, new AtomicLong());\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    String numDocsStr = String.valueOf(getReplicaInfo(leader).getVariable(\"SEARCHER.searcher.numDocs\", \"0\"));\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.CONSTRUCTION.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    // add replicas\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n//      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica_n\" + (replicaPosition.index);\n      String solrCoreName = Assign.buildSolrCoreName(collectionName, subSliceName, replicaPosition.type, Assign.incAndGetId(stateManager, collectionName, 0));\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", new AtomicLong(0));\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, new AtomicLong(replicasIndexSize));\n      replicaProps.put(Variable.coreidxsize, new AtomicDouble((Double)Type.CORE_IDX.convertVal(replicasIndexSize)));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n\n    // delay it once again to better simulate replica recoveries\n    //opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    boolean success = false;\n    try {\n      CloudTestUtils.waitForState(cloudManager, collectionName, 30, TimeUnit.SECONDS, (liveNodes, state) -> {\n        for (String subSlice : subSlices) {\n          Slice s = state.getSlice(subSlice);\n          if (s.getLeader() == null) {\n            log.debug(\"** no leader in {} / {}\", collectionName, s);\n            return false;\n          }\n          if (s.getReplicas().size() < repFactor) {\n            log.debug(\"** expected {} repFactor but there are {} replicas\", repFactor, s.getReplicas().size());\n            return false;\n          }\n        }\n        return true;\n      });\n      success = true;\n    } finally {\n      if (!success) {\n        SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      }\n    }\n    // mark the new slices as active and the old slice as inactive\n    log.trace(\"-- switching slice states after split shard: collection={}, parent={}, subSlices={}\", collectionName,\n        sliceName.get(), subSlices);\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      sProps.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      sProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      AtomicLong bufferedUpdates = (AtomicLong)sProps.remove(BUFFERED_UPDATES);\n      if (bufferedUpdates.get() > 0) {\n        // apply buffered updates\n        long perShard = bufferedUpdates.get() / subSlices.size();\n        long remainder = bufferedUpdates.get() % subSlices.size();\n        log.debug(\"-- applying {} buffered docs from {} / {}, perShard={}, remainder={}\", bufferedUpdates.get(),\n            collectionName, parentSlice.getName(), perShard, remainder);\n        for (int i = 0; i < subSlices.size(); i++) {\n          String sub = subSlices.get(i);\n          long numUpdates = perShard;\n          if (i == 0) {\n            numUpdates += remainder;\n          }\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.numDocs\", numUpdates, true, false);\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.maxDoc\", numUpdates, true, false);\n        }\n      }\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      for (String s : subSlices) {\n        Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(s, ss -> new ConcurrentHashMap<>());\n        sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n        sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      }\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      lock.unlock();\n    }\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Shard \" + collectionName +\n          \" /  \" + sliceName.get() + \" has no leader and can't be split\");\n    }\n    // start counting buffered updates\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), ss -> new ConcurrentHashMap<>());\n    if (props.containsKey(BUFFERED_UPDATES)) {\n      log.debug(\"--- SOLR-12729: Overlapping splitShard commands for {} / {}\", collectionName, sliceName.get());\n      return;\n    }\n    props.put(BUFFERED_UPDATES, new AtomicLong());\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    String numDocsStr = String.valueOf(getReplicaInfo(leader).getVariable(\"SEARCHER.searcher.numDocs\", \"0\"));\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.CONSTRUCTION.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    // add replicas\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n//      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica_n\" + (replicaPosition.index);\n      String solrCoreName = Assign.buildSolrCoreName(collectionName, subSliceName, replicaPosition.type, Assign.incAndGetId(stateManager, collectionName, 0));\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", new AtomicLong(0));\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, new AtomicLong(replicasIndexSize));\n      replicaProps.put(Variable.coreidxsize, new AtomicDouble((Double)Type.CORE_IDX.convertVal(replicasIndexSize)));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n\n    // delay it once again to better simulate replica recoveries\n    //opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    CloudTestUtils.waitForState(cloudManager, collectionName, 30, TimeUnit.SECONDS, (liveNodes, state) -> {\n      for (String subSlice : subSlices) {\n        Slice s = state.getSlice(subSlice);\n        if (s.getLeader() == null) {\n          log.debug(\"** no leader in {} / {}\", collectionName, s);\n          return false;\n        }\n        if (s.getReplicas().size() < repFactor) {\n          log.debug(\"** expected {} repFactor but there are {} replicas\", repFactor, s.getReplicas().size());\n          return false;\n        }\n      }\n      return true;\n    });\n    // mark the new slices as active and the old slice as inactive\n    log.trace(\"-- switching slice states after split shard: collection={}, parent={}, subSlices={}\", collectionName,\n        sliceName.get(), subSlices);\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      sProps.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      sProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      AtomicLong bufferedUpdates = (AtomicLong)sProps.remove(BUFFERED_UPDATES);\n      if (bufferedUpdates.get() > 0) {\n        // apply buffered updates\n        long perShard = bufferedUpdates.get() / subSlices.size();\n        long remainder = bufferedUpdates.get() % subSlices.size();\n        log.debug(\"-- applying {} buffered docs from {} / {}, perShard={}, remainder={}\", bufferedUpdates.get(),\n            collectionName, parentSlice.getName(), perShard, remainder);\n        for (int i = 0; i < subSlices.size(); i++) {\n          String sub = subSlices.get(i);\n          long numUpdates = perShard;\n          if (i == 0) {\n            numUpdates += remainder;\n          }\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.numDocs\", numUpdates, true, false);\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.maxDoc\", numUpdates, true, false);\n        }\n      }\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      for (String s : subSlices) {\n        Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(s, ss -> new ConcurrentHashMap<>());\n        sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n        sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      }\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54e3d90778c12196fe1bb9dfa019fa2e7824e3aa","date":1540200489,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Shard \" + collectionName +\n          \" /  \" + sliceName.get() + \" has no leader and can't be split\");\n    }\n    SplitShardCmd.lockForSplit(cloudManager, collectionName, sliceName.get());\n    // start counting buffered updates\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), ss -> new ConcurrentHashMap<>());\n    if (props.containsKey(BUFFERED_UPDATES)) {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      throw new Exception(\"--- SOLR-12729: Overlapping splitShard commands for \" + collectionName + \"/\" + sliceName.get());\n    }\n    props.put(BUFFERED_UPDATES, new AtomicLong());\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    String numDocsStr = String.valueOf(getReplicaInfo(leader).getVariable(\"SEARCHER.searcher.numDocs\", \"0\"));\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.CONSTRUCTION.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    // add replicas\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n//      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica_n\" + (replicaPosition.index);\n      String solrCoreName = Assign.buildSolrCoreName(collectionName, subSliceName, replicaPosition.type, Assign.incAndGetId(stateManager, collectionName, 0));\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", new AtomicLong(0));\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, new AtomicLong(replicasIndexSize));\n      replicaProps.put(Variable.coreidxsize, new AtomicDouble((Double)Type.CORE_IDX.convertVal(replicasIndexSize)));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n\n    // delay it once again to better simulate replica recoveries\n    //opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    boolean success = false;\n    try {\n      CloudTestUtils.waitForState(cloudManager, collectionName, 30, TimeUnit.SECONDS, (liveNodes, state) -> {\n        for (String subSlice : subSlices) {\n          Slice s = state.getSlice(subSlice);\n          if (s.getLeader() == null) {\n            log.debug(\"** no leader in {} / {}\", collectionName, s);\n            return false;\n          }\n          if (s.getReplicas().size() < repFactor) {\n            log.debug(\"** expected {} repFactor but there are {} replicas\", repFactor, s.getReplicas().size());\n            return false;\n          }\n        }\n        return true;\n      });\n      success = true;\n    } finally {\n      if (!success) {\n        SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      }\n    }\n    // mark the new slices as active and the old slice as inactive\n    log.trace(\"-- switching slice states after split shard: collection={}, parent={}, subSlices={}\", collectionName,\n        sliceName.get(), subSlices);\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      sProps.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      sProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      AtomicLong bufferedUpdates = (AtomicLong)sProps.remove(BUFFERED_UPDATES);\n      if (bufferedUpdates.get() > 0) {\n        // apply buffered updates\n        long perShard = bufferedUpdates.get() / subSlices.size();\n        long remainder = bufferedUpdates.get() % subSlices.size();\n        log.debug(\"-- applying {} buffered docs from {} / {}, perShard={}, remainder={}\", bufferedUpdates.get(),\n            collectionName, parentSlice.getName(), perShard, remainder);\n        for (int i = 0; i < subSlices.size(); i++) {\n          String sub = subSlices.get(i);\n          long numUpdates = perShard;\n          if (i == 0) {\n            numUpdates += remainder;\n          }\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.numDocs\", numUpdates, true, false);\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.maxDoc\", numUpdates, true, false);\n        }\n      }\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      for (String s : subSlices) {\n        Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(s, ss -> new ConcurrentHashMap<>());\n        sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n        sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      }\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      lock.unlock();\n    }\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Shard \" + collectionName +\n          \" /  \" + sliceName.get() + \" has no leader and can't be split\");\n    }\n    SplitShardCmd.lockForSplit(cloudManager, collectionName, sliceName.get());\n    // start counting buffered updates\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), ss -> new ConcurrentHashMap<>());\n    if (props.containsKey(BUFFERED_UPDATES)) {\n      throw new Exception(\"--- SOLR-12729: Overlapping splitShard commands for \" + collectionName + \"/\" + sliceName.get());\n    }\n    props.put(BUFFERED_UPDATES, new AtomicLong());\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    String numDocsStr = String.valueOf(getReplicaInfo(leader).getVariable(\"SEARCHER.searcher.numDocs\", \"0\"));\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.CONSTRUCTION.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    // add replicas\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n//      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica_n\" + (replicaPosition.index);\n      String solrCoreName = Assign.buildSolrCoreName(collectionName, subSliceName, replicaPosition.type, Assign.incAndGetId(stateManager, collectionName, 0));\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", new AtomicLong(0));\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, new AtomicLong(replicasIndexSize));\n      replicaProps.put(Variable.coreidxsize, new AtomicDouble((Double)Type.CORE_IDX.convertVal(replicasIndexSize)));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n\n    // delay it once again to better simulate replica recoveries\n    //opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    boolean success = false;\n    try {\n      CloudTestUtils.waitForState(cloudManager, collectionName, 30, TimeUnit.SECONDS, (liveNodes, state) -> {\n        for (String subSlice : subSlices) {\n          Slice s = state.getSlice(subSlice);\n          if (s.getLeader() == null) {\n            log.debug(\"** no leader in {} / {}\", collectionName, s);\n            return false;\n          }\n          if (s.getReplicas().size() < repFactor) {\n            log.debug(\"** expected {} repFactor but there are {} replicas\", repFactor, s.getReplicas().size());\n            return false;\n          }\n        }\n        return true;\n      });\n      success = true;\n    } finally {\n      if (!success) {\n        SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      }\n    }\n    // mark the new slices as active and the old slice as inactive\n    log.trace(\"-- switching slice states after split shard: collection={}, parent={}, subSlices={}\", collectionName,\n        sliceName.get(), subSlices);\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      sProps.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      sProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      AtomicLong bufferedUpdates = (AtomicLong)sProps.remove(BUFFERED_UPDATES);\n      if (bufferedUpdates.get() > 0) {\n        // apply buffered updates\n        long perShard = bufferedUpdates.get() / subSlices.size();\n        long remainder = bufferedUpdates.get() % subSlices.size();\n        log.debug(\"-- applying {} buffered docs from {} / {}, perShard={}, remainder={}\", bufferedUpdates.get(),\n            collectionName, parentSlice.getName(), perShard, remainder);\n        for (int i = 0; i < subSlices.size(); i++) {\n          String sub = subSlices.get(i);\n          long numUpdates = perShard;\n          if (i == 0) {\n            numUpdates += remainder;\n          }\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.numDocs\", numUpdates, true, false);\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.maxDoc\", numUpdates, true, false);\n        }\n      }\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      for (String s : subSlices) {\n        Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(s, ss -> new ConcurrentHashMap<>());\n        sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n        sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      }\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      lock.unlock();\n    }\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7cac1f2920f8057198f04505797cbabf74dd9a97","date":1546884894,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Shard \" + collectionName +\n          \" /  \" + sliceName.get() + \" has no leader and can't be split\");\n    }\n    SplitShardCmd.lockForSplit(cloudManager, collectionName, sliceName.get());\n    // start counting buffered updates\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), ss -> new ConcurrentHashMap<>());\n    if (props.containsKey(BUFFERED_UPDATES)) {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      throw new Exception(\"--- SOLR-12729: Overlapping splitShard commands for \" + collectionName + \"/\" + sliceName.get());\n    }\n    props.put(BUFFERED_UPDATES, new AtomicLong());\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    String numDocsStr = String.valueOf(getReplicaInfo(leader).getVariable(\"SEARCHER.searcher.numDocs\", \"0\"));\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.CONSTRUCTION.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    // add replicas\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n//      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica_n\" + (replicaPosition.index);\n      String solrCoreName = Assign.buildSolrCoreName(collectionName, subSliceName, replicaPosition.type, Assign.incAndGetId(stateManager, collectionName, 0));\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", new AtomicLong(0));\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, new AtomicLong(replicasIndexSize));\n      replicaProps.put(Variable.coreidxsize, new AtomicDouble((Double)Type.CORE_IDX.convertVal(replicasIndexSize)));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n\n    // delay it once again to better simulate replica recoveries\n    //opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    boolean success = false;\n    try {\n      CloudTestUtils.waitForState(cloudManager, collectionName, 30, TimeUnit.SECONDS, (liveNodes, state) -> {\n        for (String subSlice : subSlices) {\n          Slice s = state.getSlice(subSlice);\n          if (s.getLeader() == null) {\n            log.debug(\"** no leader in {} / {}\", collectionName, s);\n            return false;\n          }\n          if (s.getReplicas().size() < repFactor) {\n            log.debug(\"** expected {} repFactor but there are {} replicas\", repFactor, s.getReplicas().size());\n            return false;\n          }\n        }\n        return true;\n      });\n      success = true;\n    } finally {\n      if (!success) {\n        Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n        sProps.remove(BUFFERED_UPDATES);\n        SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      }\n    }\n    // mark the new slices as active and the old slice as inactive\n    log.trace(\"-- switching slice states after split shard: collection={}, parent={}, subSlices={}\", collectionName,\n        sliceName.get(), subSlices);\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      sProps.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      sProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      AtomicLong bufferedUpdates = (AtomicLong)sProps.remove(BUFFERED_UPDATES);\n      if (bufferedUpdates.get() > 0) {\n        // apply buffered updates\n        long perShard = bufferedUpdates.get() / subSlices.size();\n        long remainder = bufferedUpdates.get() % subSlices.size();\n        log.debug(\"-- applying {} buffered docs from {} / {}, perShard={}, remainder={}\", bufferedUpdates.get(),\n            collectionName, parentSlice.getName(), perShard, remainder);\n        for (int i = 0; i < subSlices.size(); i++) {\n          String sub = subSlices.get(i);\n          long numUpdates = perShard;\n          if (i == 0) {\n            numUpdates += remainder;\n          }\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.numDocs\", numUpdates, true, false);\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.maxDoc\", numUpdates, true, false);\n        }\n      }\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      for (String s : subSlices) {\n        Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(s, ss -> new ConcurrentHashMap<>());\n        sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n        sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      }\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      lock.unlock();\n    }\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Shard \" + collectionName +\n          \" /  \" + sliceName.get() + \" has no leader and can't be split\");\n    }\n    SplitShardCmd.lockForSplit(cloudManager, collectionName, sliceName.get());\n    // start counting buffered updates\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), ss -> new ConcurrentHashMap<>());\n    if (props.containsKey(BUFFERED_UPDATES)) {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      throw new Exception(\"--- SOLR-12729: Overlapping splitShard commands for \" + collectionName + \"/\" + sliceName.get());\n    }\n    props.put(BUFFERED_UPDATES, new AtomicLong());\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    String numDocsStr = String.valueOf(getReplicaInfo(leader).getVariable(\"SEARCHER.searcher.numDocs\", \"0\"));\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.CONSTRUCTION.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    // add replicas\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n//      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica_n\" + (replicaPosition.index);\n      String solrCoreName = Assign.buildSolrCoreName(collectionName, subSliceName, replicaPosition.type, Assign.incAndGetId(stateManager, collectionName, 0));\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", new AtomicLong(0));\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, new AtomicLong(replicasIndexSize));\n      replicaProps.put(Variable.coreidxsize, new AtomicDouble((Double)Type.CORE_IDX.convertVal(replicasIndexSize)));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n\n    // delay it once again to better simulate replica recoveries\n    //opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    boolean success = false;\n    try {\n      CloudTestUtils.waitForState(cloudManager, collectionName, 30, TimeUnit.SECONDS, (liveNodes, state) -> {\n        for (String subSlice : subSlices) {\n          Slice s = state.getSlice(subSlice);\n          if (s.getLeader() == null) {\n            log.debug(\"** no leader in {} / {}\", collectionName, s);\n            return false;\n          }\n          if (s.getReplicas().size() < repFactor) {\n            log.debug(\"** expected {} repFactor but there are {} replicas\", repFactor, s.getReplicas().size());\n            return false;\n          }\n        }\n        return true;\n      });\n      success = true;\n    } finally {\n      if (!success) {\n        SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      }\n    }\n    // mark the new slices as active and the old slice as inactive\n    log.trace(\"-- switching slice states after split shard: collection={}, parent={}, subSlices={}\", collectionName,\n        sliceName.get(), subSlices);\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      sProps.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      sProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      AtomicLong bufferedUpdates = (AtomicLong)sProps.remove(BUFFERED_UPDATES);\n      if (bufferedUpdates.get() > 0) {\n        // apply buffered updates\n        long perShard = bufferedUpdates.get() / subSlices.size();\n        long remainder = bufferedUpdates.get() % subSlices.size();\n        log.debug(\"-- applying {} buffered docs from {} / {}, perShard={}, remainder={}\", bufferedUpdates.get(),\n            collectionName, parentSlice.getName(), perShard, remainder);\n        for (int i = 0; i < subSlices.size(); i++) {\n          String sub = subSlices.get(i);\n          long numUpdates = perShard;\n          if (i == 0) {\n            numUpdates += remainder;\n          }\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.numDocs\", numUpdates, true, false);\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.maxDoc\", numUpdates, true, false);\n        }\n      }\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      for (String s : subSlices) {\n        Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(s, ss -> new ConcurrentHashMap<>());\n        sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n        sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      }\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      lock.unlock();\n    }\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5","date":1556572478,"type":5,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSplitShard(ZkNodeProps,NamedList).mjava","sourceNew":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Shard \" + collectionName +\n          \" /  \" + sliceName.get() + \" has no leader and can't be split\");\n    }\n    SplitShardCmd.lockForSplit(cloudManager, collectionName, sliceName.get());\n    // start counting buffered updates\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), ss -> new ConcurrentHashMap<>());\n    if (props.containsKey(BUFFERED_UPDATES)) {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      throw new Exception(\"--- SOLR-12729: Overlapping splitShard commands for \" + collectionName + \"/\" + sliceName.get());\n    }\n    props.put(BUFFERED_UPDATES, new AtomicLong());\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    String numDocsStr = String.valueOf(getReplicaInfo(leader).getVariable(\"SEARCHER.searcher.numDocs\", \"0\"));\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.CONSTRUCTION.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    // add replicas\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n//      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica_n\" + (replicaPosition.index);\n      String solrCoreName = Assign.buildSolrCoreName(collectionName, subSliceName, replicaPosition.type, Assign.incAndGetId(stateManager, collectionName, 0));\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", new AtomicLong(0));\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, new AtomicLong(replicasIndexSize));\n      replicaProps.put(Variable.coreidxsize, new AtomicDouble((Double)Type.CORE_IDX.convertVal(replicasIndexSize)));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n\n    // delay it once again to better simulate replica recoveries\n    //opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    boolean success = false;\n    try {\n      CloudUtil.waitForState(cloudManager, collectionName, 30, TimeUnit.SECONDS, (liveNodes, state) -> {\n        for (String subSlice : subSlices) {\n          Slice s = state.getSlice(subSlice);\n          if (s.getLeader() == null) {\n            log.debug(\"** no leader in {} / {}\", collectionName, s);\n            return false;\n          }\n          if (s.getReplicas().size() < repFactor) {\n            log.debug(\"** expected {} repFactor but there are {} replicas\", repFactor, s.getReplicas().size());\n            return false;\n          }\n        }\n        return true;\n      });\n      success = true;\n    } finally {\n      if (!success) {\n        Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n        sProps.remove(BUFFERED_UPDATES);\n        SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      }\n    }\n    // mark the new slices as active and the old slice as inactive\n    log.trace(\"-- switching slice states after split shard: collection={}, parent={}, subSlices={}\", collectionName,\n        sliceName.get(), subSlices);\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      sProps.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      sProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      AtomicLong bufferedUpdates = (AtomicLong)sProps.remove(BUFFERED_UPDATES);\n      if (bufferedUpdates.get() > 0) {\n        // apply buffered updates\n        long perShard = bufferedUpdates.get() / subSlices.size();\n        long remainder = bufferedUpdates.get() % subSlices.size();\n        log.debug(\"-- applying {} buffered docs from {} / {}, perShard={}, remainder={}\", bufferedUpdates.get(),\n            collectionName, parentSlice.getName(), perShard, remainder);\n        for (int i = 0; i < subSlices.size(); i++) {\n          String sub = subSlices.get(i);\n          long numUpdates = perShard;\n          if (i == 0) {\n            numUpdates += remainder;\n          }\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.numDocs\", numUpdates, true, false);\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.maxDoc\", numUpdates, true, false);\n        }\n      }\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      for (String s : subSlices) {\n        Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(s, ss -> new ConcurrentHashMap<>());\n        sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n        sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      }\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      lock.unlock();\n    }\n    results.add(\"success\", \"\");\n\n  }\n\n","sourceOld":"  /**\n   * Split a shard. This uses a similar algorithm as {@link SplitShardCmd}, including simulating its\n   * quirks, and leaving the original parent slice in place.\n   * @param message operation details\n   * @param results operation results.\n   */\n  public void simSplitShard(ZkNodeProps message, NamedList results) throws Exception {\n    ensureNotClosed();\n    if (message.getStr(CommonAdminParams.ASYNC) != null) {\n      results.add(CoreAdminParams.REQUESTID, message.getStr(CommonAdminParams.ASYNC));\n    }\n    String collectionName = message.getStr(COLLECTION_PROP);\n    AtomicReference<String> sliceName = new AtomicReference<>();\n    sliceName.set(message.getStr(SHARD_ID_PROP));\n    String splitKey = message.getStr(\"split.key\");\n\n    ClusterState clusterState = getClusterState();\n    DocCollection collection = clusterState.getCollection(collectionName);\n    Slice parentSlice = SplitShardCmd.getParentSlice(clusterState, collectionName, sliceName, splitKey);\n    Replica leader = parentSlice.getLeader();\n    // XXX leader election may not have happened yet - should we require it?\n    if (leader == null) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Shard \" + collectionName +\n          \" /  \" + sliceName.get() + \" has no leader and can't be split\");\n    }\n    SplitShardCmd.lockForSplit(cloudManager, collectionName, sliceName.get());\n    // start counting buffered updates\n    Map<String, Object> props = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n        .computeIfAbsent(sliceName.get(), ss -> new ConcurrentHashMap<>());\n    if (props.containsKey(BUFFERED_UPDATES)) {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      throw new Exception(\"--- SOLR-12729: Overlapping splitShard commands for \" + collectionName + \"/\" + sliceName.get());\n    }\n    props.put(BUFFERED_UPDATES, new AtomicLong());\n\n    List<DocRouter.Range> subRanges = new ArrayList<>();\n    List<String> subSlices = new ArrayList<>();\n    List<String> subShardNames = new ArrayList<>();\n\n    opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    SplitShardCmd.fillRanges(cloudManager, message, collection, parentSlice, subRanges, subSlices, subShardNames, true);\n    // add replicas for new subShards\n    int repFactor = parentSlice.getReplicas().size();\n    Assign.AssignRequest assignRequest = new Assign.AssignRequestBuilder()\n        .forCollection(collectionName)\n        .forShard(subSlices)\n        .assignNrtReplicas(repFactor)\n        .assignTlogReplicas(0)\n        .assignPullReplicas(0)\n        .onNodes(new ArrayList<>(clusterState.getLiveNodes()))\n        .build();\n    Assign.AssignStrategyFactory assignStrategyFactory = new Assign.AssignStrategyFactory(cloudManager);\n    Assign.AssignStrategy assignStrategy = assignStrategyFactory.create(clusterState, collection);\n    // reproduce the bug\n    List<ReplicaPosition> replicaPositions = assignStrategy.assign(cloudManager, assignRequest);\n    PolicyHelper.SessionWrapper sessionWrapper = PolicyHelper.getLastSessionWrapper(true);\n    if (sessionWrapper != null) sessionWrapper.release();\n\n    // adjust numDocs / deletedDocs / maxDoc\n    String numDocsStr = String.valueOf(getReplicaInfo(leader).getVariable(\"SEARCHER.searcher.numDocs\", \"0\"));\n    long numDocs = Long.parseLong(numDocsStr);\n    long newNumDocs = numDocs / subSlices.size();\n    long remainderDocs = numDocs % subSlices.size();\n    long newIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + newNumDocs * DEFAULT_DOC_SIZE_BYTES;\n    long remainderIndexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES + remainderDocs * DEFAULT_DOC_SIZE_BYTES;\n    String remainderSlice = null;\n\n    // add slice props\n    for (int i = 0; i < subRanges.size(); i++) {\n      String subSlice = subSlices.get(i);\n      DocRouter.Range range = subRanges.get(i);\n      Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(subSlice, ss -> new ConcurrentHashMap<>());\n      sliceProps.put(Slice.RANGE, range);\n      sliceProps.put(Slice.PARENT, sliceName.get());\n      sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.CONSTRUCTION.toString());\n      sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n    }\n    // add replicas\n    for (ReplicaPosition replicaPosition : replicaPositions) {\n      String subSliceName = replicaPosition.shard;\n      String subShardNodeName = replicaPosition.node;\n//      String solrCoreName = collectionName + \"_\" + subSliceName + \"_replica_n\" + (replicaPosition.index);\n      String solrCoreName = Assign.buildSolrCoreName(collectionName, subSliceName, replicaPosition.type, Assign.incAndGetId(stateManager, collectionName, 0));\n      Map<String, Object> replicaProps = new HashMap<>();\n      replicaProps.put(ZkStateReader.SHARD_ID_PROP, replicaPosition.shard);\n      replicaProps.put(ZkStateReader.NODE_NAME_PROP, replicaPosition.node);\n      replicaProps.put(ZkStateReader.REPLICA_TYPE, replicaPosition.type.toString());\n      replicaProps.put(ZkStateReader.BASE_URL_PROP, Utils.getBaseUrlForNodeName(subShardNodeName, \"http\"));\n\n      long replicasNumDocs = newNumDocs;\n      long replicasIndexSize = newIndexSize;\n      if (remainderSlice == null) {\n        remainderSlice = subSliceName;\n      }\n      if (remainderSlice.equals(subSliceName)) { // only add to one sub slice\n        replicasNumDocs += remainderDocs;\n        replicasIndexSize += remainderIndexSize;\n      }\n      replicaProps.put(\"SEARCHER.searcher.numDocs\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.maxDoc\", new AtomicLong(replicasNumDocs));\n      replicaProps.put(\"SEARCHER.searcher.deletedDocs\", new AtomicLong(0));\n      replicaProps.put(Type.CORE_IDX.metricsAttribute, new AtomicLong(replicasIndexSize));\n      replicaProps.put(Variable.coreidxsize, new AtomicDouble((Double)Type.CORE_IDX.convertVal(replicasIndexSize)));\n\n      ReplicaInfo ri = new ReplicaInfo(\"core_node\" + Assign.incAndGetId(stateManager, collectionName, 0),\n          solrCoreName, collectionName, replicaPosition.shard, replicaPosition.type, subShardNodeName, replicaProps);\n      simAddReplica(replicaPosition.node, ri, false);\n    }\n    simRunLeaderElection(Collections.singleton(collectionName), true);\n\n    // delay it once again to better simulate replica recoveries\n    //opDelay(collectionName, CollectionParams.CollectionAction.SPLITSHARD.name());\n\n    boolean success = false;\n    try {\n      CloudTestUtils.waitForState(cloudManager, collectionName, 30, TimeUnit.SECONDS, (liveNodes, state) -> {\n        for (String subSlice : subSlices) {\n          Slice s = state.getSlice(subSlice);\n          if (s.getLeader() == null) {\n            log.debug(\"** no leader in {} / {}\", collectionName, s);\n            return false;\n          }\n          if (s.getReplicas().size() < repFactor) {\n            log.debug(\"** expected {} repFactor but there are {} replicas\", repFactor, s.getReplicas().size());\n            return false;\n          }\n        }\n        return true;\n      });\n      success = true;\n    } finally {\n      if (!success) {\n        Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n        sProps.remove(BUFFERED_UPDATES);\n        SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      }\n    }\n    // mark the new slices as active and the old slice as inactive\n    log.trace(\"-- switching slice states after split shard: collection={}, parent={}, subSlices={}\", collectionName,\n        sliceName.get(), subSlices);\n    lock.lockInterruptibly();\n    try {\n      Map<String, Object> sProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n          .computeIfAbsent(sliceName.get(), s -> new ConcurrentHashMap<>());\n      sProps.put(ZkStateReader.STATE_PROP, Slice.State.INACTIVE.toString());\n      sProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      AtomicLong bufferedUpdates = (AtomicLong)sProps.remove(BUFFERED_UPDATES);\n      if (bufferedUpdates.get() > 0) {\n        // apply buffered updates\n        long perShard = bufferedUpdates.get() / subSlices.size();\n        long remainder = bufferedUpdates.get() % subSlices.size();\n        log.debug(\"-- applying {} buffered docs from {} / {}, perShard={}, remainder={}\", bufferedUpdates.get(),\n            collectionName, parentSlice.getName(), perShard, remainder);\n        for (int i = 0; i < subSlices.size(); i++) {\n          String sub = subSlices.get(i);\n          long numUpdates = perShard;\n          if (i == 0) {\n            numUpdates += remainder;\n          }\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.numDocs\", numUpdates, true, false);\n          simSetShardValue(collectionName, sub, \"SEARCHER.searcher.maxDoc\", numUpdates, true, false);\n        }\n      }\n      // XXX also mark replicas as down? currently SplitShardCmd doesn't do this\n\n      for (String s : subSlices) {\n        Map<String, Object> sliceProps = sliceProperties.computeIfAbsent(collectionName, c -> new ConcurrentHashMap<>())\n            .computeIfAbsent(s, ss -> new ConcurrentHashMap<>());\n        sliceProps.put(ZkStateReader.STATE_PROP, Slice.State.ACTIVE.toString());\n        sliceProps.put(ZkStateReader.STATE_TIMESTAMP_PROP, String.valueOf(cloudManager.getTimeSource().getEpochTimeNs()));\n      }\n\n      // invalidate cached state\n      collectionsStatesRef.set(null);\n    } finally {\n      SplitShardCmd.unlockForSplit(cloudManager, collectionName, sliceName.get());\n      lock.unlock();\n    }\n    results.add(\"success\", \"\");\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fc18bc8ea2e2c1e308757ff50671c774438e9f3e":["e9f71a1c6c905e9489b4d25c83c8d628d978a8ea"],"7cac1f2920f8057198f04505797cbabf74dd9a97":["54e3d90778c12196fe1bb9dfa019fa2e7824e3aa"],"e9f71a1c6c905e9489b4d25c83c8d628d978a8ea":["042b92cf48996255bedb0c3c4bf772d7e06e4dea"],"685af99397b6da31116a2cac747ed255d217d080":["6b55cd711a129fb7fc4c3c4672d652149c9a4faa"],"c9bf5e4e82a5eeecf4d057625b9698db14d6f093":["43345f1452f9510f8aaadae6156fe0c834e7d957"],"43345f1452f9510f8aaadae6156fe0c834e7d957":["d4412883c12067d8a4e2a354aa8adc58c32be1d6","ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1"],"ae70f2df00762dfce0455c0e39381848762662e5":["fc18bc8ea2e2c1e308757ff50671c774438e9f3e"],"042b92cf48996255bedb0c3c4bf772d7e06e4dea":["427edb17549d4bb82462a16eec4ee0533d12d5b7"],"9c6c0dad4932399aec99b4818086cb1772773916":["a4422b331d00607258b0ed3e43934306e67764aa"],"6b55cd711a129fb7fc4c3c4672d652149c9a4faa":["c9bf5e4e82a5eeecf4d057625b9698db14d6f093"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["c9bf5e4e82a5eeecf4d057625b9698db14d6f093","685af99397b6da31116a2cac747ed255d217d080"],"ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1":["d4412883c12067d8a4e2a354aa8adc58c32be1d6"],"54e3d90778c12196fe1bb9dfa019fa2e7824e3aa":["ae70f2df00762dfce0455c0e39381848762662e5"],"427edb17549d4bb82462a16eec4ee0533d12d5b7":["685af99397b6da31116a2cac747ed255d217d080"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d4412883c12067d8a4e2a354aa8adc58c32be1d6":["9c6c0dad4932399aec99b4818086cb1772773916"],"a4422b331d00607258b0ed3e43934306e67764aa":["1d4bf9d5308dfef350829c28f2b3b2648df1e9b1"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["7cac1f2920f8057198f04505797cbabf74dd9a97"],"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["c9bf5e4e82a5eeecf4d057625b9698db14d6f093","685af99397b6da31116a2cac747ed255d217d080"]},"commit2Childs":{"fc18bc8ea2e2c1e308757ff50671c774438e9f3e":["ae70f2df00762dfce0455c0e39381848762662e5"],"7cac1f2920f8057198f04505797cbabf74dd9a97":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"e9f71a1c6c905e9489b4d25c83c8d628d978a8ea":["fc18bc8ea2e2c1e308757ff50671c774438e9f3e"],"685af99397b6da31116a2cac747ed255d217d080":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","427edb17549d4bb82462a16eec4ee0533d12d5b7","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"c9bf5e4e82a5eeecf4d057625b9698db14d6f093":["6b55cd711a129fb7fc4c3c4672d652149c9a4faa","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"43345f1452f9510f8aaadae6156fe0c834e7d957":["c9bf5e4e82a5eeecf4d057625b9698db14d6f093"],"ae70f2df00762dfce0455c0e39381848762662e5":["54e3d90778c12196fe1bb9dfa019fa2e7824e3aa"],"042b92cf48996255bedb0c3c4bf772d7e06e4dea":["e9f71a1c6c905e9489b4d25c83c8d628d978a8ea"],"9c6c0dad4932399aec99b4818086cb1772773916":["d4412883c12067d8a4e2a354aa8adc58c32be1d6"],"6b55cd711a129fb7fc4c3c4672d652149c9a4faa":["685af99397b6da31116a2cac747ed255d217d080"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1":["43345f1452f9510f8aaadae6156fe0c834e7d957"],"54e3d90778c12196fe1bb9dfa019fa2e7824e3aa":["7cac1f2920f8057198f04505797cbabf74dd9a97"],"427edb17549d4bb82462a16eec4ee0533d12d5b7":["042b92cf48996255bedb0c3c4bf772d7e06e4dea"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1d4bf9d5308dfef350829c28f2b3b2648df1e9b1"],"d4412883c12067d8a4e2a354aa8adc58c32be1d6":["43345f1452f9510f8aaadae6156fe0c834e7d957","ce71a9836bf1eba34a0ab31884e9eb8ad3f1cef1"],"a4422b331d00607258b0ed3e43934306e67764aa":["9c6c0dad4932399aec99b4818086cb1772773916"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1":["a4422b331d00607258b0ed3e43934306e67764aa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}