{"path":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","commits":[{"id":"a371aa649cc243e82cb8677ca960a1e0232ecedf","date":1393605574,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(TopDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LookupResult> results = new ArrayList<LookupResult>();\n    BytesRef scratch = new BytesRef();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      textDV.get(fd.doc, scratch);\n      String text = scratch.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = new BytesRef();\n        payloadsDV.get(fd.doc, payload);\n      } else {\n        payload = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload);\n      } else {\n        result = new LookupResult(text, score, payload);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(TopDocs hits, int num, CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    List<LookupResult> results = new ArrayList<LookupResult>();\n    BytesRef scratch = new BytesRef();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      ScoreDoc sd = hits.scoreDocs[i];\n      textDV.get(sd.doc, scratch);\n      String text = scratch.utf8ToString();\n      long score = weightsDV.get(sd.doc);\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = new BytesRef();\n        payloadsDV.get(sd.doc, payload);\n      } else {\n        payload = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload);\n      } else {\n        result = new LookupResult(text, score, payload);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LookupResult> results = new ArrayList<>();\n    BytesRef scratch = new BytesRef();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      textDV.get(fd.doc, scratch);\n      String text = scratch.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = new BytesRef();\n        payloadsDV.get(fd.doc, payload);\n      } else {\n        payload = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload);\n      } else {\n        result = new LookupResult(text, score, payload);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LookupResult> results = new ArrayList<LookupResult>();\n    BytesRef scratch = new BytesRef();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      textDV.get(fd.doc, scratch);\n      String text = scratch.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = new BytesRef();\n        payloadsDV.get(fd.doc, payload);\n      } else {\n        payload = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload);\n      } else {\n        result = new LookupResult(text, score, payload);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b","date":1395588343,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    BytesRef scratch = new BytesRef();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      textDV.get(fd.doc, scratch);\n      String text = scratch.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = new BytesRef();\n        payloadsDV.get(fd.doc, payload);\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = new BytesRef();\n          contextsDV.lookupOrd(ord, context);\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LookupResult> results = new ArrayList<>();\n    BytesRef scratch = new BytesRef();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      textDV.get(fd.doc, scratch);\n      String text = scratch.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = new BytesRef();\n        payloadsDV.get(fd.doc, payload);\n      } else {\n        payload = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload);\n      } else {\n        result = new LookupResult(text, score, payload);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":["ec083aa3f3ecd55f91c47009d49e45553f99bd77"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","date":1401983689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    BytesRef scratch = new BytesRef();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      textDV.get(fd.doc, scratch);\n      String text = scratch.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = new BytesRef();\n        payloadsDV.get(fd.doc, payload);\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = new BytesRef();\n          contextsDV.lookupOrd(ord, context);\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ec083aa3f3ecd55f91c47009d49e45553f99bd77","date":1416002645,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        Object highlightKey = highlight(text, matchedTokens, prefixToken);\n        result = new LookupResult(highlightKey.toString(), highlightKey, score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b","f13ec1b606a28789743a563929e7c556e8218297"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"79854637616b791a00f39ee3d5257ea093804ddb","date":1422697309,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation)\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":["f13ec1b606a28789743a563929e7c556e8218297"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22aab7a3b640b0dba26cc5e9416bc7af93614b46","date":1462575761,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n\n      BytesRef term = getBinaryDocValue(searcher.getIndexReader(), TEXT_FIELD_NAME, fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload = getBinaryDocValue(searcher.getIndexReader(), \"payloads\", fd.doc);\n      if (payload != null) {\n        payload = BytesRef.deepCopyOf(payload);\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"815972da4a13279b8e975d2e32ca450649d6c295","date":1462635959,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n\n      BytesRef term = getBinaryDocValue(searcher.getIndexReader(), TEXT_FIELD_NAME, fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload = getBinaryDocValue(searcher.getIndexReader(), \"payloads\", fd.doc);\n      if (payload != null) {\n        payload = BytesRef.deepCopyOf(payload);\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n      textDV.advance(fd.doc);\n      BytesRef term = textDV.binaryValue();\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      // This will just be null if app didn't pass payloads to build():\n      // TODO: maybe just stored fields?  they compress...\n      BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        if (payloadsDV.advance(fd.doc) == fd.doc) {\n          payload = BytesRef.deepCopyOf(payloadsDV.binaryValue());\n        } else {\n          payload = new BytesRef(BytesRef.EMPTY_BYTES);\n        }\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        int targetDocID = fd.doc - leaves.get(segment).docBase;\n        if (contextsDV.advance(targetDocID) == targetDocID) {\n          long ord;\n          while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n            BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n            contexts.add(context);\n          }\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n      textDV.advance(fd.doc);\n      BytesRef term = textDV.binaryValue();\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      // This will just be null if app didn't pass payloads to build():\n      // TODO: maybe just stored fields?  they compress...\n      BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        if (payloadsDV.advance(fd.doc) == fd.doc) {\n          payload = BytesRef.deepCopyOf(payloadsDV.binaryValue());\n        } else {\n          payload = new BytesRef(BytesRef.EMPTY_BYTES);\n        }\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        int targetDocID = fd.doc - leaves.get(segment).docBase;\n        if (contextsDV.advance(targetDocID) == targetDocID) {\n          long ord;\n          while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n            BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n            contexts.add(context);\n          }\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n      textDV.advance(fd.doc);\n      BytesRef term = textDV.binaryValue();\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      // This will just be null if app didn't pass payloads to build():\n      // TODO: maybe just stored fields?  they compress...\n      BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        if (payloadsDV.advance(fd.doc) == fd.doc) {\n          payload = BytesRef.deepCopyOf(payloadsDV.binaryValue());\n        } else {\n          payload = new BytesRef(BytesRef.EMPTY_BYTES);\n        }\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        int targetDocID = fd.doc - leaves.get(segment).docBase;\n        if (contextsDV.advance(targetDocID) == targetDocID) {\n          long ord;\n          while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n            BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n            contexts.add(context);\n          }\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n\n    // This will just be null if app didn't pass payloads to build():\n    // TODO: maybe just stored fields?  they compress...\n    BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BytesRef term = textDV.get(fd.doc);\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        payload = BytesRef.deepCopyOf(payloadsDV.get(fd.doc));\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        contextsDV.setDocument(fd.doc - leaves.get(segment).docBase);\n        long ord;\n        while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n          BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n          contexts.add(context);\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d669cb2b7fbc6ceb7f966e63a1c625e6400c7d2a","date":1550036130,"type":3,"author":"Bruno P. Kinoshita","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#createResults(IndexSearcher,TopFieldDocs,int,CharSequence,boolean,Set[String],String).mjava","sourceNew":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix token (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n      textDV.advance(fd.doc);\n      BytesRef term = textDV.binaryValue();\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      // This will just be null if app didn't pass payloads to build():\n      // TODO: maybe just stored fields?  they compress...\n      BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        if (payloadsDV.advance(fd.doc) == fd.doc) {\n          payload = BytesRef.deepCopyOf(payloadsDV.binaryValue());\n        } else {\n          payload = new BytesRef(BytesRef.EMPTY_BYTES);\n        }\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        int targetDocID = fd.doc - leaves.get(segment).docBase;\n        if (contextsDV.advance(targetDocID) == targetDocID) {\n          long ord;\n          while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n            BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n            contexts.add(context);\n          }\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Create the results based on the search hits.\n   * Can be overridden by subclass to add particular behavior (e.g. weight transformation).\n   * Note that there is no prefix toke (the {@code prefixToken} argument will\n   * be null) whenever the final token in the incoming request was in fact finished\n   * (had trailing characters, such as white-space).\n   *\n   * @throws IOException If there are problems reading fields from the underlying Lucene index.\n   */\n  protected List<LookupResult> createResults(IndexSearcher searcher, TopFieldDocs hits, int num,\n                                             CharSequence charSequence,\n                                             boolean doHighlight, Set<String> matchedTokens, String prefixToken)\n      throws IOException {\n\n    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n    List<LookupResult> results = new ArrayList<>();\n    for (int i=0;i<hits.scoreDocs.length;i++) {\n      FieldDoc fd = (FieldDoc) hits.scoreDocs[i];\n      BinaryDocValues textDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), TEXT_FIELD_NAME);\n      textDV.advance(fd.doc);\n      BytesRef term = textDV.binaryValue();\n      String text = term.utf8ToString();\n      long score = (Long) fd.fields[0];\n\n      // This will just be null if app didn't pass payloads to build():\n      // TODO: maybe just stored fields?  they compress...\n      BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), \"payloads\");\n\n      BytesRef payload;\n      if (payloadsDV != null) {\n        if (payloadsDV.advance(fd.doc) == fd.doc) {\n          payload = BytesRef.deepCopyOf(payloadsDV.binaryValue());\n        } else {\n          payload = new BytesRef(BytesRef.EMPTY_BYTES);\n        }\n      } else {\n        payload = null;\n      }\n\n      // Must look up sorted-set by segment:\n      int segment = ReaderUtil.subIndex(fd.doc, leaves);\n      SortedSetDocValues contextsDV = leaves.get(segment).reader().getSortedSetDocValues(CONTEXTS_FIELD_NAME);\n      Set<BytesRef> contexts;\n      if (contextsDV != null) {\n        contexts = new HashSet<BytesRef>();\n        int targetDocID = fd.doc - leaves.get(segment).docBase;\n        if (contextsDV.advance(targetDocID) == targetDocID) {\n          long ord;\n          while ((ord = contextsDV.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {\n            BytesRef context = BytesRef.deepCopyOf(contextsDV.lookupOrd(ord));\n            contexts.add(context);\n          }\n        }\n      } else {\n        contexts = null;\n      }\n\n      LookupResult result;\n\n      if (doHighlight) {\n        result = new LookupResult(text, highlight(text, matchedTokens, prefixToken), score, payload, contexts);\n      } else {\n        result = new LookupResult(text, score, payload, contexts);\n      }\n\n      results.add(result);\n    }\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a371aa649cc243e82cb8677ca960a1e0232ecedf"],"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"a371aa649cc243e82cb8677ca960a1e0232ecedf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["79854637616b791a00f39ee3d5257ea093804ddb","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"22aab7a3b640b0dba26cc5e9416bc7af93614b46":["79854637616b791a00f39ee3d5257ea093804ddb"],"d669cb2b7fbc6ceb7f966e63a1c625e6400c7d2a":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"79854637616b791a00f39ee3d5257ea093804ddb":["ec083aa3f3ecd55f91c47009d49e45553f99bd77"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["79854637616b791a00f39ee3d5257ea093804ddb","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["79854637616b791a00f39ee3d5257ea093804ddb"],"ec083aa3f3ecd55f91c47009d49e45553f99bd77":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"815972da4a13279b8e975d2e32ca450649d6c295":["22aab7a3b640b0dba26cc5e9416bc7af93614b46"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d669cb2b7fbc6ceb7f966e63a1c625e6400c7d2a"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"a371aa649cc243e82cb8677ca960a1e0232ecedf":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["ec083aa3f3ecd55f91c47009d49e45553f99bd77"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"22aab7a3b640b0dba26cc5e9416bc7af93614b46":["815972da4a13279b8e975d2e32ca450649d6c295"],"d669cb2b7fbc6ceb7f966e63a1c625e6400c7d2a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a371aa649cc243e82cb8677ca960a1e0232ecedf"],"79854637616b791a00f39ee3d5257ea093804ddb":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","22aab7a3b640b0dba26cc5e9416bc7af93614b46","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d669cb2b7fbc6ceb7f966e63a1c625e6400c7d2a"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"ec083aa3f3ecd55f91c47009d49e45553f99bd77":["79854637616b791a00f39ee3d5257ea093804ddb"],"815972da4a13279b8e975d2e32ca450649d6c295":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","815972da4a13279b8e975d2e32ca450649d6c295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}