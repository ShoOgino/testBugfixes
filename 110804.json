{"path":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","commits":[{"id":"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","date":1475611903,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","pathOld":"/dev/null","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.weight = scorer.weight(contentLength, off.postingsEnum.freq());\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.score < right.score) {\n        return -1;\n      } else if (left.score > right.score) {\n        return 1;\n      } else {\n        return left.startOffset - right.startOffset;\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.endOffset) {\n        if (passage.startOffset >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.score *= scorer.norm(passage.startOffset);\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.score < passageQueue.peek().score) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.startOffset = Math.max(breakIterator.preceding(start + 1), 0);\n        passage.endOffset = Math.min(breakIterator.following(start), contentLength);\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.endOffset || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.score += off.weight * scorer.tf(tf, passage.endOffset - passage.startOffset);\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.startOffset - right.startOffset);\n    return passages;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","pathOld":"/dev/null","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.weight = scorer.weight(contentLength, off.postingsEnum.freq());\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.score < right.score) {\n        return -1;\n      } else if (left.score > right.score) {\n        return 1;\n      } else {\n        return left.startOffset - right.startOffset;\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.endOffset) {\n        if (passage.startOffset >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.score *= scorer.norm(passage.startOffset);\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.score < passageQueue.peek().score) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.startOffset = Math.max(breakIterator.preceding(start + 1), 0);\n        passage.endOffset = Math.min(breakIterator.following(start), contentLength);\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.endOffset || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.score += off.weight * scorer.tf(tf, passage.endOffset - passage.startOffset);\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.startOffset - right.startOffset);\n    return passages;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f714d649962c934166dedd1e83173e36356b328","date":1479399594,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.weight = scorer.weight(contentLength, off.postingsEnum.freq());\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.weight * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.getStartOffset() - right.getStartOffset());\n    return passages;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.weight = scorer.weight(contentLength, off.postingsEnum.freq());\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.score < right.score) {\n        return -1;\n      } else if (left.score > right.score) {\n        return 1;\n      } else {\n        return left.startOffset - right.startOffset;\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.endOffset) {\n        if (passage.startOffset >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.score *= scorer.norm(passage.startOffset);\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.score < passageQueue.peek().score) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.startOffset = Math.max(breakIterator.preceding(start + 1), 0);\n        passage.endOffset = Math.min(breakIterator.following(start), contentLength);\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.endOffset || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.score += off.weight * scorer.tf(tf, passage.endOffset - passage.startOffset);\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.startOffset - right.startOffset);\n    return passages;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e859e664a65796dadf8aaf65db6f66f3a885368","date":1479487334,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.weight = scorer.weight(contentLength, off.postingsEnum.freq());\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.weight * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.getStartOffset() - right.getStartOffset());\n    return passages;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.weight = scorer.weight(contentLength, off.postingsEnum.freq());\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.score < right.score) {\n        return -1;\n      } else if (left.score > right.score) {\n        return 1;\n      } else {\n        return left.startOffset - right.startOffset;\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.endOffset) {\n        if (passage.startOffset >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.score *= scorer.norm(passage.startOffset);\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.score < passageQueue.peek().score) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.startOffset = Math.max(breakIterator.preceding(start + 1), 0);\n        passage.endOffset = Math.min(breakIterator.following(start), contentLength);\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.endOffset || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.score += off.weight * scorer.tf(tf, passage.endOffset - passage.startOffset);\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.startOffset - right.startOffset);\n    return passages;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b617b6c660071491e5781addba61cf29dcfa97a","date":1484198957,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.setWeight(scorer.weight(contentLength, off.freq()));\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.getWeight() * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.getStartOffset() - right.getStartOffset());\n    return passages;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.weight = scorer.weight(contentLength, off.postingsEnum.freq());\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.weight * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.getStartOffset() - right.getStartOffset());\n    return passages;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","date":1484239864,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.setWeight(scorer.weight(contentLength, off.freq()));\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.getWeight() * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.getStartOffset() - right.getStartOffset());\n    return passages;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.weight = scorer.weight(contentLength, off.postingsEnum.freq());\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.weight * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.getStartOffset() - right.getStartOffset());\n    return passages;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571","date":1515642580,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    //TODO consider moving this part to an aggregate OffsetsEnum subclass so we have one enum that already has its weight\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.setWeight(scorer.weight(contentLength, off.freq()));\n      if (off.nextPosition()) {// go to first position\n        offsetsEnumQueue.add(off);\n      }\n    }\n    offsetsEnumQueue.add(new OffsetsEnum.OfPostings(new BytesRef(), EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.nextPosition()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.getWeight() * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, Comparator.comparingInt(Passage::getStartOffset));\n    return passages;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.setWeight(scorer.weight(contentLength, off.freq()));\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.getWeight() * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.getStartOffset() - right.getStartOffset());\n    return passages;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    //TODO consider moving this part to an aggregate OffsetsEnum subclass so we have one enum that already has its weight\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.setWeight(scorer.weight(contentLength, off.freq()));\n      if (off.nextPosition()) {// go to first position\n        offsetsEnumQueue.add(off);\n      }\n    }\n    offsetsEnumQueue.add(new OffsetsEnum.OfPostings(new BytesRef(), EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.nextPosition()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.getWeight() * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, Comparator.comparingInt(Passage::getStartOffset));\n    return passages;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.setWeight(scorer.weight(contentLength, off.freq()));\n      off.nextPosition(); // go to first position\n      offsetsEnumQueue.add(off);\n    }\n    offsetsEnumQueue.add(new OffsetsEnum(null, EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.hasMorePositions()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        off.nextPosition();\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.getWeight() * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, (left, right) -> left.getStartOffset() - right.getStartOffset());\n    return passages;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8764ca7bb74ee716c839b9545a93ec4a578c2005","date":1517564468,"type":5,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(OffsetsEnum).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldHighlighter#highlightOffsetsEnums(List[OffsetsEnum]).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(OffsetsEnum off)\n      throws IOException {\n\n    final int contentLength = this.breakIterator.getText().getEndIndex();\n\n    if (off.nextPosition() == false) {\n      return new Passage[0];\n    }\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    do {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        passage = maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(this.breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(this.breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      BytesRef term = off.getTerm();// a reference; safe to refer to\n      assert term != null;\n      passage.addMatch(start, end, term, off.freq());\n    } while (off.nextPosition());\n    maybeAddPassage(passageQueue, passageScorer, passage, contentLength);\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    // sort in ascending order\n    Arrays.sort(passages, Comparator.comparingInt(Passage::getStartOffset));\n    return passages;\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  protected Passage[] highlightOffsetsEnums(List<OffsetsEnum> offsetsEnums)\n      throws IOException {\n    PassageScorer scorer = passageScorer;\n    BreakIterator breakIterator = this.breakIterator;\n    final int contentLength = breakIterator.getText().getEndIndex();\n\n    //TODO consider moving this part to an aggregate OffsetsEnum subclass so we have one enum that already has its weight\n    PriorityQueue<OffsetsEnum> offsetsEnumQueue = new PriorityQueue<>(offsetsEnums.size() + 1);\n    for (OffsetsEnum off : offsetsEnums) {\n      off.setWeight(scorer.weight(contentLength, off.freq()));\n      if (off.nextPosition()) {// go to first position\n        offsetsEnumQueue.add(off);\n      }\n    }\n    offsetsEnumQueue.add(new OffsetsEnum.OfPostings(new BytesRef(), EMPTY)); // a sentinel for termination\n\n    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(Math.min(64, maxPassages + 1), (left, right) -> {\n      if (left.getScore() < right.getScore()) {\n        return -1;\n      } else if (left.getScore() > right.getScore()) {\n        return 1;\n      } else {\n        return left.getStartOffset() - right.getStartOffset();\n      }\n    });\n    Passage passage = new Passage(); // the current passage in-progress.  Will either get reset or added to queue.\n\n    OffsetsEnum off;\n    while ((off = offsetsEnumQueue.poll()) != null) {\n      int start = off.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = off.endOffset();\n      // LUCENE-5166: this hit would span the content limit... however more valid\n      // hits may exist (they are sorted by start). so we pretend like we never\n      // saw this term, it won't cause a passage to be added to passageQueue or anything.\n      assert EMPTY.startOffset() == Integer.MAX_VALUE;\n      if (start < contentLength && end > contentLength) {\n        continue;\n      }\n      // See if this term should be part of a new passage.\n      if (start >= passage.getEndOffset()) {\n        if (passage.getStartOffset() >= 0) { // true if this passage has terms; otherwise couldn't find any (yet)\n          // finalize passage\n          passage.setScore(passage.getScore() * scorer.norm(passage.getStartOffset()));\n          // new sentence: first add 'passage' to queue\n          if (passageQueue.size() == maxPassages && passage.getScore() < passageQueue.peek().getScore()) {\n            passage.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(passage);\n            if (passageQueue.size() > maxPassages) {\n              passage = passageQueue.poll();\n              passage.reset();\n            } else {\n              passage = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          break;\n        }\n        // advance breakIterator\n        passage.setStartOffset(Math.max(breakIterator.preceding(start + 1), 0));\n        passage.setEndOffset(Math.min(breakIterator.following(start), contentLength));\n      }\n      // Add this term to the passage.\n      int tf = 0;\n      while (true) {\n        tf++;\n        BytesRef term = off.getTerm();// a reference; safe to refer to\n        assert term != null;\n        passage.addMatch(start, end, term);\n        // see if there are multiple occurrences of this term in this passage. If so, add them.\n        if (!off.nextPosition()) {\n          break; // No more in the entire text. Already removed from pq; move on\n        }\n        start = off.startOffset();\n        end = off.endOffset();\n        if (start >= passage.getEndOffset() || end > contentLength) { // it's beyond this passage\n          offsetsEnumQueue.offer(off);\n          break;\n        }\n      }\n      passage.setScore(passage.getScore() + off.getWeight() * scorer.tf(tf, passage.getEndOffset() - passage.getStartOffset()));\n    }\n\n    Passage[] passages = passageQueue.toArray(new Passage[passageQueue.size()]);\n    for (Passage p : passages) {\n      p.sort();\n    }\n    // sort in ascending order\n    Arrays.sort(passages, Comparator.comparingInt(Passage::getStartOffset));\n    return passages;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b94236357aaa22b76c10629851fe4e376e0cea82":["7b617b6c660071491e5781addba61cf29dcfa97a","eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571"],"7b617b6c660071491e5781addba61cf29dcfa97a":["1f714d649962c934166dedd1e83173e36356b328"],"8764ca7bb74ee716c839b9545a93ec4a578c2005":["b94236357aaa22b76c10629851fe4e376e0cea82"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571":["7b617b6c660071491e5781addba61cf29dcfa97a"],"1f714d649962c934166dedd1e83173e36356b328":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":["2e859e664a65796dadf8aaf65db6f66f3a885368","7b617b6c660071491e5781addba61cf29dcfa97a"],"2e859e664a65796dadf8aaf65db6f66f3a885368":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","1f714d649962c934166dedd1e83173e36356b328"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8764ca7bb74ee716c839b9545a93ec4a578c2005"]},"commit2Childs":{"b94236357aaa22b76c10629851fe4e376e0cea82":["8764ca7bb74ee716c839b9545a93ec4a578c2005"],"7b617b6c660071491e5781addba61cf29dcfa97a":["b94236357aaa22b76c10629851fe4e376e0cea82","eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571","09ab8ee44ca898536770d0106a7c0ee4be4f0eb7"],"8764ca7bb74ee716c839b9545a93ec4a578c2005":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["1f714d649962c934166dedd1e83173e36356b328","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"eef54e3d232eae0e9fc18d75e9b0c3d9ce04b571":["b94236357aaa22b76c10629851fe4e376e0cea82"],"1f714d649962c934166dedd1e83173e36356b328":["7b617b6c660071491e5781addba61cf29dcfa97a","2e859e664a65796dadf8aaf65db6f66f3a885368"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":[],"2e859e664a65796dadf8aaf65db6f66f3a885368":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["2e859e664a65796dadf8aaf65db6f66f3a885368"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}