{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","commits":[{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletes(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    assert testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    ReadersAndUpdates mergedDeletesAndUpdates = null;\n    boolean initWritableLiveDocs = false;\n    MergePolicy.DocMap docMap = null;\n    final Map<String,NumericFieldUpdates> mergedFieldUpdates = new HashMap<String,NumericFieldUpdates>();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,NumericFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final UpdatesIterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        updatesIters = new UpdatesIterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,NumericFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          mergingFields[idx] = e.getKey();\n          updatesIters[idx] = e.getValue().getUpdates();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                  docMap = getDocMap(merge, mergeState);\n                } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                }\n                mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                // document isn't deleted, check if any of the fields have an update to it\n                int newDoc = -1;\n                for (int idx = 0; idx < mergingFields.length; idx++) {\n                  UpdatesIterator updatesIter = updatesIters[idx];\n                  if (updatesIter.doc() == j) { // document has an update\n                    if (mergedDeletesAndUpdates == null) {\n                      mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                      docMap = getDocMap(merge, mergeState);\n                    }\n                    if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                      newDoc = docMap.map(docUpto);\n                    }\n                    String field = mergingFields[idx];\n                    NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                    if (fieldUpdates == null) {\n                      // an approximantion of maxDoc, used to compute best bitsPerValue\n                      fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                      mergedFieldUpdates.put(field, fieldUpdates);\n                    }\n                    fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                    updatesIter.nextDoc(); // advance to next document\n                  } else {\n                    assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                  }\n                }\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              int newDoc = -1;\n              for (int idx = 0; idx < mergingFields.length; idx++) {\n                UpdatesIterator updatesIter = updatesIters[idx];\n                if (updatesIter.doc() == j) { // document has an update\n                  if (mergedDeletesAndUpdates == null) {\n                    mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                    docMap = getDocMap(merge, mergeState);\n                  }\n                  if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                    newDoc = docMap.map(docUpto);\n                  }\n                  String field = mergingFields[idx];\n                  NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                  if (fieldUpdates == null) {\n                    // an approximantion of maxDoc, used to compute best bitsPerValue\n                    fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                    mergedFieldUpdates.put(field, fieldUpdates);\n                  }\n                  fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                  updatesIter.nextDoc(); // advance to next document\n                } else {\n                  assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                }\n              }\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (mergedDeletesAndUpdates == null) {\n              mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n              docMap = getDocMap(merge, mergeState);\n            } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n            }\n            mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            // document isn't deleted, check if any of the fields have an update to it\n            int newDoc = -1;\n            for (int idx = 0; idx < mergingFields.length; idx++) {\n              UpdatesIterator updatesIter = updatesIters[idx];\n              if (updatesIter.doc() == j) { // document has an update\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  docMap = getDocMap(merge, mergeState);\n                }\n                if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                  newDoc = docMap.map(docUpto);\n                }\n                String field = mergingFields[idx];\n                NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                if (fieldUpdates == null) {\n                  // an approximantion of maxDoc, used to compute best bitsPerValue\n                  fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                  mergedFieldUpdates.put(field, fieldUpdates);\n                }\n                fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                updatesIter.nextDoc(); // advance to next document\n              } else {\n                assert updatesIter.doc() > j : \"field=\" + mergingFields[idx] + \" updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n              }\n            }\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          int newDoc = -1;\n          for (int idx = 0; idx < mergingFields.length; idx++) {\n            UpdatesIterator updatesIter = updatesIters[idx];\n            if (updatesIter.doc() == j) { // document has an update\n              if (mergedDeletesAndUpdates == null) {\n                mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                docMap = getDocMap(merge, mergeState);\n              }\n              if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                newDoc = docMap.map(docUpto);\n              }\n              String field = mergingFields[idx];\n              NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n              if (fieldUpdates == null) {\n                // an approximantion of maxDoc, used to compute best bitsPerValue\n                fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                mergedFieldUpdates.put(field, fieldUpdates);\n              }\n              fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n              updatesIter.nextDoc(); // advance to next document\n            } else {\n              assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n            }\n          }\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (!mergedFieldUpdates.isEmpty()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedFieldUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (!mergedFieldUpdates.isEmpty()) {\n          msg += \" and \" + mergedFieldUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  // TODO (DVU_RENAME) to commitMergedDeletesAndUpdates\n  synchronized private ReadersAndLiveDocs commitMergedDeletes(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    assert testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentInfoPerCommit> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    ReadersAndLiveDocs mergedDeletes = null; // TODO (DVU_RENAME) to mergedDeletesAndUpdates\n    boolean initWritableLiveDocs = false;\n    MergePolicy.DocMap docMap = null;\n    final Map<String,NumericFieldUpdates> mergedFieldUpdates = new HashMap<String,NumericFieldUpdates>();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentInfoPerCommit info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndLiveDocs rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,NumericFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final UpdatesIterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        updatesIters = new UpdatesIterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,NumericFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          mergingFields[idx] = e.getKey();\n          updatesIters[idx] = e.getValue().getUpdates();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (mergedDeletes == null) {\n                  mergedDeletes = readerPool.get(merge.info, true);\n                  mergedDeletes.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                  docMap = getDocMap(merge, mergeState);\n                } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n                  mergedDeletes.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                }\n                mergedDeletes.delete(docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                // document isn't deleted, check if any of the fields have an update to it\n                int newDoc = -1;\n                for (int idx = 0; idx < mergingFields.length; idx++) {\n                  UpdatesIterator updatesIter = updatesIters[idx];\n                  if (updatesIter.doc() == j) { // document has an update\n                    if (mergedDeletes == null) {\n                      mergedDeletes = readerPool.get(merge.info, true);\n                      docMap = getDocMap(merge, mergeState);\n                    }\n                    if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                      newDoc = docMap.map(docUpto);\n                    }\n                    String field = mergingFields[idx];\n                    NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                    if (fieldUpdates == null) {\n                      // an approximantion of maxDoc, used to compute best bitsPerValue\n                      fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                      mergedFieldUpdates.put(field, fieldUpdates);\n                    }\n                    fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                    updatesIter.nextDoc(); // advance to next document\n                  } else {\n                    assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                  }\n                }\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              int newDoc = -1;\n              for (int idx = 0; idx < mergingFields.length; idx++) {\n                UpdatesIterator updatesIter = updatesIters[idx];\n                if (updatesIter.doc() == j) { // document has an update\n                  if (mergedDeletes == null) {\n                    mergedDeletes = readerPool.get(merge.info, true);\n                    docMap = getDocMap(merge, mergeState);\n                  }\n                  if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                    newDoc = docMap.map(docUpto);\n                  }\n                  String field = mergingFields[idx];\n                  NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                  if (fieldUpdates == null) {\n                    // an approximantion of maxDoc, used to compute best bitsPerValue\n                    fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                    mergedFieldUpdates.put(field, fieldUpdates);\n                  }\n                  fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                  updatesIter.nextDoc(); // advance to next document\n                } else {\n                  assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                }\n              }\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (mergedDeletes == null) {\n              mergedDeletes = readerPool.get(merge.info, true);\n              mergedDeletes.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n              docMap = getDocMap(merge, mergeState);\n            } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n              mergedDeletes.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n            }\n            mergedDeletes.delete(docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            // document isn't deleted, check if any of the fields have an update to it\n            int newDoc = -1;\n            for (int idx = 0; idx < mergingFields.length; idx++) {\n              UpdatesIterator updatesIter = updatesIters[idx];\n              if (updatesIter.doc() == j) { // document has an update\n                if (mergedDeletes == null) {\n                  mergedDeletes = readerPool.get(merge.info, true);\n                  docMap = getDocMap(merge, mergeState);\n                }\n                if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                  newDoc = docMap.map(docUpto);\n                }\n                String field = mergingFields[idx];\n                NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                if (fieldUpdates == null) {\n                  // an approximantion of maxDoc, used to compute best bitsPerValue\n                  fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                  mergedFieldUpdates.put(field, fieldUpdates);\n                }\n                fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                updatesIter.nextDoc(); // advance to next document\n              } else {\n                assert updatesIter.doc() > j : \"field=\" + mergingFields[idx] + \" updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n              }\n            }\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          int newDoc = -1;\n          for (int idx = 0; idx < mergingFields.length; idx++) {\n            UpdatesIterator updatesIter = updatesIters[idx];\n            if (updatesIter.doc() == j) { // document has an update\n              if (mergedDeletes == null) {\n                mergedDeletes = readerPool.get(merge.info, true);\n                docMap = getDocMap(merge, mergeState);\n              }\n              if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                newDoc = docMap.map(docUpto);\n              }\n              String field = mergingFields[idx];\n              NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n              if (fieldUpdates == null) {\n                // an approximantion of maxDoc, used to compute best bitsPerValue\n                fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                mergedFieldUpdates.put(field, fieldUpdates);\n              }\n              fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n              updatesIter.nextDoc(); // advance to next document\n            } else {\n              assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n            }\n          }\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (!mergedFieldUpdates.isEmpty()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        mergedDeletes.writeFieldUpdates(directory, mergedFieldUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          mergedDeletes.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletes == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletes.getPendingDeleteCount() + \" new deletes\";\n        if (!mergedFieldUpdates.isEmpty()) {\n          msg += \" and \" + mergedFieldUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletes;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    assert testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    ReadersAndUpdates mergedDeletesAndUpdates = null;\n    boolean initWritableLiveDocs = false;\n    MergePolicy.DocMap docMap = null;\n    final Map<String,NumericFieldUpdates> mergedFieldUpdates = new HashMap<>();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,NumericFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final UpdatesIterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        updatesIters = new UpdatesIterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,NumericFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          mergingFields[idx] = e.getKey();\n          updatesIters[idx] = e.getValue().getUpdates();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                  docMap = getDocMap(merge, mergeState);\n                } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                }\n                mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                // document isn't deleted, check if any of the fields have an update to it\n                int newDoc = -1;\n                for (int idx = 0; idx < mergingFields.length; idx++) {\n                  UpdatesIterator updatesIter = updatesIters[idx];\n                  if (updatesIter.doc() == j) { // document has an update\n                    if (mergedDeletesAndUpdates == null) {\n                      mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                      docMap = getDocMap(merge, mergeState);\n                    }\n                    if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                      newDoc = docMap.map(docUpto);\n                    }\n                    String field = mergingFields[idx];\n                    NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                    if (fieldUpdates == null) {\n                      // an approximantion of maxDoc, used to compute best bitsPerValue\n                      fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                      mergedFieldUpdates.put(field, fieldUpdates);\n                    }\n                    fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                    updatesIter.nextDoc(); // advance to next document\n                  } else {\n                    assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                  }\n                }\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              int newDoc = -1;\n              for (int idx = 0; idx < mergingFields.length; idx++) {\n                UpdatesIterator updatesIter = updatesIters[idx];\n                if (updatesIter.doc() == j) { // document has an update\n                  if (mergedDeletesAndUpdates == null) {\n                    mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                    docMap = getDocMap(merge, mergeState);\n                  }\n                  if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                    newDoc = docMap.map(docUpto);\n                  }\n                  String field = mergingFields[idx];\n                  NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                  if (fieldUpdates == null) {\n                    // an approximantion of maxDoc, used to compute best bitsPerValue\n                    fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                    mergedFieldUpdates.put(field, fieldUpdates);\n                  }\n                  fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                  updatesIter.nextDoc(); // advance to next document\n                } else {\n                  assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                }\n              }\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (mergedDeletesAndUpdates == null) {\n              mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n              docMap = getDocMap(merge, mergeState);\n            } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n            }\n            mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            // document isn't deleted, check if any of the fields have an update to it\n            int newDoc = -1;\n            for (int idx = 0; idx < mergingFields.length; idx++) {\n              UpdatesIterator updatesIter = updatesIters[idx];\n              if (updatesIter.doc() == j) { // document has an update\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  docMap = getDocMap(merge, mergeState);\n                }\n                if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                  newDoc = docMap.map(docUpto);\n                }\n                String field = mergingFields[idx];\n                NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                if (fieldUpdates == null) {\n                  // an approximantion of maxDoc, used to compute best bitsPerValue\n                  fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                  mergedFieldUpdates.put(field, fieldUpdates);\n                }\n                fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                updatesIter.nextDoc(); // advance to next document\n              } else {\n                assert updatesIter.doc() > j : \"field=\" + mergingFields[idx] + \" updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n              }\n            }\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          int newDoc = -1;\n          for (int idx = 0; idx < mergingFields.length; idx++) {\n            UpdatesIterator updatesIter = updatesIters[idx];\n            if (updatesIter.doc() == j) { // document has an update\n              if (mergedDeletesAndUpdates == null) {\n                mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                docMap = getDocMap(merge, mergeState);\n              }\n              if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                newDoc = docMap.map(docUpto);\n              }\n              String field = mergingFields[idx];\n              NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n              if (fieldUpdates == null) {\n                // an approximantion of maxDoc, used to compute best bitsPerValue\n                fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                mergedFieldUpdates.put(field, fieldUpdates);\n              }\n              fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n              updatesIter.nextDoc(); // advance to next document\n            } else {\n              assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n            }\n          }\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (!mergedFieldUpdates.isEmpty()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedFieldUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (!mergedFieldUpdates.isEmpty()) {\n          msg += \" and \" + mergedFieldUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    assert testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    ReadersAndUpdates mergedDeletesAndUpdates = null;\n    boolean initWritableLiveDocs = false;\n    MergePolicy.DocMap docMap = null;\n    final Map<String,NumericFieldUpdates> mergedFieldUpdates = new HashMap<String,NumericFieldUpdates>();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,NumericFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final UpdatesIterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        updatesIters = new UpdatesIterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,NumericFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          mergingFields[idx] = e.getKey();\n          updatesIters[idx] = e.getValue().getUpdates();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                  docMap = getDocMap(merge, mergeState);\n                } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                }\n                mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                // document isn't deleted, check if any of the fields have an update to it\n                int newDoc = -1;\n                for (int idx = 0; idx < mergingFields.length; idx++) {\n                  UpdatesIterator updatesIter = updatesIters[idx];\n                  if (updatesIter.doc() == j) { // document has an update\n                    if (mergedDeletesAndUpdates == null) {\n                      mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                      docMap = getDocMap(merge, mergeState);\n                    }\n                    if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                      newDoc = docMap.map(docUpto);\n                    }\n                    String field = mergingFields[idx];\n                    NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                    if (fieldUpdates == null) {\n                      // an approximantion of maxDoc, used to compute best bitsPerValue\n                      fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                      mergedFieldUpdates.put(field, fieldUpdates);\n                    }\n                    fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                    updatesIter.nextDoc(); // advance to next document\n                  } else {\n                    assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                  }\n                }\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              int newDoc = -1;\n              for (int idx = 0; idx < mergingFields.length; idx++) {\n                UpdatesIterator updatesIter = updatesIters[idx];\n                if (updatesIter.doc() == j) { // document has an update\n                  if (mergedDeletesAndUpdates == null) {\n                    mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                    docMap = getDocMap(merge, mergeState);\n                  }\n                  if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                    newDoc = docMap.map(docUpto);\n                  }\n                  String field = mergingFields[idx];\n                  NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                  if (fieldUpdates == null) {\n                    // an approximantion of maxDoc, used to compute best bitsPerValue\n                    fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                    mergedFieldUpdates.put(field, fieldUpdates);\n                  }\n                  fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                  updatesIter.nextDoc(); // advance to next document\n                } else {\n                  assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                }\n              }\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (mergedDeletesAndUpdates == null) {\n              mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n              docMap = getDocMap(merge, mergeState);\n            } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n            }\n            mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            // document isn't deleted, check if any of the fields have an update to it\n            int newDoc = -1;\n            for (int idx = 0; idx < mergingFields.length; idx++) {\n              UpdatesIterator updatesIter = updatesIters[idx];\n              if (updatesIter.doc() == j) { // document has an update\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  docMap = getDocMap(merge, mergeState);\n                }\n                if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                  newDoc = docMap.map(docUpto);\n                }\n                String field = mergingFields[idx];\n                NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                if (fieldUpdates == null) {\n                  // an approximantion of maxDoc, used to compute best bitsPerValue\n                  fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                  mergedFieldUpdates.put(field, fieldUpdates);\n                }\n                fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                updatesIter.nextDoc(); // advance to next document\n              } else {\n                assert updatesIter.doc() > j : \"field=\" + mergingFields[idx] + \" updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n              }\n            }\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          int newDoc = -1;\n          for (int idx = 0; idx < mergingFields.length; idx++) {\n            UpdatesIterator updatesIter = updatesIters[idx];\n            if (updatesIter.doc() == j) { // document has an update\n              if (mergedDeletesAndUpdates == null) {\n                mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                docMap = getDocMap(merge, mergeState);\n              }\n              if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                newDoc = docMap.map(docUpto);\n              }\n              String field = mergingFields[idx];\n              NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n              if (fieldUpdates == null) {\n                // an approximantion of maxDoc, used to compute best bitsPerValue\n                fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                mergedFieldUpdates.put(field, fieldUpdates);\n              }\n              fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n              updatesIter.nextDoc(); // advance to next document\n            } else {\n              assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n            }\n          }\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (!mergedFieldUpdates.isEmpty()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedFieldUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (!mergedFieldUpdates.isEmpty()) {\n          msg += \" and \" + mergedFieldUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06805da26538ed636bd89b10c2699cc3834032ae","date":1395132972,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    assert testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    ReadersAndUpdates mergedDeletesAndUpdates = null;\n    boolean initWritableLiveDocs = false;\n    MergePolicy.DocMap docMap = null;\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.getDocCount());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                  docMap = getDocMap(merge, mergeState);\n                } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                }\n                mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                // document isn't deleted, check if any of the fields have an update to it\n                int newDoc = -1;\n                for (int idx = 0; idx < mergingFields.length; idx++) {\n                  DocValuesFieldUpdates.Iterator updatesIter = updatesIters[idx];\n                  if (updatesIter.doc() == j) { // document has an update\n                    if (mergedDeletesAndUpdates == null) {\n                      mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                      docMap = getDocMap(merge, mergeState);\n                    }\n                    if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                      newDoc = docMap.map(docUpto);\n                    }\n                    DocValuesFieldUpdates dvUpdates = dvFieldUpdates[idx];\n                    dvUpdates.add(newDoc, updatesIter.value());\n                    updatesIter.nextDoc(); // advance to next document\n                  } else {\n                    assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                  }\n                }\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              int newDoc = -1;\n              for (int idx = 0; idx < mergingFields.length; idx++) {\n                DocValuesFieldUpdates.Iterator updatesIter = updatesIters[idx];\n                if (updatesIter.doc() == j) { // document has an update\n                  if (mergedDeletesAndUpdates == null) {\n                    mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                    docMap = getDocMap(merge, mergeState);\n                  }\n                  if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                    newDoc = docMap.map(docUpto);\n                  }\n                  DocValuesFieldUpdates dvUpdates = dvFieldUpdates[idx];\n                  dvUpdates.add(newDoc, updatesIter.value());\n                  updatesIter.nextDoc(); // advance to next document\n                } else {\n                  assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                }\n              }\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (mergedDeletesAndUpdates == null) {\n              mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n              docMap = getDocMap(merge, mergeState);\n            } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n            }\n            mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            // document isn't deleted, check if any of the fields have an update to it\n            int newDoc = -1;\n            for (int idx = 0; idx < mergingFields.length; idx++) {\n              DocValuesFieldUpdates.Iterator updatesIter = updatesIters[idx];\n              if (updatesIter.doc() == j) { // document has an update\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  docMap = getDocMap(merge, mergeState);\n                }\n                if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                  newDoc = docMap.map(docUpto);\n                }\n                DocValuesFieldUpdates dvUpdates = dvFieldUpdates[idx];\n                dvUpdates.add(newDoc, updatesIter.value());\n                updatesIter.nextDoc(); // advance to next document\n              } else {\n                assert updatesIter.doc() > j : \"field=\" + mergingFields[idx] + \" updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n              }\n            }\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          int newDoc = -1;\n          for (int idx = 0; idx < mergingFields.length; idx++) {\n            DocValuesFieldUpdates.Iterator updatesIter = updatesIters[idx];\n            if (updatesIter.doc() == j) { // document has an update\n              if (mergedDeletesAndUpdates == null) {\n                mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                docMap = getDocMap(merge, mergeState);\n              }\n              if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                newDoc = docMap.map(docUpto);\n              }\n              DocValuesFieldUpdates dvUpdates = dvFieldUpdates[idx];\n              dvUpdates.add(newDoc, updatesIter.value());\n              updatesIter.nextDoc(); // advance to next document\n            } else {\n              assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n            }\n          }\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    assert testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    ReadersAndUpdates mergedDeletesAndUpdates = null;\n    boolean initWritableLiveDocs = false;\n    MergePolicy.DocMap docMap = null;\n    final Map<String,NumericFieldUpdates> mergedFieldUpdates = new HashMap<>();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,NumericFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final UpdatesIterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        updatesIters = new UpdatesIterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,NumericFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          mergingFields[idx] = e.getKey();\n          updatesIters[idx] = e.getValue().getUpdates();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                  docMap = getDocMap(merge, mergeState);\n                } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                }\n                mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                // document isn't deleted, check if any of the fields have an update to it\n                int newDoc = -1;\n                for (int idx = 0; idx < mergingFields.length; idx++) {\n                  UpdatesIterator updatesIter = updatesIters[idx];\n                  if (updatesIter.doc() == j) { // document has an update\n                    if (mergedDeletesAndUpdates == null) {\n                      mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                      docMap = getDocMap(merge, mergeState);\n                    }\n                    if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                      newDoc = docMap.map(docUpto);\n                    }\n                    String field = mergingFields[idx];\n                    NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                    if (fieldUpdates == null) {\n                      // an approximantion of maxDoc, used to compute best bitsPerValue\n                      fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                      mergedFieldUpdates.put(field, fieldUpdates);\n                    }\n                    fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                    updatesIter.nextDoc(); // advance to next document\n                  } else {\n                    assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                  }\n                }\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              int newDoc = -1;\n              for (int idx = 0; idx < mergingFields.length; idx++) {\n                UpdatesIterator updatesIter = updatesIters[idx];\n                if (updatesIter.doc() == j) { // document has an update\n                  if (mergedDeletesAndUpdates == null) {\n                    mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                    docMap = getDocMap(merge, mergeState);\n                  }\n                  if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                    newDoc = docMap.map(docUpto);\n                  }\n                  String field = mergingFields[idx];\n                  NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                  if (fieldUpdates == null) {\n                    // an approximantion of maxDoc, used to compute best bitsPerValue\n                    fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                    mergedFieldUpdates.put(field, fieldUpdates);\n                  }\n                  fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                  updatesIter.nextDoc(); // advance to next document\n                } else {\n                  assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                }\n              }\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (mergedDeletesAndUpdates == null) {\n              mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n              docMap = getDocMap(merge, mergeState);\n            } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n            }\n            mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            // document isn't deleted, check if any of the fields have an update to it\n            int newDoc = -1;\n            for (int idx = 0; idx < mergingFields.length; idx++) {\n              UpdatesIterator updatesIter = updatesIters[idx];\n              if (updatesIter.doc() == j) { // document has an update\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  docMap = getDocMap(merge, mergeState);\n                }\n                if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                  newDoc = docMap.map(docUpto);\n                }\n                String field = mergingFields[idx];\n                NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n                if (fieldUpdates == null) {\n                  // an approximantion of maxDoc, used to compute best bitsPerValue\n                  fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                  mergedFieldUpdates.put(field, fieldUpdates);\n                }\n                fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n                updatesIter.nextDoc(); // advance to next document\n              } else {\n                assert updatesIter.doc() > j : \"field=\" + mergingFields[idx] + \" updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n              }\n            }\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          int newDoc = -1;\n          for (int idx = 0; idx < mergingFields.length; idx++) {\n            UpdatesIterator updatesIter = updatesIters[idx];\n            if (updatesIter.doc() == j) { // document has an update\n              if (mergedDeletesAndUpdates == null) {\n                mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                docMap = getDocMap(merge, mergeState);\n              }\n              if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                newDoc = docMap.map(docUpto);\n              }\n              String field = mergingFields[idx];\n              NumericFieldUpdates fieldUpdates = mergedFieldUpdates.get(field);\n              if (fieldUpdates == null) {\n                // an approximantion of maxDoc, used to compute best bitsPerValue\n                fieldUpdates = new NumericFieldUpdates.PackedNumericFieldUpdates(mergeState.segmentInfo.getDocCount());\n                mergedFieldUpdates.put(field, fieldUpdates);\n              }\n              fieldUpdates.add(newDoc, updatesIter.value() == null ? NumericUpdate.MISSING : updatesIter.value());\n              updatesIter.nextDoc(); // advance to next document\n            } else {\n              assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n            }\n          }\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (!mergedFieldUpdates.isEmpty()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedFieldUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (!mergedFieldUpdates.isEmpty()) {\n          msg += \" and \" + mergedFieldUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"097495779e1cbcffe6097dfa0afe0bd40c03e418","date":1395231661,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    assert testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.getDocCount());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    assert testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    ReadersAndUpdates mergedDeletesAndUpdates = null;\n    boolean initWritableLiveDocs = false;\n    MergePolicy.DocMap docMap = null;\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.getDocCount());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                  docMap = getDocMap(merge, mergeState);\n                } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n                  mergedDeletesAndUpdates.initWritableLiveDocs();\n                  initWritableLiveDocs = true;\n                }\n                mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                // document isn't deleted, check if any of the fields have an update to it\n                int newDoc = -1;\n                for (int idx = 0; idx < mergingFields.length; idx++) {\n                  DocValuesFieldUpdates.Iterator updatesIter = updatesIters[idx];\n                  if (updatesIter.doc() == j) { // document has an update\n                    if (mergedDeletesAndUpdates == null) {\n                      mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                      docMap = getDocMap(merge, mergeState);\n                    }\n                    if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                      newDoc = docMap.map(docUpto);\n                    }\n                    DocValuesFieldUpdates dvUpdates = dvFieldUpdates[idx];\n                    dvUpdates.add(newDoc, updatesIter.value());\n                    updatesIter.nextDoc(); // advance to next document\n                  } else {\n                    assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                  }\n                }\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              int newDoc = -1;\n              for (int idx = 0; idx < mergingFields.length; idx++) {\n                DocValuesFieldUpdates.Iterator updatesIter = updatesIters[idx];\n                if (updatesIter.doc() == j) { // document has an update\n                  if (mergedDeletesAndUpdates == null) {\n                    mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                    docMap = getDocMap(merge, mergeState);\n                  }\n                  if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                    newDoc = docMap.map(docUpto);\n                  }\n                  DocValuesFieldUpdates dvUpdates = dvFieldUpdates[idx];\n                  dvUpdates.add(newDoc, updatesIter.value());\n                  updatesIter.nextDoc(); // advance to next document\n                } else {\n                  assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n                }\n              }\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (mergedDeletesAndUpdates == null) {\n              mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n              docMap = getDocMap(merge, mergeState);\n            } else if (!initWritableLiveDocs) { // mergedDeletes was initialized by field-updates changes\n              mergedDeletesAndUpdates.initWritableLiveDocs();\n              initWritableLiveDocs = true;\n            }\n            mergedDeletesAndUpdates.delete(docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            // document isn't deleted, check if any of the fields have an update to it\n            int newDoc = -1;\n            for (int idx = 0; idx < mergingFields.length; idx++) {\n              DocValuesFieldUpdates.Iterator updatesIter = updatesIters[idx];\n              if (updatesIter.doc() == j) { // document has an update\n                if (mergedDeletesAndUpdates == null) {\n                  mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                  docMap = getDocMap(merge, mergeState);\n                }\n                if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                  newDoc = docMap.map(docUpto);\n                }\n                DocValuesFieldUpdates dvUpdates = dvFieldUpdates[idx];\n                dvUpdates.add(newDoc, updatesIter.value());\n                updatesIter.nextDoc(); // advance to next document\n              } else {\n                assert updatesIter.doc() > j : \"field=\" + mergingFields[idx] + \" updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n              }\n            }\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          int newDoc = -1;\n          for (int idx = 0; idx < mergingFields.length; idx++) {\n            DocValuesFieldUpdates.Iterator updatesIter = updatesIters[idx];\n            if (updatesIter.doc() == j) { // document has an update\n              if (mergedDeletesAndUpdates == null) {\n                mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n                docMap = getDocMap(merge, mergeState);\n              }\n              if (newDoc == -1) { // map once per all field updates, but only if there are any updates\n                newDoc = docMap.map(docUpto);\n              }\n              DocValuesFieldUpdates dvUpdates = dvFieldUpdates[idx];\n              dvUpdates.add(newDoc, updatesIter.value());\n              updatesIter.nextDoc(); // advance to next document\n            } else {\n              assert updatesIter.doc() > j : \"updateDoc=\" + updatesIter.doc() + \" curDoc=\" + j;\n            }\n          }\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4","date":1414017220,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.getDocCount());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    assert testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.getDocCount());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.maxDoc() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.maxDoc();\n      }\n    }\n\n    assert docUpto == merge.info.info.maxDoc();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.getDocCount());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":["36d84416fc00253f9e834f8dba14fa89b298e64e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.maxDoc() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.maxDoc();\n      }\n    }\n\n    assert docUpto == merge.info.info.maxDoc();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.getDocCount());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.maxDoc() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.maxDoc();\n      }\n    }\n\n    assert docUpto == merge.info.info.maxDoc();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int docCount = info.info.getDocCount();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.getDocCount());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == docCount;\n        assert currentLiveDocs.length() == docCount;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < docCount; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < docCount; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == docCount;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < docCount; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < docCount; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.getDocCount();\n      }\n    }\n\n    assert docUpto == merge.info.info.getDocCount();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              if (holder.mergedDeletesAndUpdates == null || holder.initializedWritableLiveDocs == false) {\n                holder.init(readerPool, merge, true);\n              }\n              holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n              if (mergingFields != null) { // advance all iters beyond the deleted document\n                skipDeletedDoc(updatesIters, j);\n              }\n            } else if (mergingFields != null) {\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n          }\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n        }\n      }\n    }\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.maxDoc() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.maxDoc();\n      }\n    }\n\n    assert docUpto == merge.info.info.maxDoc();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":["36d84416fc00253f9e834f8dba14fa89b298e64e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              if (holder.mergedDeletesAndUpdates == null || holder.initializedWritableLiveDocs == false) {\n                holder.init(readerPool, merge, true);\n              }\n              holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n              if (mergingFields != null) { // advance all iters beyond the deleted document\n                skipDeletedDoc(updatesIters, j);\n              }\n            } else if (mergingFields != null) {\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n          }\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n        }\n      }\n    }\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.maxDoc() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.maxDoc();\n      }\n    }\n\n    assert docUpto == merge.info.info.maxDoc();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              if (holder.mergedDeletesAndUpdates == null || holder.initializedWritableLiveDocs == false) {\n                holder.init(readerPool, merge, true);\n              }\n              holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n              if (mergingFields != null) { // advance all iters beyond the deleted document\n                skipDeletedDoc(updatesIters, j);\n              }\n            } else if (mergingFields != null) {\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n          }\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n        }\n      }\n    }\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.maxDoc() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.maxDoc();\n      }\n    }\n\n    assert docUpto == merge.info.info.maxDoc();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              if (holder.mergedDeletesAndUpdates == null || holder.initializedWritableLiveDocs == false) {\n                holder.init(readerPool, merge, true);\n              }\n              holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n              if (mergingFields != null) { // advance all iters beyond the deleted document\n                skipDeletedDoc(updatesIters, j);\n              }\n            } else if (mergingFields != null) {\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n          }\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n        }\n      }\n    }\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.maxDoc() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.maxDoc();\n      }\n    }\n\n    assert docUpto == merge.info.info.maxDoc();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              if (holder.mergedDeletesAndUpdates == null || holder.initializedWritableLiveDocs == false) {\n                holder.init(readerPool, merge, true);\n              }\n              holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n              if (mergingFields != null) { // advance all iters beyond the deleted document\n                skipDeletedDoc(updatesIters, j);\n              }\n            } else if (mergingFields != null) {\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n          }\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n        }\n      }\n    }\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    int docUpto = 0;\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n    \n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (!prevLiveDocs.get(j)) {\n              assert !currentLiveDocs.get(j);\n            } else {\n              if (!currentLiveDocs.get(j)) {\n                if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n                  holder.init(readerPool, merge, mergeState, true);\n                }\n                holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n                if (mergingFields != null) { // advance all iters beyond the deleted document\n                  skipDeletedDoc(updatesIters, j);\n                }\n              } else if (mergingFields != null) {\n                maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              }\n              docUpto++;\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n              // advance docUpto for every non-deleted document\n              docUpto++;\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        } else {\n          docUpto += info.info.maxDoc() - info.getDelCount() - rld.getPendingDeleteCount();\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (!currentLiveDocs.get(j)) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, mergeState, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(holder.docMap.map(docUpto));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          }\n          docUpto++;\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);\n          // advance docUpto for every non-deleted document\n          docUpto++;\n        }\n      } else {\n        // No deletes or updates before or after\n        docUpto += info.info.maxDoc();\n      }\n    }\n\n    assert docUpto == merge.info.info.maxDoc();\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              if (holder.mergedDeletesAndUpdates == null || holder.initializedWritableLiveDocs == false) {\n                holder.init(readerPool, merge, true);\n              }\n              holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n              if (mergingFields != null) { // advance all iters beyond the deleted document\n                skipDeletedDoc(updatesIters, j);\n              }\n            } else if (mergingFields != null) {\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n          }\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n        }\n      }\n    }\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":["36d84416fc00253f9e834f8dba14fa89b298e64e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              if (holder.mergedDeletesAndUpdates == null || holder.initializedWritableLiveDocs == false) {\n                holder.init(readerPool, merge, true);\n              }\n              holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n              if (mergingFields != null) { // advance all iters beyond the deleted document\n                skipDeletedDoc(updatesIters, j);\n              }\n            } else if (mergingFields != null) {\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n          }\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n        }\n      }\n    }\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete to carry over):\n    final MergedDeletesAndUpdates holder = new MergedDeletesAndUpdates();\n    final DocValuesFieldUpdates.Container mergedDVUpdates = new DocValuesFieldUpdates.Container();\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref so it should still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n      final Map<String,DocValuesFieldUpdates> mergingFieldUpdates = rld.getMergingFieldUpdates();\n      final String[] mergingFields;\n      final DocValuesFieldUpdates[] dvFieldUpdates;\n      final DocValuesFieldUpdates.Iterator[] updatesIters;\n      if (mergingFieldUpdates.isEmpty()) {\n        mergingFields = null;\n        updatesIters = null;\n        dvFieldUpdates = null;\n      } else {\n        mergingFields = new String[mergingFieldUpdates.size()];\n        dvFieldUpdates = new DocValuesFieldUpdates[mergingFieldUpdates.size()];\n        updatesIters = new DocValuesFieldUpdates.Iterator[mergingFieldUpdates.size()];\n        int idx = 0;\n        for (Entry<String,DocValuesFieldUpdates> e : mergingFieldUpdates.entrySet()) {\n          String field = e.getKey();\n          DocValuesFieldUpdates updates = e.getValue();\n          mergingFields[idx] = field;\n          dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);\n          if (dvFieldUpdates[idx] == null) {\n            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());\n          }\n          updatesIters[idx] = updates.iterator();\n          updatesIters[idx].nextDoc(); // advance to first update doc\n          ++idx;\n        }\n      }\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: info=\" + info + \", mergingUpdates=\" + mergingUpdates);\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              if (holder.mergedDeletesAndUpdates == null || holder.initializedWritableLiveDocs == false) {\n                holder.init(readerPool, merge, true);\n              }\n              holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n              if (mergingFields != null) { // advance all iters beyond the deleted document\n                skipDeletedDoc(updatesIters, j);\n              }\n            } else if (mergingFields != null) {\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            }\n          }\n        } else if (mergingFields != null) {\n          // need to check each non-deleted document if it has any updates\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j)) {\n              // document isn't deleted, check if any of the fields have an update to it\n              maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n            } else {\n              // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {\n              holder.init(readerPool, merge, true);\n            }\n            holder.mergedDeletesAndUpdates.delete(mergeState.docMaps[i].get(mergeState.leafDocMaps[i].get(j)));\n            if (mergingFields != null) { // advance all iters beyond the deleted document\n              skipDeletedDoc(updatesIters, j);\n            }\n          } else if (mergingFields != null) {\n            maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n          }\n        }\n      } else if (mergingFields != null) {\n        // no deletions before or after, but there were updates\n        for (int j = 0; j < maxDoc; j++) {\n          maybeApplyMergedDVUpdates(merge, mergeState, holder, mergingFields, dvFieldUpdates, updatesIters, i, j);\n        }\n      }\n    }\n\n    if (mergedDVUpdates.any()) {\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.commitMergedDeletes: mergedDeletes.info=\" + mergedDeletes.info + \", mergedFieldUpdates=\" + mergedFieldUpdates);\n      boolean success = false;\n      try {\n        // if any error occurs while writing the field updates we should release\n        // the info, otherwise it stays in the pool but is considered not \"live\"\n        // which later causes false exceptions in pool.dropAll().\n        // NOTE: currently this is the only place which throws a true\n        // IOException. If this ever changes, we need to extend that try/finally\n        // block to the rest of the method too.\n        holder.mergedDeletesAndUpdates.writeFieldUpdates(directory, mergedDVUpdates);\n        success = true;\n      } finally {\n        if (!success) {\n          holder.mergedDeletesAndUpdates.dropChanges();\n          readerPool.drop(merge.info);\n        }\n      }\n    }\n    \n    if (infoStream.isEnabled(\"IW\")) {\n      if (holder.mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = holder.mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (mergedDVUpdates.any()) {\n          msg += \" and \" + mergedDVUpdates.size() + \" new field updates\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return holder.mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f0c93fedbecf591a0a34794b87e82ad8d5c754d9","date":1499287985,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e73d8d559120669b47658108d818b637df5456ea","date":1499401413,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1926100d9b67becc9701c54266fee3ba7878a5f0","date":1524472150,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = readerPool.get(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = readerPool.get(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e129bd6cb34a236558a49edf108a49d5c15e0e1","date":1525081316,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it);\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f42883db49d143abc1a0f176ba47e3388dafb608","date":1525083166,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it);\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it.value());\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"36d84416fc00253f9e834f8dba14fa89b298e64e","date":1525428963,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      carryOverHardDeletes(mergedDeletesAndUpdates, maxDoc, mergeState.liveDocs[i],  merge.hardLiveDocs.get(i), rld.getHardLiveDocs(),\n          segDocMap, segLeafDocMap);\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it);\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n      final Bits currentLiveDocs = rld.getLiveDocs();\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      if (prevLiveDocs != null) {\n\n        // If we had deletions on starting the merge we must\n        // still have deletions now:\n        assert currentLiveDocs != null;\n        assert prevLiveDocs.length() == maxDoc;\n        assert currentLiveDocs.length() == maxDoc;\n\n        // There were deletes on this segment when the merge\n        // started.  The merge has collapsed away those\n        // deletes, but, if new deletes were flushed since\n        // the merge started, we must now carefully keep any\n        // newly flushed deletes but mapping them to the new\n        // docIDs.\n\n        // Since we copy-on-write, if any new deletes were\n        // applied after merging has started, we can just\n        // check if the before/after liveDocs have changed.\n        // If so, we must carefully merge the liveDocs one\n        // doc at a time:\n        if (currentLiveDocs != prevLiveDocs) {\n          // This means this segment received new deletes\n          // since we started the merge, so we\n          // must merge them:\n          for (int j = 0; j < maxDoc; j++) {\n            if (prevLiveDocs.get(j) == false) {\n              // if the document was deleted before, it better still be deleted!\n              assert currentLiveDocs.get(j) == false;\n            } else if (currentLiveDocs.get(j) == false) {\n              // the document was deleted while we were merging:\n              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n            }\n          }\n        }\n      } else if (currentLiveDocs != null) {\n        assert currentLiveDocs.length() == maxDoc;\n        // This segment had no deletes before but now it\n        // does:\n        for (int j = 0; j < maxDoc; j++) {\n          if (currentLiveDocs.get(j) == false) {\n            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));\n          }\n        }\n      }\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      \n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it);\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":["b0267c69e2456a3477a1ad785723f2135da3117e","f4363cd33f6eff7fb4753574a441e2d18c1022a4","9ce667c6d3400b22523701c549c0d35e26da8b46","ccad4bab070f323ce610caa0040346d4a87213dc","c95a819869502635864dac0a788f874787e3395b","e072d0b1fc19e0533d8ce432eed245196bca6fde","c4ff8864209d2e972cb4393600c26082f9a6533d","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","0061262413ecc163d6eebba1b5c43ab91a0c2dc5","8a16d06e7522604de20b2d758d9b9464bb30fe02"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"33adea025f43af3243278587a46b8d9fd2e8ccf9","date":1525885077,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      carryOverHardDeletes(mergedDeletesAndUpdates, maxDoc, mergeState.liveDocs[i],  merge.hardLiveDocs.get(i), rld.getHardLiveDocs(),\n          segDocMap, segLeafDocMap);\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              if (it.hasValue()) {\n                // not deleted\n                mappedUpdates.add(mappedDoc, it);\n              } else {\n                mappedUpdates.reset(mappedDoc);\n              }\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      carryOverHardDeletes(mergedDeletesAndUpdates, maxDoc, mergeState.liveDocs[i],  merge.hardLiveDocs.get(i), rld.getHardLiveDocs(),\n          segDocMap, segLeafDocMap);\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              // not deleted\n              mappedUpdates.add(mappedDoc, it);\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aef2a94da918b657d107b616a643e1759db43b6a","date":1527706131,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    int numDeletesBefore = mergedDeletesAndUpdates.getDelCount();\n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      carryOverHardDeletes(mergedDeletesAndUpdates, maxDoc, mergeState.liveDocs[i],  merge.hardLiveDocs.get(i), rld.getHardLiveDocs(),\n          segDocMap, segLeafDocMap);\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              if (it.hasValue()) {\n                // not deleted\n                mappedUpdates.add(mappedDoc, it);\n              } else {\n                mappedUpdates.reset(mappedDoc);\n              }\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getDelCount() - numDeletesBefore + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    \n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      carryOverHardDeletes(mergedDeletesAndUpdates, maxDoc, mergeState.liveDocs[i],  merge.hardLiveDocs.get(i), rld.getHardLiveDocs(),\n          segDocMap, segLeafDocMap);\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              if (it.hasValue()) {\n                // not deleted\n                mappedUpdates.add(mappedDoc, it);\n              } else {\n                mappedUpdates.reset(mappedDoc);\n              }\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2","date":1588002560,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  private synchronized ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    int numDeletesBefore = mergedDeletesAndUpdates.getDelCount();\n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      carryOverHardDeletes(mergedDeletesAndUpdates, maxDoc, mergeState.liveDocs[i],  merge.hardLiveDocs.get(i), rld.getHardLiveDocs(),\n          segDocMap, segLeafDocMap);\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              if (it.hasValue()) {\n                // not deleted\n                mappedUpdates.add(mappedDoc, it);\n              } else {\n                mappedUpdates.reset(mappedDoc);\n              }\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getDelCount() - numDeletesBefore + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    int numDeletesBefore = mergedDeletesAndUpdates.getDelCount();\n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      carryOverHardDeletes(mergedDeletesAndUpdates, maxDoc, mergeState.liveDocs[i],  merge.hardLiveDocs.get(i), rld.getHardLiveDocs(),\n          segDocMap, segLeafDocMap);\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              if (it.hasValue()) {\n                // not deleted\n                mappedUpdates.add(mappedDoc, it);\n              } else {\n                mappedUpdates.reset(mappedDoc);\n              }\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getDelCount() - numDeletesBefore + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c2a23476693f2bd9a4b44cc3187c429a2e21dac2","date":1593289545,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#commitMergedDeletesAndUpdates(MergePolicy.OneMerge,MergeState).mjava","sourceNew":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  private synchronized ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    int numDeletesBefore = mergedDeletesAndUpdates.getDelCount();\n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      carryOverHardDeletes(mergedDeletesAndUpdates, maxDoc, mergeState.liveDocs[i],  merge.getMergeReader().get(i).hardLiveDocs, rld.getHardLiveDocs(),\n          segDocMap, segLeafDocMap);\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              if (it.hasValue()) {\n                // not deleted\n                mappedUpdates.add(mappedDoc, it);\n              } else {\n                mappedUpdates.reset(mappedDoc);\n              }\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getDelCount() - numDeletesBefore + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","sourceOld":"  /**\n   * Carefully merges deletes and updates for the segments we just merged. This\n   * is tricky because, although merging will clear all deletes (compacts the\n   * documents) and compact all the updates, new deletes and updates may have\n   * been flushed to the segments since the merge was started. This method\n   * \"carries over\" such new deletes and updates onto the newly merged segment,\n   * and saves the resulting deletes and updates files (incrementing the delete\n   * and DV generations for merge.info). If no deletes were flushed, no new\n   * deletes file is saved.\n   */\n  private synchronized ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {\n\n    mergeFinishedGen.incrementAndGet();\n    \n    testPoint(\"startCommitMergeDeletes\");\n\n    final List<SegmentCommitInfo> sourceSegments = merge.segments;\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"commitMergeDeletes \" + segString(merge.segments));\n    }\n\n    // Carefully merge deletes that occurred after we\n    // started merging:\n    long minGen = Long.MAX_VALUE;\n\n    // Lazy init (only when we find a delete or update to carry over):\n    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);\n    int numDeletesBefore = mergedDeletesAndUpdates.getDelCount();\n    // field -> delGen -> dv field updates\n    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();\n\n    boolean anyDVUpdates = false;\n\n    assert sourceSegments.size() == mergeState.docMaps.length;\n    for (int i = 0; i < sourceSegments.size(); i++) {\n      SegmentCommitInfo info = sourceSegments.get(i);\n      minGen = Math.min(info.getBufferedDeletesGen(), minGen);\n      final int maxDoc = info.info.maxDoc();\n      final ReadersAndUpdates rld = getPooledInstance(info, false);\n      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:\n      assert rld != null: \"seg=\" + info.info.name;\n\n      MergeState.DocMap segDocMap = mergeState.docMaps[i];\n      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];\n\n      carryOverHardDeletes(mergedDeletesAndUpdates, maxDoc, mergeState.liveDocs[i],  merge.hardLiveDocs.get(i), rld.getHardLiveDocs(),\n          segDocMap, segLeafDocMap);\n\n      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.\n      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed\n      // and re-resolve against the newly merged segment:\n      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();\n      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {\n\n        String field = ent.getKey();\n\n        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);\n        if (mappedField == null) {\n          mappedField = new HashMap<>();\n          mappedDVUpdates.put(field, mappedField);\n        }\n\n        for (DocValuesFieldUpdates updates : ent.getValue()) {\n\n          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {\n            continue;\n          }\n              \n          // sanity check:\n          assert field.equals(updates.field);\n\n          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);\n          if (mappedUpdates == null) {\n            switch (updates.type) {\n              case NUMERIC:\n                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              case BINARY:\n                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());\n                break;\n              default:\n                throw new AssertionError();\n            }\n            mappedField.put(updates.delGen, mappedUpdates);\n          }\n\n          DocValuesFieldUpdates.Iterator it = updates.iterator();\n          int doc;\n          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {\n            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));\n            if (mappedDoc != -1) {\n              if (it.hasValue()) {\n                // not deleted\n                mappedUpdates.add(mappedDoc, it);\n              } else {\n                mappedUpdates.reset(mappedDoc);\n              }\n              anyDVUpdates = true;\n            }\n          }\n        }\n      }\n    }\n\n    if (anyDVUpdates) {\n      // Persist the merged DV updates onto the RAU for the merged segment:\n      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {\n        for (DocValuesFieldUpdates updates : d.values()) {\n          updates.finish();\n          mergedDeletesAndUpdates.addDVUpdate(updates);\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"IW\")) {\n      if (mergedDeletesAndUpdates == null) {\n        infoStream.message(\"IW\", \"no new deletes or field updates since merge started\");\n      } else {\n        String msg = mergedDeletesAndUpdates.getDelCount() - numDeletesBefore + \" new deletes\";\n        if (anyDVUpdates) {\n          msg += \" and \" + mergedDeletesAndUpdates.getNumDVUpdates() + \" new field updates\";\n          msg += \" (\" + mergedDeletesAndUpdates.ramBytesUsed.get() + \") bytes\";\n        }\n        msg += \" since merge started\";\n        infoStream.message(\"IW\", msg);\n      }\n    }\n\n    merge.info.setBufferedDeletesGen(minGen);\n\n    return mergedDeletesAndUpdates;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"36d84416fc00253f9e834f8dba14fa89b298e64e":["f42883db49d143abc1a0f176ba47e3388dafb608"],"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4":["097495779e1cbcffe6097dfa0afe0bd40c03e418"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"33adea025f43af3243278587a46b8d9fd2e8ccf9":["36d84416fc00253f9e834f8dba14fa89b298e64e"],"06805da26538ed636bd89b10c2699cc3834032ae":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"0ad30c6a479e764150a3316e57263319775f1df2":["b0267c69e2456a3477a1ad785723f2135da3117e","3d33e731a93d4b57e662ff094f64f94a745422d4"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["b0267c69e2456a3477a1ad785723f2135da3117e","0ad30c6a479e764150a3316e57263319775f1df2"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4","b0267c69e2456a3477a1ad785723f2135da3117e"],"b06445ae1731e049327712db0454e5643ca9b7fe":["9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4","b0267c69e2456a3477a1ad785723f2135da3117e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"b0267c69e2456a3477a1ad785723f2135da3117e":["9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["f0c93fedbecf591a0a34794b87e82ad8d5c754d9"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["d470c8182e92b264680e34081b75e70a9f2b3c89","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["d470c8182e92b264680e34081b75e70a9f2b3c89","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["b0267c69e2456a3477a1ad785723f2135da3117e","d470c8182e92b264680e34081b75e70a9f2b3c89"],"c2a23476693f2bd9a4b44cc3187c429a2e21dac2":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"097495779e1cbcffe6097dfa0afe0bd40c03e418":["06805da26538ed636bd89b10c2699cc3834032ae"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["b0267c69e2456a3477a1ad785723f2135da3117e"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["aef2a94da918b657d107b616a643e1759db43b6a"],"f0c93fedbecf591a0a34794b87e82ad8d5c754d9":["28288370235ed02234a64753cdbf0c6ec096304a"],"f42883db49d143abc1a0f176ba47e3388dafb608":["1926100d9b67becc9701c54266fee3ba7878a5f0","7e129bd6cb34a236558a49edf108a49d5c15e0e1"],"e73d8d559120669b47658108d818b637df5456ea":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","f0c93fedbecf591a0a34794b87e82ad8d5c754d9"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["b0267c69e2456a3477a1ad785723f2135da3117e","ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"aef2a94da918b657d107b616a643e1759db43b6a":["33adea025f43af3243278587a46b8d9fd2e8ccf9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c2a23476693f2bd9a4b44cc3187c429a2e21dac2"]},"commit2Childs":{"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"36d84416fc00253f9e834f8dba14fa89b298e64e":["33adea025f43af3243278587a46b8d9fd2e8ccf9"],"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","b0267c69e2456a3477a1ad785723f2135da3117e"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["f42883db49d143abc1a0f176ba47e3388dafb608"],"33adea025f43af3243278587a46b8d9fd2e8ccf9":["aef2a94da918b657d107b616a643e1759db43b6a"],"06805da26538ed636bd89b10c2699cc3834032ae":["097495779e1cbcffe6097dfa0afe0bd40c03e418"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["06805da26538ed636bd89b10c2699cc3834032ae"],"b0267c69e2456a3477a1ad785723f2135da3117e":["0ad30c6a479e764150a3316e57263319775f1df2","d470c8182e92b264680e34081b75e70a9f2b3c89","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["7e129bd6cb34a236558a49edf108a49d5c15e0e1","f42883db49d143abc1a0f176ba47e3388dafb608"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["e73d8d559120669b47658108d818b637df5456ea"],"28288370235ed02234a64753cdbf0c6ec096304a":["f0c93fedbecf591a0a34794b87e82ad8d5c754d9"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"c2a23476693f2bd9a4b44cc3187c429a2e21dac2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"097495779e1cbcffe6097dfa0afe0bd40c03e418":["9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["c2a23476693f2bd9a4b44cc3187c429a2e21dac2"],"f42883db49d143abc1a0f176ba47e3388dafb608":["36d84416fc00253f9e834f8dba14fa89b298e64e"],"f0c93fedbecf591a0a34794b87e82ad8d5c754d9":["1926100d9b67becc9701c54266fee3ba7878a5f0","e73d8d559120669b47658108d818b637df5456ea"],"e73d8d559120669b47658108d818b637df5456ea":[],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"aef2a94da918b657d107b616a643e1759db43b6a":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","e73d8d559120669b47658108d818b637df5456ea","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}