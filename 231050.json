{"path":"lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList#add(int,int,List[WeightedPhraseInfo]).mjava","commits":[{"id":"b5c9602de3e9cfe5d0cc9e3a068199b57d5d85ef","date":1339509577,"type":0,"author":"Koji Sekiguchi","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList#add(int,int,List[WeightedPhraseInfo]).mjava","pathOld":"/dev/null","sourceNew":"  /* (non-Javadoc)\n   * @see org.apache.lucene.search.vectorhighlight.FieldFragList#add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList )\n   */ \n  @Override\n  public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ) {\n    \n    float totalBoost = 0;\n    \n    List<SubInfo> subInfos = new ArrayList<SubInfo>();\n    \n    HashSet<String> distinctTerms = new HashSet<String>();\n    \n    int length = 0;\n\n    for( WeightedPhraseInfo phraseInfo : phraseInfoList ){\n      \n      subInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(), phraseInfo.getSeqnum() ) );\n      \n      for ( TermInfo ti :  phraseInfo.getTermsInfos()) {\n        if ( distinctTerms.add( ti.getText() ) )\n          totalBoost += ti.getWeight() * phraseInfo.getBoost();\n        length++;\n      }\n    }\n    \n    // We want that terms per fragment (length) is included into the weight. Otherwise a one-word-query\n    // would cause an equal weight for all fragments regardless of how much words they contain.  \n    // To avoid that fragments containing a high number of words possibly \"outrank\" more relevant fragments\n    // we \"bend\" the length with a standard-normalization a little bit.  \n    totalBoost *= length * ( 1 / Math.sqrt( length ) );\n    \n    getFragInfos().add( new WeightedFragInfo( startOffset, endOffset, subInfos, totalBoost ) );\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["3f678348ac6e145f874f56992e8c0f78a1ff3c46"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3f678348ac6e145f874f56992e8c0f78a1ff3c46","date":1385743501,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList#add(int,int,List[WeightedPhraseInfo]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList#add(int,int,List[WeightedPhraseInfo]).mjava","sourceNew":"  /* (non-Javadoc)\n   * @see org.apache.lucene.search.vectorhighlight.FieldFragList#add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList )\n   */ \n  @Override\n  public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ) {\n    List<SubInfo> tempSubInfos = new ArrayList<SubInfo>();\n    List<SubInfo> realSubInfos = new ArrayList<SubInfo>();\n    HashSet<String> distinctTerms = new HashSet<String>();   \n    int length = 0;\n\n    for( WeightedPhraseInfo phraseInfo : phraseInfoList ){\n      float phraseTotalBoost = 0;\n      for ( TermInfo ti :  phraseInfo.getTermsInfos()) {\n        if ( distinctTerms.add( ti.getText() ) )\n          phraseTotalBoost += ti.getWeight() * phraseInfo.getBoost();\n        length++;\n      }\n      tempSubInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(),\n        phraseInfo.getSeqnum(), phraseTotalBoost ) );\n    }\n    \n    // We want that terms per fragment (length) is included into the weight. Otherwise a one-word-query\n    // would cause an equal weight for all fragments regardless of how much words they contain.  \n    // To avoid that fragments containing a high number of words possibly \"outrank\" more relevant fragments\n    // we \"bend\" the length with a standard-normalization a little bit.\n    float norm = length * ( 1 / (float)Math.sqrt( length ) );\n\n    float totalBoost = 0;\n    for ( SubInfo tempSubInfo : tempSubInfos ) {\n      float subInfoBoost = tempSubInfo.getBoost() * norm;\n      realSubInfos.add( new SubInfo( tempSubInfo.getText(), tempSubInfo.getTermsOffsets(),\n        tempSubInfo.getSeqnum(), subInfoBoost ));\n      totalBoost += subInfoBoost;\n    }\n\n    getFragInfos().add( new WeightedFragInfo( startOffset, endOffset, realSubInfos, totalBoost ) );\n  }\n\n","sourceOld":"  /* (non-Javadoc)\n   * @see org.apache.lucene.search.vectorhighlight.FieldFragList#add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList )\n   */ \n  @Override\n  public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ) {\n    \n    float totalBoost = 0;\n    \n    List<SubInfo> subInfos = new ArrayList<SubInfo>();\n    \n    HashSet<String> distinctTerms = new HashSet<String>();\n    \n    int length = 0;\n\n    for( WeightedPhraseInfo phraseInfo : phraseInfoList ){\n      \n      subInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(), phraseInfo.getSeqnum() ) );\n      \n      for ( TermInfo ti :  phraseInfo.getTermsInfos()) {\n        if ( distinctTerms.add( ti.getText() ) )\n          totalBoost += ti.getWeight() * phraseInfo.getBoost();\n        length++;\n      }\n    }\n    \n    // We want that terms per fragment (length) is included into the weight. Otherwise a one-word-query\n    // would cause an equal weight for all fragments regardless of how much words they contain.  \n    // To avoid that fragments containing a high number of words possibly \"outrank\" more relevant fragments\n    // we \"bend\" the length with a standard-normalization a little bit.  \n    totalBoost *= length * ( 1 / Math.sqrt( length ) );\n    \n    getFragInfos().add( new WeightedFragInfo( startOffset, endOffset, subInfos, totalBoost ) );\n  }\n\n","bugFix":["b5c9602de3e9cfe5d0cc9e3a068199b57d5d85ef"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList#add(int,int,List[WeightedPhraseInfo]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList#add(int,int,List[WeightedPhraseInfo]).mjava","sourceNew":"  /* (non-Javadoc)\n   * @see org.apache.lucene.search.vectorhighlight.FieldFragList#add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList )\n   */ \n  @Override\n  public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ) {\n    List<SubInfo> tempSubInfos = new ArrayList<SubInfo>();\n    List<SubInfo> realSubInfos = new ArrayList<SubInfo>();\n    HashSet<String> distinctTerms = new HashSet<String>();   \n    int length = 0;\n\n    for( WeightedPhraseInfo phraseInfo : phraseInfoList ){\n      float phraseTotalBoost = 0;\n      for ( TermInfo ti :  phraseInfo.getTermsInfos()) {\n        if ( distinctTerms.add( ti.getText() ) )\n          phraseTotalBoost += ti.getWeight() * phraseInfo.getBoost();\n        length++;\n      }\n      tempSubInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(),\n        phraseInfo.getSeqnum(), phraseTotalBoost ) );\n    }\n    \n    // We want that terms per fragment (length) is included into the weight. Otherwise a one-word-query\n    // would cause an equal weight for all fragments regardless of how much words they contain.  \n    // To avoid that fragments containing a high number of words possibly \"outrank\" more relevant fragments\n    // we \"bend\" the length with a standard-normalization a little bit.\n    float norm = length * ( 1 / (float)Math.sqrt( length ) );\n\n    float totalBoost = 0;\n    for ( SubInfo tempSubInfo : tempSubInfos ) {\n      float subInfoBoost = tempSubInfo.getBoost() * norm;\n      realSubInfos.add( new SubInfo( tempSubInfo.getText(), tempSubInfo.getTermsOffsets(),\n        tempSubInfo.getSeqnum(), subInfoBoost ));\n      totalBoost += subInfoBoost;\n    }\n\n    getFragInfos().add( new WeightedFragInfo( startOffset, endOffset, realSubInfos, totalBoost ) );\n  }\n\n","sourceOld":"  /* (non-Javadoc)\n   * @see org.apache.lucene.search.vectorhighlight.FieldFragList#add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList )\n   */ \n  @Override\n  public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ) {\n    \n    float totalBoost = 0;\n    \n    List<SubInfo> subInfos = new ArrayList<SubInfo>();\n    \n    HashSet<String> distinctTerms = new HashSet<String>();\n    \n    int length = 0;\n\n    for( WeightedPhraseInfo phraseInfo : phraseInfoList ){\n      \n      subInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(), phraseInfo.getSeqnum() ) );\n      \n      for ( TermInfo ti :  phraseInfo.getTermsInfos()) {\n        if ( distinctTerms.add( ti.getText() ) )\n          totalBoost += ti.getWeight() * phraseInfo.getBoost();\n        length++;\n      }\n    }\n    \n    // We want that terms per fragment (length) is included into the weight. Otherwise a one-word-query\n    // would cause an equal weight for all fragments regardless of how much words they contain.  \n    // To avoid that fragments containing a high number of words possibly \"outrank\" more relevant fragments\n    // we \"bend\" the length with a standard-normalization a little bit.  \n    totalBoost *= length * ( 1 / Math.sqrt( length ) );\n    \n    getFragInfos().add( new WeightedFragInfo( startOffset, endOffset, subInfos, totalBoost ) );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList#add(int,int,List[WeightedPhraseInfo]).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList#add(int,int,List[WeightedPhraseInfo]).mjava","sourceNew":"  /* (non-Javadoc)\n   * @see org.apache.lucene.search.vectorhighlight.FieldFragList#add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList )\n   */ \n  @Override\n  public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ) {\n    List<SubInfo> tempSubInfos = new ArrayList<>();\n    List<SubInfo> realSubInfos = new ArrayList<>();\n    HashSet<String> distinctTerms = new HashSet<>();\n    int length = 0;\n\n    for( WeightedPhraseInfo phraseInfo : phraseInfoList ){\n      float phraseTotalBoost = 0;\n      for ( TermInfo ti :  phraseInfo.getTermsInfos()) {\n        if ( distinctTerms.add( ti.getText() ) )\n          phraseTotalBoost += ti.getWeight() * phraseInfo.getBoost();\n        length++;\n      }\n      tempSubInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(),\n        phraseInfo.getSeqnum(), phraseTotalBoost ) );\n    }\n    \n    // We want that terms per fragment (length) is included into the weight. Otherwise a one-word-query\n    // would cause an equal weight for all fragments regardless of how much words they contain.  \n    // To avoid that fragments containing a high number of words possibly \"outrank\" more relevant fragments\n    // we \"bend\" the length with a standard-normalization a little bit.\n    float norm = length * ( 1 / (float)Math.sqrt( length ) );\n\n    float totalBoost = 0;\n    for ( SubInfo tempSubInfo : tempSubInfos ) {\n      float subInfoBoost = tempSubInfo.getBoost() * norm;\n      realSubInfos.add( new SubInfo( tempSubInfo.getText(), tempSubInfo.getTermsOffsets(),\n        tempSubInfo.getSeqnum(), subInfoBoost ));\n      totalBoost += subInfoBoost;\n    }\n\n    getFragInfos().add( new WeightedFragInfo( startOffset, endOffset, realSubInfos, totalBoost ) );\n  }\n\n","sourceOld":"  /* (non-Javadoc)\n   * @see org.apache.lucene.search.vectorhighlight.FieldFragList#add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList )\n   */ \n  @Override\n  public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ) {\n    List<SubInfo> tempSubInfos = new ArrayList<SubInfo>();\n    List<SubInfo> realSubInfos = new ArrayList<SubInfo>();\n    HashSet<String> distinctTerms = new HashSet<String>();   \n    int length = 0;\n\n    for( WeightedPhraseInfo phraseInfo : phraseInfoList ){\n      float phraseTotalBoost = 0;\n      for ( TermInfo ti :  phraseInfo.getTermsInfos()) {\n        if ( distinctTerms.add( ti.getText() ) )\n          phraseTotalBoost += ti.getWeight() * phraseInfo.getBoost();\n        length++;\n      }\n      tempSubInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(),\n        phraseInfo.getSeqnum(), phraseTotalBoost ) );\n    }\n    \n    // We want that terms per fragment (length) is included into the weight. Otherwise a one-word-query\n    // would cause an equal weight for all fragments regardless of how much words they contain.  \n    // To avoid that fragments containing a high number of words possibly \"outrank\" more relevant fragments\n    // we \"bend\" the length with a standard-normalization a little bit.\n    float norm = length * ( 1 / (float)Math.sqrt( length ) );\n\n    float totalBoost = 0;\n    for ( SubInfo tempSubInfo : tempSubInfos ) {\n      float subInfoBoost = tempSubInfo.getBoost() * norm;\n      realSubInfos.add( new SubInfo( tempSubInfo.getText(), tempSubInfo.getTermsOffsets(),\n        tempSubInfo.getSeqnum(), subInfoBoost ));\n      totalBoost += subInfoBoost;\n    }\n\n    getFragInfos().add( new WeightedFragInfo( startOffset, endOffset, realSubInfos, totalBoost ) );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["3f678348ac6e145f874f56992e8c0f78a1ff3c46"],"3f678348ac6e145f874f56992e8c0f78a1ff3c46":["b5c9602de3e9cfe5d0cc9e3a068199b57d5d85ef"],"b5c9602de3e9cfe5d0cc9e3a068199b57d5d85ef":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["b5c9602de3e9cfe5d0cc9e3a068199b57d5d85ef","3f678348ac6e145f874f56992e8c0f78a1ff3c46"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3f678348ac6e145f874f56992e8c0f78a1ff3c46":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"b5c9602de3e9cfe5d0cc9e3a068199b57d5d85ef":["3f678348ac6e145f874f56992e8c0f78a1ff3c46","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b5c9602de3e9cfe5d0cc9e3a068199b57d5d85ef"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}