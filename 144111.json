{"path":"src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","commits":[{"id":"e5a95ce1d7a3779af6db59b6b39d3b89172d7445","date":1228620032,"type":1,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,int,boolean,boolean,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c==0) {\n\n          }\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix) throws IOException {\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c==0) {\n\n          }\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fa89a35683d73665c61d7af1d16f41649c25e5a7","date":1228684315,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c==0) {\n\n          }\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c==0) {\n\n          }\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"861fa37cce2d9d3f8978bbb767e87a91d41ed4a8","date":1252682465,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c==0) {\n\n          }\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"fa89a35683d73665c61d7af1d16f41649c25e5a7":["e5a95ce1d7a3779af6db59b6b39d3b89172d7445"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ad94625fb8d088209f46650c8097196fec67f00c":["861fa37cce2d9d3f8978bbb767e87a91d41ed4a8"],"861fa37cce2d9d3f8978bbb767e87a91d41ed4a8":["fa89a35683d73665c61d7af1d16f41649c25e5a7"],"e5a95ce1d7a3779af6db59b6b39d3b89172d7445":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"fa89a35683d73665c61d7af1d16f41649c25e5a7":["861fa37cce2d9d3f8978bbb767e87a91d41ed4a8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["e5a95ce1d7a3779af6db59b6b39d3b89172d7445"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"e5a95ce1d7a3779af6db59b6b39d3b89172d7445":["fa89a35683d73665c61d7af1d16f41649c25e5a7"],"861fa37cce2d9d3f8978bbb767e87a91d41ed4a8":["ad94625fb8d088209f46650c8097196fec67f00c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}