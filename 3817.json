{"path":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","commits":[{"id":"91109046a59c58ee0ee5d0d2767b08d1f30d6702","date":1000830588,"type":0,"author":"Jason van Zyl","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"/dev/null","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n       throws IOException {\n    Enumeration fields  = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field)fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int position = fieldLengths[fieldNumber];\t  // position in field\n\n      if (field.isIndexed()) {\n\tif (!field.isTokenized()) {\t\t  // un-tokenized field\n\t  addPosition(fieldName, field.stringValue(), position++);\n\t} else {\n\t  Reader reader;\t\t\t  // find or make Reader\n\t  if (field.readerValue() != null)\n\t    reader = field.readerValue();\n\t  else if (field.stringValue() != null)\n\t    reader = new StringReader(field.stringValue());\n\t  else\n\t    throw new IllegalArgumentException\n\t      (\"field must have either String or Reader value\");\n\n\t  // Tokenize field and add to postingTable\n\t  TokenStream stream = analyzer.tokenStream(fieldName, reader);\n\t  try {\n\t    for (Token t = stream.next(); t != null; t = stream.next()) {\n\t      addPosition(fieldName, t.termText(), position++);\n\t      if (position > maxFieldLength) break;\n\t    }\n\t  } finally {\n\t    stream.close();\n\t  }\n\t}\n\n\tfieldLengths[fieldNumber] = position;\t  // save field length\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14aec0a40da5a9c26f8752701a5aa10f78f5017d","date":1027969875,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n       throws IOException {\n    Enumeration fields  = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field)fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int position = fieldLengths[fieldNumber];\t  // position in field\n\n      if (field.isIndexed()) {\n\tif (!field.isTokenized()) {\t\t  // un-tokenized field\n\t  addPosition(fieldName, field.stringValue(), position++);\n\t} else {\n\t  Reader reader;\t\t\t  // find or make Reader\n\t  if (field.readerValue() != null)\n\t    reader = field.readerValue();\n\t  else if (field.stringValue() != null)\n\t    reader = new StringReader(field.stringValue());\n\t  else\n\t    throw new IllegalArgumentException\n\t      (\"field must have either String or Reader value\");\n\n\t  // Tokenize field and add to postingTable\n\t  TokenStream stream = analyzer.tokenStream(fieldName, reader);\n\t  try {\n\t    for (Token t = stream.next(); t != null; t = stream.next()) {\n\t      addPosition(fieldName, t.termText(), position++);\n\t      if (position > maxFieldLength) break;\n\t    }\n\t  } finally {\n\t    stream.close();\n\t  }\n\t}\n\n\tfieldLengths[fieldNumber] = position;\t  // save field length\n        fieldBoosts[fieldNumber] *= field.getBoost();\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n       throws IOException {\n    Enumeration fields  = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field)fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int position = fieldLengths[fieldNumber];\t  // position in field\n\n      if (field.isIndexed()) {\n\tif (!field.isTokenized()) {\t\t  // un-tokenized field\n\t  addPosition(fieldName, field.stringValue(), position++);\n\t} else {\n\t  Reader reader;\t\t\t  // find or make Reader\n\t  if (field.readerValue() != null)\n\t    reader = field.readerValue();\n\t  else if (field.stringValue() != null)\n\t    reader = new StringReader(field.stringValue());\n\t  else\n\t    throw new IllegalArgumentException\n\t      (\"field must have either String or Reader value\");\n\n\t  // Tokenize field and add to postingTable\n\t  TokenStream stream = analyzer.tokenStream(fieldName, reader);\n\t  try {\n\t    for (Token t = stream.next(); t != null; t = stream.next()) {\n\t      addPosition(fieldName, t.termText(), position++);\n\t      if (position > maxFieldLength) break;\n\t    }\n\t  } finally {\n\t    stream.close();\n\t  }\n\t}\n\n\tfieldLengths[fieldNumber] = position;\t  // save field length\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"052d6a9887bbf0e7b3620c505ab07df06740289f","date":1028567700,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n       throws IOException {\n    Enumeration fields  = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field)fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int position = fieldLengths[fieldNumber];\t  // position in field\n\n      if (field.isIndexed()) {\n\tif (!field.isTokenized()) {\t\t  // un-tokenized field\n\t  addPosition(fieldName, field.stringValue(), position++);\n\t} else {\n\t  Reader reader;\t\t\t  // find or make Reader\n\t  if (field.readerValue() != null)\n\t    reader = field.readerValue();\n\t  else if (field.stringValue() != null)\n\t    reader = new StringReader(field.stringValue());\n\t  else\n\t    throw new IllegalArgumentException\n\t      (\"field must have either String or Reader value\");\n\n\t  // Tokenize field and add to postingTable\n\t  TokenStream stream = analyzer.tokenStream(fieldName, reader);\n\t  try {\n\t    for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n\t      addPosition(fieldName, t.termText(), position++);\n\t      if (position > maxFieldLength) break;\n\t    }\n\t  } finally {\n\t    stream.close();\n\t  }\n\t}\n\n\tfieldLengths[fieldNumber] = position;\t  // save field length\n        fieldBoosts[fieldNumber] *= field.getBoost();\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n       throws IOException {\n    Enumeration fields  = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field)fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int position = fieldLengths[fieldNumber];\t  // position in field\n\n      if (field.isIndexed()) {\n\tif (!field.isTokenized()) {\t\t  // un-tokenized field\n\t  addPosition(fieldName, field.stringValue(), position++);\n\t} else {\n\t  Reader reader;\t\t\t  // find or make Reader\n\t  if (field.readerValue() != null)\n\t    reader = field.readerValue();\n\t  else if (field.stringValue() != null)\n\t    reader = new StringReader(field.stringValue());\n\t  else\n\t    throw new IllegalArgumentException\n\t      (\"field must have either String or Reader value\");\n\n\t  // Tokenize field and add to postingTable\n\t  TokenStream stream = analyzer.tokenStream(fieldName, reader);\n\t  try {\n\t    for (Token t = stream.next(); t != null; t = stream.next()) {\n\t      addPosition(fieldName, t.termText(), position++);\n\t      if (position > maxFieldLength) break;\n\t    }\n\t  } finally {\n\t    stream.close();\n\t  }\n\t}\n\n\tfieldLengths[fieldNumber] = position;\t  // save field length\n        fieldBoosts[fieldNumber] *= field.getBoost();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"66f8ac981e8707cfae011613a8168a2edeb0b6e3","date":1064079760,"type":3,"author":"Erik Hatcher","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n    throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int position = fieldLengths[fieldNumber];\t  // position in field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          addPosition(fieldName, field.stringValue(), position++);\n        } else {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              addPosition(fieldName, t.termText(), position++);\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = position;\t  // save field length\n        fieldBoosts[fieldNumber] *= field.getBoost();\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n       throws IOException {\n    Enumeration fields  = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field)fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int position = fieldLengths[fieldNumber];\t  // position in field\n\n      if (field.isIndexed()) {\n\tif (!field.isTokenized()) {\t\t  // un-tokenized field\n\t  addPosition(fieldName, field.stringValue(), position++);\n\t} else {\n\t  Reader reader;\t\t\t  // find or make Reader\n\t  if (field.readerValue() != null)\n\t    reader = field.readerValue();\n\t  else if (field.stringValue() != null)\n\t    reader = new StringReader(field.stringValue());\n\t  else\n\t    throw new IllegalArgumentException\n\t      (\"field must have either String or Reader value\");\n\n\t  // Tokenize field and add to postingTable\n\t  TokenStream stream = analyzer.tokenStream(fieldName, reader);\n\t  try {\n\t    for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n\t      addPosition(fieldName, t.termText(), position++);\n\t      if (position > maxFieldLength) break;\n\t    }\n\t  } finally {\n\t    stream.close();\n\t  }\n\t}\n\n\tfieldLengths[fieldNumber] = position;\t  // save field length\n        fieldBoosts[fieldNumber] *= field.getBoost();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2ef9d8f446efc33e4fc919333c39cd20410ffa11","date":1072129218,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n    throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          addPosition(fieldName, field.stringValue(), position++);\n          length++;\n        } else {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              addPosition(fieldName, t.termText(), position++);\n              if (++length > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n    throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int position = fieldLengths[fieldNumber];\t  // position in field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          addPosition(fieldName, field.stringValue(), position++);\n        } else {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              addPosition(fieldName, t.termText(), position++);\n              if (position > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = position;\t  // save field length\n        fieldBoosts[fieldNumber] *= field.getBoost();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"770281b8a8459cafcdd2354b6a06078fea2d83c9","date":1077308096,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          addPosition(fieldName, field.stringValue(), position++);\n          length++;\n        } else {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              addPosition(fieldName, t.termText(), position++);\n              if (++length > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n    throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          addPosition(fieldName, field.stringValue(), position++);\n          length++;\n        } else {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n              (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              addPosition(fieldName, t.termText(), position++);\n              if (++length > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0","date":1096997448,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length > maxFieldLength) \n                break;\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          addPosition(fieldName, field.stringValue(), position++);\n          length++;\n        } else {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              addPosition(fieldName, t.termText(), position++);\n              if (++length > maxFieldLength) break;\n            }\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0fea872534d6ef91a1b497a238a935761aa51e64","date":1102883187,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length > maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length > maxFieldLength) \n                break;\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["82f0d8199c97a56b4747ae4e8e83608110fe30f2","b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"71776a043ec5499f36208dcdf114f371cbbb27f5","date":1132623984,"type":3,"author":"Erik Hatcher","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length > maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length > maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"12d40284fd9481f79444bc63bc5d13847caddd3d","date":1149902602,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Fieldable field = (Fieldable) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length > maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Field field = (Field) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length > maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"82f0d8199c97a56b4747ae4e8e83608110fe30f2","date":1154260617,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Fieldable field = (Fieldable) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Fieldable field = (Fieldable) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length > maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","bugFix":["0fea872534d6ef91a1b497a238a935761aa51e64"],"bugIntro":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c205cd73e4706c6feceba2eed84d820ec83e0850","date":1164584418,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Iterator fieldIterator = doc.getFields().iterator();\n    while (fieldIterator.hasNext()) {\n      Fieldable field = (Fieldable) fieldIterator.next();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Enumeration fields = doc.fields();\n    while (fields.hasMoreElements()) {\n      Fieldable field = (Fieldable) fields.nextElement();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8522ae207a56c6db28ca06fe6cc33e70911c3600","date":1173935743,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Iterator fieldIterator = doc.getFields().iterator();\n    while (fieldIterator.hasNext()) {\n      Fieldable field = (Fieldable) fieldIterator.next();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, null, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              Payload payload = t.getPayload();\n              if (payload != null) {\n                // enable payloads for this field\n              \tfieldStoresPayloads.set(fieldNumber);\n              }\n              \n              TermVectorOffsetInfo termVectorOffsetInfo;\n              if (field.isStoreOffsetWithTermVector()) {\n                termVectorOffsetInfo = new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset());\n              } else {\n                termVectorOffsetInfo = null;\n              }\n              addPosition(fieldName, t.termText(), position++, payload, termVectorOffsetInfo);\n              \n              lastToken = t;\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n    \n    // update fieldInfos for all fields that have one or more tokens with payloads\n    for (int i = fieldStoresPayloads.nextSetBit(0); i >= 0; i = fieldStoresPayloads.nextSetBit(i+1)) { \n    \tfieldInfos.fieldInfo(i).storePayloads = true;\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Iterator fieldIterator = doc.getFields().iterator();\n    while (fieldIterator.hasNext()) {\n      Fieldable field = (Fieldable) fieldIterator.next();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              if(field.isStoreOffsetWithTermVector())\n                addPosition(fieldName, t.termText(), position++, new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset()));\n              else\n                addPosition(fieldName, t.termText(), position++, null);\n              \n              lastToken = t;\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"42579622cc27f9908e64f29fa1130bfc28306009","date":1177874771,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Iterator fieldIterator = doc.getFields().iterator();\n    while (fieldIterator.hasNext()) {\n      Fieldable field = (Fieldable) fieldIterator.next();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, null, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        { // tokenized field\n          TokenStream stream = field.tokenStreamValue();\n          \n          // the field does not have a TokenStream,\n          // so we have to obtain one from the analyzer\n          if (stream == null) {\n            Reader reader;\t\t\t  // find or make Reader\n            if (field.readerValue() != null)\n              reader = field.readerValue();\n            else if (field.stringValue() != null)\n              reader = new StringReader(field.stringValue());\n            else\n              throw new IllegalArgumentException\n                      (\"field must have either String or Reader value\");\n  \n            // Tokenize field and add to postingTable\n            stream = analyzer.tokenStream(fieldName, reader);\n          }\n          \n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              Payload payload = t.getPayload();\n              if (payload != null) {\n                // enable payloads for this field\n              \tfieldStoresPayloads.set(fieldNumber);\n              }\n              \n              TermVectorOffsetInfo termVectorOffsetInfo;\n              if (field.isStoreOffsetWithTermVector()) {\n                termVectorOffsetInfo = new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset());\n              } else {\n                termVectorOffsetInfo = null;\n              }\n              addPosition(fieldName, t.termText(), position++, payload, termVectorOffsetInfo);\n              \n              lastToken = t;\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n    \n    // update fieldInfos for all fields that have one or more tokens with payloads\n    for (int i = fieldStoresPayloads.nextSetBit(0); i >= 0; i = fieldStoresPayloads.nextSetBit(i+1)) { \n    \tfieldInfos.fieldInfo(i).storePayloads = true;\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Iterator fieldIterator = doc.getFields().iterator();\n    while (fieldIterator.hasNext()) {\n      Fieldable field = (Fieldable) fieldIterator.next();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, null, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        {\n          Reader reader;\t\t\t  // find or make Reader\n          if (field.readerValue() != null)\n            reader = field.readerValue();\n          else if (field.stringValue() != null)\n            reader = new StringReader(field.stringValue());\n          else\n            throw new IllegalArgumentException\n                    (\"field must have either String or Reader value\");\n\n          // Tokenize field and add to postingTable\n          TokenStream stream = analyzer.tokenStream(fieldName, reader);\n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              Payload payload = t.getPayload();\n              if (payload != null) {\n                // enable payloads for this field\n              \tfieldStoresPayloads.set(fieldNumber);\n              }\n              \n              TermVectorOffsetInfo termVectorOffsetInfo;\n              if (field.isStoreOffsetWithTermVector()) {\n                termVectorOffsetInfo = new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset());\n              } else {\n                termVectorOffsetInfo = null;\n              }\n              addPosition(fieldName, t.termText(), position++, payload, termVectorOffsetInfo);\n              \n              lastToken = t;\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n    \n    // update fieldInfos for all fields that have one or more tokens with payloads\n    for (int i = fieldStoresPayloads.nextSetBit(0); i >= 0; i = fieldStoresPayloads.nextSetBit(i+1)) { \n    \tfieldInfos.fieldInfo(i).storePayloads = true;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b443dda8f84ed86d67aeecb48ce98151c485dbe6","date":1179405523,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Iterator fieldIterator = doc.getFields().iterator();\n    while (fieldIterator.hasNext()) {\n      Fieldable field = (Fieldable) fieldIterator.next();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, null, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        { // tokenized field\n          TokenStream stream = field.tokenStreamValue();\n          \n          // the field does not have a TokenStream,\n          // so we have to obtain one from the analyzer\n          if (stream == null) {\n            Reader reader;\t\t\t  // find or make Reader\n            if (field.readerValue() != null)\n              reader = field.readerValue();\n            else if (field.stringValue() != null)\n              reader = new StringReader(field.stringValue());\n            else\n              throw new IllegalArgumentException\n                      (\"field must have either String or Reader value\");\n  \n            // Tokenize field and add to postingTable\n            stream = analyzer.tokenStream(fieldName, reader);\n          }\n          \n          // remember this TokenStream, we must close it later\n          openTokenStreams.add(stream);\n          \n          // reset the TokenStream to the first token\n          stream.reset();\n          \n\n          Token lastToken = null;\n          for (Token t = stream.next(); t != null; t = stream.next()) {\n            position += (t.getPositionIncrement() - 1);\n              \n            Payload payload = t.getPayload();\n            if (payload != null) {\n              // enable payloads for this field\n              fieldStoresPayloads.set(fieldNumber);\n            }\n              \n            TermVectorOffsetInfo termVectorOffsetInfo;\n            if (field.isStoreOffsetWithTermVector()) {\n              termVectorOffsetInfo = new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset());\n            } else {\n              termVectorOffsetInfo = null;\n            }\n            addPosition(fieldName, t.termText(), position++, payload, termVectorOffsetInfo);\n              \n            lastToken = t;\n            if (++length >= maxFieldLength) {\n              if (infoStream != null)\n                infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n              break;\n            }\n          }\n            \n          if(lastToken != null)\n            offset += lastToken.endOffset() + 1;\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n    \n    // update fieldInfos for all fields that have one or more tokens with payloads\n    for (int i = fieldStoresPayloads.nextSetBit(0); i >= 0; i = fieldStoresPayloads.nextSetBit(i+1)) { \n    \tfieldInfos.fieldInfo(i).storePayloads = true;\n    }\n  }\n\n","sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Iterator fieldIterator = doc.getFields().iterator();\n    while (fieldIterator.hasNext()) {\n      Fieldable field = (Fieldable) fieldIterator.next();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, null, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        { // tokenized field\n          TokenStream stream = field.tokenStreamValue();\n          \n          // the field does not have a TokenStream,\n          // so we have to obtain one from the analyzer\n          if (stream == null) {\n            Reader reader;\t\t\t  // find or make Reader\n            if (field.readerValue() != null)\n              reader = field.readerValue();\n            else if (field.stringValue() != null)\n              reader = new StringReader(field.stringValue());\n            else\n              throw new IllegalArgumentException\n                      (\"field must have either String or Reader value\");\n  \n            // Tokenize field and add to postingTable\n            stream = analyzer.tokenStream(fieldName, reader);\n          }\n          \n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            Token lastToken = null;\n            for (Token t = stream.next(); t != null; t = stream.next()) {\n              position += (t.getPositionIncrement() - 1);\n              \n              Payload payload = t.getPayload();\n              if (payload != null) {\n                // enable payloads for this field\n              \tfieldStoresPayloads.set(fieldNumber);\n              }\n              \n              TermVectorOffsetInfo termVectorOffsetInfo;\n              if (field.isStoreOffsetWithTermVector()) {\n                termVectorOffsetInfo = new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset());\n              } else {\n                termVectorOffsetInfo = null;\n              }\n              addPosition(fieldName, t.termText(), position++, payload, termVectorOffsetInfo);\n              \n              lastToken = t;\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n                break;\n              }\n            }\n            \n            if(lastToken != null)\n              offset += lastToken.endOffset() + 1;\n            \n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n    \n    // update fieldInfos for all fields that have one or more tokens with payloads\n    for (int i = fieldStoresPayloads.nextSetBit(0); i >= 0; i = fieldStoresPayloads.nextSetBit(i+1)) { \n    \tfieldInfos.fieldInfo(i).storePayloads = true;\n    }\n  }\n\n","bugFix":["0fea872534d6ef91a1b497a238a935761aa51e64","66f8ac981e8707cfae011613a8168a2edeb0b6e3","052d6a9887bbf0e7b3620c505ab07df06740289f","82f0d8199c97a56b4747ae4e8e83608110fe30f2","8522ae207a56c6db28ca06fe6cc33e70911c3600","6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b20bed3506d9b128ea30a7a62e2a8b1d7df697b0","date":1185569419,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#invertDocument(Document).mjava","sourceNew":null,"sourceOld":"  // Tokenizes the fields of a document into Postings.\n  private final void invertDocument(Document doc)\n          throws IOException {\n    Iterator fieldIterator = doc.getFields().iterator();\n    while (fieldIterator.hasNext()) {\n      Fieldable field = (Fieldable) fieldIterator.next();\n      String fieldName = field.name();\n      int fieldNumber = fieldInfos.fieldNumber(fieldName);\n\n      int length = fieldLengths[fieldNumber];     // length of field\n      int position = fieldPositions[fieldNumber]; // position in field\n      if (length>0) position+=analyzer.getPositionIncrementGap(fieldName);\n      int offset = fieldOffsets[fieldNumber];       // offset field\n\n      if (field.isIndexed()) {\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          if(field.isStoreOffsetWithTermVector())\n            addPosition(fieldName, stringValue, position++, null, new TermVectorOffsetInfo(offset, offset + stringValue.length()));\n          else\n            addPosition(fieldName, stringValue, position++, null, null);\n          offset += stringValue.length();\n          length++;\n        } else \n        { // tokenized field\n          TokenStream stream = field.tokenStreamValue();\n          \n          // the field does not have a TokenStream,\n          // so we have to obtain one from the analyzer\n          if (stream == null) {\n            Reader reader;\t\t\t  // find or make Reader\n            if (field.readerValue() != null)\n              reader = field.readerValue();\n            else if (field.stringValue() != null)\n              reader = new StringReader(field.stringValue());\n            else\n              throw new IllegalArgumentException\n                      (\"field must have either String or Reader value\");\n  \n            // Tokenize field and add to postingTable\n            stream = analyzer.tokenStream(fieldName, reader);\n          }\n          \n          // remember this TokenStream, we must close it later\n          openTokenStreams.add(stream);\n          \n          // reset the TokenStream to the first token\n          stream.reset();\n          \n\n          Token lastToken = null;\n          for (Token t = stream.next(); t != null; t = stream.next()) {\n            position += (t.getPositionIncrement() - 1);\n              \n            Payload payload = t.getPayload();\n            if (payload != null) {\n              // enable payloads for this field\n              fieldStoresPayloads.set(fieldNumber);\n            }\n              \n            TermVectorOffsetInfo termVectorOffsetInfo;\n            if (field.isStoreOffsetWithTermVector()) {\n              termVectorOffsetInfo = new TermVectorOffsetInfo(offset + t.startOffset(), offset + t.endOffset());\n            } else {\n              termVectorOffsetInfo = null;\n            }\n            addPosition(fieldName, t.termText(), position++, payload, termVectorOffsetInfo);\n              \n            lastToken = t;\n            if (++length >= maxFieldLength) {\n              if (infoStream != null)\n                infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached, ignoring following tokens\");\n              break;\n            }\n          }\n            \n          if(lastToken != null)\n            offset += lastToken.endOffset() + 1;\n        }\n\n        fieldLengths[fieldNumber] = length;\t  // save field length\n        fieldPositions[fieldNumber] = position;\t  // save field position\n        fieldBoosts[fieldNumber] *= field.getBoost();\n        fieldOffsets[fieldNumber] = offset;\n      }\n    }\n    \n    // update fieldInfos for all fields that have one or more tokens with payloads\n    for (int i = fieldStoresPayloads.nextSetBit(0); i >= 0; i = fieldStoresPayloads.nextSetBit(i+1)) { \n    \tfieldInfos.fieldInfo(i).storePayloads = true;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"71776a043ec5499f36208dcdf114f371cbbb27f5":["0fea872534d6ef91a1b497a238a935761aa51e64"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8522ae207a56c6db28ca06fe6cc33e70911c3600":["c205cd73e4706c6feceba2eed84d820ec83e0850"],"b20bed3506d9b128ea30a7a62e2a8b1d7df697b0":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"12d40284fd9481f79444bc63bc5d13847caddd3d":["71776a043ec5499f36208dcdf114f371cbbb27f5"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["2ef9d8f446efc33e4fc919333c39cd20410ffa11"],"14aec0a40da5a9c26f8752701a5aa10f78f5017d":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"82f0d8199c97a56b4747ae4e8e83608110fe30f2":["12d40284fd9481f79444bc63bc5d13847caddd3d"],"2ef9d8f446efc33e4fc919333c39cd20410ffa11":["66f8ac981e8707cfae011613a8168a2edeb0b6e3"],"b443dda8f84ed86d67aeecb48ce98151c485dbe6":["42579622cc27f9908e64f29fa1130bfc28306009"],"0fea872534d6ef91a1b497a238a935761aa51e64":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"42579622cc27f9908e64f29fa1130bfc28306009":["8522ae207a56c6db28ca06fe6cc33e70911c3600"],"052d6a9887bbf0e7b3620c505ab07df06740289f":["14aec0a40da5a9c26f8752701a5aa10f78f5017d"],"66f8ac981e8707cfae011613a8168a2edeb0b6e3":["052d6a9887bbf0e7b3620c505ab07df06740289f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b20bed3506d9b128ea30a7a62e2a8b1d7df697b0"],"c205cd73e4706c6feceba2eed84d820ec83e0850":["82f0d8199c97a56b4747ae4e8e83608110fe30f2"]},"commit2Childs":{"71776a043ec5499f36208dcdf114f371cbbb27f5":["12d40284fd9481f79444bc63bc5d13847caddd3d"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["14aec0a40da5a9c26f8752701a5aa10f78f5017d"],"8522ae207a56c6db28ca06fe6cc33e70911c3600":["42579622cc27f9908e64f29fa1130bfc28306009"],"b20bed3506d9b128ea30a7a62e2a8b1d7df697b0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["0fea872534d6ef91a1b497a238a935761aa51e64"],"12d40284fd9481f79444bc63bc5d13847caddd3d":["82f0d8199c97a56b4747ae4e8e83608110fe30f2"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"14aec0a40da5a9c26f8752701a5aa10f78f5017d":["052d6a9887bbf0e7b3620c505ab07df06740289f"],"82f0d8199c97a56b4747ae4e8e83608110fe30f2":["c205cd73e4706c6feceba2eed84d820ec83e0850"],"2ef9d8f446efc33e4fc919333c39cd20410ffa11":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"0fea872534d6ef91a1b497a238a935761aa51e64":["71776a043ec5499f36208dcdf114f371cbbb27f5"],"b443dda8f84ed86d67aeecb48ce98151c485dbe6":["b20bed3506d9b128ea30a7a62e2a8b1d7df697b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"42579622cc27f9908e64f29fa1130bfc28306009":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"052d6a9887bbf0e7b3620c505ab07df06740289f":["66f8ac981e8707cfae011613a8168a2edeb0b6e3"],"66f8ac981e8707cfae011613a8168a2edeb0b6e3":["2ef9d8f446efc33e4fc919333c39cd20410ffa11"],"c205cd73e4706c6feceba2eed84d820ec83e0850":["8522ae207a56c6db28ca06fe6cc33e70911c3600"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}