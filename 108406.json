{"path":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/dict/UserDictionary#UserDictionary(Reader).mjava","commits":[{"id":"6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2","date":1326399048,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"/dev/null","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, fstOutput.get(ord));\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3be20ca1091c0b7cdb2308b9023606a5e451cec","date":1327877325,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, fstOutput.get(ord));\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817882884229bace7dc5d1b75f6b0e4aa1e47122","date":1327879145,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, fstOutput.get(ord));\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5b6fdfce35d0adb18836cf8711abe487a934df33","date":1327946200,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, fstOutput.get(ord));\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"98d45c1ff2c99694b6de2201175f9b8b8b27b597","date":1332757908,"type":5,"author":"Christian Moen","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b3be20ca1091c0b7cdb2308b9023606a5e451cec":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2"],"817882884229bace7dc5d1b75f6b0e4aa1e47122":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2","b3be20ca1091c0b7cdb2308b9023606a5e451cec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"98d45c1ff2c99694b6de2201175f9b8b8b27b597":["b3be20ca1091c0b7cdb2308b9023606a5e451cec"],"5b6fdfce35d0adb18836cf8711abe487a934df33":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2","b3be20ca1091c0b7cdb2308b9023606a5e451cec"],"6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["98d45c1ff2c99694b6de2201175f9b8b8b27b597"]},"commit2Childs":{"b3be20ca1091c0b7cdb2308b9023606a5e451cec":["817882884229bace7dc5d1b75f6b0e4aa1e47122","98d45c1ff2c99694b6de2201175f9b8b8b27b597","5b6fdfce35d0adb18836cf8711abe487a934df33"],"817882884229bace7dc5d1b75f6b0e4aa1e47122":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2"],"98d45c1ff2c99694b6de2201175f9b8b8b27b597":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5b6fdfce35d0adb18836cf8711abe487a934df33":[],"6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2":["b3be20ca1091c0b7cdb2308b9023606a5e451cec","817882884229bace7dc5d1b75f6b0e4aa1e47122","5b6fdfce35d0adb18836cf8711abe487a934df33"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["817882884229bace7dc5d1b75f6b0e4aa1e47122","5b6fdfce35d0adb18836cf8711abe487a934df33","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}