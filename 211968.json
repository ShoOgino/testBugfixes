{"path":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb10b6bcde550b87d8f10e5f010bd8f3021023b6","date":1274974592,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"833a7987bc1c94455fde83e3311f72bddedcfb93","date":1279951470,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    //nocommit\n//    if (!mergeDocStores) {\n//      // When we are not merging by doc stores, their field\n//      // name -> number mapping are the same.  So, we start\n//      // with the fieldInfos of the last segment in this\n//      // case, to keep that numbering.\n//      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n//      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n//    } else {\n//      fieldInfos = new FieldInfos();\t\t  // merge field names\n//    }\n\n    fieldInfos = new FieldInfos();      // merge field names\n    \n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    // merge field values\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength) {\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    }\n      \n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c740bdcaf9781b9822969a3305e51cfa4eaaf673","date":1280775080,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90dd9e0118d85e8451e26b0e3c18172a42d673ce","date":1280782731,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5972dc9caad764a4e6e2e25f3e1a8de2489a8487","date":1280787041,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01f60198ece724a6e96cd0b45f289cf42ff83d4f","date":1286864103,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Values fiIndexValues = fi.indexValues;\n          final Values mergedIndexValues = merged.indexValues;\n          if (mergedIndexValues == null) {\n            merged.setIndexValues(fiIndexValues);\n          } else if (mergedIndexValues != fiIndexValues) {\n            // nocommit -- what to do?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedIndexValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n\n        // nocommit -- how should we handle index values here?\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e28c49f1fb6215a550fdadcf3805aa629b63ec0","date":1288081775,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Values fiIndexValues = fi.indexValues;\n          final Values mergedIndexValues = merged.indexValues;\n          if (mergedIndexValues == null) {\n            merged.setIndexValues(fiIndexValues);\n          } else if (mergedIndexValues != fiIndexValues) {\n            // TODO -- can we recover from this?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedIndexValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Values fiIndexValues = fi.indexValues;\n          final Values mergedIndexValues = merged.indexValues;\n          if (mergedIndexValues == null) {\n            merged.setIndexValues(fiIndexValues);\n          } else if (mergedIndexValues != fiIndexValues) {\n            // nocommit -- what to do?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedIndexValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n\n        // nocommit -- how should we handle index values here?\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"39f3757037aa8f710c0cbf9a76a332de735f58b0","date":1288384416,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Values fiIndexValues = fi.docValues;\n          final Values mergedDocValues = merged.docValues;\n          if (mergedDocValues == null) {\n            merged.setDocValues(fiIndexValues);\n          } else if (mergedDocValues != fiIndexValues) {\n            // TODO -- can we recover from this?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedDocValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Values fiIndexValues = fi.indexValues;\n          final Values mergedIndexValues = merged.indexValues;\n          if (mergedIndexValues == null) {\n            merged.setIndexValues(fiIndexValues);\n          } else if (mergedIndexValues != fiIndexValues) {\n            // TODO -- can we recover from this?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedIndexValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a493e6d0c3ad86bd55c0a1360d110142e948f2bd","date":1289406991,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();// merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecInfo);\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();// merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Values fiIndexValues = fi.docValues;\n          final Values mergedDocValues = merged.docValues;\n          if (mergedDocValues == null) {\n            merged.setDocValues(fiIndexValues);\n          } else if (mergedDocValues != fiIndexValues) {\n            // TODO -- can we recover from this?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedDocValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecInfo);\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();\t\t  // merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Values fiIndexValues = fi.docValues;\n          final Values mergedDocValues = merged.docValues;\n          if (mergedDocValues == null) {\n            merged.setDocValues(fiIndexValues);\n          } else if (mergedDocValues != fiIndexValues) {\n            // TODO -- can we recover from this?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedDocValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4","date":1291128345,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();// merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Values fiIndexValues = fi.docValues;\n          final Values mergedDocValues = merged.docValues;\n          if (mergedDocValues == null) {\n            merged.setDocValues(fiIndexValues);\n          } else if (mergedDocValues != fiIndexValues) {\n            // TODO -- can we recover from this?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedDocValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecInfo, new AtomicLong(0));\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();// merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Values fiIndexValues = fi.docValues;\n          final Values mergedDocValues = merged.docValues;\n          if (mergedDocValues == null) {\n            merged.setDocValues(fiIndexValues);\n          } else if (mergedDocValues != fiIndexValues) {\n            // TODO -- can we recover from this?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedDocValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecInfo);\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aa2fc2eb37a1f19e90850f787d9e085950ebfa04","date":1291597075,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();// merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Type fiIndexValues = fi.docValues;\n          final Type mergedDocValues = merged.docValues;\n          if (mergedDocValues == null) {\n            merged.setDocValues(fiIndexValues);\n          } else if (mergedDocValues != fiIndexValues) {\n            // TODO -- can we recover from this?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedDocValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecInfo, new AtomicLong(0));\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();// merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Values fiIndexValues = fi.docValues;\n          final Values mergedDocValues = merged.docValues;\n          if (mergedDocValues == null) {\n            merged.setDocValues(fiIndexValues);\n          } else if (mergedDocValues != fiIndexValues) {\n            // TODO -- can we recover from this?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedDocValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecInfo, new AtomicLong(0));\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    fieldInfos = new FieldInfos();// merge field names\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo);\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();// merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecInfo);\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5","date":1292711882,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo);\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    fieldInfos = new FieldInfos();// merge field names\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo);\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, new AtomicLong(0));\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    if (!mergeDocStores) {\n      // When we are not merging by doc stores, their field\n      // name -> number mapping are the same.  So, we start\n      // with the fieldInfos of the last segment in this\n      // case, to keep that numbering.\n      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n    } else {\n      fieldInfos = new FieldInfos();// merge field names\n    }\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          FieldInfo merged = fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n                                            fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n                                            !reader.hasNorms(fi.name), fi.storePayloads,\n                                            fi.omitTermFreqAndPositions);\n          final Type fiIndexValues = fi.docValues;\n          final Type mergedDocValues = merged.docValues;\n          if (mergedDocValues == null) {\n            merged.setDocValues(fiIndexValues);\n          } else if (mergedDocValues != fiIndexValues) {\n            // TODO -- can we recover from this?\n            throw new IllegalStateException(\"cannot merge field \" + fi.name + \" indexValues changed from \" + mergedDocValues + \" to \" + fiIndexValues);\n          }\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    if (mergeDocStores) {\n      // merge field values\n      final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n      try {\n        int idx = 0;\n        for (IndexReader reader : readers) {\n          final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n          FieldsReader matchingFieldsReader = null;\n          if (matchingSegmentReader != null) {\n            final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n            if (fieldsReader != null) {\n              matchingFieldsReader = fieldsReader;\n            }\n          }\n          if (reader.hasDeletions()) {\n            docCount += copyFieldsWithDeletions(fieldsWriter,\n                                                reader, matchingFieldsReader);\n          } else {\n            docCount += copyFieldsNoDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n          }\n        }\n      } finally {\n        fieldsWriter.close();\n      }\n\n      final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n      final long fdxFileLength = directory.fileLength(fileName);\n\n      if (4+((long) docCount)*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    } else {\n      // If we are skipping the doc stores, that means there\n      // are no deletions in any of these segments, so we\n      // just sum numDocs() of each segment to get total docCount\n      for (final IndexReader reader : readers) {\n        docCount += reader.numDocs();\n      }\n    }\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, null, docCount, 0, termIndexInterval, codecInfo, new AtomicLong(0));\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    // merge field values\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int mergeFields() throws CorruptIndexException, IOException {\n\n    //nocommit\n//    if (!mergeDocStores) {\n//      // When we are not merging by doc stores, their field\n//      // name -> number mapping are the same.  So, we start\n//      // with the fieldInfos of the last segment in this\n//      // case, to keep that numbering.\n//      final SegmentReader sr = (SegmentReader) readers.get(readers.size()-1);\n//      fieldInfos = (FieldInfos) sr.core.fieldInfos.clone();\n//    } else {\n//      fieldInfos = new FieldInfos();\t\t  // merge field names\n//    }\n\n    fieldInfos = new FieldInfos();      // merge field names\n    \n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          FieldInfo fi = readerFieldInfos.fieldInfo(j);\n          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector,\n              fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,\n              !reader.hasNorms(fi.name), fi.storePayloads,\n              fi.omitTermFreqAndPositions);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    // merge field values\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null && fieldsReader.canReadRawDocs()) {            \n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength) {\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    }\n      \n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecs);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"69a6d2d525aeab53c867ed26934185e5bb627d0e","date":1296516902,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo);\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, new AtomicLong(0));\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, new AtomicLong(0));\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    // merge field values\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e92442af786151ee55bc283eb472f629e3c7b52b","date":1301070252,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, new AtomicLong(0));\n    \n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        int numReaderFieldInfos = readerFieldInfos.size();\n        for (int j = 0; j < numReaderFieldInfos; j++) {\n          fieldInfos.add(readerFieldInfos.fieldInfo(j));\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, new AtomicLong(0));\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = SegmentCodecs.build(fieldInfos, this.codecs);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, new AtomicLong(0));\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   * \n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n    \n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e8d7ba2175f47e280231533f7d3016249cea88b","date":1307711934,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    // nocommit - should we rather use IOContext.MERGE here?\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, IOContext.DEFAULT);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    //nocommit if Merge then what to initialize OneMerge with ?\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, IOContext.DEFAULT);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6f9be74ca7baaef11857ad002cad40419979516","date":1309449808,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    // nocommit - should we rather use IOContext.MERGE here?\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    // nocommit - should we rather use IOContext.MERGE here?\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, IOContext.DEFAULT);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    //nocommit if Merge then what to initialize OneMerge with ?\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, IOContext.DEFAULT);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e248526ae3a33286a678d7833da022fd95695f2d","date":1309450587,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    // nocommit - should we rather use IOContext.MERGE here?\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment);\n\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2afd23a6f1242190c3409d8d81d5c5912d607fc9","date":1310477482,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_POSITIONS), false, false, false, false, IndexOptions.DOCS_AND_FREQS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, IndexOptions.DOCS_ONLY);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, true);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0061262413ecc163d6eebba1b5c43ab91a0c2dc5","date":1311195279,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (MergeState.IndexReaderAndLiveDocs readerAndLiveDocs : readers) {\n      final IndexReader reader = readerAndLiveDocs.reader;\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_POSITIONS), false, false, false, false, IndexOptions.DOCS_AND_FREQS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, IndexOptions.DOCS_ONLY);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.liveDocs != null) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (IndexReader reader : readers) {\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_POSITIONS), false, false, false, false, IndexOptions.DOCS_AND_FREQS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, IndexOptions.DOCS_ONLY);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (IndexReader reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.hasDeletions()) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b1110660886afcc62f57e9af901cd3f5dd294bc","date":1317830374,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (MergeState.IndexReaderAndLiveDocs readerAndLiveDocs : readers) {\n      final IndexReader reader = readerAndLiveDocs.reader;\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_POSITIONS), false, false, false, false, IndexOptions.DOCS_AND_FREQS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, IndexOptions.DOCS_ONLY);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = codecInfo.provider.fieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.liveDocs != null) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (MergeState.IndexReaderAndLiveDocs readerAndLiveDocs : readers) {\n      final IndexReader reader = readerAndLiveDocs.reader;\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_POSITIONS), false, false, false, false, IndexOptions.DOCS_AND_FREQS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, IndexOptions.DOCS_ONLY);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.liveDocs != null) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"32aca6bb0a6aa0a1813e7d035ac0e039f54269f4","date":1318260487,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (MergeState.IndexReaderAndLiveDocs readerAndLiveDocs : readers) {\n      final IndexReader reader = readerAndLiveDocs.reader;\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_POSITIONS), false, false, false, false, IndexOptions.DOCS_AND_FREQS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, IndexOptions.DOCS_ONLY);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = codecInfo.provider.fieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.liveDocs != null) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (MergeState.IndexReaderAndLiveDocs readerAndLiveDocs : readers) {\n      final IndexReader reader = readerAndLiveDocs.reader;\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_POSITIONS), false, false, false, false, IndexOptions.DOCS_AND_FREQS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, IndexOptions.DOCS_ONLY);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n    fieldInfos.write(directory, segment + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = codecInfo.provider.fieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.liveDocs != null) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (MergeState.IndexReaderAndLiveDocs readerAndLiveDocs : readers) {\n      final IndexReader reader = readerAndLiveDocs.reader;\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_POSITIONS), false, false, false, false, IndexOptions.DOCS_AND_FREQS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, IndexOptions.DOCS_ONLY);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = codec.fieldsFormat().fieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.liveDocs != null) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codec, null, context);\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (MergeState.IndexReaderAndLiveDocs readerAndLiveDocs : readers) {\n      final IndexReader reader = readerAndLiveDocs.reader;\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_POSITIONS), false, false, false, false, IndexOptions.DOCS_AND_FREQS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, IndexOptions.DOCS_ONLY);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n    final SegmentCodecs codecInfo = fieldInfos.buildSegmentCodecs(false);\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = codecInfo.provider.fieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.liveDocs != null) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codecInfo, null, context);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    int docCount = 0;\n\n    final StoredFieldsWriter fieldsWriter = codec.storedFieldsFormat().fieldsWriter(directory, segment, context);\n    try {\n      docCount = fieldsWriter.merge(mergeState);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    for (MergeState.IndexReaderAndLiveDocs readerAndLiveDocs : readers) {\n      final IndexReader reader = readerAndLiveDocs.reader;\n      if (reader instanceof SegmentReader) {\n        SegmentReader segmentReader = (SegmentReader) reader;\n        FieldInfos readerFieldInfos = segmentReader.fieldInfos();\n        for (FieldInfo fi : readerFieldInfos) {\n          fieldInfos.add(fi);\n        }\n      } else {\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR), true, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_POSITIONS), false, false, false, false, IndexOptions.DOCS_AND_FREQS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.OMIT_TERM_FREQ_AND_POSITIONS), false, false, false, false, IndexOptions.DOCS_ONLY);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.UNINDEXED), false);\n        fieldInfos.addOrUpdate(reader.getFieldNames(FieldOption.DOC_VALUES), false);\n      }\n    }\n\n    int docCount = 0;\n\n    setMatchingSegmentReaders();\n    final FieldsWriter fieldsWriter = codec.fieldsFormat().fieldsWriter(directory, segment, context);\n    try {\n      int idx = 0;\n      for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n        final SegmentReader matchingSegmentReader = matchingSegmentReaders[idx++];\n        FieldsReader matchingFieldsReader = null;\n        if (matchingSegmentReader != null) {\n          final FieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n          if (fieldsReader != null) {\n            matchingFieldsReader = fieldsReader;\n          }\n        }\n        if (reader.liveDocs != null) {\n          docCount += copyFieldsWithDeletions(fieldsWriter,\n                                              reader, matchingFieldsReader);\n        } else {\n          docCount += copyFieldsNoDeletions(fieldsWriter,\n                                            reader, matchingFieldsReader);\n        }\n      }\n    } finally {\n      fieldsWriter.close();\n    }\n\n    final String fileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.FIELDS_INDEX_EXTENSION);\n    final long fdxFileLength = directory.fileLength(fileName);\n\n    if (4+((long) docCount)*8 != fdxFileLength)\n      // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n      // we detect that the bug has struck, here, and\n      // throw an exception to prevent the corruption from\n      // entering the index.  See LUCENE-1282 for\n      // details.\n      throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \" file=\" + fileName + \" file exists?=\" + directory.fileExists(fileName) + \"; now aborting this merge to prevent index corruption\");\n    segmentWriteState = new SegmentWriteState(null, directory, segment, fieldInfos, docCount, termIndexInterval, codec, null, context);\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    final StoredFieldsWriter fieldsWriter = codec.storedFieldsFormat().fieldsWriter(directory, segment, context);\n    \n    try {\n      return fieldsWriter.merge(mergeState);\n    } finally {\n      fieldsWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    int docCount = 0;\n\n    final StoredFieldsWriter fieldsWriter = codec.storedFieldsFormat().fieldsWriter(directory, segment, context);\n    try {\n      docCount = fieldsWriter.merge(mergeState);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeFields().mjava","sourceNew":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    final StoredFieldsWriter fieldsWriter = codec.storedFieldsFormat().fieldsWriter(directory, segment, context);\n    \n    try {\n      return fieldsWriter.merge(mergeState);\n    } finally {\n      fieldsWriter.close();\n    }\n  }\n\n","sourceOld":"  /**\n   *\n   * @return The number of documents in all of the readers\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private int mergeFields() throws CorruptIndexException, IOException {\n    final StoredFieldsWriter fieldsWriter = codec.storedFieldsFormat().fieldsWriter(directory, segment, context);\n    \n    try {\n      return fieldsWriter.merge(mergeState);\n    } finally {\n      fieldsWriter.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"2afd23a6f1242190c3409d8d81d5c5912d607fc9":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4":["85a883878c0af761245ab048babc63d099f835f3"],"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["2afd23a6f1242190c3409d8d81d5c5912d607fc9"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["3cc749c053615f5871f3b95715fe292f34e70a53"],"c740bdcaf9781b9822969a3305e51cfa4eaaf673":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["5972dc9caad764a4e6e2e25f3e1a8de2489a8487"],"4b1110660886afcc62f57e9af901cd3f5dd294bc":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["833a7987bc1c94455fde83e3311f72bddedcfb93","94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"01f60198ece724a6e96cd0b45f289cf42ff83d4f":["5972dc9caad764a4e6e2e25f3e1a8de2489a8487"],"90dd9e0118d85e8451e26b0e3c18172a42d673ce":["c740bdcaf9781b9822969a3305e51cfa4eaaf673"],"e92442af786151ee55bc283eb472f629e3c7b52b":["1224a4027481acce15495b03bce9b48b93b42722"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["bde51b089eb7f86171eb3406e38a274743f9b7ac","e92442af786151ee55bc283eb472f629e3c7b52b"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","69a6d2d525aeab53c867ed26934185e5bb627d0e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b6f9be74ca7baaef11857ad002cad40419979516":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"69a6d2d525aeab53c867ed26934185e5bb627d0e":["94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"5972dc9caad764a4e6e2e25f3e1a8de2489a8487":["90dd9e0118d85e8451e26b0e3c18172a42d673ce"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["2e8d7ba2175f47e280231533f7d3016249cea88b","e248526ae3a33286a678d7833da022fd95695f2d"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"1224a4027481acce15495b03bce9b48b93b42722":["14ec33385f6fbb6ce172882d14605790418a5d31"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","2e8d7ba2175f47e280231533f7d3016249cea88b"],"39f3757037aa8f710c0cbf9a76a332de735f58b0":["0e28c49f1fb6215a550fdadcf3805aa629b63ec0"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"85a883878c0af761245ab048babc63d099f835f3":["39f3757037aa8f710c0cbf9a76a332de735f58b0","a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3cc749c053615f5871f3b95715fe292f34e70a53":["06584e6e98d592b34e1329b384182f368d2025e8"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["e92442af786151ee55bc283eb472f629e3c7b52b","c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["aa2fc2eb37a1f19e90850f787d9e085950ebfa04","94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"aa2fc2eb37a1f19e90850f787d9e085950ebfa04":["5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"7b91922b55d15444d554721b352861d028eb8278":["32aca6bb0a6aa0a1813e7d035ac0e039f54269f4"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["29ef99d61cda9641b6250bf9567329a6e65f901d","e92442af786151ee55bc283eb472f629e3c7b52b"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["69a6d2d525aeab53c867ed26934185e5bb627d0e"],"a3776dccca01c11e7046323cfad46a3b4a471233":["e92442af786151ee55bc283eb472f629e3c7b52b","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"e248526ae3a33286a678d7833da022fd95695f2d":["b6f9be74ca7baaef11857ad002cad40419979516"],"32aca6bb0a6aa0a1813e7d035ac0e039f54269f4":["4b1110660886afcc62f57e9af901cd3f5dd294bc"],"0e28c49f1fb6215a550fdadcf3805aa629b63ec0":["01f60198ece724a6e96cd0b45f289cf42ff83d4f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"2afd23a6f1242190c3409d8d81d5c5912d607fc9":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5"],"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4":["aa2fc2eb37a1f19e90850f787d9e085950ebfa04"],"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["4b1110660886afcc62f57e9af901cd3f5dd294bc"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c740bdcaf9781b9822969a3305e51cfa4eaaf673":["90dd9e0118d85e8451e26b0e3c18172a42d673ce"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","85a883878c0af761245ab048babc63d099f835f3"],"4b1110660886afcc62f57e9af901cd3f5dd294bc":["32aca6bb0a6aa0a1813e7d035ac0e039f54269f4"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["b6f9be74ca7baaef11857ad002cad40419979516"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"01f60198ece724a6e96cd0b45f289cf42ff83d4f":["0e28c49f1fb6215a550fdadcf3805aa629b63ec0"],"90dd9e0118d85e8451e26b0e3c18172a42d673ce":["5972dc9caad764a4e6e2e25f3e1a8de2489a8487"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd","ddc4c914be86e34b54f70023f45a60fa7f04e929","a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"e92442af786151ee55bc283eb472f629e3c7b52b":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","b3e06be49006ecac364d39d12b9c9f74882f9b9f","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"14ec33385f6fbb6ce172882d14605790418a5d31":["1224a4027481acce15495b03bce9b48b93b42722"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b6f9be74ca7baaef11857ad002cad40419979516":["e248526ae3a33286a678d7833da022fd95695f2d"],"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","69a6d2d525aeab53c867ed26934185e5bb627d0e","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["2afd23a6f1242190c3409d8d81d5c5912d607fc9","5d004d0e0b3f65bb40da76d476d659d7888270e8"],"5972dc9caad764a4e6e2e25f3e1a8de2489a8487":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd","01f60198ece724a6e96cd0b45f289cf42ff83d4f"],"69a6d2d525aeab53c867ed26934185e5bb627d0e":["29ef99d61cda9641b6250bf9567329a6e65f901d","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"1224a4027481acce15495b03bce9b48b93b42722":["e92442af786151ee55bc283eb472f629e3c7b52b"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"39f3757037aa8f710c0cbf9a76a332de735f58b0":["85a883878c0af761245ab048babc63d099f835f3"],"06584e6e98d592b34e1329b384182f368d2025e8":["3cc749c053615f5871f3b95715fe292f34e70a53"],"85a883878c0af761245ab048babc63d099f835f3":["5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"3cc749c053615f5871f3b95715fe292f34e70a53":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["2e8d7ba2175f47e280231533f7d3016249cea88b","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"aa2fc2eb37a1f19e90850f787d9e085950ebfa04":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["c740bdcaf9781b9822969a3305e51cfa4eaaf673","833a7987bc1c94455fde83e3311f72bddedcfb93"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"e248526ae3a33286a678d7833da022fd95695f2d":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"0e28c49f1fb6215a550fdadcf3805aa629b63ec0":["39f3757037aa8f710c0cbf9a76a332de735f58b0"],"32aca6bb0a6aa0a1813e7d035ac0e039f54269f4":["7b91922b55d15444d554721b352861d028eb8278"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5d004d0e0b3f65bb40da76d476d659d7888270e8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}