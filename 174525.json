{"path":"lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort#mergePartitions(List[File],File).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort#mergePartitions(List[File],File).mjava","pathOld":"modules/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort#mergePartitions(List[File],File).mjava","sourceNew":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<File> merges, File outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current);\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","sourceOld":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<File> merges, File outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current);\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7530de27b87b961b51f01bd1299b7004d46e8823","date":1355236261,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort#mergePartitions(List[File],File).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort#mergePartitions(List[File],File).mjava","sourceNew":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<File> merges, File outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current);\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","sourceOld":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<File> merges, File outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current);\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort#mergePartitions(List[File],File).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort#mergePartitions(List[File],File).mjava","sourceNew":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<File> merges, File outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current);\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","sourceOld":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<File> merges, File outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current);\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"847d1294c8ff2f0172af20892ef98a1c6a952e09","date":1359503775,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/Sort#mergePartitions(List[File],File).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort#mergePartitions(List[File],File).mjava","sourceNew":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<File> merges, File outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current);\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","sourceOld":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<File> merges, File outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current);\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/Sort#mergePartitions(List[File],File).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort#mergePartitions(List[File],File).mjava","sourceNew":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<File> merges, File outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current);\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","sourceOld":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<File> merges, File outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current);\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["b89678825b68eccaf09e6ab71675fc0b0af1e099","7530de27b87b961b51f01bd1299b7004d46e8823"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["7530de27b87b961b51f01bd1299b7004d46e8823","847d1294c8ff2f0172af20892ef98a1c6a952e09"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"847d1294c8ff2f0172af20892ef98a1c6a952e09":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064"],"7530de27b87b961b51f01bd1299b7004d46e8823":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d4d69c535930b5cce125cff868d40f6373dc27d4"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["847d1294c8ff2f0172af20892ef98a1c6a952e09"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","7530de27b87b961b51f01bd1299b7004d46e8823"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"847d1294c8ff2f0172af20892ef98a1c6a952e09":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"7530de27b87b961b51f01bd1299b7004d46e8823":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","d4d69c535930b5cce125cff868d40f6373dc27d4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}