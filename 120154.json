{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,String,boolean).mjava","commits":[{"id":"22859cb40e09867e7da8de84a31956c07259f82f","date":1441822065,"type":1,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,String,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCoreNodeName); // core node name of current leader\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, Replica.State.DOWN, leaderCoreNodeName);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();\n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n          ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(),\n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \" + forcePublishState,\n          replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n      overseerJobQueue.offer(Utils.toJSON(m));\n    }\n\n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e0300d1df37d7e9662d491269e91b6f66dca8bd","date":1443011762,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,String,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      CoreDescriptor leaderCd, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCd);\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCoreNodeName); // core node name of current leader\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"22859cb40e09867e7da8de84a31956c07259f82f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["22859cb40e09867e7da8de84a31956c07259f82f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"]},"commit2Childs":{"22859cb40e09867e7da8de84a31956c07259f82f":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["22859cb40e09867e7da8de84a31956c07259f82f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}