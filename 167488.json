{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(s.getHasVectors());\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      assertTrue(files.isEmpty());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(s.getHasVectors());\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      assertTrue(files.isEmpty());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(s.getHasVectors());\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      assertTrue(files.isEmpty());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(s.getHasVectors());\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      assertTrue(files.isEmpty());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4a8b14bc4241c302311422d5c6f7627f8febb86e","date":1337291675,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      List<String> filesExisting = SegmentInfo.findMatchingFiles(s.name, dir, files);\n      assertTrue(filesExisting.isEmpty());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(s.getHasVectors());\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      assertTrue(files.isEmpty());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1494abe5dc85557ec2e2772f87660d48f831c3a5","date":1337614370,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n      // nocommit\n      /*\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      List<String> filesExisting = SegmentInfo.findMatchingFiles(s.name, dir, files);\n      assertTrue(filesExisting.isEmpty());\n      */\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      List<String> filesExisting = SegmentInfo.findMatchingFiles(s.name, dir, files);\n      assertTrue(filesExisting.isEmpty());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d153abcf92dc5329d98571a8c3035df9bd80648","date":1337702630,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n      // nocommit\n      /*\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      List<String> filesExisting = SegmentInfo.findMatchingFiles(s.name, dir, files);\n      assertTrue(filesExisting.isEmpty());\n      */\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n      // nocommit\n      /*\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      List<String> filesExisting = SegmentInfo.findMatchingFiles(s.name, dir, files);\n      assertTrue(filesExisting.isEmpty());\n      */\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"224453c981b599f6026e0f088a3678f894913874","date":1337804410,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n      // nocommit\n      /*\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      List<String> filesExisting = SegmentInfo.findMatchingFiles(s.name, dir, files);\n      assertTrue(filesExisting.isEmpty());\n      */\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(s.getHasVectors());\n      Set<String> files = new HashSet<String>();\n      s.getCodec().termVectorsFormat().files(s, files);\n      assertTrue(files.isEmpty());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = IndexReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ced66195b26fdb1f77ee00e2a77ec6918dedd766","date":1344948886,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":["4a8b14bc4241c302311422d5c6f7627f8febb86e","3cc749c053615f5871f3b95715fe292f34e70a53","9d153abcf92dc5329d98571a8c3035df9bd80648"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c188105a9aae04f56c24996f98f8333fc825d2e","date":1345031914,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c93396a1df03720cb20e2c2f513a6fa59b21e4c","date":1345032673,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (IndexReader r : r0.getSequentialSubReaders()) {\n      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();\n      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"088a7ef694fd43d5d9a4d200c4005865f773d1e7","date":1371136274,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    ((LogMergePolicy) indexWriter.getConfig().getMergePolicy()).setUseCompoundFile(false);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    _TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.shutdown();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.shutdown();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setRAMBufferSizeMB(0.01).setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.shutdown();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.shutdown();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13","date":1409346855,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // test uses IW unref'ed check which is unaware of retries\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // test uses IW unref'ed check which is unaware of retries\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // test uses IW unref'ed check which is unaware of retries\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (AtomicReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bfe104fc023fadc9e709f8d17403d2cc61133fe","date":1454446396,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // test uses IW unref'ed check which is unaware of retries\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b470f36a9372c97283360b1304eacbde22df6c0d","date":1454765175,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // test uses IW unref'ed check which is unaware of retries\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // test uses IW unref'ed check which is unaware of retries\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a207d19eac354d649c3f0e2cce070017c78125e","date":1454776470,"type":3,"author":"Erick Erickson","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    if (dir instanceof MockDirectoryWrapper) {\n      // test uses IW unref'ed check which is unaware of retries\n      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);\n    }\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe39f1a106531207c028defebbc9eb5bb489ac50","date":1592513789,"type":3,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n\n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n\n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1182fe36fb5df768dc2da53f6d5338cbc07268ae","date":1592861749,"type":3,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n\n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n\n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c2a23476693f2bd9a4b44cc3187c429a2e21dac2","date":1593289545,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testNoUnwantedTVFiles().mjava","sourceNew":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n\n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n\n    r0.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testNoUnwantedTVFiles() throws Exception {\n\n    Directory dir = newDirectory();\n    IndexWriter indexWriter = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                                     .setRAMBufferSizeMB(0.01)\n                                                     .setMergePolicy(newLogMergePolicy()));\n    indexWriter.getConfig().getMergePolicy().setNoCFSRatio(0.0);\n\n    String BIG=\"alskjhlaksjghlaksjfhalksvjepgjioefgjnsdfjgefgjhelkgjhqewlrkhgwlekgrhwelkgjhwelkgrhwlkejg\";\n    BIG=BIG+BIG+BIG+BIG;\n\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setOmitNorms(true);\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    customType2.setTokenized(false);\n    FieldType customType3 = new FieldType(TextField.TYPE_STORED);\n    customType3.setTokenized(false);\n    customType3.setOmitNorms(true);\n    \n    for (int i=0; i<2; i++) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", Integer.toString(i)+BIG, customType3));\n      doc.add(new Field(\"str\", Integer.toString(i)+BIG, customType2));\n      doc.add(new Field(\"str2\", Integer.toString(i)+BIG, storedTextType));\n      doc.add(new Field(\"str3\", Integer.toString(i)+BIG, customType));\n      indexWriter.addDocument(doc);\n    }\n\n    indexWriter.close();\n\n    TestUtil.checkIndex(dir);\n\n    assertNoUnreferencedFiles(dir, \"no tv files\");\n    DirectoryReader r0 = DirectoryReader.open(dir);\n    for (LeafReaderContext ctx : r0.leaves()) {\n      SegmentReader sr = (SegmentReader) ctx.reader();\n      assertFalse(sr.getFieldInfos().hasVectors());\n    }\n    \n    r0.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4a8b14bc4241c302311422d5c6f7627f8febb86e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c","ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5a207d19eac354d649c3f0e2cce070017c78125e":["c9fb5f46e264daf5ba3860defe623a89d202dd87","b470f36a9372c97283360b1304eacbde22df6c0d"],"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13"],"1494abe5dc85557ec2e2772f87660d48f831c3a5":["4a8b14bc4241c302311422d5c6f7627f8febb86e"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"224453c981b599f6026e0f088a3678f894913874":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"ced66195b26fdb1f77ee00e2a77ec6918dedd766":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["1494abe5dc85557ec2e2772f87660d48f831c3a5"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["c9fb5f46e264daf5ba3860defe623a89d202dd87","b470f36a9372c97283360b1304eacbde22df6c0d"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"fe39f1a106531207c028defebbc9eb5bb489ac50":["5a207d19eac354d649c3f0e2cce070017c78125e"],"c2a23476693f2bd9a4b44cc3187c429a2e21dac2":["1182fe36fb5df768dc2da53f6d5338cbc07268ae"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","224453c981b599f6026e0f088a3678f894913874"],"b470f36a9372c97283360b1304eacbde22df6c0d":["c9fb5f46e264daf5ba3860defe623a89d202dd87","6bfe104fc023fadc9e709f8d17403d2cc61133fe"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"1182fe36fb5df768dc2da53f6d5338cbc07268ae":["fe39f1a106531207c028defebbc9eb5bb489ac50"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c2a23476693f2bd9a4b44cc3187c429a2e21dac2"]},"commit2Childs":{"4a8b14bc4241c302311422d5c6f7627f8febb86e":["1494abe5dc85557ec2e2772f87660d48f831c3a5"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"5a207d19eac354d649c3f0e2cce070017c78125e":["fe39f1a106531207c028defebbc9eb5bb489ac50"],"98d2deb8c96c79ebef084a1f8e5a1a6c08608f13":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["5a207d19eac354d649c3f0e2cce070017c78125e","6bfe104fc023fadc9e709f8d17403d2cc61133fe","1e6acbaae7af722f17204ceccf0f7db5753eccf3","b470f36a9372c97283360b1304eacbde22df6c0d"],"1494abe5dc85557ec2e2772f87660d48f831c3a5":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["3c188105a9aae04f56c24996f98f8333fc825d2e","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"224453c981b599f6026e0f088a3678f894913874":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["b470f36a9372c97283360b1304eacbde22df6c0d"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ced66195b26fdb1f77ee00e2a77ec6918dedd766":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["224453c981b599f6026e0f088a3678f894913874"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"fe39f1a106531207c028defebbc9eb5bb489ac50":["1182fe36fb5df768dc2da53f6d5338cbc07268ae"],"c2a23476693f2bd9a4b44cc3187c429a2e21dac2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["6613659748fe4411a7dcf85266e55db1f95f7315"],"b470f36a9372c97283360b1304eacbde22df6c0d":["5a207d19eac354d649c3f0e2cce070017c78125e","1e6acbaae7af722f17204ceccf0f7db5753eccf3"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["98d2deb8c96c79ebef084a1f8e5a1a6c08608f13"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["4a8b14bc4241c302311422d5c6f7627f8febb86e","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"1182fe36fb5df768dc2da53f6d5338cbc07268ae":["c2a23476693f2bd9a4b44cc3187c429a2e21dac2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","1e6acbaae7af722f17204ceccf0f7db5753eccf3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}