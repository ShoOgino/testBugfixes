{"path":"solr/contrib/solr-mr/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","commits":[{"id":"d6e604e9030fb0cabf0c5a85ae6039921a81419c","date":1386009743,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/solr-mr/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","pathOld":"solr/contrib/solr-mr/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","sourceNew":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","sourceOld":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"d6e604e9030fb0cabf0c5a85ae6039921a81419c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d6e604e9030fb0cabf0c5a85ae6039921a81419c":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}