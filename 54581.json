{"path":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","commits":[{"id":"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b","date":1395588343,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","pathOld":"/dev/null","sourceNew":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRef[] lastTokens = new BytesRef[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      BytesRef tokenBytes = termBytesAtt.getBytesRef();\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        termBytesAtt.fillBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        lastTokens[gramCount-1] = BytesRef.deepCopyOf(tokenBytes);\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRef token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.grow(token.length+1);\n          token.bytes[token.length] = separator;\n          token.length++;\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRef();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRef token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token, arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length-1;i>=0;i--) {\n          if (token.bytes[token.offset+i] == separator) {\n            BytesRef context = new BytesRef(token.bytes, token.offset, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRef()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes, token.offset + i + 1, token.length - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRef finalLastToken;\n        \n        if (lastTokenFragment == null) {\n          finalLastToken = BytesRef.deepCopyOf(token);\n        } else {\n          finalLastToken = BytesRef.deepCopyOf(lastTokenFragment);\n        }\n        assert finalLastToken.offset == 0;\n        \n        CharsRef spare = new CharsRef();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRef scratchBytes = new BytesRef();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length + scratchBytes.length);\n              int lenSav = finalLastToken.length;\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken) == false;\n              \n              finalLastToken.length = lenSav;\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRef());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length;\n        \n        BytesRef suffix = new BytesRef(8);\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.length = prefixLength;\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token;\n            for(int i=token.length-1;i>=0;i--) {\n              if (token.bytes[token.offset+i] == separator) {\n                assert token.length-i-1 > 0;\n                lastToken = new BytesRef(token.bytes, token.offset+i+1, token.length-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.grow(token.length);\n            UnicodeUtil.UTF8toUTF16(token, spare);\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","sourceNew":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRefBuilder[] lastTokens = new BytesRefBuilder[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      BytesRef tokenBytes = termBytesAtt.getBytesRef();\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        termBytesAtt.fillBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        BytesRefBuilder b = new BytesRefBuilder();\n        b.append(tokenBytes);\n        lastTokens[gramCount-1] = b;\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRefBuilder token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.append(separator);\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRefBuilder();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRefBuilder token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length() == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token.get(), arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length()-1;i>=0;i--) {\n          if (token.byteAt(i) == separator) {\n            BytesRef context = new BytesRef(token.bytes(), 0, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRefBuilder()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes(), i + 1, token.length() - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRefBuilder finalLastToken = new BytesRefBuilder();\n        if (lastTokenFragment == null) {\n          finalLastToken.copyBytes(token.get());\n        } else {\n          finalLastToken.copyBytes(lastTokenFragment);\n        }\n        \n        CharsRefBuilder spare = new CharsRefBuilder();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRefBuilder scratchBytes = new BytesRefBuilder();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length() + scratchBytes.length());\n              int lenSav = finalLastToken.length();\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken.get()) == false;\n              \n              finalLastToken.setLength(lenSav);\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRefBuilder());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length();\n        \n        BytesRefBuilder suffix = new BytesRefBuilder();\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.setLength(prefixLength);\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token.get();\n            for(int i=token.length()-1;i>=0;i--) {\n              if (token.byteAt(i) == separator) {\n                assert token.length()-i-1 > 0;\n                lastToken = new BytesRef(token.bytes(), i+1, token.length()-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.copyUTF8Bytes(token.get());\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","sourceOld":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRef[] lastTokens = new BytesRef[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      BytesRef tokenBytes = termBytesAtt.getBytesRef();\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        termBytesAtt.fillBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        lastTokens[gramCount-1] = BytesRef.deepCopyOf(tokenBytes);\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRef token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.grow(token.length+1);\n          token.bytes[token.length] = separator;\n          token.length++;\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRef();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRef token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token, arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length-1;i>=0;i--) {\n          if (token.bytes[token.offset+i] == separator) {\n            BytesRef context = new BytesRef(token.bytes, token.offset, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRef()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes, token.offset + i + 1, token.length - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRef finalLastToken;\n        \n        if (lastTokenFragment == null) {\n          finalLastToken = BytesRef.deepCopyOf(token);\n        } else {\n          finalLastToken = BytesRef.deepCopyOf(lastTokenFragment);\n        }\n        assert finalLastToken.offset == 0;\n        \n        CharsRef spare = new CharsRef();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRef scratchBytes = new BytesRef();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length + scratchBytes.length);\n              int lenSav = finalLastToken.length;\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken) == false;\n              \n              finalLastToken.length = lenSav;\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRef());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length;\n        \n        BytesRef suffix = new BytesRef(8);\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.length = prefixLength;\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token;\n            for(int i=token.length-1;i>=0;i--) {\n              if (token.bytes[token.offset+i] == separator) {\n                assert token.length-i-1 > 0;\n                lastToken = new BytesRef(token.bytes, token.offset+i+1, token.length-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.grow(token.length);\n            UnicodeUtil.UTF8toUTF16(token, spare);\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","bugFix":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c67436a5d24444dae3dfb0f08bdb28f62672628b","date":1423736313,"type":3,"author":"Jan HÃ¸ydahl","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","sourceNew":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n    if (fst == null) {\n      throw new IllegalStateException(\"Lookup not supported at this time\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRefBuilder[] lastTokens = new BytesRefBuilder[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      BytesRef tokenBytes = termBytesAtt.getBytesRef();\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        termBytesAtt.fillBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        BytesRefBuilder b = new BytesRefBuilder();\n        b.append(tokenBytes);\n        lastTokens[gramCount-1] = b;\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRefBuilder token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.append(separator);\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRefBuilder();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRefBuilder token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length() == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token.get(), arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length()-1;i>=0;i--) {\n          if (token.byteAt(i) == separator) {\n            BytesRef context = new BytesRef(token.bytes(), 0, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRefBuilder()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes(), i + 1, token.length() - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRefBuilder finalLastToken = new BytesRefBuilder();\n        if (lastTokenFragment == null) {\n          finalLastToken.copyBytes(token.get());\n        } else {\n          finalLastToken.copyBytes(lastTokenFragment);\n        }\n        \n        CharsRefBuilder spare = new CharsRefBuilder();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRefBuilder scratchBytes = new BytesRefBuilder();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length() + scratchBytes.length());\n              int lenSav = finalLastToken.length();\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken.get()) == false;\n              \n              finalLastToken.setLength(lenSav);\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRefBuilder());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length();\n        \n        BytesRefBuilder suffix = new BytesRefBuilder();\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.setLength(prefixLength);\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token.get();\n            for(int i=token.length()-1;i>=0;i--) {\n              if (token.byteAt(i) == separator) {\n                assert token.length()-i-1 > 0;\n                lastToken = new BytesRef(token.bytes(), i+1, token.length()-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.copyUTF8Bytes(token.get());\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","sourceOld":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRefBuilder[] lastTokens = new BytesRefBuilder[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      BytesRef tokenBytes = termBytesAtt.getBytesRef();\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        termBytesAtt.fillBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        BytesRefBuilder b = new BytesRefBuilder();\n        b.append(tokenBytes);\n        lastTokens[gramCount-1] = b;\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRefBuilder token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.append(separator);\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRefBuilder();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRefBuilder token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length() == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token.get(), arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length()-1;i>=0;i--) {\n          if (token.byteAt(i) == separator) {\n            BytesRef context = new BytesRef(token.bytes(), 0, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRefBuilder()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes(), i + 1, token.length() - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRefBuilder finalLastToken = new BytesRefBuilder();\n        if (lastTokenFragment == null) {\n          finalLastToken.copyBytes(token.get());\n        } else {\n          finalLastToken.copyBytes(lastTokenFragment);\n        }\n        \n        CharsRefBuilder spare = new CharsRefBuilder();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRefBuilder scratchBytes = new BytesRefBuilder();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length() + scratchBytes.length());\n              int lenSav = finalLastToken.length();\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken.get()) == false;\n              \n              finalLastToken.setLength(lenSav);\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRefBuilder());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length();\n        \n        BytesRefBuilder suffix = new BytesRefBuilder();\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.setLength(prefixLength);\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token.get();\n            for(int i=token.length()-1;i>=0;i--) {\n              if (token.byteAt(i) == separator) {\n                assert token.length()-i-1 > 0;\n                lastToken = new BytesRef(token.bytes(), i+1, token.length()-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.copyUTF8Bytes(token.get());\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"804b857d1066ab5185b3b9101bde41b0b71426ec","date":1435846169,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","sourceNew":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n    if (fst == null) {\n      throw new IllegalStateException(\"Lookup not supported at this time\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRefBuilder[] lastTokens = new BytesRefBuilder[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        BytesRef tokenBytes = termBytesAtt.getBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        BytesRefBuilder b = new BytesRefBuilder();\n        b.append(tokenBytes);\n        lastTokens[gramCount-1] = b;\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRefBuilder token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.append(separator);\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRefBuilder();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRefBuilder token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length() == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token.get(), arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length()-1;i>=0;i--) {\n          if (token.byteAt(i) == separator) {\n            BytesRef context = new BytesRef(token.bytes(), 0, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRefBuilder()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes(), i + 1, token.length() - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRefBuilder finalLastToken = new BytesRefBuilder();\n        if (lastTokenFragment == null) {\n          finalLastToken.copyBytes(token.get());\n        } else {\n          finalLastToken.copyBytes(lastTokenFragment);\n        }\n        \n        CharsRefBuilder spare = new CharsRefBuilder();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRefBuilder scratchBytes = new BytesRefBuilder();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length() + scratchBytes.length());\n              int lenSav = finalLastToken.length();\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken.get()) == false;\n              \n              finalLastToken.setLength(lenSav);\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRefBuilder());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length();\n        \n        BytesRefBuilder suffix = new BytesRefBuilder();\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.setLength(prefixLength);\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token.get();\n            for(int i=token.length()-1;i>=0;i--) {\n              if (token.byteAt(i) == separator) {\n                assert token.length()-i-1 > 0;\n                lastToken = new BytesRef(token.bytes(), i+1, token.length()-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.copyUTF8Bytes(token.get());\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","sourceOld":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n    if (fst == null) {\n      throw new IllegalStateException(\"Lookup not supported at this time\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRefBuilder[] lastTokens = new BytesRefBuilder[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      BytesRef tokenBytes = termBytesAtt.getBytesRef();\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        termBytesAtt.fillBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        BytesRefBuilder b = new BytesRefBuilder();\n        b.append(tokenBytes);\n        lastTokens[gramCount-1] = b;\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRefBuilder token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.append(separator);\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRefBuilder();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRefBuilder token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length() == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token.get(), arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length()-1;i>=0;i--) {\n          if (token.byteAt(i) == separator) {\n            BytesRef context = new BytesRef(token.bytes(), 0, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRefBuilder()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes(), i + 1, token.length() - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRefBuilder finalLastToken = new BytesRefBuilder();\n        if (lastTokenFragment == null) {\n          finalLastToken.copyBytes(token.get());\n        } else {\n          finalLastToken.copyBytes(lastTokenFragment);\n        }\n        \n        CharsRefBuilder spare = new CharsRefBuilder();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRefBuilder scratchBytes = new BytesRefBuilder();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length() + scratchBytes.length());\n              int lenSav = finalLastToken.length();\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken.get()) == false;\n              \n              finalLastToken.setLength(lenSav);\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRefBuilder());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length();\n        \n        BytesRefBuilder suffix = new BytesRefBuilder();\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.setLength(prefixLength);\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token.get();\n            for(int i=token.length()-1;i>=0;i--) {\n              if (token.byteAt(i) == separator) {\n                assert token.length()-i-1 > 0;\n                lastToken = new BytesRef(token.bytes(), i+1, token.length()-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.copyUTF8Bytes(token.get());\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54204c8a3ca26aeafd273139fc29baf70d0f6786","date":1564170395,"type":3,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","sourceNew":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n    if (fst == null) {\n      throw new IllegalStateException(\"Lookup not supported at this time\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRefBuilder[] lastTokens = new BytesRefBuilder[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        BytesRef tokenBytes = termBytesAtt.getBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        BytesRefBuilder b = new BytesRefBuilder();\n        b.append(tokenBytes);\n        lastTokens[gramCount-1] = b;\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRefBuilder token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.append(separator);\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRefBuilder();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRefBuilder token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length() == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token.get(), arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length()-1;i>=0;i--) {\n          if (token.byteAt(i) == separator) {\n            BytesRef context = new BytesRef(token.bytes(), 0, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRefBuilder()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes(), i + 1, token.length() - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRefBuilder finalLastToken = new BytesRefBuilder();\n        if (lastTokenFragment == null) {\n          finalLastToken.copyBytes(token.get());\n        } else {\n          finalLastToken.copyBytes(lastTokenFragment);\n        }\n        \n        CharsRefBuilder spare = new CharsRefBuilder();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRefBuilder scratchBytes = new BytesRefBuilder();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label() != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length() + scratchBytes.length());\n              int lenSav = finalLastToken.length();\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken.get()) == false;\n              \n              finalLastToken.setLength(lenSav);\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRefBuilder());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length();\n        \n        BytesRefBuilder suffix = new BytesRefBuilder();\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.setLength(prefixLength);\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token.get();\n            for(int i=token.length()-1;i>=0;i--) {\n              if (token.byteAt(i) == separator) {\n                assert token.length()-i-1 > 0;\n                lastToken = new BytesRef(token.bytes(), i+1, token.length()-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.copyUTF8Bytes(token.get());\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","sourceOld":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n    if (fst == null) {\n      throw new IllegalStateException(\"Lookup not supported at this time\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRefBuilder[] lastTokens = new BytesRefBuilder[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        BytesRef tokenBytes = termBytesAtt.getBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        BytesRefBuilder b = new BytesRefBuilder();\n        b.append(tokenBytes);\n        lastTokens[gramCount-1] = b;\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRefBuilder token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.append(separator);\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRefBuilder();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRefBuilder token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length() == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token.get(), arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length()-1;i>=0;i--) {\n          if (token.byteAt(i) == separator) {\n            BytesRef context = new BytesRef(token.bytes(), 0, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRefBuilder()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes(), i + 1, token.length() - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRefBuilder finalLastToken = new BytesRefBuilder();\n        if (lastTokenFragment == null) {\n          finalLastToken.copyBytes(token.get());\n        } else {\n          finalLastToken.copyBytes(lastTokenFragment);\n        }\n        \n        CharsRefBuilder spare = new CharsRefBuilder();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRefBuilder scratchBytes = new BytesRefBuilder();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length() + scratchBytes.length());\n              int lenSav = finalLastToken.length();\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken.get()) == false;\n              \n              finalLastToken.setLength(lenSav);\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRefBuilder());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length();\n        \n        BytesRefBuilder suffix = new BytesRefBuilder();\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.setLength(prefixLength);\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token.get();\n            for(int i=token.length()-1;i>=0;i--) {\n              if (token.byteAt(i) == separator) {\n                assert token.length()-i-1 > 0;\n                lastToken = new BytesRef(token.bytes(), i+1, token.length()-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.copyUTF8Bytes(token.get());\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8061ddd97f3352007d927dae445884a6f3d857b","date":1564988276,"type":3,"author":"Atri Sharma","isMerge":true,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester#lookup(CharSequence,Set[BytesRef],int).mjava","sourceNew":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n    if (fst == null) {\n      throw new IllegalStateException(\"Lookup not supported at this time\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRefBuilder[] lastTokens = new BytesRefBuilder[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        BytesRef tokenBytes = termBytesAtt.getBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        BytesRefBuilder b = new BytesRefBuilder();\n        b.append(tokenBytes);\n        lastTokens[gramCount-1] = b;\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRefBuilder token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.append(separator);\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRefBuilder();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRefBuilder token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length() == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token.get(), arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length()-1;i>=0;i--) {\n          if (token.byteAt(i) == separator) {\n            BytesRef context = new BytesRef(token.bytes(), 0, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRefBuilder()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes(), i + 1, token.length() - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRefBuilder finalLastToken = new BytesRefBuilder();\n        if (lastTokenFragment == null) {\n          finalLastToken.copyBytes(token.get());\n        } else {\n          finalLastToken.copyBytes(lastTokenFragment);\n        }\n        \n        CharsRefBuilder spare = new CharsRefBuilder();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRefBuilder scratchBytes = new BytesRefBuilder();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label() != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length() + scratchBytes.length());\n              int lenSav = finalLastToken.length();\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken.get()) == false;\n              \n              finalLastToken.setLength(lenSav);\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRefBuilder());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length();\n        \n        BytesRefBuilder suffix = new BytesRefBuilder();\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.setLength(prefixLength);\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token.get();\n            for(int i=token.length()-1;i>=0;i--) {\n              if (token.byteAt(i) == separator) {\n                assert token.length()-i-1 > 0;\n                lastToken = new BytesRef(token.bytes(), i+1, token.length()-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.copyUTF8Bytes(token.get());\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","sourceOld":"  /** Retrieve suggestions. */\n  public List<LookupResult> lookup(final CharSequence key, Set<BytesRef> contexts, int num) throws IOException {\n    if (contexts != null) {\n      throw new IllegalArgumentException(\"this suggester doesn't support contexts\");\n    }\n    if (fst == null) {\n      throw new IllegalStateException(\"Lookup not supported at this time\");\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", key.toString())) {\n      TermToBytesRefAttribute termBytesAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      PositionLengthAttribute posLenAtt = ts.addAttribute(PositionLengthAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      ts.reset();\n      \n      BytesRefBuilder[] lastTokens = new BytesRefBuilder[grams];\n      //System.out.println(\"lookup: key='\" + key + \"'\");\n      \n      // Run full analysis, but save only the\n      // last 1gram, last 2gram, etc.:\n      int maxEndOffset = -1;\n      boolean sawRealToken = false;\n      while(ts.incrementToken()) {\n        BytesRef tokenBytes = termBytesAtt.getBytesRef();\n        sawRealToken |= tokenBytes.length > 0;\n        // TODO: this is somewhat iffy; today, ShingleFilter\n        // sets posLen to the gram count; maybe we should make\n        // a separate dedicated att for this?\n        int gramCount = posLenAtt.getPositionLength();\n        \n        assert gramCount <= grams;\n        \n        // Safety: make sure the recalculated count \"agrees\":\n        if (countGrams(tokenBytes) != gramCount) {\n          throw new IllegalArgumentException(\"tokens must not contain separator byte; got token=\" + tokenBytes + \" but gramCount=\" + gramCount + \" does not match recalculated count=\" + countGrams(tokenBytes));\n        }\n        maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        BytesRefBuilder b = new BytesRefBuilder();\n        b.append(tokenBytes);\n        lastTokens[gramCount-1] = b;\n      }\n      ts.end();\n      \n      if (!sawRealToken) {\n        throw new IllegalArgumentException(\"no tokens produced by analyzer, or the only tokens were empty strings\");\n      }\n      \n      // Carefully fill last tokens with _ tokens;\n      // ShingleFilter appraently won't emit \"only hole\"\n      // tokens:\n      int endPosInc = posIncAtt.getPositionIncrement();\n      \n      // Note this will also be true if input is the empty\n      // string (in which case we saw no tokens and\n      // maxEndOffset is still -1), which in fact works out OK\n      // because we fill the unigram with an empty BytesRef\n      // below:\n      boolean lastTokenEnded = offsetAtt.endOffset() > maxEndOffset || endPosInc > 0;\n      //System.out.println(\"maxEndOffset=\" + maxEndOffset + \" vs \" + offsetAtt.endOffset());\n      \n      if (lastTokenEnded) {\n        //System.out.println(\"  lastTokenEnded\");\n        // If user hit space after the last token, then\n        // \"upgrade\" all tokens.  This way \"foo \" will suggest\n        // all bigrams starting w/ foo, and not any unigrams\n        // starting with \"foo\":\n        for(int i=grams-1;i>0;i--) {\n          BytesRefBuilder token = lastTokens[i-1];\n          if (token == null) {\n            continue;\n          }\n          token.append(separator);\n          lastTokens[i] = token;\n        }\n        lastTokens[0] = new BytesRefBuilder();\n      }\n      \n      Arc<Long> arc = new Arc<>();\n      \n      BytesReader bytesReader = fst.getBytesReader();\n      \n      // Try highest order models first, and if they return\n      // results, return that; else, fallback:\n      double backoff = 1.0;\n      \n      List<LookupResult> results = new ArrayList<>(num);\n      \n      // We only add a given suffix once, from the highest\n      // order model that saw it; for subsequent lower order\n      // models we skip it:\n      final Set<BytesRef> seen = new HashSet<>();\n      \n      for(int gram=grams-1;gram>=0;gram--) {\n        BytesRefBuilder token = lastTokens[gram];\n        // Don't make unigram predictions from empty string:\n        if (token == null || (token.length() == 0 && key.length() > 0)) {\n          // Input didn't have enough tokens:\n          //System.out.println(\"  gram=\" + gram + \": skip: not enough input\");\n          continue;\n        }\n        \n        if (endPosInc > 0 && gram <= endPosInc) {\n          // Skip hole-only predictions; in theory we\n          // shouldn't have to do this, but we'd need to fix\n          // ShingleFilter to produce only-hole tokens:\n          //System.out.println(\"  break: only holes now\");\n          break;\n        }\n        \n        //System.out.println(\"try \" + (gram+1) + \" gram token=\" + token.utf8ToString());\n        \n        // TODO: we could add fuzziness here\n        // match the prefix portion exactly\n        //Pair<Long,BytesRef> prefixOutput = null;\n        Long prefixOutput = null;\n        try {\n          prefixOutput = lookupPrefix(fst, bytesReader, token.get(), arc);\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        //System.out.println(\"  prefixOutput=\" + prefixOutput);\n        \n        if (prefixOutput == null) {\n          // This model never saw this prefix, e.g. the\n          // trigram model never saw context \"purple mushroom\"\n          backoff *= ALPHA;\n          continue;\n        }\n        \n        // TODO: we could do this division at build time, and\n        // bake it into the FST?\n        \n        // Denominator for computing scores from current\n        // model's predictions:\n        long contextCount = totTokens;\n        \n        BytesRef lastTokenFragment = null;\n        \n        for(int i=token.length()-1;i>=0;i--) {\n          if (token.byteAt(i) == separator) {\n            BytesRef context = new BytesRef(token.bytes(), 0, i);\n            Long output = Util.get(fst, Util.toIntsRef(context, new IntsRefBuilder()));\n            assert output != null;\n            contextCount = decodeWeight(output);\n            lastTokenFragment = new BytesRef(token.bytes(), i + 1, token.length() - i - 1);\n            break;\n          }\n        }\n        \n        final BytesRefBuilder finalLastToken = new BytesRefBuilder();\n        if (lastTokenFragment == null) {\n          finalLastToken.copyBytes(token.get());\n        } else {\n          finalLastToken.copyBytes(lastTokenFragment);\n        }\n        \n        CharsRefBuilder spare = new CharsRefBuilder();\n        \n        // complete top-N\n        TopResults<Long> completions = null;\n        try {\n          \n          // Because we store multiple models in one FST\n          // (1gram, 2gram, 3gram), we must restrict the\n          // search so that it only considers the current\n          // model.  For highest order model, this is not\n          // necessary since all completions in the FST\n          // must be from this model, but for lower order\n          // models we have to filter out the higher order\n          // ones:\n          \n          // Must do num+seen.size() for queue depth because we may\n          // reject up to seen.size() paths in acceptResult():\n          Util.TopNSearcher<Long> searcher = new Util.TopNSearcher<Long>(fst, num, num+seen.size(), weightComparator) {\n            \n            BytesRefBuilder scratchBytes = new BytesRefBuilder();\n            \n            @Override\n            protected void addIfCompetitive(Util.FSTPath<Long> path) {\n              if (path.arc.label != separator) {\n                //System.out.println(\"    keep path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n                super.addIfCompetitive(path);\n              } else {\n                //System.out.println(\"    prevent path: \" + Util.toBytesRef(path.input, new BytesRef()).utf8ToString() + \"; \" + path + \"; arc=\" + path.arc);\n              }\n            }\n            \n            @Override\n            protected boolean acceptResult(IntsRef input, Long output) {\n              Util.toBytesRef(input, scratchBytes);\n              finalLastToken.grow(finalLastToken.length() + scratchBytes.length());\n              int lenSav = finalLastToken.length();\n              finalLastToken.append(scratchBytes);\n              //System.out.println(\"    accept? input='\" + scratchBytes.utf8ToString() + \"'; lastToken='\" + finalLastToken.utf8ToString() + \"'; return \" + (seen.contains(finalLastToken) == false));\n              boolean ret = seen.contains(finalLastToken.get()) == false;\n              \n              finalLastToken.setLength(lenSav);\n              return ret;\n            }\n          };\n          \n          // since this search is initialized with a single start node \n          // it is okay to start with an empty input path here\n          searcher.addStartPaths(arc, prefixOutput, true, new IntsRefBuilder());\n          \n          completions = searcher.search();\n          assert completions.isComplete;\n        } catch (IOException bogus) {\n          throw new RuntimeException(bogus);\n        }\n        \n        int prefixLength = token.length();\n        \n        BytesRefBuilder suffix = new BytesRefBuilder();\n        //System.out.println(\"    \" + completions.length + \" completions\");\n        \n        nextCompletion:\n          for (Result<Long> completion : completions) {\n            token.setLength(prefixLength);\n            // append suffix\n            Util.toBytesRef(completion.input, suffix);\n            token.append(suffix);\n            \n            //System.out.println(\"    completion \" + token.utf8ToString());\n            \n            // Skip this path if a higher-order model already\n            // saw/predicted its last token:\n            BytesRef lastToken = token.get();\n            for(int i=token.length()-1;i>=0;i--) {\n              if (token.byteAt(i) == separator) {\n                assert token.length()-i-1 > 0;\n                lastToken = new BytesRef(token.bytes(), i+1, token.length()-i-1);\n                break;\n              }\n            }\n            if (seen.contains(lastToken)) {\n              //System.out.println(\"      skip dup \" + lastToken.utf8ToString());\n              continue nextCompletion;\n            }\n            seen.add(BytesRef.deepCopyOf(lastToken));\n            spare.copyUTF8Bytes(token.get());\n            LookupResult result = new LookupResult(spare.toString(), (long) (Long.MAX_VALUE * backoff * ((double) decodeWeight(completion.output)) / contextCount));\n            results.add(result);\n            assert results.size() == seen.size();\n            //System.out.println(\"  add result=\" + result);\n          }\n        backoff *= ALPHA;\n      }\n      \n      Collections.sort(results, new Comparator<LookupResult>() {\n        @Override\n        public int compare(LookupResult a, LookupResult b) {\n          if (a.value > b.value) {\n            return -1;\n          } else if (a.value < b.value) {\n            return 1;\n          } else {\n            // Tie break by UTF16 sort order:\n            return ((String) a.key).compareTo((String) b.key);\n          }\n        }\n      });\n      \n      if (results.size() > num) {\n        results.subList(num, results.size()).clear();\n      }\n      \n      return results;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54204c8a3ca26aeafd273139fc29baf70d0f6786":["804b857d1066ab5185b3b9101bde41b0b71426ec"],"804b857d1066ab5185b3b9101bde41b0b71426ec":["c67436a5d24444dae3dfb0f08bdb28f62672628b"],"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c67436a5d24444dae3dfb0f08bdb28f62672628b":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"f8061ddd97f3352007d927dae445884a6f3d857b":["804b857d1066ab5185b3b9101bde41b0b71426ec","54204c8a3ca26aeafd273139fc29baf70d0f6786"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["54204c8a3ca26aeafd273139fc29baf70d0f6786"]},"commit2Childs":{"54204c8a3ca26aeafd273139fc29baf70d0f6786":["f8061ddd97f3352007d927dae445884a6f3d857b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"804b857d1066ab5185b3b9101bde41b0b71426ec":["54204c8a3ca26aeafd273139fc29baf70d0f6786","f8061ddd97f3352007d927dae445884a6f3d857b"],"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"c67436a5d24444dae3dfb0f08bdb28f62672628b":["804b857d1066ab5185b3b9101bde41b0b71426ec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["c67436a5d24444dae3dfb0f08bdb28f62672628b"],"f8061ddd97f3352007d927dae445884a6f3d857b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f8061ddd97f3352007d927dae445884a6f3d857b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}