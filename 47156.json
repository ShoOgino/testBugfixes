{"path":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new Field(\"noTV\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new Field(\"noTV\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new Field(\"noTV\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new Field(\"noTV\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new Field(\"noTV\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","bugFix":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f21ce13f410ee015e1ba14687ab4b8518ac52a11","date":1359713213,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0fa6955ed1b1007ded1349ab72cea4555640432f","date":1359721908,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n    searcher = newSearcher(reader);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"088a7ef694fd43d5d9a4d200c4005865f773d1e7","date":1371136274,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setNoCFSRatio(1.0);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setUseCompoundFile(true);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setNoCFSRatio(1.0);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.shutdown();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setNoCFSRatio(1.0);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setNoCFSRatio(1.0);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.shutdown();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setNoCFSRatio(1.0);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setNoCFSRatio(1.0);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setNoCFSRatio(1.0);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","date":1420599177,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermVectors#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setNoCFSRatio(1.0);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {                  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy()));\n    //writer.setNoCFSRatio(1.0);\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 1000; i++) {\n      Document doc = new Document();\n      FieldType ft = new FieldType(TextField.TYPE_STORED);\n      int mod3 = i % 3;\n      int mod2 = i % 2;\n      if (mod2 == 0 && mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod2 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorPositions(true);\n      } else if (mod3 == 0) {\n        ft.setStoreTermVectors(true);\n        ft.setStoreTermVectorOffsets(true);\n      } else {\n        ft.setStoreTermVectors(true);\n      }\n      doc.add(new Field(\"field\", English.intToEnglish(i), ft));\n      //test no term vectors too\n      doc.add(new TextField(\"noTV\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"04f07771a2a7dd3a395700665ed839c3dae2def2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"0fa6955ed1b1007ded1349ab72cea4555640432f":["04f07771a2a7dd3a395700665ed839c3dae2def2","f21ce13f410ee015e1ba14687ab4b8518ac52a11"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["f21ce13f410ee015e1ba14687ab4b8518ac52a11"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"f21ce13f410ee015e1ba14687ab4b8518ac52a11":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"]},"commit2Childs":{"04f07771a2a7dd3a395700665ed839c3dae2def2":["0fa6955ed1b1007ded1349ab72cea4555640432f","f21ce13f410ee015e1ba14687ab4b8518ac52a11"],"0fa6955ed1b1007ded1349ab72cea4555640432f":[],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"f21ce13f410ee015e1ba14687ab4b8518ac52a11":["0fa6955ed1b1007ded1349ab72cea4555640432f","088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["0fa6955ed1b1007ded1349ab72cea4555640432f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}