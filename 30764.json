{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newField(\"f\", \"doctor who\", TextField.TYPE_UNSTORED));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newField(\"f\", \"doctor who\", TextField.TYPE_UNSTORED));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newField(\"f\", \"doctor who\", TextField.TYPE_UNSTORED));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newField(\"f\", \"doctor who\", TextField.TYPE_UNSTORED));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newField(\"f\", \"doctor who\", TextField.TYPE_UNSTORED));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d19974432be9aed28ee7dca73bdf01d139e763a9","date":1342822166,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5","date":1376515204,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    _TestUtil.keepFullyDeletedSegments(w);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    _TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.shutdown();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.shutdown();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(true).\n            setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.shutdown();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.shutdown();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"19e497fe4da591a79332da97681b8017d9c61165","date":1409030374,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9e22bdf0692bfa61e342b04a6ac7078670c1e16","date":1436866730,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    try {\n      w.addDocument(doc);\n      fail(\"writer was not closed by merge exception\");\n    } catch (AlreadyClosedException ace) {\n      // expected\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    w.addDocument(doc);\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["05fe562aa248790944d43cdd478f512572835ba0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05fe562aa248790944d43cdd478f512572835ba0","date":1455901667,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    expectThrows(IOException.class, () -> {\n      w.commit();\n    });\n    assertTrue(ftdm.didFail1 || ftdm.didFail2);\n\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    expectThrows(AlreadyClosedException.class, () -> {\n      w.addDocument(doc);\n    });\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    try {\n      w.commit();\n      fail(\"fake disk full IOExceptions not hit\");\n    } catch (IOException ioe) {\n      // expected\n      assertTrue(ftdm.didFail1 || ftdm.didFail2);\n    }\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    try {\n      w.addDocument(doc);\n      fail(\"writer was not closed by merge exception\");\n    } catch (AlreadyClosedException ace) {\n      // expected\n    }\n\n    dir.close();\n  }\n\n","bugFix":["1085ea837da8f1e96697e17cf73e1d08e7329261","d9e22bdf0692bfa61e342b04a6ac7078670c1e16","c00afe74a80796ed1f30a9509b150ff104746a1f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5ee0394b8176abd7c90a4be8c05465be1879db79","date":1522842314,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(new MergePolicyWrapper(newLogMergePolicy(2)) {\n            @Override\n            public boolean keepFullyDeletedSegment(CodecReader reader) throws IOException {\n              // we can do this because we add/delete/add (and dont merge to \"nothing\")\n              return true;\n            }\n          })\n    );\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    expectThrows(IOException.class, () -> {\n      w.commit();\n    });\n    assertTrue(ftdm.didFail1 || ftdm.didFail2);\n\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    expectThrows(AlreadyClosedException.class, () -> {\n      w.addDocument(doc);\n    });\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(newLogMergePolicy(2))\n    );\n    // we can do this because we add/delete/add (and dont merge to \"nothing\")\n    w.setKeepFullyDeletedSegments(true);\n\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    expectThrows(IOException.class, () -> {\n      w.commit();\n    });\n    assertTrue(ftdm.didFail1 || ftdm.didFail2);\n\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    expectThrows(AlreadyClosedException.class, () -> {\n      w.addDocument(doc);\n    });\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["15e716649e2bd79a98b5e68c464154ea4c44677a"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6bca5135c8cc6e6cd90c5f6f451ff7445dd4f7d8","date":1523648719,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(new FilterMergePolicy(newLogMergePolicy(2)) {\n            @Override\n            public boolean keepFullyDeletedSegment(CodecReader reader) throws IOException {\n              // we can do this because we add/delete/add (and dont merge to \"nothing\")\n              return true;\n            }\n          })\n    );\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    expectThrows(IOException.class, () -> {\n      w.commit();\n    });\n    assertTrue(ftdm.didFail1 || ftdm.didFail2);\n\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    expectThrows(AlreadyClosedException.class, () -> {\n      w.addDocument(doc);\n    });\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(new MergePolicyWrapper(newLogMergePolicy(2)) {\n            @Override\n            public boolean keepFullyDeletedSegment(CodecReader reader) throws IOException {\n              // we can do this because we add/delete/add (and dont merge to \"nothing\")\n              return true;\n            }\n          })\n    );\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    expectThrows(IOException.class, () -> {\n      w.commit();\n    });\n    assertTrue(ftdm.didFail1 || ftdm.didFail2);\n\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    expectThrows(AlreadyClosedException.class, () -> {\n      w.addDocument(doc);\n    });\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"15e716649e2bd79a98b5e68c464154ea4c44677a","date":1523975212,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testCorruptionAfterDiskFullDuringMerge().mjava","sourceNew":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(new FilterMergePolicy(newLogMergePolicy(2)) {\n            @Override\n            public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) throws IOException {\n              // we can do this because we add/delete/add (and dont merge to \"nothing\")\n              return true;\n            }\n          })\n    );\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    expectThrows(IOException.class, () -> {\n      w.commit();\n    });\n    assertTrue(ftdm.didFail1 || ftdm.didFail2);\n\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    expectThrows(AlreadyClosedException.class, () -> {\n      w.addDocument(doc);\n    });\n\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-2593\n  public void testCorruptionAfterDiskFullDuringMerge() throws IOException {\n    MockDirectoryWrapper dir = newMockDirectory();\n    //IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)).setReaderPooling(true));\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(new MockAnalyzer(random()))\n          .setMergeScheduler(new SerialMergeScheduler())\n          .setReaderPooling(true)\n          .setMergePolicy(new FilterMergePolicy(newLogMergePolicy(2)) {\n            @Override\n            public boolean keepFullyDeletedSegment(CodecReader reader) throws IOException {\n              // we can do this because we add/delete/add (and dont merge to \"nothing\")\n              return true;\n            }\n          })\n    );\n    Document doc = new Document();\n\n    doc.add(newTextField(\"f\", \"doctor who\", Field.Store.NO));\n    w.addDocument(doc);\n    w.commit();\n\n    w.deleteDocuments(new Term(\"f\", \"who\"));\n    w.addDocument(doc);\n    \n    // disk fills up!\n    FailTwiceDuringMerge ftdm = new FailTwiceDuringMerge();\n    ftdm.setDoFail();\n    dir.failOn(ftdm);\n\n    expectThrows(IOException.class, () -> {\n      w.commit();\n    });\n    assertTrue(ftdm.didFail1 || ftdm.didFail2);\n\n    TestUtil.checkIndex(dir);\n    ftdm.clearDoFail();\n    expectThrows(AlreadyClosedException.class, () -> {\n      w.addDocument(doc);\n    });\n\n    dir.close();\n  }\n\n","bugFix":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6bca5135c8cc6e6cd90c5f6f451ff7445dd4f7d8":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["d19974432be9aed28ee7dca73bdf01d139e763a9","da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5"],"5ee0394b8176abd7c90a4be8c05465be1879db79":["05fe562aa248790944d43cdd478f512572835ba0"],"05fe562aa248790944d43cdd478f512572835ba0":["d9e22bdf0692bfa61e342b04a6ac7078670c1e16"],"19e497fe4da591a79332da97681b8017d9c61165":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"d9e22bdf0692bfa61e342b04a6ac7078670c1e16":["19e497fe4da591a79332da97681b8017d9c61165"],"aba371508186796cc6151d8223a5b4e16d02e26e":["04f07771a2a7dd3a395700665ed839c3dae2def2","d19974432be9aed28ee7dca73bdf01d139e763a9"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"15e716649e2bd79a98b5e68c464154ea4c44677a":["6bca5135c8cc6e6cd90c5f6f451ff7445dd4f7d8"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["04f07771a2a7dd3a395700665ed839c3dae2def2","d19974432be9aed28ee7dca73bdf01d139e763a9"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["15e716649e2bd79a98b5e68c464154ea4c44677a"]},"commit2Childs":{"6bca5135c8cc6e6cd90c5f6f451ff7445dd4f7d8":["15e716649e2bd79a98b5e68c464154ea4c44677a"],"da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5":["6613659748fe4411a7dcf85266e55db1f95f7315","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"5ee0394b8176abd7c90a4be8c05465be1879db79":["6bca5135c8cc6e6cd90c5f6f451ff7445dd4f7d8"],"05fe562aa248790944d43cdd478f512572835ba0":["5ee0394b8176abd7c90a4be8c05465be1879db79"],"19e497fe4da591a79332da97681b8017d9c61165":["d9e22bdf0692bfa61e342b04a6ac7078670c1e16"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["aba371508186796cc6151d8223a5b4e16d02e26e","d19974432be9aed28ee7dca73bdf01d139e763a9","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"d9e22bdf0692bfa61e342b04a6ac7078670c1e16":["05fe562aa248790944d43cdd478f512572835ba0"],"aba371508186796cc6151d8223a5b4e16d02e26e":[],"d19974432be9aed28ee7dca73bdf01d139e763a9":["da6a6dcc9fdd946001f6161fb9c16afdb5ed84c5","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["19e497fe4da591a79332da97681b8017d9c61165"],"15e716649e2bd79a98b5e68c464154ea4c44677a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}