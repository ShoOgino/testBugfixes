{"path":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","commits":[{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"/dev/null","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName);\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n\n    boolean success = false;\n\n    try {\n      int totDocCount = 0;\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        if (infoStream != null)\n          totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      if (infoStream != null)\n        assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n      if (!success) {\n        if (infoStream != null)\n          message(\"hit exception during merge; now refresh deleter on segment \" + mergedName);\n        synchronized(this) {\n          addMergeException(merge);\n          deleter.refresh(mergedName);\n        }\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n      \n      success = false;\n      boolean skip = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        try {\n          merger.createCompoundFile(compoundFileName);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (segmentInfos.indexOf(merge.info) == -1) {\n              // If another merge kicked in and merged our\n              // new segment away while we were trying to\n              // build the compound file, we can hit a\n              // FileNotFoundException and possibly\n              // IOException over NFS.  We can tell this has\n              // happened because our SegmentInfo is no\n              // longer in the segments; if this has\n              // happened it is safe to ignore the exception\n              // & skip finishing/committing our compound\n              // file creating.\n              if (infoStream != null)\n                message(\"hit exception creating compound file; ignoring it because our info (segment \" + merge.info.name + \") has been merged away\");\n              skip = true;\n            } else\n              throw ioe;\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge: skip=\" + skip);\n\n          synchronized(this) {\n            if (!skip)\n              addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (!skip) {\n\n        synchronized(this) {\n          if (skip || segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n            // Our segment (committed in non-compound\n            // format) got merged away while we were\n            // building the compound format.\n            deleter.deleteFile(compoundFileName);\n          } else {\n            success = false;\n            try {\n              merge.info.setUseCompoundFile(true);\n              checkpoint();\n              success = true;\n            } finally {\n              if (!success) {  \n                if (infoStream != null)\n                  message(\"hit exception checkpointing compound file during merge\");\n\n                // Must rollback:\n                addMergeException(merge);\n                merge.info.setUseCompoundFile(false);\n                deletePartialSegmentsFile();\n                deleter.deleteFile(compoundFileName);\n              }\n            }\n      \n            // Give deleter a chance to remove files now.\n            deleter.checkpoint(segmentInfos, autoCommit);\n          }\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["6a7626d7c0746aa25a85a8c7a27742a411e9b9fd","346d5897e4c4e77ed5dbd31f7730ff30973d5971","4d3e8520fd031bab31fd0e4d480e55958bc45efe","8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"52e9359ec09a0ce311e6ce95805998bebc7f7fd0","date":1196417385,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName);\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n\n    boolean success = false;\n\n    try {\n      int totDocCount = 0;\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n      if (!success) {\n        if (infoStream != null)\n          message(\"hit exception during merge; now refresh deleter on segment \" + mergedName);\n        synchronized(this) {\n          addMergeException(merge);\n          deleter.refresh(mergedName);\n        }\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n      \n      success = false;\n      boolean skip = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        try {\n          merger.createCompoundFile(compoundFileName);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (segmentInfos.indexOf(merge.info) == -1) {\n              // If another merge kicked in and merged our\n              // new segment away while we were trying to\n              // build the compound file, we can hit a\n              // FileNotFoundException and possibly\n              // IOException over NFS.  We can tell this has\n              // happened because our SegmentInfo is no\n              // longer in the segments; if this has\n              // happened it is safe to ignore the exception\n              // & skip finishing/committing our compound\n              // file creating.\n              if (infoStream != null)\n                message(\"hit exception creating compound file; ignoring it because our info (segment \" + merge.info.name + \") has been merged away\");\n              skip = true;\n            } else\n              throw ioe;\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge: skip=\" + skip);\n\n          synchronized(this) {\n            if (!skip)\n              addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (!skip) {\n\n        synchronized(this) {\n          if (skip || segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n            // Our segment (committed in non-compound\n            // format) got merged away while we were\n            // building the compound format.\n            deleter.deleteFile(compoundFileName);\n          } else {\n            success = false;\n            try {\n              merge.info.setUseCompoundFile(true);\n              checkpoint();\n              success = true;\n            } finally {\n              if (!success) {  \n                if (infoStream != null)\n                  message(\"hit exception checkpointing compound file during merge\");\n\n                // Must rollback:\n                addMergeException(merge);\n                merge.info.setUseCompoundFile(false);\n                deletePartialSegmentsFile();\n                deleter.deleteFile(compoundFileName);\n              }\n            }\n      \n            // Give deleter a chance to remove files now.\n            deleter.checkpoint(segmentInfos, autoCommit);\n          }\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName);\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n\n    boolean success = false;\n\n    try {\n      int totDocCount = 0;\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        if (infoStream != null)\n          totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      if (infoStream != null)\n        assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n      if (!success) {\n        if (infoStream != null)\n          message(\"hit exception during merge; now refresh deleter on segment \" + mergedName);\n        synchronized(this) {\n          addMergeException(merge);\n          deleter.refresh(mergedName);\n        }\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n      \n      success = false;\n      boolean skip = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        try {\n          merger.createCompoundFile(compoundFileName);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (segmentInfos.indexOf(merge.info) == -1) {\n              // If another merge kicked in and merged our\n              // new segment away while we were trying to\n              // build the compound file, we can hit a\n              // FileNotFoundException and possibly\n              // IOException over NFS.  We can tell this has\n              // happened because our SegmentInfo is no\n              // longer in the segments; if this has\n              // happened it is safe to ignore the exception\n              // & skip finishing/committing our compound\n              // file creating.\n              if (infoStream != null)\n                message(\"hit exception creating compound file; ignoring it because our info (segment \" + merge.info.name + \") has been merged away\");\n              skip = true;\n            } else\n              throw ioe;\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge: skip=\" + skip);\n\n          synchronized(this) {\n            if (!skip)\n              addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (!skip) {\n\n        synchronized(this) {\n          if (skip || segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n            // Our segment (committed in non-compound\n            // format) got merged away while we were\n            // building the compound format.\n            deleter.deleteFile(compoundFileName);\n          } else {\n            success = false;\n            try {\n              merge.info.setUseCompoundFile(true);\n              checkpoint();\n              success = true;\n            } finally {\n              if (!success) {  \n                if (infoStream != null)\n                  message(\"hit exception checkpointing compound file during merge\");\n\n                // Must rollback:\n                addMergeException(merge);\n                merge.info.setUseCompoundFile(false);\n                deletePartialSegmentsFile();\n                deleter.deleteFile(compoundFileName);\n              }\n            }\n      \n            // Give deleter a chance to remove files now.\n            deleter.checkpoint(segmentInfos, autoCommit);\n          }\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c","date":1196806748,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName);\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n\n    boolean success = false;\n\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      if (merge.isAborted())\n        throw new IOException(\"merge is aborted\");\n\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n      if (!success) {\n        if (infoStream != null)\n          message(\"hit exception during merge; now refresh deleter on segment \" + mergedName);\n        synchronized(this) {\n          addMergeException(merge);\n          deleter.refresh(mergedName);\n        }\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n      \n      success = false;\n      boolean skip = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        try {\n          merger.createCompoundFile(compoundFileName);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (segmentInfos.indexOf(merge.info) == -1) {\n              // If another merge kicked in and merged our\n              // new segment away while we were trying to\n              // build the compound file, we can hit a\n              // FileNotFoundException and possibly\n              // IOException over NFS.  We can tell this has\n              // happened because our SegmentInfo is no\n              // longer in the segments; if this has\n              // happened it is safe to ignore the exception\n              // & skip finishing/committing our compound\n              // file creating.\n              if (infoStream != null)\n                message(\"hit exception creating compound file; ignoring it because our info (segment \" + merge.info.name + \") has been merged away\");\n              skip = true;\n            } else\n              throw ioe;\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge: skip=\" + skip);\n\n          synchronized(this) {\n            if (!skip)\n              addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (!skip) {\n\n        synchronized(this) {\n          if (skip || segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n            // Our segment (committed in non-compound\n            // format) got merged away while we were\n            // building the compound format.\n            deleter.deleteFile(compoundFileName);\n          } else {\n            success = false;\n            try {\n              merge.info.setUseCompoundFile(true);\n              checkpoint();\n              success = true;\n            } finally {\n              if (!success) {  \n                if (infoStream != null)\n                  message(\"hit exception checkpointing compound file during merge\");\n\n                // Must rollback:\n                addMergeException(merge);\n                merge.info.setUseCompoundFile(false);\n                deletePartialSegmentsFile();\n                deleter.deleteFile(compoundFileName);\n              }\n            }\n      \n            // Give deleter a chance to remove files now.\n            deleter.checkpoint(segmentInfos, autoCommit);\n          }\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName);\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n\n    boolean success = false;\n\n    try {\n      int totDocCount = 0;\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n      if (!success) {\n        if (infoStream != null)\n          message(\"hit exception during merge; now refresh deleter on segment \" + mergedName);\n        synchronized(this) {\n          addMergeException(merge);\n          deleter.refresh(mergedName);\n        }\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n      \n      success = false;\n      boolean skip = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        try {\n          merger.createCompoundFile(compoundFileName);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (segmentInfos.indexOf(merge.info) == -1) {\n              // If another merge kicked in and merged our\n              // new segment away while we were trying to\n              // build the compound file, we can hit a\n              // FileNotFoundException and possibly\n              // IOException over NFS.  We can tell this has\n              // happened because our SegmentInfo is no\n              // longer in the segments; if this has\n              // happened it is safe to ignore the exception\n              // & skip finishing/committing our compound\n              // file creating.\n              if (infoStream != null)\n                message(\"hit exception creating compound file; ignoring it because our info (segment \" + merge.info.name + \") has been merged away\");\n              skip = true;\n            } else\n              throw ioe;\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge: skip=\" + skip);\n\n          synchronized(this) {\n            if (!skip)\n              addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (!skip) {\n\n        synchronized(this) {\n          if (skip || segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n            // Our segment (committed in non-compound\n            // format) got merged away while we were\n            // building the compound format.\n            deleter.deleteFile(compoundFileName);\n          } else {\n            success = false;\n            try {\n              merge.info.setUseCompoundFile(true);\n              checkpoint();\n              success = true;\n            } finally {\n              if (!success) {  \n                if (infoStream != null)\n                  message(\"hit exception checkpointing compound file during merge\");\n\n                // Must rollback:\n                addMergeException(merge);\n                merge.info.setUseCompoundFile(false);\n                deletePartialSegmentsFile();\n                deleter.deleteFile(compoundFileName);\n              }\n            }\n      \n            // Give deleter a chance to remove files now.\n            deleter.checkpoint(segmentInfos, autoCommit);\n          }\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"346d5897e4c4e77ed5dbd31f7730ff30973d5971","date":1198317988,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n\n    boolean success = false;\n\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n      if (!success) {\n        if (infoStream != null)\n          message(\"hit exception during merge; now refresh deleter on segment \" + mergedName);\n        synchronized(this) {\n          addMergeException(merge);\n          deleter.refresh(mergedName);\n        }\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n      \n      success = false;\n      boolean skip = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        try {\n          merger.createCompoundFile(compoundFileName);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (segmentInfos.indexOf(merge.info) == -1) {\n              // If another merge kicked in and merged our\n              // new segment away while we were trying to\n              // build the compound file, we can hit a\n              // FileNotFoundException and possibly\n              // IOException over NFS.  We can tell this has\n              // happened because our SegmentInfo is no\n              // longer in the segments; if this has\n              // happened it is safe to ignore the exception\n              // & skip finishing/committing our compound\n              // file creating.\n              if (infoStream != null)\n                message(\"hit exception creating compound file; ignoring it because our info (segment \" + merge.info.name + \") has been merged away\");\n              skip = true;\n            } else\n              throw ioe;\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge: skip=\" + skip);\n\n          synchronized(this) {\n            if (!skip)\n              addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (!skip) {\n\n        synchronized(this) {\n          if (skip || segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n            // Our segment (committed in non-compound\n            // format) got merged away while we were\n            // building the compound format.\n            deleter.deleteFile(compoundFileName);\n          } else {\n            success = false;\n            try {\n              merge.info.setUseCompoundFile(true);\n              checkpoint();\n              success = true;\n            } finally {\n              if (!success) {  \n                if (infoStream != null)\n                  message(\"hit exception checkpointing compound file during merge\");\n\n                // Must rollback:\n                addMergeException(merge);\n                merge.info.setUseCompoundFile(false);\n                deletePartialSegmentsFile();\n                deleter.deleteFile(compoundFileName);\n              }\n            }\n      \n            // Give deleter a chance to remove files now.\n            deleter.checkpoint(segmentInfos, autoCommit);\n          }\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName);\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n\n    boolean success = false;\n\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      if (merge.isAborted())\n        throw new IOException(\"merge is aborted\");\n\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n      if (!success) {\n        if (infoStream != null)\n          message(\"hit exception during merge; now refresh deleter on segment \" + mergedName);\n        synchronized(this) {\n          addMergeException(merge);\n          deleter.refresh(mergedName);\n        }\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n      \n      success = false;\n      boolean skip = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        try {\n          merger.createCompoundFile(compoundFileName);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (segmentInfos.indexOf(merge.info) == -1) {\n              // If another merge kicked in and merged our\n              // new segment away while we were trying to\n              // build the compound file, we can hit a\n              // FileNotFoundException and possibly\n              // IOException over NFS.  We can tell this has\n              // happened because our SegmentInfo is no\n              // longer in the segments; if this has\n              // happened it is safe to ignore the exception\n              // & skip finishing/committing our compound\n              // file creating.\n              if (infoStream != null)\n                message(\"hit exception creating compound file; ignoring it because our info (segment \" + merge.info.name + \") has been merged away\");\n              skip = true;\n            } else\n              throw ioe;\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge: skip=\" + skip);\n\n          synchronized(this) {\n            if (!skip)\n              addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (!skip) {\n\n        synchronized(this) {\n          if (skip || segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n            // Our segment (committed in non-compound\n            // format) got merged away while we were\n            // building the compound format.\n            deleter.deleteFile(compoundFileName);\n          } else {\n            success = false;\n            try {\n              merge.info.setUseCompoundFile(true);\n              checkpoint();\n              success = true;\n            } finally {\n              if (!success) {  \n                if (infoStream != null)\n                  message(\"hit exception checkpointing compound file during merge\");\n\n                // Must rollback:\n                addMergeException(merge);\n                merge.info.setUseCompoundFile(false);\n                deletePartialSegmentsFile();\n                deleter.deleteFile(compoundFileName);\n              }\n            }\n      \n            // Give deleter a chance to remove files now.\n            deleter.checkpoint(segmentInfos, autoCommit);\n          }\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":["d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c","b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e82780afe6097066eb5befb86e9432f077667e3d","date":1202756169,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge))\n        sync(false, merge.info.sizeInBytes());\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit)\n      sync(false, merge.info.sizeInBytes());\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n\n    boolean success = false;\n\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n      if (!success) {\n        if (infoStream != null)\n          message(\"hit exception during merge; now refresh deleter on segment \" + mergedName);\n        synchronized(this) {\n          addMergeException(merge);\n          deleter.refresh(mergedName);\n        }\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n      \n      success = false;\n      boolean skip = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        try {\n          merger.createCompoundFile(compoundFileName);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (segmentInfos.indexOf(merge.info) == -1) {\n              // If another merge kicked in and merged our\n              // new segment away while we were trying to\n              // build the compound file, we can hit a\n              // FileNotFoundException and possibly\n              // IOException over NFS.  We can tell this has\n              // happened because our SegmentInfo is no\n              // longer in the segments; if this has\n              // happened it is safe to ignore the exception\n              // & skip finishing/committing our compound\n              // file creating.\n              if (infoStream != null)\n                message(\"hit exception creating compound file; ignoring it because our info (segment \" + merge.info.name + \") has been merged away\");\n              skip = true;\n            } else\n              throw ioe;\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge: skip=\" + skip);\n\n          synchronized(this) {\n            if (!skip)\n              addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (!skip) {\n\n        synchronized(this) {\n          if (skip || segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n            // Our segment (committed in non-compound\n            // format) got merged away while we were\n            // building the compound format.\n            deleter.deleteFile(compoundFileName);\n          } else {\n            success = false;\n            try {\n              merge.info.setUseCompoundFile(true);\n              checkpoint();\n              success = true;\n            } finally {\n              if (!success) {  \n                if (infoStream != null)\n                  message(\"hit exception checkpointing compound file during merge\");\n\n                // Must rollback:\n                addMergeException(merge);\n                merge.info.setUseCompoundFile(false);\n                deletePartialSegmentsFile();\n                deleter.deleteFile(compoundFileName);\n              }\n            }\n      \n            // Give deleter a chance to remove files now.\n            deleter.checkpoint(segmentInfos, autoCommit);\n          }\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["b6b0122d107a2f2a35007aca038d2a7fde039266","7c7146ecc4ef8e4b4f1637894d18b850eea7721b","5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a","8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c7146ecc4ef8e4b4f1637894d18b850eea7721b","date":1202840834,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        sync(false, size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      sync(false, size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge))\n        sync(false, merge.info.sizeInBytes());\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit)\n      sync(false, merge.info.sizeInBytes());\n\n    return mergedDocCount;\n  }\n\n","bugFix":["e82780afe6097066eb5befb86e9432f077667e3d"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be","date":1204801324,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(false, size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(false, size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        sync(false, size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      sync(false, size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"455aeff4fef915340c5b19d71d5e147034e83093","date":1210099270,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(false, size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(false, size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5712975970a595c28f1988efd007e1b8a617a92f","date":1219499238,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(true, si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2586f96f60332eb97ecd2934b0763791462568b2","date":1220116589,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(true, si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            throw ioe;\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(true, si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["b6b0122d107a2f2a35007aca038d2a7fde039266","8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6b0122d107a2f2a35007aca038d2a7fde039266","date":1222123999,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(true, si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(true, si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            throw ioe;\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            addMergeException(merge);\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":["2586f96f60332eb97ecd2934b0763791462568b2","e82780afe6097066eb5befb86e9432f077667e3d"],"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6a7626d7c0746aa25a85a8c7a27742a411e9b9fd","date":1236202493,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(true, si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      boolean success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    boolean success = false;\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(true, si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      success = true;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c4ff8864209d2e972cb4393600c26082f9a6533d","date":1239297466,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    checkpoint();\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    SegmentInfos sourceSegmentsClone = merge.segmentsClone;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n        SegmentInfo si = sourceSegmentsClone.info(i);\n        IndexReader reader = SegmentReader.get(true, si, MERGE_READ_BUFFER_SIZE, merge.mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n        totDocCount += reader.numDocs();\n      }\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n    } finally {\n      // close readers before we attempt to delete\n      // now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    if (!commitMerge(merge, merger, mergedDocCount))\n      // commitMerge will return false if this merge was aborted\n      return 0;\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      boolean success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["43b5b9b7b6370cc2e7a725ac424aefca77cd3eb3","47737b13e8268ad290e1540fe7cd264a29355092","1085ea837da8f1e96697e17cf73e1d08e7329261","8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"47737b13e8268ad290e1540fe7cd264a29355092","date":1242408643,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    checkpoint();\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"43b5b9b7b6370cc2e7a725ac424aefca77cd3eb3","date":1245425405,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores(merge.readers[i]);\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"66f3dadb253a44f4cccc81c8a21b685b18b201fb","date":1247245699,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores(merge.readers[i]);\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"961159f13aece73fbb30aea720e77a2237e8bafd","date":1247258916,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ce99e2b80b5a9cb2b9b59c01219e5397b081dcd8","date":1255049357,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      // Maybe force a sync here to allow reclaiming of the\n      // disk space used by the segments we just merged:\n      if (autoCommit && doCommitBeforeMergeCFS(merge)) {\n        final long size;\n        synchronized(this) {\n          size = merge.info.sizeInBytes();\n        }\n        commit(size);\n      }\n      \n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    // Force a sync after commiting the merge.  Once this\n    // sync completes then all index files referenced by the\n    // current segmentInfos are on stable storage so if the\n    // OS/machine crashes, or power cord is yanked, the\n    // index will be intact.  Note that this is just one\n    // (somewhat arbitrary) policy; we could try other\n    // policies like only sync if it's been > X minutes or\n    // more than Y bytes have been written, etc.\n    if (autoCommit) {\n      final long size;\n      synchronized(this) {\n        size = merge.info.sizeInBytes();\n      }\n      commit(size);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ef82ff03e4016c705811b2658e81471a645c0e49","date":1255900293,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set dss = new HashSet();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"775efee7f959e0dd3df7960b93767d9e00b78751","date":1267203159,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = IndexFileNames.segmentFileName(mergedName, IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"6a7626d7c0746aa25a85a8c7a27742a411e9b9fd":["b6b0122d107a2f2a35007aca038d2a7fde039266"],"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5712975970a595c28f1988efd007e1b8a617a92f":["455aeff4fef915340c5b19d71d5e147034e83093"],"775efee7f959e0dd3df7960b93767d9e00b78751":["ef82ff03e4016c705811b2658e81471a645c0e49"],"455aeff4fef915340c5b19d71d5e147034e83093":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"ce99e2b80b5a9cb2b9b59c01219e5397b081dcd8":["961159f13aece73fbb30aea720e77a2237e8bafd"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["6a7626d7c0746aa25a85a8c7a27742a411e9b9fd"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["7c7146ecc4ef8e4b4f1637894d18b850eea7721b"],"66f3dadb253a44f4cccc81c8a21b685b18b201fb":["43b5b9b7b6370cc2e7a725ac424aefca77cd3eb3"],"d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c":["52e9359ec09a0ce311e6ce95805998bebc7f7fd0"],"43b5b9b7b6370cc2e7a725ac424aefca77cd3eb3":["47737b13e8268ad290e1540fe7cd264a29355092"],"b6b0122d107a2f2a35007aca038d2a7fde039266":["2586f96f60332eb97ecd2934b0763791462568b2"],"52e9359ec09a0ce311e6ce95805998bebc7f7fd0":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"961159f13aece73fbb30aea720e77a2237e8bafd":["66f3dadb253a44f4cccc81c8a21b685b18b201fb"],"ef82ff03e4016c705811b2658e81471a645c0e49":["ce99e2b80b5a9cb2b9b59c01219e5397b081dcd8"],"7c7146ecc4ef8e4b4f1637894d18b850eea7721b":["e82780afe6097066eb5befb86e9432f077667e3d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e82780afe6097066eb5befb86e9432f077667e3d":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"2586f96f60332eb97ecd2934b0763791462568b2":["5712975970a595c28f1988efd007e1b8a617a92f"],"47737b13e8268ad290e1540fe7cd264a29355092":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["775efee7f959e0dd3df7960b93767d9e00b78751"]},"commit2Childs":{"6a7626d7c0746aa25a85a8c7a27742a411e9b9fd":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["e82780afe6097066eb5befb86e9432f077667e3d"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["52e9359ec09a0ce311e6ce95805998bebc7f7fd0"],"5712975970a595c28f1988efd007e1b8a617a92f":["2586f96f60332eb97ecd2934b0763791462568b2"],"775efee7f959e0dd3df7960b93767d9e00b78751":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"455aeff4fef915340c5b19d71d5e147034e83093":["5712975970a595c28f1988efd007e1b8a617a92f"],"ce99e2b80b5a9cb2b9b59c01219e5397b081dcd8":["ef82ff03e4016c705811b2658e81471a645c0e49"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["47737b13e8268ad290e1540fe7cd264a29355092"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["455aeff4fef915340c5b19d71d5e147034e83093"],"66f3dadb253a44f4cccc81c8a21b685b18b201fb":["961159f13aece73fbb30aea720e77a2237e8bafd"],"d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"b6b0122d107a2f2a35007aca038d2a7fde039266":["6a7626d7c0746aa25a85a8c7a27742a411e9b9fd"],"43b5b9b7b6370cc2e7a725ac424aefca77cd3eb3":["66f3dadb253a44f4cccc81c8a21b685b18b201fb"],"52e9359ec09a0ce311e6ce95805998bebc7f7fd0":["d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c"],"961159f13aece73fbb30aea720e77a2237e8bafd":["ce99e2b80b5a9cb2b9b59c01219e5397b081dcd8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"ef82ff03e4016c705811b2658e81471a645c0e49":["775efee7f959e0dd3df7960b93767d9e00b78751"],"7c7146ecc4ef8e4b4f1637894d18b850eea7721b":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"e82780afe6097066eb5befb86e9432f077667e3d":["7c7146ecc4ef8e4b4f1637894d18b850eea7721b"],"2586f96f60332eb97ecd2934b0763791462568b2":["b6b0122d107a2f2a35007aca038d2a7fde039266"],"47737b13e8268ad290e1540fe7cd264a29355092":["43b5b9b7b6370cc2e7a725ac424aefca77cd3eb3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}