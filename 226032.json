{"path":"src/java/org/apache/lucene/search/ParallelMultiSearcher#search(QueryWeight,Filter,Collector).mjava","commits":[{"id":"052fac7830290bd38a04cddee1a121ee07656b56","date":1245780702,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/ParallelMultiSearcher#search(QueryWeight,Filter,Collector).mjava","pathOld":"src/java/org/apache/lucene/search/ParallelMultiSearcher#search(Weight,Filter,Collector).mjava","sourceNew":"  /** Lower-level search API.\n  *\n  * <p>{@link Collector#collect(int)} is called for every matching document.\n  *\n  * <p>Applications should only use this if they need <i>all</i> of the\n  * matching documents.  The high-level search API ({@link\n  * Searcher#search(Query)}) is usually more efficient, as it skips\n  * non-high-scoring hits.\n  *\n  * @param weight to match documents\n  * @param filter if non-null, a bitset used to eliminate some documents\n  * @param collector to receive hits\n  * \n  * @todo parallelize this one too\n  */\n  public void search(QueryWeight weight, Filter filter, final Collector collector)\n   throws IOException {\n   for (int i = 0; i < searchables.length; i++) {\n\n     final int start = starts[i];\n\n     final Collector hc = new Collector() {\n       public void setScorer(Scorer scorer) throws IOException {\n         collector.setScorer(scorer);\n       }\n       public void collect(int doc) throws IOException {\n         collector.collect(doc);\n       }\n       public void setNextReader(IndexReader reader, int docBase) throws IOException {\n         collector.setNextReader(reader, start + docBase);\n       }\n       public boolean acceptsDocsOutOfOrder() {\n         return collector.acceptsDocsOutOfOrder();\n       }\n     };\n     \n     searchables[i].search(weight, filter, hc);\n   }\n }\n\n","sourceOld":"  /** Lower-level search API.\n  *\n  * <p>{@link Collector#collect(int)} is called for every matching document.\n  *\n  * <p>Applications should only use this if they need <i>all</i> of the\n  * matching documents.  The high-level search API ({@link\n  * Searcher#search(Query)}) is usually more efficient, as it skips\n  * non-high-scoring hits.\n  *\n  * @param weight to match documents\n  * @param filter if non-null, a bitset used to eliminate some documents\n  * @param collector to receive hits\n  * \n  * @todo parallelize this one too\n  */\n  public void search(Weight weight, Filter filter, final Collector collector)\n   throws IOException {\n   for (int i = 0; i < searchables.length; i++) {\n\n     final int start = starts[i];\n\n     final Collector hc = new Collector() {\n       public void setScorer(Scorer scorer) throws IOException {\n         collector.setScorer(scorer);\n       }\n       public void collect(int doc) throws IOException {\n         collector.collect(doc);\n       }\n       \n       public void setNextReader(IndexReader reader, int docBase) throws IOException {\n         collector.setNextReader(reader, start + docBase);\n       }\n     };\n     \n     searchables[i].search(weight, filter, hc);\n   }\n }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe941135bdfc28c81e20b4d21422f8726af34925","date":1250040150,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/ParallelMultiSearcher#search(Weight,Filter,Collector).mjava","pathOld":"src/java/org/apache/lucene/search/ParallelMultiSearcher#search(QueryWeight,Filter,Collector).mjava","sourceNew":"  /** Lower-level search API.\n  *\n  * <p>{@link Collector#collect(int)} is called for every matching document.\n  *\n  * <p>Applications should only use this if they need <i>all</i> of the\n  * matching documents.  The high-level search API ({@link\n  * Searcher#search(Query)}) is usually more efficient, as it skips\n  * non-high-scoring hits.\n  *\n  * @param weight to match documents\n  * @param filter if non-null, a bitset used to eliminate some documents\n  * @param collector to receive hits\n  * \n  * @todo parallelize this one too\n  */\n  public void search(Weight weight, Filter filter, final Collector collector)\n   throws IOException {\n   for (int i = 0; i < searchables.length; i++) {\n\n     final int start = starts[i];\n\n     final Collector hc = new Collector() {\n       public void setScorer(Scorer scorer) throws IOException {\n         collector.setScorer(scorer);\n       }\n       public void collect(int doc) throws IOException {\n         collector.collect(doc);\n       }\n       public void setNextReader(IndexReader reader, int docBase) throws IOException {\n         collector.setNextReader(reader, start + docBase);\n       }\n       public boolean acceptsDocsOutOfOrder() {\n         return collector.acceptsDocsOutOfOrder();\n       }\n     };\n     \n     searchables[i].search(weight, filter, hc);\n   }\n }\n\n","sourceOld":"  /** Lower-level search API.\n  *\n  * <p>{@link Collector#collect(int)} is called for every matching document.\n  *\n  * <p>Applications should only use this if they need <i>all</i> of the\n  * matching documents.  The high-level search API ({@link\n  * Searcher#search(Query)}) is usually more efficient, as it skips\n  * non-high-scoring hits.\n  *\n  * @param weight to match documents\n  * @param filter if non-null, a bitset used to eliminate some documents\n  * @param collector to receive hits\n  * \n  * @todo parallelize this one too\n  */\n  public void search(QueryWeight weight, Filter filter, final Collector collector)\n   throws IOException {\n   for (int i = 0; i < searchables.length; i++) {\n\n     final int start = starts[i];\n\n     final Collector hc = new Collector() {\n       public void setScorer(Scorer scorer) throws IOException {\n         collector.setScorer(scorer);\n       }\n       public void collect(int doc) throws IOException {\n         collector.collect(doc);\n       }\n       public void setNextReader(IndexReader reader, int docBase) throws IOException {\n         collector.setNextReader(reader, start + docBase);\n       }\n       public boolean acceptsDocsOutOfOrder() {\n         return collector.acceptsDocsOutOfOrder();\n       }\n     };\n     \n     searchables[i].search(weight, filter, hc);\n   }\n }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fe941135bdfc28c81e20b4d21422f8726af34925":["052fac7830290bd38a04cddee1a121ee07656b56"],"052fac7830290bd38a04cddee1a121ee07656b56":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fe941135bdfc28c81e20b4d21422f8726af34925"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["052fac7830290bd38a04cddee1a121ee07656b56"],"fe941135bdfc28c81e20b4d21422f8726af34925":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"052fac7830290bd38a04cddee1a121ee07656b56":["fe941135bdfc28c81e20b4d21422f8726af34925"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}