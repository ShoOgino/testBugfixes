{"path":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeNumericDVUpdates(Map[String,LinkedHashMap[Term,NumericDocValuesUpdate]]).mjava","commits":[{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeNumericDVUpdates(Map[String,LinkedHashMap[Term,NumericDocValuesUpdate]]).mjava","pathOld":"/dev/null","sourceNew":"  private byte[] freezeNumericDVUpdates(Map<String,LinkedHashMap<Term,NumericDocValuesUpdate>> numericDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    RAMOutputStream out = new RAMOutputStream();\n    String lastTermField = null;\n    String lastUpdateField = null;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : numericDVUpdates.values()) {\n      numericDVUpdateCount += numericUpdates.size();\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n\n        int code = update.term.bytes().length << 2;\n\n        String termField = update.term.field();\n        if (termField.equals(lastTermField) == false) {\n          code |= 1;\n        }\n        String updateField = update.field;\n        if (updateField.equals(lastUpdateField) == false) {\n          code |= 2;\n        }\n        out.writeVInt(code);\n        out.writeVInt(update.docIDUpto);\n        if ((code & 1) != 0) {\n          out.writeString(termField);\n          lastTermField = termField;\n        }\n        if ((code & 2) != 0) {\n          out.writeString(updateField);\n          lastUpdateField = updateField;\n        }\n\n        out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n        out.writeZLong(((Long) update.value).longValue());\n      }\n    }\n    byte[] bytes = new byte[(int) out.getFilePointer()];\n    out.writeTo(bytes, 0);\n    return bytes;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeNumericDVUpdates(Map[String,LinkedHashMap[Term,NumericDocValuesUpdate]]).mjava","pathOld":"/dev/null","sourceNew":"  private byte[] freezeNumericDVUpdates(Map<String,LinkedHashMap<Term,NumericDocValuesUpdate>> numericDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    RAMOutputStream out = new RAMOutputStream();\n    String lastTermField = null;\n    String lastUpdateField = null;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : numericDVUpdates.values()) {\n      numericDVUpdateCount += numericUpdates.size();\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n\n        int code = update.term.bytes().length << 2;\n\n        String termField = update.term.field();\n        if (termField.equals(lastTermField) == false) {\n          code |= 1;\n        }\n        String updateField = update.field;\n        if (updateField.equals(lastUpdateField) == false) {\n          code |= 2;\n        }\n        out.writeVInt(code);\n        out.writeVInt(update.docIDUpto);\n        if ((code & 1) != 0) {\n          out.writeString(termField);\n          lastTermField = termField;\n        }\n        if ((code & 2) != 0) {\n          out.writeString(updateField);\n          lastUpdateField = updateField;\n        }\n\n        out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n        out.writeZLong(((Long) update.value).longValue());\n      }\n    }\n    byte[] bytes = new byte[(int) out.getFilePointer()];\n    out.writeTo(bytes, 0);\n    return bytes;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeNumericDVUpdates(Map[String,LinkedHashMap[Term,NumericDocValuesUpdate]]).mjava","pathOld":"/dev/null","sourceNew":"  private byte[] freezeNumericDVUpdates(Map<String,LinkedHashMap<Term,NumericDocValuesUpdate>> numericDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    RAMOutputStream out = new RAMOutputStream();\n    String lastTermField = null;\n    String lastUpdateField = null;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : numericDVUpdates.values()) {\n      numericDVUpdateCount += numericUpdates.size();\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n\n        int code = update.term.bytes().length << 2;\n\n        String termField = update.term.field();\n        if (termField.equals(lastTermField) == false) {\n          code |= 1;\n        }\n        String updateField = update.field;\n        if (updateField.equals(lastUpdateField) == false) {\n          code |= 2;\n        }\n        out.writeVInt(code);\n        out.writeVInt(update.docIDUpto);\n        if ((code & 1) != 0) {\n          out.writeString(termField);\n          lastTermField = termField;\n        }\n        if ((code & 2) != 0) {\n          out.writeString(updateField);\n          lastUpdateField = updateField;\n        }\n\n        out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n        out.writeZLong(((Long) update.value).longValue());\n      }\n    }\n    byte[] bytes = new byte[(int) out.getFilePointer()];\n    out.writeTo(bytes, 0);\n    return bytes;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"110125c995236a7f61057dd04b039ed2d267f3a1","date":1521014987,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeNumericDVUpdates(Map[String,LinkedHashMap[Term,NumericDocValuesUpdate]]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeNumericDVUpdates(Map[String,LinkedHashMap[Term,NumericDocValuesUpdate]]).mjava","sourceNew":"  private byte[] freezeNumericDVUpdates(Map<String,LinkedHashMap<Term,NumericDocValuesUpdate>> numericDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, NumericDocValuesUpdate> numericUpdates : numericDVUpdates.values()) {\n        numericDVUpdateCount += numericUpdates.size();\n        for (NumericDocValuesUpdate update : numericUpdates.values()) {\n\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if ((code & 1) != 0) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if ((code & 2) != 0) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n          out.writeZLong(((Long) update.value).longValue());\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","sourceOld":"  private byte[] freezeNumericDVUpdates(Map<String,LinkedHashMap<Term,NumericDocValuesUpdate>> numericDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    RAMOutputStream out = new RAMOutputStream();\n    String lastTermField = null;\n    String lastUpdateField = null;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : numericDVUpdates.values()) {\n      numericDVUpdateCount += numericUpdates.size();\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n\n        int code = update.term.bytes().length << 2;\n\n        String termField = update.term.field();\n        if (termField.equals(lastTermField) == false) {\n          code |= 1;\n        }\n        String updateField = update.field;\n        if (updateField.equals(lastUpdateField) == false) {\n          code |= 2;\n        }\n        out.writeVInt(code);\n        out.writeVInt(update.docIDUpto);\n        if ((code & 1) != 0) {\n          out.writeString(termField);\n          lastTermField = termField;\n        }\n        if ((code & 2) != 0) {\n          out.writeString(updateField);\n          lastUpdateField = updateField;\n        }\n\n        out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n        out.writeZLong(((Long) update.value).longValue());\n      }\n    }\n    byte[] bytes = new byte[(int) out.getFilePointer()];\n    out.writeTo(bytes, 0);\n    return bytes;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e129bd6cb34a236558a49edf108a49d5c15e0e1","date":1525081316,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeNumericDVUpdates(Map[String,LinkedHashMap[Term,NumericDocValuesUpdate]]).mjava","sourceNew":null,"sourceOld":"  private byte[] freezeNumericDVUpdates(Map<String,LinkedHashMap<Term,NumericDocValuesUpdate>> numericDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, NumericDocValuesUpdate> numericUpdates : numericDVUpdates.values()) {\n        numericDVUpdateCount += numericUpdates.size();\n        for (NumericDocValuesUpdate update : numericUpdates.values()) {\n\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if ((code & 1) != 0) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if ((code & 2) != 0) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n          out.writeZLong(((Long) update.value).longValue());\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f42883db49d143abc1a0f176ba47e3388dafb608","date":1525083166,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeNumericDVUpdates(Map[String,LinkedHashMap[Term,NumericDocValuesUpdate]]).mjava","sourceNew":null,"sourceOld":"  private byte[] freezeNumericDVUpdates(Map<String,LinkedHashMap<Term,NumericDocValuesUpdate>> numericDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, NumericDocValuesUpdate> numericUpdates : numericDVUpdates.values()) {\n        numericDVUpdateCount += numericUpdates.size();\n        for (NumericDocValuesUpdate update : numericUpdates.values()) {\n\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if ((code & 1) != 0) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if ((code & 2) != 0) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n          out.writeZLong(((Long) update.value).longValue());\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["110125c995236a7f61057dd04b039ed2d267f3a1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f42883db49d143abc1a0f176ba47e3388dafb608":["110125c995236a7f61057dd04b039ed2d267f3a1","7e129bd6cb34a236558a49edf108a49d5c15e0e1"],"110125c995236a7f61057dd04b039ed2d267f3a1":["28288370235ed02234a64753cdbf0c6ec096304a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f42883db49d143abc1a0f176ba47e3388dafb608"]},"commit2Childs":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["f42883db49d143abc1a0f176ba47e3388dafb608"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"f42883db49d143abc1a0f176ba47e3388dafb608":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"110125c995236a7f61057dd04b039ed2d267f3a1":["7e129bd6cb34a236558a49edf108a49d5c15e0e1","f42883db49d143abc1a0f176ba47e3388dafb608"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["110125c995236a7f61057dd04b039ed2d267f3a1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}