{"path":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","commits":[{"id":"28211671436f185419b3f7e53ccfc3911441ab65","date":1544026960,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","pathOld":"/dev/null","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      while ((bufferedUpdate = iterator.next()) != null) {\n        Terms terms = segState.reader.terms(bufferedUpdate.termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          // no terms in this segment for this field\n          continue;\n        }\n\n        final int limit;\n        if (delGen == segState.delGen) {\n          assert segmentPrivateDeletes;\n          limit = bufferedUpdate.docUpTo;\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        \n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n\n        // TODO: we could at least *collate* by field?\n\n\n        final BytesRef binaryValue;\n        final long longValue;\n        if (bufferedUpdate.hasValue == false) {\n          longValue = -1;\n          binaryValue = null;\n        } else {\n          longValue = bufferedUpdate.numericValue;\n          binaryValue = bufferedUpdate.binaryValue;\n        }\n\n        if (termsEnum.seekExact(bufferedUpdate.termValue)) {\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n          DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            holder.put(updateField, dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c58aa602dcba92248bb2498aa7b9c8daecc898b0","date":1544085659,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      while ((bufferedUpdate = iterator.next()) != null) {\n        Terms terms = segState.reader.terms(bufferedUpdate.termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          // no terms in this segment for this field\n          continue;\n        }\n\n        final int limit;\n        if (delGen == segState.delGen) {\n          assert segmentPrivateDeletes;\n          limit = bufferedUpdate.docUpTo;\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        \n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n\n        // TODO: we could at least *collate* by field?\n\n\n        final BytesRef binaryValue;\n        final long longValue;\n        if (bufferedUpdate.hasValue == false) {\n          longValue = -1;\n          binaryValue = null;\n        } else {\n          longValue = bufferedUpdate.numericValue;\n          binaryValue = bufferedUpdate.binaryValue;\n        }\n\n        if (termsEnum.seekExact(bufferedUpdate.termValue)) {\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      while ((bufferedUpdate = iterator.next()) != null) {\n        Terms terms = segState.reader.terms(bufferedUpdate.termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          // no terms in this segment for this field\n          continue;\n        }\n\n        final int limit;\n        if (delGen == segState.delGen) {\n          assert segmentPrivateDeletes;\n          limit = bufferedUpdate.docUpTo;\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        \n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n\n        // TODO: we could at least *collate* by field?\n\n\n        final BytesRef binaryValue;\n        final long longValue;\n        if (bufferedUpdate.hasValue == false) {\n          longValue = -1;\n          binaryValue = null;\n        } else {\n          longValue = bufferedUpdate.numericValue;\n          binaryValue = bufferedUpdate.binaryValue;\n        }\n\n        if (termsEnum.seekExact(bufferedUpdate.termValue)) {\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n          DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            holder.put(updateField, dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6dbaff854eb04c7da5e30a9ade1644b7fbf7c3b9","date":1544461568,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      while ((bufferedUpdate = iterator.next()) != null) {\n        Terms terms = segState.reader.terms(bufferedUpdate.termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          // no terms in this segment for this field\n          continue;\n        }\n\n        final int limit;\n        if (delGen == segState.delGen) {\n          assert segmentPrivateDeletes;\n          limit = bufferedUpdate.docUpTo;\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        \n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n\n        // TODO: we could at least *collate* by field?\n\n\n        final BytesRef binaryValue;\n        final long longValue;\n        if (bufferedUpdate.hasValue == false) {\n          longValue = -1;\n          binaryValue = null;\n        } else {\n          longValue = bufferedUpdate.numericValue;\n          binaryValue = bufferedUpdate.binaryValue;\n        }\n\n        if (termsEnum.seekExact(bufferedUpdate.termValue)) {\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, value.getMinNumeric(),\n                    value.getMaxNumeric(), segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      while ((bufferedUpdate = iterator.next()) != null) {\n        Terms terms = segState.reader.terms(bufferedUpdate.termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          // no terms in this segment for this field\n          continue;\n        }\n\n        final int limit;\n        if (delGen == segState.delGen) {\n          assert segmentPrivateDeletes;\n          limit = bufferedUpdate.docUpTo;\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        \n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n\n        // TODO: we could at least *collate* by field?\n\n\n        final BytesRef binaryValue;\n        final long longValue;\n        if (bufferedUpdate.hasValue == false) {\n          longValue = -1;\n          binaryValue = null;\n        } else {\n          longValue = bufferedUpdate.numericValue;\n          binaryValue = bufferedUpdate.binaryValue;\n        }\n\n        if (termsEnum.seekExact(bufferedUpdate.termValue)) {\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e67f4692aaca7e208efb63e367caa54f3f33de9","date":1544552056,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      String previousField = null;\n      while ((bufferedUpdate = iterator.next()) != null) {\n        if (previousField == null || previousField.equals(bufferedUpdate.termField) == false) {\n          previousField = bufferedUpdate.termField;\n          Terms terms = segState.reader.terms(previousField);\n          termsEnum = terms == null ? null : terms.iterator();\n        }\n        if (termsEnum == null) {\n          // no terms in this segment for this field\n          continue;\n        }\n\n        final int limit;\n        if (delGen == segState.delGen) {\n          assert segmentPrivateDeletes;\n          limit = bufferedUpdate.docUpTo;\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        \n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n\n        // TODO: we could at least *collate* by field?\n\n\n        final BytesRef binaryValue;\n        final long longValue;\n        if (bufferedUpdate.hasValue == false) {\n          longValue = -1;\n          binaryValue = null;\n        } else {\n          longValue = bufferedUpdate.numericValue;\n          binaryValue = bufferedUpdate.binaryValue;\n        }\n\n        if (termsEnum.seekExact(bufferedUpdate.termValue)) {\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, value.getMinNumeric(),\n                    value.getMaxNumeric(), segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      while ((bufferedUpdate = iterator.next()) != null) {\n        Terms terms = segState.reader.terms(bufferedUpdate.termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          // no terms in this segment for this field\n          continue;\n        }\n\n        final int limit;\n        if (delGen == segState.delGen) {\n          assert segmentPrivateDeletes;\n          limit = bufferedUpdate.docUpTo;\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        \n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n\n        // TODO: we could at least *collate* by field?\n\n\n        final BytesRef binaryValue;\n        final long longValue;\n        if (bufferedUpdate.hasValue == false) {\n          longValue = -1;\n          binaryValue = null;\n        } else {\n          longValue = bufferedUpdate.numericValue;\n          binaryValue = bufferedUpdate.binaryValue;\n        }\n\n        if (termsEnum.seekExact(bufferedUpdate.termValue)) {\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, value.getMinNumeric(),\n                    value.getMaxNumeric(), segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e0abaca9e5481b5c3660805111683e8ce53bef3","date":1544554124,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      String previousField = null;\n      while ((bufferedUpdate = iterator.next()) != null) {\n        if (previousField == null || previousField.equals(bufferedUpdate.termField) == false) {\n          previousField = bufferedUpdate.termField;\n          Terms terms = segState.reader.terms(previousField);\n          termsEnum = terms == null ? null : terms.iterator();\n        }\n        if (termsEnum == null) {\n          // no terms in this segment for this field\n          continue;\n        }\n\n        final int limit;\n        if (delGen == segState.delGen) {\n          assert segmentPrivateDeletes;\n          limit = bufferedUpdate.docUpTo;\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        \n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n\n        // TODO: we could at least *collate* by field?\n\n\n        final BytesRef binaryValue;\n        final long longValue;\n        if (bufferedUpdate.hasValue == false) {\n          longValue = -1;\n          binaryValue = null;\n        } else {\n          longValue = bufferedUpdate.numericValue;\n          binaryValue = bufferedUpdate.binaryValue;\n        }\n\n        if (termsEnum.seekExact(bufferedUpdate.termValue)) {\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, value.getMinNumeric(),\n                    value.getMaxNumeric(), segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      while ((bufferedUpdate = iterator.next()) != null) {\n        Terms terms = segState.reader.terms(bufferedUpdate.termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          // no terms in this segment for this field\n          continue;\n        }\n\n        final int limit;\n        if (delGen == segState.delGen) {\n          assert segmentPrivateDeletes;\n          limit = bufferedUpdate.docUpTo;\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        \n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n\n        // TODO: we could at least *collate* by field?\n\n\n        final BytesRef binaryValue;\n        final long longValue;\n        if (bufferedUpdate.hasValue == false) {\n          longValue = -1;\n          binaryValue = null;\n        } else {\n          longValue = bufferedUpdate.numericValue;\n          binaryValue = bufferedUpdate.binaryValue;\n        }\n\n        if (termsEnum.seekExact(bufferedUpdate.termValue)) {\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, value.getMinNumeric(),\n                    value.getMaxNumeric(), segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83b6ce113ec151d7bf9175578d92d5320f91ab2e","date":1544711434,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      TermDocsIterator termDocsIterator = new TermDocsIterator(segState.reader, false);\n      while ((bufferedUpdate = iterator.next()) != null) {\n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n        // TODO: we could at least *collate* by field?\n        final DocIdSetIterator docIdSetIterator = termDocsIterator.nextTerm(bufferedUpdate.termField, bufferedUpdate.termValue);\n        if (docIdSetIterator != null) {\n          final int limit;\n          if (delGen == segState.delGen) {\n            assert segmentPrivateDeletes;\n            limit = bufferedUpdate.docUpTo;\n          } else {\n            limit = Integer.MAX_VALUE;\n          }\n          final BytesRef binaryValue;\n          final long longValue;\n          if (bufferedUpdate.hasValue == false) {\n            longValue = -1;\n            binaryValue = null;\n          } else {\n            longValue = bufferedUpdate.numericValue;\n            binaryValue = bufferedUpdate.binaryValue;\n          }\n           termDocsIterator.getDocs();\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, value.getMinNumeric(),\n                    value.getMaxNumeric(), segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = docIdSetIterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = docIdSetIterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      String previousField = null;\n      while ((bufferedUpdate = iterator.next()) != null) {\n        if (previousField == null || previousField.equals(bufferedUpdate.termField) == false) {\n          previousField = bufferedUpdate.termField;\n          Terms terms = segState.reader.terms(previousField);\n          termsEnum = terms == null ? null : terms.iterator();\n        }\n        if (termsEnum == null) {\n          // no terms in this segment for this field\n          continue;\n        }\n\n        final int limit;\n        if (delGen == segState.delGen) {\n          assert segmentPrivateDeletes;\n          limit = bufferedUpdate.docUpTo;\n        } else {\n          limit = Integer.MAX_VALUE;\n        }\n        \n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n\n        // TODO: we could at least *collate* by field?\n\n\n        final BytesRef binaryValue;\n        final long longValue;\n        if (bufferedUpdate.hasValue == false) {\n          longValue = -1;\n          binaryValue = null;\n        } else {\n          longValue = bufferedUpdate.numericValue;\n          binaryValue = bufferedUpdate.binaryValue;\n        }\n\n        if (termsEnum.seekExact(bufferedUpdate.termValue)) {\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, value.getMinNumeric(),\n                    value.getMaxNumeric(), segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb0b91dbd7ffa9329d88b6cf6d606e542fed61d8","date":1582222690,"type":3,"author":"Nhat Nguyen","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,Map[String,FieldUpdatesBuffer],long,boolean).mjava","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      TermDocsIterator termDocsIterator = new TermDocsIterator(segState.reader, iterator.isSortedTerms());\n      while ((bufferedUpdate = iterator.next()) != null) {\n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n        // TODO: we could at least *collate* by field?\n        final DocIdSetIterator docIdSetIterator = termDocsIterator.nextTerm(bufferedUpdate.termField, bufferedUpdate.termValue);\n        if (docIdSetIterator != null) {\n          final int limit;\n          if (delGen == segState.delGen) {\n            assert segmentPrivateDeletes;\n            limit = bufferedUpdate.docUpTo;\n          } else {\n            limit = Integer.MAX_VALUE;\n          }\n          final BytesRef binaryValue;\n          final long longValue;\n          if (bufferedUpdate.hasValue == false) {\n            longValue = -1;\n            binaryValue = null;\n          } else {\n            longValue = bufferedUpdate.numericValue;\n            binaryValue = bufferedUpdate.binaryValue;\n          }\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, value.getMinNumeric(),\n                    value.getMaxNumeric(), segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = docIdSetIterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = docIdSetIterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                            Map<String, FieldUpdatesBuffer> updates,\n                                            long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    final List<DocValuesFieldUpdates> resolvedUpdates = new ArrayList<>();\n    for (Map.Entry<String, FieldUpdatesBuffer> fieldUpdate : updates.entrySet()) {\n      String updateField = fieldUpdate.getKey();\n      DocValuesFieldUpdates dvUpdates = null;\n      FieldUpdatesBuffer value = fieldUpdate.getValue();\n      boolean isNumeric = value.isNumeric();\n      FieldUpdatesBuffer.BufferedUpdateIterator iterator = value.iterator();\n      FieldUpdatesBuffer.BufferedUpdate bufferedUpdate;\n      TermDocsIterator termDocsIterator = new TermDocsIterator(segState.reader, false);\n      while ((bufferedUpdate = iterator.next()) != null) {\n        // TODO: we traverse the terms in update order (not term order) so that we\n        // apply the updates in the correct order, i.e. if two terms update the\n        // same document, the last one that came in wins, irrespective of the\n        // terms lexical order.\n        // we can apply the updates in terms order if we keep an updatesGen (and\n        // increment it with every update) and attach it to each NumericUpdate. Note\n        // that we cannot rely only on docIDUpto because an app may send two updates\n        // which will get same docIDUpto, yet will still need to respect the order\n        // those updates arrived.\n        // TODO: we could at least *collate* by field?\n        final DocIdSetIterator docIdSetIterator = termDocsIterator.nextTerm(bufferedUpdate.termField, bufferedUpdate.termValue);\n        if (docIdSetIterator != null) {\n          final int limit;\n          if (delGen == segState.delGen) {\n            assert segmentPrivateDeletes;\n            limit = bufferedUpdate.docUpTo;\n          } else {\n            limit = Integer.MAX_VALUE;\n          }\n          final BytesRef binaryValue;\n          final long longValue;\n          if (bufferedUpdate.hasValue == false) {\n            longValue = -1;\n            binaryValue = null;\n          } else {\n            longValue = bufferedUpdate.numericValue;\n            binaryValue = bufferedUpdate.binaryValue;\n          }\n           termDocsIterator.getDocs();\n          if (dvUpdates == null) {\n            if (isNumeric) {\n              if (value.hasSingleValue()) {\n                dvUpdates = new NumericDocValuesFieldUpdates\n                    .SingleValueNumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc(),\n                    value.getNumericValue(0));\n              } else {\n                dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, value.getMinNumeric(),\n                    value.getMaxNumeric(), segState.reader.maxDoc());\n              }\n            } else {\n              dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n            }\n            resolvedUpdates.add(dvUpdates);\n          }\n          final IntConsumer docIdConsumer;\n          final DocValuesFieldUpdates update = dvUpdates;\n          if (bufferedUpdate.hasValue == false) {\n            docIdConsumer = doc -> update.reset(doc);\n          } else if (isNumeric) {\n            docIdConsumer = doc -> update.add(doc, longValue);\n          } else {\n            docIdConsumer = doc -> update.add(doc, binaryValue);\n          }\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n            // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n            int doc;\n            while ((doc = docIdSetIterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                // The limit is in the pre-sorted doc space:\n                if (segState.rld.sortMap.newToOld(doc) < limit) {\n                  docIdConsumer.accept(doc);\n                  updateCount++;\n                }\n              }\n            }\n          } else {\n            int doc;\n            while ((doc = docIdSetIterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n              if (doc >= limit) {\n                break; // no more docs that can be updated for this term\n              }\n              if (acceptDocs == null || acceptDocs.get(doc)) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : resolvedUpdates) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fb0b91dbd7ffa9329d88b6cf6d606e542fed61d8":["83b6ce113ec151d7bf9175578d92d5320f91ab2e"],"83b6ce113ec151d7bf9175578d92d5320f91ab2e":["7e0abaca9e5481b5c3660805111683e8ce53bef3"],"c58aa602dcba92248bb2498aa7b9c8daecc898b0":["28211671436f185419b3f7e53ccfc3911441ab65"],"28211671436f185419b3f7e53ccfc3911441ab65":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6dbaff854eb04c7da5e30a9ade1644b7fbf7c3b9":["c58aa602dcba92248bb2498aa7b9c8daecc898b0"],"7e67f4692aaca7e208efb63e367caa54f3f33de9":["6dbaff854eb04c7da5e30a9ade1644b7fbf7c3b9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7e0abaca9e5481b5c3660805111683e8ce53bef3":["6dbaff854eb04c7da5e30a9ade1644b7fbf7c3b9","7e67f4692aaca7e208efb63e367caa54f3f33de9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fb0b91dbd7ffa9329d88b6cf6d606e542fed61d8"]},"commit2Childs":{"fb0b91dbd7ffa9329d88b6cf6d606e542fed61d8":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"83b6ce113ec151d7bf9175578d92d5320f91ab2e":["fb0b91dbd7ffa9329d88b6cf6d606e542fed61d8"],"c58aa602dcba92248bb2498aa7b9c8daecc898b0":["6dbaff854eb04c7da5e30a9ade1644b7fbf7c3b9"],"28211671436f185419b3f7e53ccfc3911441ab65":["c58aa602dcba92248bb2498aa7b9c8daecc898b0"],"6dbaff854eb04c7da5e30a9ade1644b7fbf7c3b9":["7e67f4692aaca7e208efb63e367caa54f3f33de9","7e0abaca9e5481b5c3660805111683e8ce53bef3"],"7e67f4692aaca7e208efb63e367caa54f3f33de9":["7e0abaca9e5481b5c3660805111683e8ce53bef3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["28211671436f185419b3f7e53ccfc3911441ab65"],"7e0abaca9e5481b5c3660805111683e8ce53bef3":["83b6ce113ec151d7bf9175578d92d5320f91ab2e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}