{"path":"modules/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29Tokenizer#populateAttributes(String).mjava","commits":[{"id":"683d3f90dda2bbb999c3ce855706d74563a53680","date":1285654576,"type":0,"author":"Steven Rowe","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29Tokenizer#populateAttributes(String).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Populates this TokenStream's CharTermAttribute and OffsetAttribute from\n   * the current match, the TypeAttribute from the passed-in tokenType, and\n   * the PositionIncrementAttribute to one, unless the immediately previous\n   * token(s) was/were skipped because maxTokenLength was exceeded, in which\n   * case the PositionIncrementAttribute is set to one plus the number of\n   * skipped overly long tokens. \n   * <p/> \n   * If maxTokenLength is exceeded, the CharTermAttribute is set back to empty\n   * and false is returned.\n   * \n   * @param tokenType The type of the matching token\n   * @return true there is a token available (not too long); false otherwise \n   */\n  private boolean populateAttributes(String tokenType) {\n    boolean isTokenAvailable = false;\n    if (yylength() > maxTokenLength) {\n      // When we skip a too-long token, we treat it like a stopword, introducing\n      // a position increment gap\n      ++posIncr;\n    } else {\n      termAtt.copyBuffer(zzBuffer, zzStartRead, yylength());\n      posIncrAtt.setPositionIncrement(posIncr);\n      offsetAtt.setOffset(correctOffset(yychar),\n                          correctOffset(yychar + yylength()));\n      typeAtt.setType(tokenType);\n      isTokenAvailable = true;\n    }\n    return isTokenAvailable;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"351c452f1c3ded97338e6d3db2b585c5f89b0410","date":1291733593,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer#populateAttributes(String).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29Tokenizer#populateAttributes(String).mjava","sourceNew":"  /**\n   * Populates this TokenStream's CharTermAttribute and OffsetAttribute from\n   * the current match, the TypeAttribute from the passed-in tokenType, and\n   * the PositionIncrementAttribute to one, unless the immediately previous\n   * token(s) was/were skipped because maxTokenLength was exceeded, in which\n   * case the PositionIncrementAttribute is set to one plus the number of\n   * skipped overly long tokens. \n   * <p/> \n   * If maxTokenLength is exceeded, the CharTermAttribute is set back to empty\n   * and false is returned.\n   * \n   * @param tokenType The type of the matching token\n   * @return true there is a token available (not too long); false otherwise \n   */\n  private boolean populateAttributes(String tokenType) {\n    boolean isTokenAvailable = false;\n    if (yylength() > maxTokenLength) {\n      // When we skip a too-long token, we treat it like a stopword, introducing\n      // a position increment gap\n      ++posIncr;\n    } else {\n      termAtt.copyBuffer(zzBuffer, zzStartRead, yylength());\n      posIncrAtt.setPositionIncrement(posIncr);\n      offsetAtt.setOffset(correctOffset(yychar),\n                          correctOffset(yychar + yylength()));\n      typeAtt.setType(tokenType);\n      isTokenAvailable = true;\n    }\n    return isTokenAvailable;\n  }\n\n","sourceOld":"  /**\n   * Populates this TokenStream's CharTermAttribute and OffsetAttribute from\n   * the current match, the TypeAttribute from the passed-in tokenType, and\n   * the PositionIncrementAttribute to one, unless the immediately previous\n   * token(s) was/were skipped because maxTokenLength was exceeded, in which\n   * case the PositionIncrementAttribute is set to one plus the number of\n   * skipped overly long tokens. \n   * <p/> \n   * If maxTokenLength is exceeded, the CharTermAttribute is set back to empty\n   * and false is returned.\n   * \n   * @param tokenType The type of the matching token\n   * @return true there is a token available (not too long); false otherwise \n   */\n  private boolean populateAttributes(String tokenType) {\n    boolean isTokenAvailable = false;\n    if (yylength() > maxTokenLength) {\n      // When we skip a too-long token, we treat it like a stopword, introducing\n      // a position increment gap\n      ++posIncr;\n    } else {\n      termAtt.copyBuffer(zzBuffer, zzStartRead, yylength());\n      posIncrAtt.setPositionIncrement(posIncr);\n      offsetAtt.setOffset(correctOffset(yychar),\n                          correctOffset(yychar + yylength()));\n      typeAtt.setType(tokenType);\n      isTokenAvailable = true;\n    }\n    return isTokenAvailable;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d8c4bb144102e537495ae5b321f77a9898f7b0b8","date":1291834816,"type":5,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer#populateAttributes(String).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29Tokenizer#populateAttributes(String).mjava","sourceNew":"  /**\n   * Populates this TokenStream's CharTermAttribute and OffsetAttribute from\n   * the current match, the TypeAttribute from the passed-in tokenType, and\n   * the PositionIncrementAttribute to one, unless the immediately previous\n   * token(s) was/were skipped because maxTokenLength was exceeded, in which\n   * case the PositionIncrementAttribute is set to one plus the number of\n   * skipped overly long tokens. \n   * <p/> \n   * If maxTokenLength is exceeded, the CharTermAttribute is set back to empty\n   * and false is returned.\n   * \n   * @param tokenType The type of the matching token\n   * @return true there is a token available (not too long); false otherwise \n   */\n  private boolean populateAttributes(String tokenType) {\n    boolean isTokenAvailable = false;\n    if (yylength() > maxTokenLength) {\n      // When we skip a too-long token, we treat it like a stopword, introducing\n      // a position increment gap\n      ++posIncr;\n    } else {\n      termAtt.copyBuffer(zzBuffer, zzStartRead, yylength());\n      posIncrAtt.setPositionIncrement(posIncr);\n      offsetAtt.setOffset(correctOffset(yychar),\n                          correctOffset(yychar + yylength()));\n      typeAtt.setType(tokenType);\n      isTokenAvailable = true;\n    }\n    return isTokenAvailable;\n  }\n\n","sourceOld":"  /**\n   * Populates this TokenStream's CharTermAttribute and OffsetAttribute from\n   * the current match, the TypeAttribute from the passed-in tokenType, and\n   * the PositionIncrementAttribute to one, unless the immediately previous\n   * token(s) was/were skipped because maxTokenLength was exceeded, in which\n   * case the PositionIncrementAttribute is set to one plus the number of\n   * skipped overly long tokens. \n   * <p/> \n   * If maxTokenLength is exceeded, the CharTermAttribute is set back to empty\n   * and false is returned.\n   * \n   * @param tokenType The type of the matching token\n   * @return true there is a token available (not too long); false otherwise \n   */\n  private boolean populateAttributes(String tokenType) {\n    boolean isTokenAvailable = false;\n    if (yylength() > maxTokenLength) {\n      // When we skip a too-long token, we treat it like a stopword, introducing\n      // a position increment gap\n      ++posIncr;\n    } else {\n      termAtt.copyBuffer(zzBuffer, zzStartRead, yylength());\n      posIncrAtt.setPositionIncrement(posIncr);\n      offsetAtt.setOffset(correctOffset(yychar),\n                          correctOffset(yychar + yylength()));\n      typeAtt.setType(tokenType);\n      isTokenAvailable = true;\n    }\n    return isTokenAvailable;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"683d3f90dda2bbb999c3ce855706d74563a53680":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"351c452f1c3ded97338e6d3db2b585c5f89b0410":["683d3f90dda2bbb999c3ce855706d74563a53680"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d8c4bb144102e537495ae5b321f77a9898f7b0b8":["683d3f90dda2bbb999c3ce855706d74563a53680","351c452f1c3ded97338e6d3db2b585c5f89b0410"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["351c452f1c3ded97338e6d3db2b585c5f89b0410"]},"commit2Childs":{"683d3f90dda2bbb999c3ce855706d74563a53680":["351c452f1c3ded97338e6d3db2b585c5f89b0410","d8c4bb144102e537495ae5b321f77a9898f7b0b8"],"351c452f1c3ded97338e6d3db2b585c5f89b0410":["d8c4bb144102e537495ae5b321f77a9898f7b0b8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["683d3f90dda2bbb999c3ce855706d74563a53680"],"d8c4bb144102e537495ae5b321f77a9898f7b0b8":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d8c4bb144102e537495ae5b321f77a9898f7b0b8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}