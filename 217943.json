{"path":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","commits":[{"id":"48bedd31c61edafb8baaff4bcbcac19449fb7c3a","date":1251468037,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/miscellaneous/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4256bc1b3c94786287ccdfc751230374521843cf","date":1254612273,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"87c966e9308847938a7c905c2e46a56d8df788b8","date":1255035452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9","date":1256127131,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Version.LUCENE_CURRENT, Collections.emptySet()), true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a","date":1267298041,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet()), true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Version.LUCENE_CURRENT, Collections.emptySet()), true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1cedb00d2dd44640194401179358a2e3ba6051bf","date":1268243626,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT).setAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet())));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet()), true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e52fea2c4081a1e552b98506691990be59503168","date":1268250331,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet()), true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT).setAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet())));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8","date":1268494368,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet())));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet()), true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet())));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet())));\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir, false);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["e52fea2c4081a1e552b98506691990be59503168"],"48bedd31c61edafb8baaff4bcbcac19449fb7c3a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1cedb00d2dd44640194401179358a2e3ba6051bf":["a9ac13b5f0ce5ef1b2ce168367d993a79594b23a"],"e52fea2c4081a1e552b98506691990be59503168":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9":["87c966e9308847938a7c905c2e46a56d8df788b8"],"87c966e9308847938a7c905c2e46a56d8df788b8":["4256bc1b3c94786287ccdfc751230374521843cf"],"4256bc1b3c94786287ccdfc751230374521843cf":["48bedd31c61edafb8baaff4bcbcac19449fb7c3a"],"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a":["4b41b991de69ba7b72d5e90cfcee25699a1a7fc9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"]},"commit2Childs":{"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"48bedd31c61edafb8baaff4bcbcac19449fb7c3a":["4256bc1b3c94786287ccdfc751230374521843cf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["48bedd31c61edafb8baaff4bcbcac19449fb7c3a"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["e52fea2c4081a1e552b98506691990be59503168"],"e52fea2c4081a1e552b98506691990be59503168":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9":["a9ac13b5f0ce5ef1b2ce168367d993a79594b23a"],"87c966e9308847938a7c905c2e46a56d8df788b8":["4b41b991de69ba7b72d5e90cfcee25699a1a7fc9"],"4256bc1b3c94786287ccdfc751230374521843cf":["87c966e9308847938a7c905c2e46a56d8df788b8"],"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}