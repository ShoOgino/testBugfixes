{"path":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#test().mjava","commits":[{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":1,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    boolean testFinished = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n\n      // todo: do I have to do this here?\n      waitForRecoveriesToFinish(true);\n\n      doTriLevelHashingTest();\n      doTriLevelHashingTestWithBitMask();\n\n      testFinished = true;\n    } finally {\n      if (!testFinished) {\n        printLayoutOnTearDown = true;\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testFinished = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n\n      // todo: do I have to do this here?\n      waitForRecoveriesToFinish(true);\n\n      doTriLevelHashingTest();\n      doTriLevelHashingTestWithBitMask();\n\n      testFinished = true;\n    } finally {\n      if (!testFinished) {\n        printLayoutOnTearDown = true;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d35c84fdef07284c122012ca4000d3b7285a66e","date":1545962630,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#test().mjava","sourceNew":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // annotated on: 24-Dec-2018\n  public void test() throws Exception {\n    boolean testFinished = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n\n      // todo: do I have to do this here?\n      waitForRecoveriesToFinish(true);\n\n      doTriLevelHashingTest();\n      doTriLevelHashingTestWithBitMask();\n\n      testFinished = true;\n    } finally {\n      if (!testFinished) {\n        printLayoutOnTearDown = true;\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    boolean testFinished = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n\n      // todo: do I have to do this here?\n      waitForRecoveriesToFinish(true);\n\n      doTriLevelHashingTest();\n      doTriLevelHashingTestWithBitMask();\n\n      testFinished = true;\n    } finally {\n      if (!testFinished) {\n        printLayoutOnTearDown = true;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0949f20e74c954865e5cfd653de9059bfc1a9aca","date":1548956712,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    boolean testFinished = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n\n      // todo: do I have to do this here?\n      waitForRecoveriesToFinish(true);\n\n      doTriLevelHashingTest();\n      del(\"*:*\");\n      commit();\n      doTriLevelHashingTestWithBitMask();\n\n      testFinished = true;\n    } finally {\n      if (!testFinished) {\n        printLayoutOnTearDown = true;\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // annotated on: 24-Dec-2018\n  public void test() throws Exception {\n    boolean testFinished = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n\n      // todo: do I have to do this here?\n      waitForRecoveriesToFinish(true);\n\n      doTriLevelHashingTest();\n      doTriLevelHashingTestWithBitMask();\n\n      testFinished = true;\n    } finally {\n      if (!testFinished) {\n        printLayoutOnTearDown = true;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06e68e6090ed0bd8f1f9b96e03328d9a4591e451","date":1549489350,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    boolean testFinished = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n\n      // todo: do I have to do this here?\n      waitForRecoveriesToFinish(true);\n\n      // NOTE: we might randomly generate the same uniqueKey value multiple times,\n      // (which is a valid test case, they should route to the same shard both times)\n      // so we track each expectedId in a set for later sanity checking\n      final Set<String> expectedUniqueKeys = new HashSet<>();\n      for (int i = 0; i < NUM_ADDS; i++) {\n        final int appId = r.nextInt(MAX_APP_ID) + 1;\n        final int userId = r.nextInt(MAX_USER_ID) + 1;\n        // skew the odds so half the time we have no mask, and half the time we\n        // have an even distribution of 1-16 bits\n        final int bitMask = Math.max(0, r.nextInt(32)-15);\n        \n        String id = \"app\" + appId + (bitMask <= 0 ? \"\" : (\"/\" + bitMask))\n          + \"!\" + \"user\" + userId\n          + \"!\" + \"doc\" + r.nextInt(MAX_DOC_ID);\n        \n        doAddDoc(id);\n        expectedUniqueKeys.add(id);\n      }\n      \n      commit();\n      \n      final Map<String, String> routePrefixMap = new HashMap<>();\n      final Set<String> actualUniqueKeys = new HashSet<>();\n      for (int i = 1; i <= sliceCount; i++) {\n        final String shardId = \"shard\" + i;\n        final Set<String> uniqueKeysInShard = fetchUniqueKeysFromShard(shardId);\n        \n        { // sanity check our uniqueKey values aren't duplicated across shards\n          final Set<String> uniqueKeysOnDuplicateShards = new HashSet<>(uniqueKeysInShard);\n          uniqueKeysOnDuplicateShards.retainAll(actualUniqueKeys);\n          assertEquals(shardId + \" contains some uniqueKeys that were already found on a previous shard\",\n                       Collections.emptySet(),  uniqueKeysOnDuplicateShards);\n          actualUniqueKeys.addAll(uniqueKeysInShard);\n        }\n        \n        // foreach uniqueKey, extract it's route prefix and confirm those aren't spread across multiple shards\n        for (String uniqueKey : uniqueKeysInShard) {\n          final String routePrefix = uniqueKey.substring(0, uniqueKey.lastIndexOf('!'));\n          log.debug(\"shard( {} ) : uniqueKey( {} ) -> routePrefix( {} )\", shardId, uniqueKey, routePrefix);\n          assertNotNull(\"null prefix WTF? \" + uniqueKey, routePrefix);\n          \n          final String otherShard = routePrefixMap.put(routePrefix, shardId);\n          if (null != otherShard)\n            // if we already had a mapping, make sure it's an earlier doc from our current shard...\n            assertEquals(\"routePrefix \" + routePrefix + \" found in multiple shards\",\n                         shardId, otherShard);\n        }\n      }\n\n      assertEquals(\"Docs missing?\", expectedUniqueKeys.size(), actualUniqueKeys.size());\n      \n      testFinished = true;\n    } finally {\n      if (!testFinished) {\n        printLayoutOnTearDown = true;\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    boolean testFinished = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n\n      // todo: do I have to do this here?\n      waitForRecoveriesToFinish(true);\n\n      doTriLevelHashingTest();\n      del(\"*:*\");\n      commit();\n      doTriLevelHashingTestWithBitMask();\n\n      testFinished = true;\n    } finally {\n      if (!testFinished) {\n        printLayoutOnTearDown = true;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e02808bac106227463c64df6f39bb6bc0e71ea21","date":1554772649,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest#test().mjava","sourceNew":"  @AwaitsFix(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-13369\")\n  @Test\n  public void test() throws Exception {\n    boolean testFinished = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n\n      // todo: do I have to do this here?\n      waitForRecoveriesToFinish(true);\n\n      // NOTE: we might randomly generate the same uniqueKey value multiple times,\n      // (which is a valid test case, they should route to the same shard both times)\n      // so we track each expectedId in a set for later sanity checking\n      final Set<String> expectedUniqueKeys = new HashSet<>();\n      for (int i = 0; i < NUM_ADDS; i++) {\n        final int appId = r.nextInt(MAX_APP_ID) + 1;\n        final int userId = r.nextInt(MAX_USER_ID) + 1;\n        // skew the odds so half the time we have no mask, and half the time we\n        // have an even distribution of 1-16 bits\n        final int bitMask = Math.max(0, r.nextInt(32)-15);\n        \n        String id = \"app\" + appId + (bitMask <= 0 ? \"\" : (\"/\" + bitMask))\n          + \"!\" + \"user\" + userId\n          + \"!\" + \"doc\" + r.nextInt(MAX_DOC_ID);\n        \n        doAddDoc(id);\n        expectedUniqueKeys.add(id);\n      }\n      \n      commit();\n      \n      final Map<String, String> routePrefixMap = new HashMap<>();\n      final Set<String> actualUniqueKeys = new HashSet<>();\n      for (int i = 1; i <= sliceCount; i++) {\n        final String shardId = \"shard\" + i;\n        final Set<String> uniqueKeysInShard = fetchUniqueKeysFromShard(shardId);\n        \n        { // sanity check our uniqueKey values aren't duplicated across shards\n          final Set<String> uniqueKeysOnDuplicateShards = new HashSet<>(uniqueKeysInShard);\n          uniqueKeysOnDuplicateShards.retainAll(actualUniqueKeys);\n          assertEquals(shardId + \" contains some uniqueKeys that were already found on a previous shard\",\n                       Collections.emptySet(),  uniqueKeysOnDuplicateShards);\n          actualUniqueKeys.addAll(uniqueKeysInShard);\n        }\n        \n        // foreach uniqueKey, extract it's route prefix and confirm those aren't spread across multiple shards\n        for (String uniqueKey : uniqueKeysInShard) {\n          final String routePrefix = uniqueKey.substring(0, uniqueKey.lastIndexOf('!'));\n          log.debug(\"shard( {} ) : uniqueKey( {} ) -> routePrefix( {} )\", shardId, uniqueKey, routePrefix);\n          assertNotNull(\"null prefix WTF? \" + uniqueKey, routePrefix);\n          \n          final String otherShard = routePrefixMap.put(routePrefix, shardId);\n          if (null != otherShard)\n            // if we already had a mapping, make sure it's an earlier doc from our current shard...\n            assertEquals(\"routePrefix \" + routePrefix + \" found in multiple shards\",\n                         shardId, otherShard);\n        }\n      }\n\n      assertEquals(\"Docs missing?\", expectedUniqueKeys.size(), actualUniqueKeys.size());\n      \n      testFinished = true;\n    } finally {\n      if (!testFinished) {\n        printLayoutOnTearDown = true;\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    boolean testFinished = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n\n      // todo: do I have to do this here?\n      waitForRecoveriesToFinish(true);\n\n      // NOTE: we might randomly generate the same uniqueKey value multiple times,\n      // (which is a valid test case, they should route to the same shard both times)\n      // so we track each expectedId in a set for later sanity checking\n      final Set<String> expectedUniqueKeys = new HashSet<>();\n      for (int i = 0; i < NUM_ADDS; i++) {\n        final int appId = r.nextInt(MAX_APP_ID) + 1;\n        final int userId = r.nextInt(MAX_USER_ID) + 1;\n        // skew the odds so half the time we have no mask, and half the time we\n        // have an even distribution of 1-16 bits\n        final int bitMask = Math.max(0, r.nextInt(32)-15);\n        \n        String id = \"app\" + appId + (bitMask <= 0 ? \"\" : (\"/\" + bitMask))\n          + \"!\" + \"user\" + userId\n          + \"!\" + \"doc\" + r.nextInt(MAX_DOC_ID);\n        \n        doAddDoc(id);\n        expectedUniqueKeys.add(id);\n      }\n      \n      commit();\n      \n      final Map<String, String> routePrefixMap = new HashMap<>();\n      final Set<String> actualUniqueKeys = new HashSet<>();\n      for (int i = 1; i <= sliceCount; i++) {\n        final String shardId = \"shard\" + i;\n        final Set<String> uniqueKeysInShard = fetchUniqueKeysFromShard(shardId);\n        \n        { // sanity check our uniqueKey values aren't duplicated across shards\n          final Set<String> uniqueKeysOnDuplicateShards = new HashSet<>(uniqueKeysInShard);\n          uniqueKeysOnDuplicateShards.retainAll(actualUniqueKeys);\n          assertEquals(shardId + \" contains some uniqueKeys that were already found on a previous shard\",\n                       Collections.emptySet(),  uniqueKeysOnDuplicateShards);\n          actualUniqueKeys.addAll(uniqueKeysInShard);\n        }\n        \n        // foreach uniqueKey, extract it's route prefix and confirm those aren't spread across multiple shards\n        for (String uniqueKey : uniqueKeysInShard) {\n          final String routePrefix = uniqueKey.substring(0, uniqueKey.lastIndexOf('!'));\n          log.debug(\"shard( {} ) : uniqueKey( {} ) -> routePrefix( {} )\", shardId, uniqueKey, routePrefix);\n          assertNotNull(\"null prefix WTF? \" + uniqueKey, routePrefix);\n          \n          final String otherShard = routePrefixMap.put(routePrefix, shardId);\n          if (null != otherShard)\n            // if we already had a mapping, make sure it's an earlier doc from our current shard...\n            assertEquals(\"routePrefix \" + routePrefix + \" found in multiple shards\",\n                         shardId, otherShard);\n        }\n      }\n\n      assertEquals(\"Docs missing?\", expectedUniqueKeys.size(), actualUniqueKeys.size());\n      \n      testFinished = true;\n    } finally {\n      if (!testFinished) {\n        printLayoutOnTearDown = true;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e02808bac106227463c64df6f39bb6bc0e71ea21":["06e68e6090ed0bd8f1f9b96e03328d9a4591e451"],"abb23fcc2461782ab204e61213240feb77d355aa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0949f20e74c954865e5cfd653de9059bfc1a9aca":["8d35c84fdef07284c122012ca4000d3b7285a66e"],"06e68e6090ed0bd8f1f9b96e03328d9a4591e451":["0949f20e74c954865e5cfd653de9059bfc1a9aca"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e02808bac106227463c64df6f39bb6bc0e71ea21"],"8d35c84fdef07284c122012ca4000d3b7285a66e":["abb23fcc2461782ab204e61213240feb77d355aa"]},"commit2Childs":{"e02808bac106227463c64df6f39bb6bc0e71ea21":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"abb23fcc2461782ab204e61213240feb77d355aa":["8d35c84fdef07284c122012ca4000d3b7285a66e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["abb23fcc2461782ab204e61213240feb77d355aa"],"0949f20e74c954865e5cfd653de9059bfc1a9aca":["06e68e6090ed0bd8f1f9b96e03328d9a4591e451"],"06e68e6090ed0bd8f1f9b96e03328d9a4591e451":["e02808bac106227463c64df6f39bb6bc0e71ea21"],"8d35c84fdef07284c122012ca4000d3b7285a66e":["0949f20e74c954865e5cfd653de9059bfc1a9aca"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}