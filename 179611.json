{"path":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#maybeApplyDeletes(boolean).mjava","sourceNew":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    if (bufferedDeleteTerms.size() > 0) {\n      if (infoStream != null)\n        infoStream.println(\"flush \" + numBufferedDeleteTerms + \" buffered deleted terms on \"\n                           + segmentInfos.size() + \" segments.\");\n\n      if (flushedNewSegment) {\n        IndexReader reader = null;\n        try {\n          // Open readers w/o opening the stored fields /\n          // vectors because these files may still be held\n          // open for writing by docWriter\n          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n          // Apply delete terms to the segment just flushed from ram\n          // apply appropriately so that a delete term is only applied to\n          // the documents buffered before it, not those buffered after it.\n          applyDeletesSelectively(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      int infosEnd = segmentInfos.size();\n      if (flushedNewSegment) {\n        infosEnd--;\n      }\n\n      for (int i = 0; i < infosEnd; i++) {\n        IndexReader reader = null;\n        try {\n          reader = SegmentReader.get(segmentInfos.info(i), false);\n\n          // Apply delete terms to disk segments\n          // except the one just flushed from ram.\n          applyDeletes(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      // Clean up bufferedDeleteTerms.\n      bufferedDeleteTerms.clear();\n      numBufferedDeleteTerms = 0;\n    }\n  }\n\n","sourceOld":"  // Called during flush to apply any buffered deletes.  If\n  // doMerge is true then a new segment was just created and\n  // flushed from the ram segments.\n  private final void maybeApplyDeletes(boolean doMerge) throws CorruptIndexException, IOException {\n\n    if (bufferedDeleteTerms.size() > 0) {\n      if (infoStream != null)\n        infoStream.println(\"flush \" + numBufferedDeleteTerms + \" buffered deleted terms on \"\n                           + segmentInfos.size() + \" segments.\");\n\n      if (doMerge) {\n        IndexReader reader = null;\n        try {\n          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1));\n\n          // Apply delete terms to the segment just flushed from ram\n          // apply appropriately so that a delete term is only applied to\n          // the documents buffered before it, not those buffered after it.\n          applyDeletesSelectively(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      int infosEnd = segmentInfos.size();\n      if (doMerge) {\n        infosEnd--;\n      }\n\n      for (int i = 0; i < infosEnd; i++) {\n        IndexReader reader = null;\n        try {\n          reader = SegmentReader.get(segmentInfos.info(i));\n\n          // Apply delete terms to disk segments\n          // except the one just flushed from ram.\n          applyDeletes(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      // Clean up bufferedDeleteTerms.\n      bufferedDeleteTerms.clear();\n      numBufferedDeleteTerms = 0;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b","date":1184336627,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","sourceNew":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    if (bufferedDeleteTerms.size() > 0) {\n      if (infoStream != null)\n        infoStream.println(\"flush \" + numBufferedDeleteTerms + \" buffered deleted terms on \"\n                           + segmentInfos.size() + \" segments.\");\n\n      if (flushedNewSegment) {\n        IndexReader reader = null;\n        try {\n          // Open readers w/o opening the stored fields /\n          // vectors because these files may still be held\n          // open for writing by docWriter\n          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n          // Apply delete terms to the segment just flushed from ram\n          // apply appropriately so that a delete term is only applied to\n          // the documents buffered before it, not those buffered after it.\n          applyDeletesSelectively(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      int infosEnd = segmentInfos.size();\n      if (flushedNewSegment) {\n        infosEnd--;\n      }\n\n      for (int i = 0; i < infosEnd; i++) {\n        IndexReader reader = null;\n        try {\n          reader = SegmentReader.get(segmentInfos.info(i), false);\n\n          // Apply delete terms to disk segments\n          // except the one just flushed from ram.\n          applyDeletes(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      // Clean up bufferedDeleteTerms.\n\n      // Rollbacks of buffered deletes are based on restoring the old\n      // map, so don't modify this one. Rare enough that the gc\n      // overhead is almost certainly lower than the alternate, which\n      // would be clone to support rollback.\n\n      bufferedDeleteTerms = new HashMap();\n      numBufferedDeleteTerms = 0;\n    }\n  }\n\n","sourceOld":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    if (bufferedDeleteTerms.size() > 0) {\n      if (infoStream != null)\n        infoStream.println(\"flush \" + numBufferedDeleteTerms + \" buffered deleted terms on \"\n                           + segmentInfos.size() + \" segments.\");\n\n      if (flushedNewSegment) {\n        IndexReader reader = null;\n        try {\n          // Open readers w/o opening the stored fields /\n          // vectors because these files may still be held\n          // open for writing by docWriter\n          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n          // Apply delete terms to the segment just flushed from ram\n          // apply appropriately so that a delete term is only applied to\n          // the documents buffered before it, not those buffered after it.\n          applyDeletesSelectively(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      int infosEnd = segmentInfos.size();\n      if (flushedNewSegment) {\n        infosEnd--;\n      }\n\n      for (int i = 0; i < infosEnd; i++) {\n        IndexReader reader = null;\n        try {\n          reader = SegmentReader.get(segmentInfos.info(i), false);\n\n          // Apply delete terms to disk segments\n          // except the one just flushed from ram.\n          applyDeletes(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      // Clean up bufferedDeleteTerms.\n      bufferedDeleteTerms.clear();\n      numBufferedDeleteTerms = 0;\n    }\n  }\n\n","bugFix":["42a18cb0bca2c4ac9747f31c7a74fac90c661f39"],"bugIntro":["fde68de507dbf344495d7b5e8052866fe5f254ab"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fde68de507dbf344495d7b5e8052866fe5f254ab","date":1189434831,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","sourceNew":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final int applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();\n\n    int delCount = 0;\n    if (bufferedDeleteTerms.size() > 0) {\n      if (infoStream != null)\n        infoStream.println(\"flush \" + docWriter.getNumBufferedDeleteTerms() + \" buffered deleted terms on \"\n                           + segmentInfos.size() + \" segments.\");\n\n      if (flushedNewSegment) {\n        IndexReader reader = null;\n        try {\n          // Open readers w/o opening the stored fields /\n          // vectors because these files may still be held\n          // open for writing by docWriter\n          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n          // Apply delete terms to the segment just flushed from ram\n          // apply appropriately so that a delete term is only applied to\n          // the documents buffered before it, not those buffered after it.\n          delCount += applyDeletesSelectively(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      int infosEnd = segmentInfos.size();\n      if (flushedNewSegment) {\n        infosEnd--;\n      }\n\n      for (int i = 0; i < infosEnd; i++) {\n        IndexReader reader = null;\n        try {\n          reader = SegmentReader.get(segmentInfos.info(i), false);\n\n          // Apply delete terms to disk segments\n          // except the one just flushed from ram.\n          delCount += applyDeletes(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      // Clean up bufferedDeleteTerms.\n      docWriter.clearBufferedDeleteTerms();\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    if (bufferedDeleteTerms.size() > 0) {\n      if (infoStream != null)\n        infoStream.println(\"flush \" + numBufferedDeleteTerms + \" buffered deleted terms on \"\n                           + segmentInfos.size() + \" segments.\");\n\n      if (flushedNewSegment) {\n        IndexReader reader = null;\n        try {\n          // Open readers w/o opening the stored fields /\n          // vectors because these files may still be held\n          // open for writing by docWriter\n          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n          // Apply delete terms to the segment just flushed from ram\n          // apply appropriately so that a delete term is only applied to\n          // the documents buffered before it, not those buffered after it.\n          applyDeletesSelectively(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      int infosEnd = segmentInfos.size();\n      if (flushedNewSegment) {\n        infosEnd--;\n      }\n\n      for (int i = 0; i < infosEnd; i++) {\n        IndexReader reader = null;\n        try {\n          reader = SegmentReader.get(segmentInfos.info(i), false);\n\n          // Apply delete terms to disk segments\n          // except the one just flushed from ram.\n          applyDeletes(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      // Clean up bufferedDeleteTerms.\n\n      // Rollbacks of buffered deletes are based on restoring the old\n      // map, so don't modify this one. Rare enough that the gc\n      // overhead is almost certainly lower than the alternate, which\n      // would be clone to support rollback.\n\n      bufferedDeleteTerms = new HashMap();\n      numBufferedDeleteTerms = 0;\n    }\n  }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c","abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b","42a18cb0bca2c4ac9747f31c7a74fac90c661f39"],"bugIntro":["5a251aa47d1808cbae42c0e172d698c377430e60"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","sourceNew":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final int applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();\n\n    int delCount = 0;\n    if (bufferedDeleteTerms.size() > 0) {\n      if (infoStream != null)\n        message(\"flush \" + docWriter.getNumBufferedDeleteTerms() + \" buffered deleted terms on \"\n                + segmentInfos.size() + \" segments.\");\n\n      if (flushedNewSegment) {\n        IndexReader reader = null;\n        try {\n          // Open readers w/o opening the stored fields /\n          // vectors because these files may still be held\n          // open for writing by docWriter\n          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n          // Apply delete terms to the segment just flushed from ram\n          // apply appropriately so that a delete term is only applied to\n          // the documents buffered before it, not those buffered after it.\n          delCount += applyDeletesSelectively(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      int infosEnd = segmentInfos.size();\n      if (flushedNewSegment) {\n        infosEnd--;\n      }\n\n      for (int i = 0; i < infosEnd; i++) {\n        IndexReader reader = null;\n        try {\n          reader = SegmentReader.get(segmentInfos.info(i), false);\n\n          // Apply delete terms to disk segments\n          // except the one just flushed from ram.\n          delCount += applyDeletes(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      // Clean up bufferedDeleteTerms.\n      docWriter.clearBufferedDeleteTerms();\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final int applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();\n\n    int delCount = 0;\n    if (bufferedDeleteTerms.size() > 0) {\n      if (infoStream != null)\n        infoStream.println(\"flush \" + docWriter.getNumBufferedDeleteTerms() + \" buffered deleted terms on \"\n                           + segmentInfos.size() + \" segments.\");\n\n      if (flushedNewSegment) {\n        IndexReader reader = null;\n        try {\n          // Open readers w/o opening the stored fields /\n          // vectors because these files may still be held\n          // open for writing by docWriter\n          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n          // Apply delete terms to the segment just flushed from ram\n          // apply appropriately so that a delete term is only applied to\n          // the documents buffered before it, not those buffered after it.\n          delCount += applyDeletesSelectively(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      int infosEnd = segmentInfos.size();\n      if (flushedNewSegment) {\n        infosEnd--;\n      }\n\n      for (int i = 0; i < infosEnd; i++) {\n        IndexReader reader = null;\n        try {\n          reader = SegmentReader.get(segmentInfos.info(i), false);\n\n          // Apply delete terms to disk segments\n          // except the one just flushed from ram.\n          delCount += applyDeletes(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      // Clean up bufferedDeleteTerms.\n      docWriter.clearBufferedDeleteTerms();\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":["5a251aa47d1808cbae42c0e172d698c377430e60"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a251aa47d1808cbae42c0e172d698c377430e60","date":1199375390,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","sourceNew":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();\n    final List bufferedDeleteDocIDs = docWriter.getBufferedDeleteDocIDs();\n\n    if (infoStream != null)\n      message(\"flush \" + docWriter.getNumBufferedDeleteTerms() + \" buffered deleted terms and \" +\n              bufferedDeleteDocIDs.size() + \" deleted docIDs on \"\n              + segmentInfos.size() + \" segments.\");\n\n    if (flushedNewSegment) {\n      IndexReader reader = null;\n      try {\n        // Open readers w/o opening the stored fields /\n        // vectors because these files may still be held\n        // open for writing by docWriter\n        reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n        // Apply delete terms to the segment just flushed from ram\n        // apply appropriately so that a delete term is only applied to\n        // the documents buffered before it, not those buffered after it.\n        applyDeletesSelectively(bufferedDeleteTerms, bufferedDeleteDocIDs, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n\n    int infosEnd = segmentInfos.size();\n    if (flushedNewSegment) {\n      infosEnd--;\n    }\n\n    for (int i = 0; i < infosEnd; i++) {\n      IndexReader reader = null;\n      try {\n        reader = SegmentReader.get(segmentInfos.info(i), false);\n\n        // Apply delete terms to disk segments\n        // except the one just flushed from ram.\n        applyDeletes(bufferedDeleteTerms, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n\n    // Clean up bufferedDeleteTerms.\n    docWriter.clearBufferedDeletes();\n  }\n\n","sourceOld":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final int applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();\n\n    int delCount = 0;\n    if (bufferedDeleteTerms.size() > 0) {\n      if (infoStream != null)\n        message(\"flush \" + docWriter.getNumBufferedDeleteTerms() + \" buffered deleted terms on \"\n                + segmentInfos.size() + \" segments.\");\n\n      if (flushedNewSegment) {\n        IndexReader reader = null;\n        try {\n          // Open readers w/o opening the stored fields /\n          // vectors because these files may still be held\n          // open for writing by docWriter\n          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n          // Apply delete terms to the segment just flushed from ram\n          // apply appropriately so that a delete term is only applied to\n          // the documents buffered before it, not those buffered after it.\n          delCount += applyDeletesSelectively(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      int infosEnd = segmentInfos.size();\n      if (flushedNewSegment) {\n        infosEnd--;\n      }\n\n      for (int i = 0; i < infosEnd; i++) {\n        IndexReader reader = null;\n        try {\n          reader = SegmentReader.get(segmentInfos.info(i), false);\n\n          // Apply delete terms to disk segments\n          // except the one just flushed from ram.\n          delCount += applyDeletes(bufferedDeleteTerms, reader);\n        } finally {\n          if (reader != null) {\n            try {\n              reader.doCommit();\n            } finally {\n              reader.doClose();\n            }\n          }\n        }\n      }\n\n      // Clean up bufferedDeleteTerms.\n      docWriter.clearBufferedDeleteTerms();\n    }\n\n    return delCount;\n  }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c","b1405362241b561f5590ff4a87d5d6e173bcd9cf","d72db039743bd6a2da9be6306f57c71654ca1bf6","42a18cb0bca2c4ac9747f31c7a74fac90c661f39","fde68de507dbf344495d7b5e8052866fe5f254ab"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e82780afe6097066eb5befb86e9432f077667e3d","date":1202756169,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(SegmentInfo).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","sourceNew":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final void applyDeletes(SegmentInfo newSegment) throws CorruptIndexException, IOException {\n\n    final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();\n    final List bufferedDeleteDocIDs = docWriter.getBufferedDeleteDocIDs();\n\n    if (infoStream != null)\n      message(\"flush \" + docWriter.getNumBufferedDeleteTerms() + \" buffered deleted terms and \" +\n              bufferedDeleteDocIDs.size() + \" deleted docIDs on \"\n              + segmentInfos.size() + \" segments.\");\n\n    if (newSegment != null) {\n      IndexReader reader = null;\n      try {\n        // Open readers w/o opening the stored fields /\n        // vectors because these files may still be held\n        // open for writing by docWriter\n        reader = SegmentReader.get(newSegment, false);\n\n        // Apply delete terms to the segment just flushed from ram\n        // apply appropriately so that a delete term is only applied to\n        // the documents buffered before it, not those buffered after it.\n        applyDeletesSelectively(bufferedDeleteTerms, bufferedDeleteDocIDs, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n\n    final int infosEnd = segmentInfos.size();\n\n    for (int i = 0; i < infosEnd; i++) {\n      IndexReader reader = null;\n      try {\n        reader = SegmentReader.get(segmentInfos.info(i), false);\n\n        // Apply delete terms to disk segments\n        // except the one just flushed from ram.\n        applyDeletes(bufferedDeleteTerms, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();\n    final List bufferedDeleteDocIDs = docWriter.getBufferedDeleteDocIDs();\n\n    if (infoStream != null)\n      message(\"flush \" + docWriter.getNumBufferedDeleteTerms() + \" buffered deleted terms and \" +\n              bufferedDeleteDocIDs.size() + \" deleted docIDs on \"\n              + segmentInfos.size() + \" segments.\");\n\n    if (flushedNewSegment) {\n      IndexReader reader = null;\n      try {\n        // Open readers w/o opening the stored fields /\n        // vectors because these files may still be held\n        // open for writing by docWriter\n        reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n        // Apply delete terms to the segment just flushed from ram\n        // apply appropriately so that a delete term is only applied to\n        // the documents buffered before it, not those buffered after it.\n        applyDeletesSelectively(bufferedDeleteTerms, bufferedDeleteDocIDs, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n\n    int infosEnd = segmentInfos.size();\n    if (flushedNewSegment) {\n      infosEnd--;\n    }\n\n    for (int i = 0; i < infosEnd; i++) {\n      IndexReader reader = null;\n      try {\n        reader = SegmentReader.get(segmentInfos.info(i), false);\n\n        // Apply delete terms to disk segments\n        // except the one just flushed from ram.\n        applyDeletes(bufferedDeleteTerms, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n\n    // Clean up bufferedDeleteTerms.\n    docWriter.clearBufferedDeletes();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["fde68de507dbf344495d7b5e8052866fe5f254ab"],"fde68de507dbf344495d7b5e8052866fe5f254ab":["abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5a251aa47d1808cbae42c0e172d698c377430e60":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"e82780afe6097066eb5befb86e9432f077667e3d":["5a251aa47d1808cbae42c0e172d698c377430e60"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e82780afe6097066eb5befb86e9432f077667e3d"]},"commit2Childs":{"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["5a251aa47d1808cbae42c0e172d698c377430e60"],"fde68de507dbf344495d7b5e8052866fe5f254ab":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"5a251aa47d1808cbae42c0e172d698c377430e60":["e82780afe6097066eb5befb86e9432f077667e3d"],"abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b":["fde68de507dbf344495d7b5e8052866fe5f254ab"],"e82780afe6097066eb5befb86e9432f077667e3d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}