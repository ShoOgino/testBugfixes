{"path":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","commits":[{"id":"043c298cb215f13ba7b9b81d20760704e8f93d66","date":1107566743,"type":1,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"sandbox/contributions/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous)\n    {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream\n        {\n            Token tokens[];\n            int currentToken=0;\n            StoredTokenStream(Token tokens[])\n            {\n                this.tokens=tokens;\n            }\n            public Token next() throws IOException\n            {\n                if(currentToken>=tokens.length)\n                {\n                    return null;\n                }\n                return tokens[currentToken++];\n            }            \n        }        \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                    unsortedTokens.add(new Token(terms[t],\n                        offsets[tp].getStartOffset(),\n                        offsets[tp].getEndOffset()));\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                    tokensInOriginalOrder[pos[tp]]=new Token(terms[t],\n                            offsets[tp].getStartOffset(),\n                            offsets[tp].getEndOffset());\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.startOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous)\n    {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream\n        {\n            Token tokens[];\n            int currentToken=0;\n            StoredTokenStream(Token tokens[])\n            {\n                this.tokens=tokens;\n            }\n            public Token next() throws IOException\n            {\n                if(currentToken>=tokens.length)\n                {\n                    return null;\n                }\n                return tokens[currentToken++];\n            }            \n        }        \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                    unsortedTokens.add(new Token(terms[t],\n                        offsets[tp].getStartOffset(),\n                        offsets[tp].getEndOffset()));\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                    tokensInOriginalOrder[pos[tp]]=new Token(terms[t],\n                            offsets[tp].getStartOffset(),\n                            offsets[tp].getEndOffset());\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.startOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e6ec63b41d741f5323f34c2820265518550b50dd","date":1108585215,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous)\n    {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream\n        {\n            Token tokens[];\n            int currentToken=0;\n            StoredTokenStream(Token tokens[])\n            {\n                this.tokens=tokens;\n            }\n            public Token next()\n            {\n                if(currentToken>=tokens.length)\n                {\n                    return null;\n                }\n                return tokens[currentToken++];\n            }            \n        }        \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                    unsortedTokens.add(new Token(terms[t],\n                        offsets[tp].getStartOffset(),\n                        offsets[tp].getEndOffset()));\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                    tokensInOriginalOrder[pos[tp]]=new Token(terms[t],\n                            offsets[tp].getStartOffset(),\n                            offsets[tp].getEndOffset());\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.startOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous)\n    {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream\n        {\n            Token tokens[];\n            int currentToken=0;\n            StoredTokenStream(Token tokens[])\n            {\n                this.tokens=tokens;\n            }\n            public Token next() throws IOException\n            {\n                if(currentToken>=tokens.length)\n                {\n                    return null;\n                }\n                return tokens[currentToken++];\n            }            \n        }        \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                    unsortedTokens.add(new Token(terms[t],\n                        offsets[tp].getStartOffset(),\n                        offsets[tp].getEndOffset()));\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                    tokensInOriginalOrder[pos[tp]]=new Token(terms[t],\n                            offsets[tp].getStartOffset(),\n                            offsets[tp].getEndOffset());\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.startOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous)\n    {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream\n        {\n            Token tokens[];\n            int currentToken=0;\n            StoredTokenStream(Token tokens[])\n            {\n                this.tokens=tokens;\n            }\n            public Token next(final Token reusableToken)\n            {\n                assert reusableToken != null;\n                if(currentToken>=tokens.length)\n                {\n                    return null;\n                }\n                return tokens[currentToken++];\n            }            \n        }        \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n        Token newToken = new Token();\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  newToken.reinit(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  unsortedTokens.add(newToken.clone());\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  newToken.reinit(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = (Token) newToken.clone();\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.startOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous)\n    {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream\n        {\n            Token tokens[];\n            int currentToken=0;\n            StoredTokenStream(Token tokens[])\n            {\n                this.tokens=tokens;\n            }\n            public Token next()\n            {\n                if(currentToken>=tokens.length)\n                {\n                    return null;\n                }\n                return tokens[currentToken++];\n            }            \n        }        \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                    unsortedTokens.add(new Token(terms[t],\n                        offsets[tp].getStartOffset(),\n                        offsets[tp].getEndOffset()));\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                    tokensInOriginalOrder[pos[tp]]=new Token(terms[t],\n                            offsets[tp].getStartOffset(),\n                            offsets[tp].getEndOffset());\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.startOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b4471b2ef75c0e11869f60b23cabe292b895c3ee","date":1248991247,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n            offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          }\n    \n          public Token next(final Token reusableToken) {\n            System.out.println(\"next token\");\n            assert reusableToken != null;\n            if (currentToken >= tokens.length) {\n              return null;\n            }\n            return tokens[currentToken++];\n          }\n    \n          public boolean incrementToken() throws IOException {\n            System.out.println(\"inc token\");\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous)\n    {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream\n        {\n            Token tokens[];\n            int currentToken=0;\n            StoredTokenStream(Token tokens[])\n            {\n                this.tokens=tokens;\n            }\n            public Token next(final Token reusableToken)\n            {\n                assert reusableToken != null;\n                if(currentToken>=tokens.length)\n                {\n                    return null;\n                }\n                return tokens[currentToken++];\n            }            \n        }        \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n        Token newToken = new Token();\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  newToken.reinit(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  unsortedTokens.add(newToken.clone());\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  newToken.reinit(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = (Token) newToken.clone();\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.startOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c8352a07236bd88b3963b4678e13d75b9e87da34","date":1249136299,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n            offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          }\n    \n          public boolean incrementToken() throws IOException {\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n            offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          }\n    \n          public Token next(final Token reusableToken) {\n            System.out.println(\"next token\");\n            assert reusableToken != null;\n            if (currentToken >= tokens.length) {\n              return null;\n            }\n            return tokens[currentToken++];\n          }\n    \n          public boolean incrementToken() throws IOException {\n            System.out.println(\"inc token\");\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"add7d922e63099fbce8f0a1b31216df7ef5067f1","date":1252002701,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster due to compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n            offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          }\n    \n          public boolean incrementToken() throws IOException {\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n            offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          }\n    \n          public boolean incrementToken() throws IOException {\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster due to compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = addAttribute(TermAttribute.class);\n            offsetAtt = addAttribute(OffsetAttribute.class);\n          }\n    \n          public boolean incrementToken() throws IOException {\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster due to compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n            offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n          }\n    \n          public boolean incrementToken() throws IOException {\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ffdf794cee8d43eb612df752c592cef2dc3e75ae","date":1256465578,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster due to compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = addAttribute(TermAttribute.class);\n            offsetAtt = addAttribute(OffsetAttribute.class);\n          }\n    \n          public boolean incrementToken() throws IOException {\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList<Token> unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList<Token>();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null) {\n            tokensInOriginalOrder= unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator<Token>(){\n                public int compare(Token t1, Token t2) {\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster due to compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = addAttribute(TermAttribute.class);\n            offsetAtt = addAttribute(OffsetAttribute.class);\n          }\n    \n          public boolean incrementToken() throws IOException {\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d57eb7c98c08c03af6e4cd83509df31c81ac16af","date":1257684312,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster due to compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = addAttribute(TermAttribute.class);\n            offsetAtt = addAttribute(OffsetAttribute.class);\n          }\n    \n          @Override\n          public boolean incrementToken() throws IOException {\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList<Token> unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList<Token>();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null) {\n            tokensInOriginalOrder= unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator<Token>(){\n                public int compare(Token t1, Token t2) {\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster due to compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = addAttribute(TermAttribute.class);\n            offsetAtt = addAttribute(OffsetAttribute.class);\n          }\n    \n          public boolean incrementToken() throws IOException {\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList<Token> unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList<Token>();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null) {\n            tokensInOriginalOrder= unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator<Token>(){\n                public int compare(Token t1, Token t2) {\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fafd002a407d38098f1f0edf4365f971102ae0ef","date":1262804916,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      TermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(TermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        termAtt.setTermBuffer(token.term());\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          token.setTermBuffer(terms[t]);\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster due to compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous) {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream {\n          Token tokens[];\n          int currentToken = 0;\n          TermAttribute termAtt;\n          OffsetAttribute offsetAtt;\n    \n          StoredTokenStream(Token tokens[]) {\n            this.tokens = tokens;\n            termAtt = addAttribute(TermAttribute.class);\n            offsetAtt = addAttribute(OffsetAttribute.class);\n          }\n    \n          @Override\n          public boolean incrementToken() throws IOException {\n            if (currentToken >= tokens.length) {\n              return false;\n            }\n            Token token = tokens[currentToken++];\n            termAtt.setTermBuffer(token.term());\n            offsetAtt.setOffset(token.startOffset(), token.endOffset());\n            return true;\n          }\n        }      \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList<Token> unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList<Token>();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                  Token token = new Token(offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  token.setTermBuffer(terms[t]);\n                  unsortedTokens.add(token);\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                  Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n                  tokensInOriginalOrder[pos[tp]] = token;\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null) {\n            tokensInOriginalOrder= unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator<Token>(){\n                public int compare(Token t1, Token t2) {\n                    if(t1.startOffset()>t2.endOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":["be4c30aa8665022511179534b83b929f65ec86f5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"360d15dc189fb48153cb62234f7d20819e4e292e","date":1263562938,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      TermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(TermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setTermBuffer(token.term());\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          token.setTermBuffer(terms[t]);\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      TermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(TermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        termAtt.setTermBuffer(token.term());\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          token.setTermBuffer(terms[t]);\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      TermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(TermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setTermBuffer(token.term());\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          token.setTermBuffer(terms[t]);\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      TermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(TermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setTermBuffer(token.term());\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          token.setTermBuffer(terms[t]);\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["e6ec63b41d741f5323f34c2820265518550b50dd"],"043c298cb215f13ba7b9b81d20760704e8f93d66":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c8352a07236bd88b3963b4678e13d75b9e87da34":["b4471b2ef75c0e11869f60b23cabe292b895c3ee"],"e6ec63b41d741f5323f34c2820265518550b50dd":["043c298cb215f13ba7b9b81d20760704e8f93d66"],"d57eb7c98c08c03af6e4cd83509df31c81ac16af":["ffdf794cee8d43eb612df752c592cef2dc3e75ae"],"360d15dc189fb48153cb62234f7d20819e4e292e":["fafd002a407d38098f1f0edf4365f971102ae0ef"],"ffdf794cee8d43eb612df752c592cef2dc3e75ae":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"fafd002a407d38098f1f0edf4365f971102ae0ef":["d57eb7c98c08c03af6e4cd83509df31c81ac16af"],"add7d922e63099fbce8f0a1b31216df7ef5067f1":["c8352a07236bd88b3963b4678e13d75b9e87da34"],"b4471b2ef75c0e11869f60b23cabe292b895c3ee":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8d78f014fded44fbde905f4f84cdc21907b371e8":["add7d922e63099fbce8f0a1b31216df7ef5067f1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["360d15dc189fb48153cb62234f7d20819e4e292e"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["b4471b2ef75c0e11869f60b23cabe292b895c3ee"],"043c298cb215f13ba7b9b81d20760704e8f93d66":["e6ec63b41d741f5323f34c2820265518550b50dd"],"c8352a07236bd88b3963b4678e13d75b9e87da34":["add7d922e63099fbce8f0a1b31216df7ef5067f1"],"e6ec63b41d741f5323f34c2820265518550b50dd":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"d57eb7c98c08c03af6e4cd83509df31c81ac16af":["fafd002a407d38098f1f0edf4365f971102ae0ef"],"360d15dc189fb48153cb62234f7d20819e4e292e":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"ffdf794cee8d43eb612df752c592cef2dc3e75ae":["d57eb7c98c08c03af6e4cd83509df31c81ac16af"],"fafd002a407d38098f1f0edf4365f971102ae0ef":["360d15dc189fb48153cb62234f7d20819e4e292e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["043c298cb215f13ba7b9b81d20760704e8f93d66"],"b4471b2ef75c0e11869f60b23cabe292b895c3ee":["c8352a07236bd88b3963b4678e13d75b9e87da34"],"add7d922e63099fbce8f0a1b31216df7ef5067f1":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["ffdf794cee8d43eb612df752c592cef2dc3e75ae"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}