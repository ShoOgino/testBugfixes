{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ).</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public synchronized void deleteAll() throws IOException {\n    ensureOpen();\n    boolean success = false;\n    try {\n\n      // Abort any running merges\n      finishMerges(false);\n\n      // Remove any buffered docs\n      docWriter.abort();\n\n      // Remove all segments\n      segmentInfos.clear();\n\n      // Ask deleter to locate unreferenced files & remove them:\n      deleter.checkpoint(segmentInfos, false);\n      deleter.refresh();\n\n      // Don't bother saving any changes in our segmentInfos\n      readerPool.dropAll(false);\n\n      // Mark that the index has changed\n      ++changeCount;\n      segmentInfos.changed();\n      success = true;\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"deleteAll\");\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during deleteAll\");\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ).</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public synchronized void deleteAll() throws IOException {\n    ensureOpen();\n    boolean success = false;\n    try {\n\n      // Abort any running merges\n      finishMerges(false);\n\n      // Remove any buffered docs\n      docWriter.abort();\n\n      // Remove all segments\n      segmentInfos.clear();\n\n      // Ask deleter to locate unreferenced files & remove them:\n      deleter.checkpoint(segmentInfos, false);\n      deleter.refresh();\n\n      // Don't bother saving any changes in our segmentInfos\n      readerPool.dropAll(false);\n\n      // Mark that the index has changed\n      ++changeCount;\n      segmentInfos.changed();\n      success = true;\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"deleteAll\");\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during deleteAll\");\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e2893fd5349134af382d33ccc3d84840394c6c1","date":1353682567,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ).</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public synchronized void deleteAll() throws IOException {\n    ensureOpen();\n    boolean success = false;\n    try {\n\n      // Abort any running merges\n      finishMerges(false);\n\n      // Remove any buffered docs\n      docWriter.abort();\n\n      // Remove all segments\n      segmentInfos.clear();\n\n      // Ask deleter to locate unreferenced files & remove them:\n      deleter.checkpoint(segmentInfos, false);\n      deleter.refresh();\n\n      globalFieldNumberMap.clear();\n\n      // Don't bother saving any changes in our segmentInfos\n      readerPool.dropAll(false);\n\n      // Mark that the index has changed\n      ++changeCount;\n      segmentInfos.changed();\n      success = true;\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"deleteAll\");\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during deleteAll\");\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ).</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public synchronized void deleteAll() throws IOException {\n    ensureOpen();\n    boolean success = false;\n    try {\n\n      // Abort any running merges\n      finishMerges(false);\n\n      // Remove any buffered docs\n      docWriter.abort();\n\n      // Remove all segments\n      segmentInfos.clear();\n\n      // Ask deleter to locate unreferenced files & remove them:\n      deleter.checkpoint(segmentInfos, false);\n      deleter.refresh();\n\n      // Don't bother saving any changes in our segmentInfos\n      readerPool.dropAll(false);\n\n      // Mark that the index has changed\n      ++changeCount;\n      segmentInfos.changed();\n      success = true;\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"deleteAll\");\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during deleteAll\");\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["a4278fc65afbb35739525c37f818cded6fe6e9ae"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ).</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public synchronized void deleteAll() throws IOException {\n    ensureOpen();\n    boolean success = false;\n    try {\n\n      // Abort any running merges\n      finishMerges(false);\n\n      // Remove any buffered docs\n      docWriter.abort();\n\n      // Remove all segments\n      segmentInfos.clear();\n\n      // Ask deleter to locate unreferenced files & remove them:\n      deleter.checkpoint(segmentInfos, false);\n      deleter.refresh();\n\n      globalFieldNumberMap.clear();\n\n      // Don't bother saving any changes in our segmentInfos\n      readerPool.dropAll(false);\n\n      // Mark that the index has changed\n      ++changeCount;\n      segmentInfos.changed();\n      success = true;\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"deleteAll\");\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during deleteAll\");\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ).</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public synchronized void deleteAll() throws IOException {\n    ensureOpen();\n    boolean success = false;\n    try {\n\n      // Abort any running merges\n      finishMerges(false);\n\n      // Remove any buffered docs\n      docWriter.abort();\n\n      // Remove all segments\n      segmentInfos.clear();\n\n      // Ask deleter to locate unreferenced files & remove them:\n      deleter.checkpoint(segmentInfos, false);\n      deleter.refresh();\n\n      // Don't bother saving any changes in our segmentInfos\n      readerPool.dropAll(false);\n\n      // Mark that the index has changed\n      ++changeCount;\n      segmentInfos.changed();\n      success = true;\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"deleteAll\");\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during deleteAll\");\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a4278fc65afbb35739525c37f818cded6fe6e9ae","date":1369132128,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ). \n   *    Yet, this method also has different semantics compared to {@link #deleteDocuments(Query)} \n   *    / {@link #deleteDocuments(Query...)} since internal data-structures are cleared as well \n   *    as all segment information is forcefully dropped anti-viral semantics like omitting norms\n   *    are reset or doc value types are cleared. Essentially a call to {@link #deleteAll()} is equivalent\n   *    to creating a new {@link IndexWriter} with {@link OpenMode#CREATE} which a delete query only marks\n   *    documents as deleted.</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll();\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ).</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public synchronized void deleteAll() throws IOException {\n    ensureOpen();\n    boolean success = false;\n    try {\n\n      // Abort any running merges\n      finishMerges(false);\n\n      // Remove any buffered docs\n      docWriter.abort();\n\n      // Remove all segments\n      segmentInfos.clear();\n\n      // Ask deleter to locate unreferenced files & remove them:\n      deleter.checkpoint(segmentInfos, false);\n      deleter.refresh();\n\n      globalFieldNumberMap.clear();\n\n      // Don't bother saving any changes in our segmentInfos\n      readerPool.dropAll(false);\n\n      // Mark that the index has changed\n      ++changeCount;\n      segmentInfos.changed();\n      success = true;\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"deleteAll\");\n    } finally {\n      if (!success) {\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"hit exception during deleteAll\");\n        }\n      }\n    }\n  }\n\n","bugFix":["7e0644fd1d7df35f8fa936857eb37b7d3c92aff5","9ce667c6d3400b22523701c549c0d35e26da8b46","6e2893fd5349134af382d33ccc3d84840394c6c1","7b91922b55d15444d554721b352861d028eb8278","c5df35ab57c223ea11aec64b53bf611904f3dced","c5947ccd7ba3770dcba5a0713dbd5496678256d9"],"bugIntro":["25e07bf0d9fa18cd8f0185e309d09a873c45017c","901d103ab7c2eeae92b111fc91bb1b00580a3fd7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7af110b00ea8df9429309d83e38e0533d82e144f","date":1376924768,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ). \n   *    Yet, this method also has different semantics compared to {@link #deleteDocuments(Query)} \n   *    / {@link #deleteDocuments(Query...)} since internal data-structures are cleared as well \n   *    as all segment information is forcefully dropped anti-viral semantics like omitting norms\n   *    are reset or doc value types are cleared. Essentially a call to {@link #deleteAll()} is equivalent\n   *    to creating a new {@link IndexWriter} with {@link OpenMode#CREATE} which a delete query only marks\n   *    documents as deleted.</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ). \n   *    Yet, this method also has different semantics compared to {@link #deleteDocuments(Query)} \n   *    / {@link #deleteDocuments(Query...)} since internal data-structures are cleared as well \n   *    as all segment information is forcefully dropped anti-viral semantics like omitting norms\n   *    are reset or doc value types are cleared. Essentially a call to {@link #deleteAll()} is equivalent\n   *    to creating a new {@link IndexWriter} with {@link OpenMode#CREATE} which a delete query only marks\n   *    documents as deleted.</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll();\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e","901d103ab7c2eeae92b111fc91bb1b00580a3fd7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"31d4861802ca404d78ca1d15f4550eec415b9199","date":1376947894,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ). \n   *    Yet, this method also has different semantics compared to {@link #deleteDocuments(Query)} \n   *    / {@link #deleteDocuments(Query...)} since internal data-structures are cleared as well \n   *    as all segment information is forcefully dropped anti-viral semantics like omitting norms\n   *    are reset or doc value types are cleared. Essentially a call to {@link #deleteAll()} is equivalent\n   *    to creating a new {@link IndexWriter} with {@link OpenMode#CREATE} which a delete query only marks\n   *    documents as deleted.</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ). \n   *    Yet, this method also has different semantics compared to {@link #deleteDocuments(Query)} \n   *    / {@link #deleteDocuments(Query...)} since internal data-structures are cleared as well \n   *    as all segment information is forcefully dropped anti-viral semantics like omitting norms\n   *    are reset or doc value types are cleared. Essentially a call to {@link #deleteAll()} is equivalent\n   *    to creating a new {@link IndexWriter} with {@link OpenMode#CREATE} which a delete query only marks\n   *    documents as deleted.</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll();\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ). \n   *    Yet, this method also has different semantics compared to {@link #deleteDocuments(Query)} \n   *    / {@link #deleteDocuments(Query...)} since internal data-structures are cleared as well \n   *    as all segment information is forcefully dropped anti-viral semantics like omitting norms\n   *    are reset or doc value types are cleared. Essentially a call to {@link #deleteAll()} is equivalent\n   *    to creating a new {@link IndexWriter} with {@link OpenMode#CREATE} which a delete query only marks\n   *    documents as deleted.</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ). \n   *    Yet, this method also has different semantics compared to {@link #deleteDocuments(Query)} \n   *    / {@link #deleteDocuments(Query...)} since internal data-structures are cleared as well \n   *    as all segment information is forcefully dropped anti-viral semantics like omitting norms\n   *    are reset or doc value types are cleared. Essentially a call to {@link #deleteAll()} is equivalent\n   *    to creating a new {@link IndexWriter} with {@link OpenMode#CREATE} which a delete query only marks\n   *    documents as deleted.</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll();\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c3762f22fc1bf57957efeaf962d3393a5bb1b151","date":1400601035,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(IndexReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ). \n   *    Yet, this method also has different semantics compared to {@link #deleteDocuments(Query)} \n   *    / {@link #deleteDocuments(Query...)} since internal data-structures are cleared as well \n   *    as all segment information is forcefully dropped anti-viral semantics like omitting norms\n   *    are reset or doc value types are cleared. Essentially a call to {@link #deleteAll()} is equivalent\n   *    to creating a new {@link IndexWriter} with {@link OpenMode#CREATE} which a delete query only marks\n   *    documents as deleted.</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(IndexReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   *\n   * <p>This method will drop all buffered documents and will\n   *    remove all segments from the index. This change will not be\n   *    visible until a {@link #commit()} has been called. This method\n   *    can be rolled back using {@link #rollback()}.</p>\n   *\n   * <p>NOTE: this method is much faster than using deleteDocuments( new MatchAllDocsQuery() ). \n   *    Yet, this method also has different semantics compared to {@link #deleteDocuments(Query)} \n   *    / {@link #deleteDocuments(Query...)} since internal data-structures are cleared as well \n   *    as all segment information is forcefully dropped anti-viral semantics like omitting norms\n   *    are reset or doc value types are cleared. Essentially a call to {@link #deleteAll()} is equivalent\n   *    to creating a new {@link IndexWriter} with {@link OpenMode#CREATE} which a delete query only marks\n   *    documents as deleted.</p>\n   *\n   * <p>NOTE: this method will forcefully abort all merges\n   *    in progress.  If other threads are running {@link\n   *    #forceMerge}, {@link #addIndexes(IndexReader[])} or\n   *    {@link #forceMergeDeletes} methods, they may receive\n   *    {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a64ffebe62143a8d4c37d99b6ece6d430d948ebc","date":1408382164,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(IndexReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(IndexReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            finishMerges(false);\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"949847c0040cd70a68222d526cb0da7bf6cbb3c2","date":1410997182,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(IndexReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            tragicEvent(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(IndexReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            handleOOM(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["25e07bf0d9fa18cd8f0185e309d09a873c45017c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"25e07bf0d9fa18cd8f0185e309d09a873c45017c","date":1411478085,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(IndexReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n      /*\n       * We first abort and trash everything we have in-memory\n       * and keep the thread-states locked, the lockAndAbortAll operation\n       * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n       * of logical happens-before relationship in the DW. So we do\n       * abort all in memory structures \n       * We also drop global field numbering before during abort to make\n       * sure it's just like a fresh index.\n       */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } catch (OutOfMemoryError oom) {\n        tragicEvent(oom, \"deleteAll\");\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(IndexReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n        /*\n         * We first abort and trash everything we have in-memory\n         * and keep the thread-states locked, the lockAndAbortAll operation\n         * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n         * of logical happens-before relationship in the DW. So we do\n         * abort all in memory structures \n         * We also drop global field numbering before during abort to make\n         * sure it's just like a fresh index.\n         */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } catch (OutOfMemoryError oom) {\n            tragicEvent(oom, \"deleteAll\");\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","bugFix":["949847c0040cd70a68222d526cb0da7bf6cbb3c2","a4278fc65afbb35739525c37f818cded6fe6e9ae"],"bugIntro":["901d103ab7c2eeae92b111fc91bb1b00580a3fd7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","date":1420599177,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(LeafReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n      /*\n       * We first abort and trash everything we have in-memory\n       * and keep the thread-states locked, the lockAndAbortAll operation\n       * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n       * of logical happens-before relationship in the DW. So we do\n       * abort all in memory structures \n       * We also drop global field numbering before during abort to make\n       * sure it's just like a fresh index.\n       */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } catch (OutOfMemoryError oom) {\n        tragicEvent(oom, \"deleteAll\");\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(IndexReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n      /*\n       * We first abort and trash everything we have in-memory\n       * and keep the thread-states locked, the lockAndAbortAll operation\n       * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n       * of logical happens-before relationship in the DW. So we do\n       * abort all in memory structures \n       * We also drop global field numbering before during abort to make\n       * sure it's just like a fresh index.\n       */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } catch (OutOfMemoryError oom) {\n        tragicEvent(oom, \"deleteAll\");\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","bugFix":["c3762f22fc1bf57957efeaf962d3393a5bb1b151"],"bugIntro":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"505bff044e47a553f461b6f4484d1d08faf4ac85","date":1420728783,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n      /*\n       * We first abort and trash everything we have in-memory\n       * and keep the thread-states locked, the lockAndAbortAll operation\n       * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n       * of logical happens-before relationship in the DW. So we do\n       * abort all in memory structures \n       * We also drop global field numbering before during abort to make\n       * sure it's just like a fresh index.\n       */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } catch (OutOfMemoryError oom) {\n        tragicEvent(oom, \"deleteAll\");\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(LeafReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n      /*\n       * We first abort and trash everything we have in-memory\n       * and keep the thread-states locked, the lockAndAbortAll operation\n       * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n       * of logical happens-before relationship in the DW. So we do\n       * abort all in memory structures \n       * We also drop global field numbering before during abort to make\n       * sure it's just like a fresh index.\n       */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } catch (OutOfMemoryError oom) {\n        tragicEvent(oom, \"deleteAll\");\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","bugFix":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"901d103ab7c2eeae92b111fc91bb1b00580a3fd7","date":1422827173,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    synchronized (fullFlushLock) { \n      /*\n       * We first abort and trash everything we have in-memory\n       * and keep the thread-states locked, the lockAndAbortAll operation\n       * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n       * of logical happens-before relationship in the DW. So we do\n       * abort all in memory structures \n       * We also drop global field numbering before during abort to make\n       * sure it's just like a fresh index.\n       */\n      try {\n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } finally {\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      } catch (OutOfMemoryError oom) {\n        tragicEvent(oom, \"deleteAll\");\n      } finally {\n        docWriter.unlockAllAfterAbortAll(this);\n      }\n    }\n  }\n\n","bugFix":["25e07bf0d9fa18cd8f0185e309d09a873c45017c","a4278fc65afbb35739525c37f818cded6fe6e9ae","7af110b00ea8df9429309d83e38e0533d82e144f"],"bugIntro":["c48871ed951104729f5e17a8ee1091b43fa18980"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98b44240f64a2d6935543ff25faee750b29204eb","date":1424972040,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            pendingNumDocs.set(0);\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2","date":1424979404,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalDocCount());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            pendingNumDocs.set(0);\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98a04f56464afdffd4c430d6c47a0c868a38354e","date":1424985833,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalDocCount());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","bugFix":["7af110b00ea8df9429309d83e38e0533d82e144f"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalDocCount());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalDocCount());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b869e42fbd9c52c4728652ba51faf7266b239a6f","date":1428140988,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            ++changeCount;\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9e22bdf0692bfa61e342b04a6ac7078670c1e16","date":1436866730,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c48871ed951104729f5e17a8ee1091b43fa18980","date":1446564542,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"deleteAll\");\n    }\n  }\n\n","bugFix":["901d103ab7c2eeae92b111fc91bb1b00580a3fd7"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f6df47cbfd656ea50ca2996361f7954531ee18b","date":1464133540,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n            return docWriter.deleteQueue.seqNo.get();\n\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"16ebfabc294f23b88b6a39722a02c9d39b353195","date":1464343867,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n            return docWriter.deleteQueue.getNextSequenceNumber();\n\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n            return docWriter.deleteQueue.seqNo.get();\n\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6483e4260c08168709c02238ae083a51519a28dd","date":1465117546,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n            return docWriter.deleteQueue.getNextSequenceNumber();\n\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"191128ac5b85671b1671e2c857437694283b6ebf","date":1465297861,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n            return docWriter.deleteQueue.getNextSequenceNumber();\n\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"481f7e7bbb6cd3648a79ea3343aacbdfb04f7b0c","date":1467811770,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n            long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n            docWriter.setLastSeqNo(seqNo);\n            return seqNo;\n\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n            return docWriter.deleteQueue.getNextSequenceNumber();\n\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n            long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n            docWriter.setLastSeqNo(seqNo);\n            return seqNo;\n\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   */\n  public void deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef","date":1512420564,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n            long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n            docWriter.setLastSeqNo(seqNo);\n            return seqNo;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        long abortedDocCount = docWriter.lockAndAbortAll(this);\n        pendingNumDocs.addAndGet(-abortedDocCount);\n        \n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            // Remove all segments\n            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n\n            success = true;\n            long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n            docWriter.setLastSeqNo(seqNo);\n            return seqNo;\n\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","bugFix":["b0267c69e2456a3477a1ad785723f2135da3117e","acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2","0f6df47cbfd656ea50ca2996361f7954531ee18b"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"845b760a99e5f369fcd0a5d723a87b8def6a3f56","date":1521117993,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n            long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n            docWriter.setLastSeqNo(seqNo);\n            return seqNo;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false, true);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n            long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n            docWriter.setLastSeqNo(seqNo);\n            return seqNo;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n\n      // dead code but javac disagrees\n      return -1;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"af02a5a3ff2c1e52a02c0f07ff02c7197e43e59c","date":1521393811,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable release = docWriter.lockAndAbortAll(this)) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll(false);\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n            long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n            docWriter.setLastSeqNo(seqNo);\n            return seqNo;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"203e3fcf513c02ee2c07015f2ce277e26dc60907","date":1521404157,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable release = docWriter.lockAndAbortAll(this)) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll(false);\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) { \n        docWriter.lockAndAbortAll(this);\n        processEvents(false);\n        synchronized (this) {\n          try {\n            // Abort any running merges\n            abortMerges();\n            // Let merges run again\n            stopMerges = false;\n            adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n            // Remove all segments\n            segmentInfos.clear();\n            // Ask deleter to locate unreferenced files & remove them:\n            deleter.checkpoint(segmentInfos, false);\n\n            /* don't refresh the deleter here since there might\n             * be concurrent indexing requests coming in opening\n             * files on the directory after we called DW#abort()\n             * if we do so these indexing requests might hit FNF exceptions.\n             * We will remove the files incrementally as we go...\n             */\n            // Don't bother saving any changes in our segmentInfos\n            readerPool.dropAll(false);\n            // Mark that the index has changed\n            changeCount.incrementAndGet();\n            segmentInfos.changed();\n            globalFieldNumberMap.clear();\n            success = true;\n            long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n            docWriter.setLastSeqNo(seqNo);\n            return seqNo;\n          } finally {\n            docWriter.unlockAllAfterAbortAll(this);\n            if (!success) {\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit exception during deleteAll\");\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14d66d86a8b184a86bcaebcf6e15fcef486e0876","date":1521539412,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll(this)) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll(false);\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable release = docWriter.lockAndAbortAll(this)) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll(false);\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","date":1521731438,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll(this)) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll(false);\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable release = docWriter.lockAndAbortAll(this)) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll(false);\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1926100d9b67becc9701c54266fee3ba7878a5f0","date":1524472150,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll(this)) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll(this)) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll(false);\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b8498afacfc8322268ca0d659d274fcce08d557","date":1524577248,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll()) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll(this)) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92b74780b4efed2011d2d1a19183689db904519e","date":1586516102,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll()) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll()) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              // Let merges run again\n              stopMerges = false;\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"264935965977b4a9e2f3920420647072c9c49176","date":1586600626,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll()) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll()) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              docWriter.setLastSeqNo(seqNo);\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b07024a7318c25225dc4d070cf6d047315b73aaf","date":1586885963,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll()) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.getNextSequenceNumber();\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll()) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a775b68a26e2d19d1b5f16cd18a3bc8df738a302","date":1598253342,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#deleteAll().mjava","sourceNew":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll()) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              try {\n                abortMerges();\n                assert merges.areEnabled() == false : \"merges should be disabled - who enabled them?\";\n                assert mergingSegments.isEmpty() : \"found merging segments but merges are disabled: \" + mergingSegments;\n              } finally {\n                // abortMerges disables all merges and we need to re-enable them here to make sure\n                // IW can function properly. An exception in abortMerges() might be fatal for IW but just to be sure\n                // lets re-enable merges anyway.\n                merges.enable();\n              }\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.getNextSequenceNumber();\n              return seqNo;\n            } finally {\n              if (success == false) {\n\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","sourceOld":"  /**\n   * Delete all documents in the index.\n   * \n   * <p>\n   * This method will drop all buffered documents and will remove all segments\n   * from the index. This change will not be visible until a {@link #commit()}\n   * has been called. This method can be rolled back using {@link #rollback()}.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method is much faster than using deleteDocuments( new\n   * MatchAllDocsQuery() ). Yet, this method also has different semantics\n   * compared to {@link #deleteDocuments(Query...)} since internal\n   * data-structures are cleared as well as all segment information is\n   * forcefully dropped anti-viral semantics like omitting norms are reset or\n   * doc value types are cleared. Essentially a call to {@link #deleteAll()} is\n   * equivalent to creating a new {@link IndexWriter} with\n   * {@link OpenMode#CREATE} which a delete query only marks documents as\n   * deleted.\n   * </p>\n   * \n   * <p>\n   * NOTE: this method will forcefully abort all merges in progress. If other\n   * threads are running {@link #forceMerge}, {@link #addIndexes(CodecReader[])}\n   * or {@link #forceMergeDeletes} methods, they may receive\n   * {@link MergePolicy.MergeAbortedException}s.\n   *\n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   */\n  @SuppressWarnings(\"try\")\n  public long deleteAll() throws IOException {\n    ensureOpen();\n    // Remove any buffered docs\n    boolean success = false;\n    /* hold the full flush lock to prevent concurrency commits / NRT reopens to\n     * get in our way and do unnecessary work. -- if we don't lock this here we might\n     * get in trouble if */\n    /*\n     * We first abort and trash everything we have in-memory\n     * and keep the thread-states locked, the lockAndAbortAll operation\n     * also guarantees \"point in time semantics\" ie. the checkpoint that we need in terms\n     * of logical happens-before relationship in the DW. So we do\n     * abort all in memory structures \n     * We also drop global field numbering before during abort to make\n     * sure it's just like a fresh index.\n     */\n    try {\n      synchronized (fullFlushLock) {\n        try (Closeable finalizer = docWriter.lockAndAbortAll()) {\n          processEvents(false);\n          synchronized (this) {\n            try {\n              // Abort any running merges\n              abortMerges();\n              adjustPendingNumDocs(-segmentInfos.totalMaxDoc());\n              // Remove all segments\n              segmentInfos.clear();\n              // Ask deleter to locate unreferenced files & remove them:\n              deleter.checkpoint(segmentInfos, false);\n\n              /* don't refresh the deleter here since there might\n               * be concurrent indexing requests coming in opening\n               * files on the directory after we called DW#abort()\n               * if we do so these indexing requests might hit FNF exceptions.\n               * We will remove the files incrementally as we go...\n               */\n              // Don't bother saving any changes in our segmentInfos\n              readerPool.dropAll();\n              // Mark that the index has changed\n              changeCount.incrementAndGet();\n              segmentInfos.changed();\n              globalFieldNumberMap.clear();\n              success = true;\n              long seqNo = docWriter.getNextSequenceNumber();\n              return seqNo;\n            } finally {\n              if (success == false) {\n                if (infoStream.isEnabled(\"IW\")) {\n                  infoStream.message(\"IW\", \"hit exception during deleteAll\");\n                }\n              }\n            }\n          }\n        }\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"deleteAll\");\n      throw tragedy;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b869e42fbd9c52c4728652ba51faf7266b239a6f":["b0267c69e2456a3477a1ad785723f2135da3117e"],"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["203e3fcf513c02ee2c07015f2ce277e26dc60907","14d66d86a8b184a86bcaebcf6e15fcef486e0876"],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"7af110b00ea8df9429309d83e38e0533d82e144f":["a4278fc65afbb35739525c37f818cded6fe6e9ae"],"c3762f22fc1bf57957efeaf962d3393a5bb1b151":["7af110b00ea8df9429309d83e38e0533d82e144f"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a775b68a26e2d19d1b5f16cd18a3bc8df738a302":["b07024a7318c25225dc4d070cf6d047315b73aaf"],"191128ac5b85671b1671e2c857437694283b6ebf":["c48871ed951104729f5e17a8ee1091b43fa18980","6483e4260c08168709c02238ae083a51519a28dd"],"b7605579001505896d48b07160075a5c8b8e128e":["7af110b00ea8df9429309d83e38e0533d82e144f","c3762f22fc1bf57957efeaf962d3393a5bb1b151"],"af02a5a3ff2c1e52a02c0f07ff02c7197e43e59c":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["901d103ab7c2eeae92b111fc91bb1b00580a3fd7","b0267c69e2456a3477a1ad785723f2135da3117e"],"6b8498afacfc8322268ca0d659d274fcce08d557":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2":["98b44240f64a2d6935543ff25faee750b29204eb"],"b06445ae1731e049327712db0454e5643ca9b7fe":["98a04f56464afdffd4c430d6c47a0c868a38354e","b0267c69e2456a3477a1ad785723f2135da3117e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"25e07bf0d9fa18cd8f0185e309d09a873c45017c":["949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"949847c0040cd70a68222d526cb0da7bf6cbb3c2":["a64ffebe62143a8d4c37d99b6ece6d430d948ebc"],"a4278fc65afbb35739525c37f818cded6fe6e9ae":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"98b44240f64a2d6935543ff25faee750b29204eb":["901d103ab7c2eeae92b111fc91bb1b00580a3fd7"],"14d66d86a8b184a86bcaebcf6e15fcef486e0876":["203e3fcf513c02ee2c07015f2ce277e26dc60907"],"264935965977b4a9e2f3920420647072c9c49176":["92b74780b4efed2011d2d1a19183689db904519e"],"98a04f56464afdffd4c430d6c47a0c868a38354e":["901d103ab7c2eeae92b111fc91bb1b00580a3fd7","acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["a4278fc65afbb35739525c37f818cded6fe6e9ae","7af110b00ea8df9429309d83e38e0533d82e144f"],"31d4861802ca404d78ca1d15f4550eec415b9199":["a4278fc65afbb35739525c37f818cded6fe6e9ae","7af110b00ea8df9429309d83e38e0533d82e144f"],"b0267c69e2456a3477a1ad785723f2135da3117e":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["25e07bf0d9fa18cd8f0185e309d09a873c45017c"],"203e3fcf513c02ee2c07015f2ce277e26dc60907":["845b760a99e5f369fcd0a5d723a87b8def6a3f56","af02a5a3ff2c1e52a02c0f07ff02c7197e43e59c"],"6483e4260c08168709c02238ae083a51519a28dd":["c48871ed951104729f5e17a8ee1091b43fa18980","16ebfabc294f23b88b6a39722a02c9d39b353195"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"901d103ab7c2eeae92b111fc91bb1b00580a3fd7":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"16ebfabc294f23b88b6a39722a02c9d39b353195":["0f6df47cbfd656ea50ca2996361f7954531ee18b"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["c48871ed951104729f5e17a8ee1091b43fa18980","481f7e7bbb6cd3648a79ea3343aacbdfb04f7b0c"],"481f7e7bbb6cd3648a79ea3343aacbdfb04f7b0c":["191128ac5b85671b1671e2c857437694283b6ebf"],"c48871ed951104729f5e17a8ee1091b43fa18980":["d9e22bdf0692bfa61e342b04a6ac7078670c1e16"],"b07024a7318c25225dc4d070cf6d047315b73aaf":["264935965977b4a9e2f3920420647072c9c49176"],"d9e22bdf0692bfa61e342b04a6ac7078670c1e16":["b869e42fbd9c52c4728652ba51faf7266b239a6f"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","6e2893fd5349134af382d33ccc3d84840394c6c1"],"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef":["481f7e7bbb6cd3648a79ea3343aacbdfb04f7b0c"],"a64ffebe62143a8d4c37d99b6ece6d430d948ebc":["c3762f22fc1bf57957efeaf962d3393a5bb1b151"],"0f6df47cbfd656ea50ca2996361f7954531ee18b":["c48871ed951104729f5e17a8ee1091b43fa18980"],"92b74780b4efed2011d2d1a19183689db904519e":["6b8498afacfc8322268ca0d659d274fcce08d557"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a775b68a26e2d19d1b5f16cd18a3bc8df738a302"]},"commit2Childs":{"b869e42fbd9c52c4728652ba51faf7266b239a6f":["d9e22bdf0692bfa61e342b04a6ac7078670c1e16"],"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["af02a5a3ff2c1e52a02c0f07ff02c7197e43e59c","203e3fcf513c02ee2c07015f2ce277e26dc60907"],"7af110b00ea8df9429309d83e38e0533d82e144f":["c3762f22fc1bf57957efeaf962d3393a5bb1b151","b7605579001505896d48b07160075a5c8b8e128e","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199"],"c3762f22fc1bf57957efeaf962d3393a5bb1b151":["b7605579001505896d48b07160075a5c8b8e128e","a64ffebe62143a8d4c37d99b6ece6d430d948ebc"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["6e2893fd5349134af382d33ccc3d84840394c6c1","d4d69c535930b5cce125cff868d40f6373dc27d4"],"a775b68a26e2d19d1b5f16cd18a3bc8df738a302":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"191128ac5b85671b1671e2c857437694283b6ebf":["481f7e7bbb6cd3648a79ea3343aacbdfb04f7b0c"],"b7605579001505896d48b07160075a5c8b8e128e":[],"af02a5a3ff2c1e52a02c0f07ff02c7197e43e59c":["203e3fcf513c02ee2c07015f2ce277e26dc60907"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"6b8498afacfc8322268ca0d659d274fcce08d557":["92b74780b4efed2011d2d1a19183689db904519e"],"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"25e07bf0d9fa18cd8f0185e309d09a873c45017c":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"949847c0040cd70a68222d526cb0da7bf6cbb3c2":["25e07bf0d9fa18cd8f0185e309d09a873c45017c"],"a4278fc65afbb35739525c37f818cded6fe6e9ae":["7af110b00ea8df9429309d83e38e0533d82e144f","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199"],"98b44240f64a2d6935543ff25faee750b29204eb":["acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2"],"14d66d86a8b184a86bcaebcf6e15fcef486e0876":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"264935965977b4a9e2f3920420647072c9c49176":["b07024a7318c25225dc4d070cf6d047315b73aaf"],"98a04f56464afdffd4c430d6c47a0c868a38354e":["b06445ae1731e049327712db0454e5643ca9b7fe","b0267c69e2456a3477a1ad785723f2135da3117e"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"31d4861802ca404d78ca1d15f4550eec415b9199":[],"b0267c69e2456a3477a1ad785723f2135da3117e":["b869e42fbd9c52c4728652ba51faf7266b239a6f","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"203e3fcf513c02ee2c07015f2ce277e26dc60907":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","14d66d86a8b184a86bcaebcf6e15fcef486e0876"],"6483e4260c08168709c02238ae083a51519a28dd":["191128ac5b85671b1671e2c857437694283b6ebf"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["6b8498afacfc8322268ca0d659d274fcce08d557"],"901d103ab7c2eeae92b111fc91bb1b00580a3fd7":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","98b44240f64a2d6935543ff25faee750b29204eb","98a04f56464afdffd4c430d6c47a0c868a38354e"],"16ebfabc294f23b88b6a39722a02c9d39b353195":["6483e4260c08168709c02238ae083a51519a28dd"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["901d103ab7c2eeae92b111fc91bb1b00580a3fd7"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"c48871ed951104729f5e17a8ee1091b43fa18980":["191128ac5b85671b1671e2c857437694283b6ebf","6483e4260c08168709c02238ae083a51519a28dd","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","0f6df47cbfd656ea50ca2996361f7954531ee18b"],"481f7e7bbb6cd3648a79ea3343aacbdfb04f7b0c":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"b07024a7318c25225dc4d070cf6d047315b73aaf":["a775b68a26e2d19d1b5f16cd18a3bc8df738a302"],"d9e22bdf0692bfa61e342b04a6ac7078670c1e16":["c48871ed951104729f5e17a8ee1091b43fa18980"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["a4278fc65afbb35739525c37f818cded6fe6e9ae"],"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"a64ffebe62143a8d4c37d99b6ece6d430d948ebc":["949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"0f6df47cbfd656ea50ca2996361f7954531ee18b":["16ebfabc294f23b88b6a39722a02c9d39b353195"],"92b74780b4efed2011d2d1a19183689db904519e":["264935965977b4a9e2f3920420647072c9c49176"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7605579001505896d48b07160075a5c8b8e128e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}