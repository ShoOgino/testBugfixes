{"path":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","commits":[{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","pathOld":"solr/contrib/solr-mr/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","sourceNew":"  @Test\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    ReduceDriver<Text, SolrInputDocumentWritable, Text, SolrInputDocumentWritable> reduceDriver = ReduceDriver.newReduceDriver(myReducer);\n\n    Configuration config = reduceDriver.getConfiguration();\n    setupHadoopConfig(config);\n\n    List<SolrInputDocumentWritable> values = new ArrayList<SolrInputDocumentWritable>();\n    SolrInputDocument sid = new SolrInputDocument();\n    String id = \"myid1\";\n    sid.addField(\"id\", id);\n    sid.addField(\"text\", \"some unique text\");\n    SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n    values.add(sidw);\n    reduceDriver.withInput(new Text(id), values);\n\n    reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n    \n    reduceDriver.withOutputFormat(SolrOutputFormat.class, NullInputFormat.class);\n\n    reduceDriver.run();\n\n    assertEquals(\"Expected 1 counter increment\", 1, reduceDriver.getCounters()\n        .findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n  }\n\n","sourceOld":"  @Test\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    ReduceDriver<Text, SolrInputDocumentWritable, Text, SolrInputDocumentWritable> reduceDriver = ReduceDriver.newReduceDriver(myReducer);\n\n    Configuration config = reduceDriver.getConfiguration();\n    setupHadoopConfig(config);\n\n    List<SolrInputDocumentWritable> values = new ArrayList<SolrInputDocumentWritable>();\n    SolrInputDocument sid = new SolrInputDocument();\n    String id = \"myid1\";\n    sid.addField(\"id\", id);\n    sid.addField(\"text\", \"some unique text\");\n    SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n    values.add(sidw);\n    reduceDriver.withInput(new Text(id), values);\n\n    reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n    \n    reduceDriver.withOutputFormat(SolrOutputFormat.class, NullInputFormat.class);\n\n    reduceDriver.run();\n\n    assertEquals(\"Expected 1 counter increment\", 1, reduceDriver.getCounters()\n        .findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42d384b06aa87eae925b668b65f3246154f0b0fa","date":1386181725,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","sourceNew":"  @Test\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<SolrInputDocumentWritable>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    ReduceDriver<Text, SolrInputDocumentWritable, Text, SolrInputDocumentWritable> reduceDriver = ReduceDriver.newReduceDriver(myReducer);\n\n    Configuration config = reduceDriver.getConfiguration();\n    setupHadoopConfig(config);\n\n    List<SolrInputDocumentWritable> values = new ArrayList<SolrInputDocumentWritable>();\n    SolrInputDocument sid = new SolrInputDocument();\n    String id = \"myid1\";\n    sid.addField(\"id\", id);\n    sid.addField(\"text\", \"some unique text\");\n    SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n    values.add(sidw);\n    reduceDriver.withInput(new Text(id), values);\n\n    reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n    \n    reduceDriver.withOutputFormat(SolrOutputFormat.class, NullInputFormat.class);\n\n    reduceDriver.run();\n\n    assertEquals(\"Expected 1 counter increment\", 1, reduceDriver.getCounters()\n        .findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b0d372d32bc2f431c90e9d5bd34cd074a70479e","date":1386349487,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","sourceNew":"  @Test\n  @Ignore(\"This test cannot currently work because it uses a local filesystem output path for the indexes and Solr requires hdfs output paths\")\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<SolrInputDocumentWritable>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<SolrInputDocumentWritable>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  @Ignore(\"This test cannot currently work because it uses a local filesystem output path for the indexes and Solr requires hdfs output paths\")\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<SolrInputDocumentWritable>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","sourceNew":"  @Test\n  @Ignore(\"This test cannot currently work because it uses a local filesystem output path for the indexes and Solr requires hdfs output paths\")\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","sourceOld":"  @Test\n  @Ignore(\"This test cannot currently work because it uses a local filesystem output path for the indexes and Solr requires hdfs output paths\")\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<SolrInputDocumentWritable>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17ac431e37b605d1d64aff99512d9729b6f8c8c5","date":1485908823,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","sourceNew":"  @Test\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","sourceOld":"  @Test\n  @Ignore(\"This test cannot currently work because it uses a local filesystem output path for the indexes and Solr requires hdfs output paths\")\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c3523a0ab04c3002eee3896c75ea5f10f388bcc","date":1485968422,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","sourceNew":"  @Test\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","sourceOld":"  @Test\n  @Ignore(\"This test cannot currently work because it uses a local filesystem output path for the indexes and Solr requires hdfs output paths\")\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12109b652e9210b8d58fca47f6c4a725d058a58e","date":1490373076,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe1c4aa9af769a38e878f608070f672efbeac27f","date":1490594650,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineReducerTest#testReducer().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void testReducer() throws Exception {\n    MySolrReducer myReducer = new MySolrReducer();\n    try {\n      ReduceDriver<Text,SolrInputDocumentWritable,Text,SolrInputDocumentWritable> reduceDriver = ReduceDriver\n          .newReduceDriver(myReducer);\n      \n      Configuration config = reduceDriver.getConfiguration();\n      setupHadoopConfig(config);\n      \n      List<SolrInputDocumentWritable> values = new ArrayList<>();\n      SolrInputDocument sid = new SolrInputDocument();\n      String id = \"myid1\";\n      sid.addField(\"id\", id);\n      sid.addField(\"text\", \"some unique text\");\n      SolrInputDocumentWritable sidw = new SolrInputDocumentWritable(sid);\n      values.add(sidw);\n      reduceDriver.withInput(new Text(id), values);\n      \n      reduceDriver.withCacheArchive(solrHomeZip.getAbsolutePath());\n      \n      reduceDriver.withOutputFormat(SolrOutputFormat.class,\n          NullInputFormat.class);\n      \n      reduceDriver.run();\n      \n      assertEquals(\"Expected 1 counter increment\", 1,\n          reduceDriver.getCounters().findCounter(SolrCounters.class.getName(),\n                  SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    } finally {\n      myReducer.cleanup(myReducer.context);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"17ac431e37b605d1d64aff99512d9729b6f8c8c5":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["9b0d372d32bc2f431c90e9d5bd34cd074a70479e"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["17ac431e37b605d1d64aff99512d9729b6f8c8c5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7c3523a0ab04c3002eee3896c75ea5f10f388bcc":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","17ac431e37b605d1d64aff99512d9729b6f8c8c5"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9b0d372d32bc2f431c90e9d5bd34cd074a70479e"],"fe1c4aa9af769a38e878f608070f672efbeac27f":["17ac431e37b605d1d64aff99512d9729b6f8c8c5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12109b652e9210b8d58fca47f6c4a725d058a58e"],"9b0d372d32bc2f431c90e9d5bd34cd074a70479e":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"42d384b06aa87eae925b668b65f3246154f0b0fa":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"17ac431e37b605d1d64aff99512d9729b6f8c8c5":["12109b652e9210b8d58fca47f6c4a725d058a58e","7c3523a0ab04c3002eee3896c75ea5f10f388bcc","fe1c4aa9af769a38e878f608070f672efbeac27f"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["17ac431e37b605d1d64aff99512d9729b6f8c8c5","7c3523a0ab04c3002eee3896c75ea5f10f388bcc"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70f91c8322fbffe3a3a897ef20ea19119cac10cd","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"7c3523a0ab04c3002eee3896c75ea5f10f388bcc":[],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"fe1c4aa9af769a38e878f608070f672efbeac27f":[],"9b0d372d32bc2f431c90e9d5bd34cd074a70479e":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"42d384b06aa87eae925b668b65f3246154f0b0fa":["9b0d372d32bc2f431c90e9d5bd34cd074a70479e"]},"heads":["7c3523a0ab04c3002eee3896c75ea5f10f388bcc","74f45af4339b0daf7a95c820ab88c1aea74fbce0","fe1c4aa9af769a38e878f608070f672efbeac27f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}