{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestConcatenatingTokenStream#testInconsistentAttributes().mjava","commits":[{"id":"1516d411048ee5b9655104d318ff9a9f0a4a6e5f","date":1498119193,"type":0,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestConcatenatingTokenStream#testInconsistentAttributes().mjava","pathOld":"/dev/null","sourceNew":"  public void testInconsistentAttributes() throws IOException {\n\n    AttributeFactory factory = newAttributeFactory();\n\n    final MockTokenizer first = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    first.setReader(new StringReader(\"first words \"));\n    first.addAttribute(PayloadAttribute.class);\n    final MockTokenizer second = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    second.setReader(new StringReader(\"second words\"));\n    second.addAttribute(FlagsAttribute.class);\n\n    TokenStream ts = new ConcatenatingTokenStream(first, second);\n    assertTrue(ts.hasAttribute(FlagsAttribute.class));\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n\n    assertTokenStreamContents(ts,\n        new String[] { \"first\", \"words\", \"second\", \"words\" },\n        new int[]{ 0, 6, 12, 19, },\n        new int[]{ 5, 11, 18, 24, });\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestConcatenatingTokenStream#testInconsistentAttributes().mjava","pathOld":"/dev/null","sourceNew":"  public void testInconsistentAttributes() throws IOException {\n\n    AttributeFactory factory = newAttributeFactory();\n\n    final MockTokenizer first = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    first.setReader(new StringReader(\"first words \"));\n    first.addAttribute(PayloadAttribute.class);\n    final MockTokenizer second = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    second.setReader(new StringReader(\"second words\"));\n    second.addAttribute(FlagsAttribute.class);\n\n    TokenStream ts = new ConcatenatingTokenStream(first, second);\n    assertTrue(ts.hasAttribute(FlagsAttribute.class));\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n\n    assertTokenStreamContents(ts,\n        new String[] { \"first\", \"words\", \"second\", \"words\" },\n        new int[]{ 0, 6, 12, 19, },\n        new int[]{ 5, 11, 18, 24, });\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestConcatenatingTokenStream#testInconsistentAttributes().mjava","pathOld":"/dev/null","sourceNew":"  public void testInconsistentAttributes() throws IOException {\n\n    AttributeFactory factory = newAttributeFactory();\n\n    final MockTokenizer first = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    first.setReader(new StringReader(\"first words \"));\n    first.addAttribute(PayloadAttribute.class);\n    final MockTokenizer second = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    second.setReader(new StringReader(\"second words\"));\n    second.addAttribute(FlagsAttribute.class);\n\n    TokenStream ts = new ConcatenatingTokenStream(first, second);\n    assertTrue(ts.hasAttribute(FlagsAttribute.class));\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n\n    assertTokenStreamContents(ts,\n        new String[] { \"first\", \"words\", \"second\", \"words\" },\n        new int[]{ 0, 6, 12, 19, },\n        new int[]{ 5, 11, 18, 24, });\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1516d411048ee5b9655104d318ff9a9f0a4a6e5f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1516d411048ee5b9655104d318ff9a9f0a4a6e5f"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1516d411048ee5b9655104d318ff9a9f0a4a6e5f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"]},"commit2Childs":{"1516d411048ee5b9655104d318ff9a9f0a4a6e5f":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1516d411048ee5b9655104d318ff9a9f0a4a6e5f","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}