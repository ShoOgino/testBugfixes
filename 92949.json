{"path":"solr/core/src/java/org/apache/solr/cloud/api/collections/ReindexCollectionCmd#call(ClusterState,CloudConfig,ZkNodeProps,NamedList).mjava","commits":[{"id":"c526352db87264a72a7a9ad68c1b769b81e54305","date":1598780188,"type":1,"author":"Ilan Ginzburg","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/ReindexCollectionCmd#call(ClusterState,CloudConfig,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/ReindexCollectionCmd#call(ClusterState,ZkNodeProps,NamedList).mjava","sourceNew":"  @Override\n  @SuppressWarnings({\"unchecked\"})\n  public void call(ClusterState clusterState, CloudConfig cloudConfig, ZkNodeProps message, @SuppressWarnings({\"rawtypes\"})NamedList results) throws Exception {\n\n    log.debug(\"*** called: {}\", message);\n\n    String extCollection = message.getStr(CommonParams.NAME);\n\n    if (extCollection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Source collection name must be specified\");\n    }\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collection;\n    if (followAliases) {\n      collection = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollection);\n    } else {\n      collection = extCollection;\n    }\n    if (!clusterState.hasCollection(collection)) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Source collection name must exist\");\n    }\n    String target = message.getStr(TARGET);\n    if (target == null) {\n      target = collection;\n    } else {\n      if (followAliases) {\n        target = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(target);\n      }\n    }\n    boolean sameTarget = target.equals(collection) || target.equals(extCollection);\n    boolean removeSource = message.getBool(REMOVE_SOURCE, false);\n    Cmd command = Cmd.get(message.getStr(COMMAND, Cmd.START.toLower()));\n    if (command == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown command: \" + message.getStr(COMMAND));\n    }\n    Map<String, Object> reindexingState = getReindexingState(ocmh.cloudManager.getDistribStateManager(), collection);\n    if (!reindexingState.containsKey(STATE)) {\n      reindexingState.put(STATE, State.IDLE.toLower());\n    }\n    State state = State.get(reindexingState.get(STATE));\n    if (command == Cmd.ABORT) {\n      log.info(\"Abort requested for collection {}, setting the state to ABORTED.\", collection);\n      // check that it's running\n      if (state != State.RUNNING) {\n        log.debug(\"Abort requested for collection {} but command is not running: {}\", collection, state);\n        return;\n      }\n      setReindexingState(collection, State.ABORTED, null);\n      reindexingState.put(STATE, \"aborting\");\n      results.add(REINDEX_STATUS, reindexingState);\n      // if needed the cleanup will be performed by the running instance of the command\n      return;\n    } else if (command == Cmd.STATUS) {\n      results.add(REINDEX_STATUS, reindexingState);\n      return;\n    }\n    // command == Cmd.START\n\n    // check it's not already running\n    if (state == State.RUNNING) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Reindex is already running for collection \" + collection +\n          \". If you are sure this is not the case you can issue &cmd=abort to clean up this state.\");\n    }\n\n    DocCollection coll = clusterState.getCollection(collection);\n    boolean aborted = false;\n    int batchSize = message.getInt(CommonParams.ROWS, 100);\n    String query = message.getStr(CommonParams.Q, \"*:*\");\n    String fl = message.getStr(CommonParams.FL, \"*\");\n    Integer rf = message.getInt(ZkStateReader.REPLICATION_FACTOR, coll.getReplicationFactor());\n    Integer numNrt = message.getInt(ZkStateReader.NRT_REPLICAS, coll.getNumNrtReplicas());\n    Integer numTlog = message.getInt(ZkStateReader.TLOG_REPLICAS, coll.getNumTlogReplicas());\n    Integer numPull = message.getInt(ZkStateReader.PULL_REPLICAS, coll.getNumPullReplicas());\n    int numShards = message.getInt(ZkStateReader.NUM_SHARDS_PROP, coll.getActiveSlices().size());\n    DocRouter router = coll.getRouter();\n    if (router == null) {\n      router = DocRouter.DEFAULT;\n    }\n\n    String configName = message.getStr(ZkStateReader.CONFIGNAME_PROP, ocmh.zkStateReader.readConfigName(collection));\n    String targetCollection;\n    int seq = tmpCollectionSeq.getAndIncrement();\n    if (sameTarget) {\n      do {\n        targetCollection = TARGET_COL_PREFIX + extCollection + \"_\" + seq;\n        if (!clusterState.hasCollection(targetCollection)) {\n          break;\n        }\n        seq = tmpCollectionSeq.getAndIncrement();\n      } while (clusterState.hasCollection(targetCollection));\n    } else {\n      targetCollection = target;\n    }\n    String chkCollection = CHK_COL_PREFIX + extCollection;\n    String daemonUrl = null;\n    Exception exc = null;\n    boolean createdTarget = false;\n    try {\n      zkHost = ocmh.zkStateReader.getZkClient().getZkServerAddress();\n      // set the running flag\n      reindexingState.clear();\n      reindexingState.put(\"actualSourceCollection\", collection);\n      reindexingState.put(\"actualTargetCollection\", targetCollection);\n      reindexingState.put(\"checkpointCollection\", chkCollection);\n      reindexingState.put(\"inputDocs\", getNumberOfDocs(collection));\n      reindexingState.put(PHASE, \"creating target and checkpoint collections\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      // 0. set up target and checkpoint collections\n      NamedList<Object> cmdResults = new NamedList<>();\n      ZkNodeProps cmd;\n      if (clusterState.hasCollection(targetCollection)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Target collection \" + targetCollection + \" already exists! Delete it first.\");\n      }\n      if (clusterState.hasCollection(chkCollection)) {\n        // delete the checkpoint collection\n        cmd = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n            CommonParams.NAME, chkCollection,\n            CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n        );\n        ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cloudConfig, cmd, cmdResults);\n        ocmh.checkResults(\"deleting old checkpoint collection \" + chkCollection, cmdResults, true);\n      }\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      Map<String, Object> propMap = new HashMap<>();\n      propMap.put(Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.CREATE.toLower());\n      propMap.put(CommonParams.NAME, targetCollection);\n      propMap.put(ZkStateReader.NUM_SHARDS_PROP, numShards);\n      propMap.put(CollectionAdminParams.COLL_CONF, configName);\n      // init first from the same router\n      propMap.put(\"router.name\", router.getName());\n      for (String key : coll.keySet()) {\n        if (key.startsWith(\"router.\")) {\n          propMap.put(key, coll.get(key));\n        }\n      }\n      // then apply overrides if present\n      for (String key : message.keySet()) {\n        if (key.startsWith(\"router.\")) {\n          propMap.put(key, message.getStr(key));\n        } else if (COLLECTION_PARAMS.contains(key)) {\n          propMap.put(key, message.get(key));\n        }\n      }\n\n      propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, true);\n      if (rf != null) {\n        propMap.put(ZkStateReader.REPLICATION_FACTOR, rf);\n      }\n      if (numNrt != null) {\n        propMap.put(ZkStateReader.NRT_REPLICAS, numNrt);\n      }\n      if (numTlog != null) {\n        propMap.put(ZkStateReader.TLOG_REPLICAS, numTlog);\n      }\n      if (numPull != null) {\n        propMap.put(ZkStateReader.PULL_REPLICAS, numPull);\n      }\n      // create the target collection\n      cmd = new ZkNodeProps(propMap);\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.CREATE).call(clusterState, cloudConfig, cmd, cmdResults);\n      createdTarget = true;\n      ocmh.checkResults(\"creating target collection \" + targetCollection, cmdResults, true);\n\n      // create the checkpoint collection - use RF=1 and 1 shard\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.CREATE.toLower(),\n          CommonParams.NAME, chkCollection,\n          ZkStateReader.NUM_SHARDS_PROP, \"1\",\n          ZkStateReader.REPLICATION_FACTOR, \"1\",\n          CollectionAdminParams.COLL_CONF, \"_default\",\n          CommonAdminParams.WAIT_FOR_FINAL_STATE, \"true\"\n      );\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.CREATE).call(clusterState, cloudConfig, cmd, cmdResults);\n      ocmh.checkResults(\"creating checkpoint collection \" + chkCollection, cmdResults, true);\n      // wait for a while until we see both collections\n      TimeOut waitUntil = new TimeOut(30, TimeUnit.SECONDS, ocmh.timeSource);\n      boolean created = false;\n      while (!waitUntil.hasTimedOut()) {\n        waitUntil.sleep(100);\n        // this also refreshes our local var clusterState\n        clusterState = ocmh.cloudManager.getClusterStateProvider().getClusterState();\n        created = clusterState.hasCollection(targetCollection) && clusterState.hasCollection(chkCollection);\n        if (created) break;\n      }\n      if (!created) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Could not fully create temporary collection(s)\");\n      }\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      // 1. put the source collection in read-only mode\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n          ZkStateReader.COLLECTION_PROP, collection,\n          ZkStateReader.READ_ONLY, \"true\");\n      ocmh.overseer.offerStateUpdate(Utils.toJSON(cmd));\n\n      TestInjection.injectReindexLatch();\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      // 2. copy the documents to target\n      // Recipe taken from: http://joelsolr.blogspot.com/2016/10/solr-63-batch-jobs-parallel-etl-and.html\n      ModifiableSolrParams q = new ModifiableSolrParams();\n      q.set(CommonParams.QT, \"/stream\");\n      q.set(\"collection\", collection);\n      q.set(\"expr\",\n          \"daemon(id=\\\"\" + targetCollection + \"\\\",\" +\n              \"terminate=\\\"true\\\",\" +\n              \"commit(\" + targetCollection + \",\" +\n                \"update(\" + targetCollection + \",\" +\n                  \"batchSize=\" + batchSize + \",\" +\n                  \"topic(\" + chkCollection + \",\" +\n                    collection + \",\" +\n                    \"q=\\\"\" + query + \"\\\",\" +\n                    \"fl=\\\"\" + fl + \"\\\",\" +\n                    \"id=\\\"topic_\" + targetCollection + \"\\\",\" +\n                    // some of the documents eg. in .system contain large blobs\n                    \"rows=\\\"\" + batchSize + \"\\\",\" +\n                    \"initialCheckpoint=\\\"0\\\"))))\");\n      log.debug(\"- starting copying documents from {} to {}\", collection, targetCollection);\n      SolrResponse rsp = null;\n      try {\n        rsp = ocmh.cloudManager.request(new QueryRequest(q));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to copy documents from \" +\n            collection + \" to \" + targetCollection, e);\n      }\n      daemonUrl = getDaemonUrl(rsp, coll);\n      if (daemonUrl == null) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to copy documents from \" +\n            collection + \" to \" + targetCollection + \": \" + Utils.toJSONString(rsp));\n      }\n      reindexingState.put(\"daemonUrl\", daemonUrl);\n      reindexingState.put(\"daemonName\", targetCollection);\n      reindexingState.put(PHASE, \"copying documents\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      // wait for the daemon to finish\n      waitForDaemon(targetCollection, daemonUrl, collection, targetCollection, reindexingState);\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n      log.debug(\"- finished copying from {} to {}\", collection, targetCollection);\n      // fail here or earlier during daemon run\n      TestInjection.injectReindexFailure();\n\n      // 5. if (sameTarget) set up an alias to use targetCollection as the source name\n      if (sameTarget) {\n        log.debug(\"- setting up alias from {} to {}\", extCollection, targetCollection);\n        cmd = new ZkNodeProps(\n            CommonParams.NAME, extCollection,\n            \"collections\", targetCollection);\n        cmdResults = new NamedList<>();\n        ocmh.commandMap.get(CollectionParams.CollectionAction.CREATEALIAS).call(clusterState, cloudConfig, cmd, cmdResults);\n        ocmh.checkResults(\"setting up alias \" + extCollection + \" -> \" + targetCollection, cmdResults, true);\n        reindexingState.put(\"alias\", extCollection + \" -> \" + targetCollection);\n      }\n\n      reindexingState.remove(\"daemonUrl\");\n      reindexingState.remove(\"daemonName\");\n      reindexingState.put(\"processedDocs\", getNumberOfDocs(targetCollection));\n      reindexingState.put(PHASE, \"copying done, finalizing\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n      // 6. delete the checkpoint collection\n      log.debug(\"- deleting {}\", chkCollection);\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n          CommonParams.NAME, chkCollection,\n          CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n      );\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cloudConfig, cmd, cmdResults);\n      ocmh.checkResults(\"deleting checkpoint collection \" + chkCollection, cmdResults, true);\n\n      // 7. optionally delete the source collection\n      if (removeSource) {\n        log.debug(\"- deleting source collection\");\n        cmd = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n            CommonParams.NAME, collection,\n            FOLLOW_ALIASES, \"false\",\n            CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n        );\n        cmdResults = new NamedList<>();\n        ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cloudConfig, cmd, cmdResults);\n        ocmh.checkResults(\"deleting source collection \" + collection, cmdResults, true);\n      } else {\n        // 8. clear readOnly on source\n        ZkNodeProps props = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n            ZkStateReader.COLLECTION_PROP, collection,\n            ZkStateReader.READ_ONLY, null);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n      }\n      // 9. set FINISHED state on the target and clear the state on the source\n      ZkNodeProps props = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n          ZkStateReader.COLLECTION_PROP, targetCollection,\n          REINDEXING_STATE, State.FINISHED.toLower());\n      ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n      reindexingState.put(STATE, State.FINISHED.toLower());\n      reindexingState.put(PHASE, \"done\");\n      removeReindexingState(collection);\n    } catch (Exception e) {\n      log.warn(\"Error during reindexing of {}\", extCollection, e);\n      exc = e;\n      aborted = true;\n    } finally {\n      if (aborted) {\n        cleanup(cloudConfig, collection, targetCollection, chkCollection, daemonUrl, targetCollection, createdTarget);\n        if (exc != null) {\n          results.add(\"error\", exc.toString());\n        }\n        reindexingState.put(STATE, State.ABORTED.toLower());\n      }\n      results.add(REINDEX_STATUS, reindexingState);\n    }\n  }\n\n","sourceOld":"  @Override\n  @SuppressWarnings({\"unchecked\"})\n  public void call(ClusterState clusterState, ZkNodeProps message, @SuppressWarnings({\"rawtypes\"})NamedList results) throws Exception {\n\n    log.debug(\"*** called: {}\", message);\n\n    String extCollection = message.getStr(CommonParams.NAME);\n\n    if (extCollection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Source collection name must be specified\");\n    }\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collection;\n    if (followAliases) {\n      collection = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollection);\n    } else {\n      collection = extCollection;\n    }\n    if (!clusterState.hasCollection(collection)) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Source collection name must exist\");\n    }\n    String target = message.getStr(TARGET);\n    if (target == null) {\n      target = collection;\n    } else {\n      if (followAliases) {\n        target = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(target);\n      }\n    }\n    boolean sameTarget = target.equals(collection) || target.equals(extCollection);\n    boolean removeSource = message.getBool(REMOVE_SOURCE, false);\n    Cmd command = Cmd.get(message.getStr(COMMAND, Cmd.START.toLower()));\n    if (command == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown command: \" + message.getStr(COMMAND));\n    }\n    Map<String, Object> reindexingState = getReindexingState(ocmh.cloudManager.getDistribStateManager(), collection);\n    if (!reindexingState.containsKey(STATE)) {\n      reindexingState.put(STATE, State.IDLE.toLower());\n    }\n    State state = State.get(reindexingState.get(STATE));\n    if (command == Cmd.ABORT) {\n      log.info(\"Abort requested for collection {}, setting the state to ABORTED.\", collection);\n      // check that it's running\n      if (state != State.RUNNING) {\n        log.debug(\"Abort requested for collection {} but command is not running: {}\", collection, state);\n        return;\n      }\n      setReindexingState(collection, State.ABORTED, null);\n      reindexingState.put(STATE, \"aborting\");\n      results.add(REINDEX_STATUS, reindexingState);\n      // if needed the cleanup will be performed by the running instance of the command\n      return;\n    } else if (command == Cmd.STATUS) {\n      results.add(REINDEX_STATUS, reindexingState);\n      return;\n    }\n    // command == Cmd.START\n\n    // check it's not already running\n    if (state == State.RUNNING) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Reindex is already running for collection \" + collection +\n          \". If you are sure this is not the case you can issue &cmd=abort to clean up this state.\");\n    }\n\n    DocCollection coll = clusterState.getCollection(collection);\n    boolean aborted = false;\n    int batchSize = message.getInt(CommonParams.ROWS, 100);\n    String query = message.getStr(CommonParams.Q, \"*:*\");\n    String fl = message.getStr(CommonParams.FL, \"*\");\n    Integer rf = message.getInt(ZkStateReader.REPLICATION_FACTOR, coll.getReplicationFactor());\n    Integer numNrt = message.getInt(ZkStateReader.NRT_REPLICAS, coll.getNumNrtReplicas());\n    Integer numTlog = message.getInt(ZkStateReader.TLOG_REPLICAS, coll.getNumTlogReplicas());\n    Integer numPull = message.getInt(ZkStateReader.PULL_REPLICAS, coll.getNumPullReplicas());\n    int numShards = message.getInt(ZkStateReader.NUM_SHARDS_PROP, coll.getActiveSlices().size());\n    DocRouter router = coll.getRouter();\n    if (router == null) {\n      router = DocRouter.DEFAULT;\n    }\n\n    String configName = message.getStr(ZkStateReader.CONFIGNAME_PROP, ocmh.zkStateReader.readConfigName(collection));\n    String targetCollection;\n    int seq = tmpCollectionSeq.getAndIncrement();\n    if (sameTarget) {\n      do {\n        targetCollection = TARGET_COL_PREFIX + extCollection + \"_\" + seq;\n        if (!clusterState.hasCollection(targetCollection)) {\n          break;\n        }\n        seq = tmpCollectionSeq.getAndIncrement();\n      } while (clusterState.hasCollection(targetCollection));\n    } else {\n      targetCollection = target;\n    }\n    String chkCollection = CHK_COL_PREFIX + extCollection;\n    String daemonUrl = null;\n    Exception exc = null;\n    boolean createdTarget = false;\n    try {\n      zkHost = ocmh.zkStateReader.getZkClient().getZkServerAddress();\n      // set the running flag\n      reindexingState.clear();\n      reindexingState.put(\"actualSourceCollection\", collection);\n      reindexingState.put(\"actualTargetCollection\", targetCollection);\n      reindexingState.put(\"checkpointCollection\", chkCollection);\n      reindexingState.put(\"inputDocs\", getNumberOfDocs(collection));\n      reindexingState.put(PHASE, \"creating target and checkpoint collections\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      // 0. set up target and checkpoint collections\n      NamedList<Object> cmdResults = new NamedList<>();\n      ZkNodeProps cmd;\n      if (clusterState.hasCollection(targetCollection)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Target collection \" + targetCollection + \" already exists! Delete it first.\");\n      }\n      if (clusterState.hasCollection(chkCollection)) {\n        // delete the checkpoint collection\n        cmd = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n            CommonParams.NAME, chkCollection,\n            CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n        );\n        ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cmd, cmdResults);\n        ocmh.checkResults(\"deleting old checkpoint collection \" + chkCollection, cmdResults, true);\n      }\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      Map<String, Object> propMap = new HashMap<>();\n      propMap.put(Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.CREATE.toLower());\n      propMap.put(CommonParams.NAME, targetCollection);\n      propMap.put(ZkStateReader.NUM_SHARDS_PROP, numShards);\n      propMap.put(CollectionAdminParams.COLL_CONF, configName);\n      // init first from the same router\n      propMap.put(\"router.name\", router.getName());\n      for (String key : coll.keySet()) {\n        if (key.startsWith(\"router.\")) {\n          propMap.put(key, coll.get(key));\n        }\n      }\n      // then apply overrides if present\n      for (String key : message.keySet()) {\n        if (key.startsWith(\"router.\")) {\n          propMap.put(key, message.getStr(key));\n        } else if (COLLECTION_PARAMS.contains(key)) {\n          propMap.put(key, message.get(key));\n        }\n      }\n\n      propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, true);\n      if (rf != null) {\n        propMap.put(ZkStateReader.REPLICATION_FACTOR, rf);\n      }\n      if (numNrt != null) {\n        propMap.put(ZkStateReader.NRT_REPLICAS, numNrt);\n      }\n      if (numTlog != null) {\n        propMap.put(ZkStateReader.TLOG_REPLICAS, numTlog);\n      }\n      if (numPull != null) {\n        propMap.put(ZkStateReader.PULL_REPLICAS, numPull);\n      }\n      // create the target collection\n      cmd = new ZkNodeProps(propMap);\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.CREATE).call(clusterState, cmd, cmdResults);\n      createdTarget = true;\n      ocmh.checkResults(\"creating target collection \" + targetCollection, cmdResults, true);\n\n      // create the checkpoint collection - use RF=1 and 1 shard\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.CREATE.toLower(),\n          CommonParams.NAME, chkCollection,\n          ZkStateReader.NUM_SHARDS_PROP, \"1\",\n          ZkStateReader.REPLICATION_FACTOR, \"1\",\n          CollectionAdminParams.COLL_CONF, \"_default\",\n          CommonAdminParams.WAIT_FOR_FINAL_STATE, \"true\"\n      );\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.CREATE).call(clusterState, cmd, cmdResults);\n      ocmh.checkResults(\"creating checkpoint collection \" + chkCollection, cmdResults, true);\n      // wait for a while until we see both collections\n      TimeOut waitUntil = new TimeOut(30, TimeUnit.SECONDS, ocmh.timeSource);\n      boolean created = false;\n      while (!waitUntil.hasTimedOut()) {\n        waitUntil.sleep(100);\n        // this also refreshes our local var clusterState\n        clusterState = ocmh.cloudManager.getClusterStateProvider().getClusterState();\n        created = clusterState.hasCollection(targetCollection) && clusterState.hasCollection(chkCollection);\n        if (created) break;\n      }\n      if (!created) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Could not fully create temporary collection(s)\");\n      }\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      // 1. put the source collection in read-only mode\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n          ZkStateReader.COLLECTION_PROP, collection,\n          ZkStateReader.READ_ONLY, \"true\");\n      ocmh.overseer.offerStateUpdate(Utils.toJSON(cmd));\n\n      TestInjection.injectReindexLatch();\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      // 2. copy the documents to target\n      // Recipe taken from: http://joelsolr.blogspot.com/2016/10/solr-63-batch-jobs-parallel-etl-and.html\n      ModifiableSolrParams q = new ModifiableSolrParams();\n      q.set(CommonParams.QT, \"/stream\");\n      q.set(\"collection\", collection);\n      q.set(\"expr\",\n          \"daemon(id=\\\"\" + targetCollection + \"\\\",\" +\n              \"terminate=\\\"true\\\",\" +\n              \"commit(\" + targetCollection + \",\" +\n                \"update(\" + targetCollection + \",\" +\n                  \"batchSize=\" + batchSize + \",\" +\n                  \"topic(\" + chkCollection + \",\" +\n                    collection + \",\" +\n                    \"q=\\\"\" + query + \"\\\",\" +\n                    \"fl=\\\"\" + fl + \"\\\",\" +\n                    \"id=\\\"topic_\" + targetCollection + \"\\\",\" +\n                    // some of the documents eg. in .system contain large blobs\n                    \"rows=\\\"\" + batchSize + \"\\\",\" +\n                    \"initialCheckpoint=\\\"0\\\"))))\");\n      log.debug(\"- starting copying documents from {} to {}\", collection, targetCollection);\n      SolrResponse rsp = null;\n      try {\n        rsp = ocmh.cloudManager.request(new QueryRequest(q));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to copy documents from \" +\n            collection + \" to \" + targetCollection, e);\n      }\n      daemonUrl = getDaemonUrl(rsp, coll);\n      if (daemonUrl == null) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to copy documents from \" +\n            collection + \" to \" + targetCollection + \": \" + Utils.toJSONString(rsp));\n      }\n      reindexingState.put(\"daemonUrl\", daemonUrl);\n      reindexingState.put(\"daemonName\", targetCollection);\n      reindexingState.put(PHASE, \"copying documents\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      // wait for the daemon to finish\n      waitForDaemon(targetCollection, daemonUrl, collection, targetCollection, reindexingState);\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n      log.debug(\"- finished copying from {} to {}\", collection, targetCollection);\n      // fail here or earlier during daemon run\n      TestInjection.injectReindexFailure();\n\n      // 5. if (sameTarget) set up an alias to use targetCollection as the source name\n      if (sameTarget) {\n        log.debug(\"- setting up alias from {} to {}\", extCollection, targetCollection);\n        cmd = new ZkNodeProps(\n            CommonParams.NAME, extCollection,\n            \"collections\", targetCollection);\n        cmdResults = new NamedList<>();\n        ocmh.commandMap.get(CollectionParams.CollectionAction.CREATEALIAS).call(clusterState, cmd, cmdResults);\n        ocmh.checkResults(\"setting up alias \" + extCollection + \" -> \" + targetCollection, cmdResults, true);\n        reindexingState.put(\"alias\", extCollection + \" -> \" + targetCollection);\n      }\n\n      reindexingState.remove(\"daemonUrl\");\n      reindexingState.remove(\"daemonName\");\n      reindexingState.put(\"processedDocs\", getNumberOfDocs(targetCollection));\n      reindexingState.put(PHASE, \"copying done, finalizing\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n      // 6. delete the checkpoint collection\n      log.debug(\"- deleting {}\", chkCollection);\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n          CommonParams.NAME, chkCollection,\n          CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n      );\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cmd, cmdResults);\n      ocmh.checkResults(\"deleting checkpoint collection \" + chkCollection, cmdResults, true);\n\n      // 7. optionally delete the source collection\n      if (removeSource) {\n        log.debug(\"- deleting source collection\");\n        cmd = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n            CommonParams.NAME, collection,\n            FOLLOW_ALIASES, \"false\",\n            CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n        );\n        cmdResults = new NamedList<>();\n        ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cmd, cmdResults);\n        ocmh.checkResults(\"deleting source collection \" + collection, cmdResults, true);\n      } else {\n        // 8. clear readOnly on source\n        ZkNodeProps props = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n            ZkStateReader.COLLECTION_PROP, collection,\n            ZkStateReader.READ_ONLY, null);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n      }\n      // 9. set FINISHED state on the target and clear the state on the source\n      ZkNodeProps props = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n          ZkStateReader.COLLECTION_PROP, targetCollection,\n          REINDEXING_STATE, State.FINISHED.toLower());\n      ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n      reindexingState.put(STATE, State.FINISHED.toLower());\n      reindexingState.put(PHASE, \"done\");\n      removeReindexingState(collection);\n    } catch (Exception e) {\n      log.warn(\"Error during reindexing of {}\", extCollection, e);\n      exc = e;\n      aborted = true;\n    } finally {\n      if (aborted) {\n        cleanup(collection, targetCollection, chkCollection, daemonUrl, targetCollection, createdTarget);\n        if (exc != null) {\n          results.add(\"error\", exc.toString());\n        }\n        reindexingState.put(STATE, State.ABORTED.toLower());\n      }\n      results.add(REINDEX_STATUS, reindexingState);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7b17e79a71117668ecbf8d3417c876e41396565","date":1598973672,"type":5,"author":"Ilan Ginzburg","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/api/collections/ReindexCollectionCmd#call(ClusterState,ZkNodeProps,NamedList).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/api/collections/ReindexCollectionCmd#call(ClusterState,CloudConfig,ZkNodeProps,NamedList).mjava","sourceNew":"  @Override\n  @SuppressWarnings({\"unchecked\"})\n  public void call(ClusterState clusterState, ZkNodeProps message, @SuppressWarnings({\"rawtypes\"})NamedList results) throws Exception {\n\n    log.debug(\"*** called: {}\", message);\n\n    String extCollection = message.getStr(CommonParams.NAME);\n\n    if (extCollection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Source collection name must be specified\");\n    }\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collection;\n    if (followAliases) {\n      collection = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollection);\n    } else {\n      collection = extCollection;\n    }\n    if (!clusterState.hasCollection(collection)) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Source collection name must exist\");\n    }\n    String target = message.getStr(TARGET);\n    if (target == null) {\n      target = collection;\n    } else {\n      if (followAliases) {\n        target = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(target);\n      }\n    }\n    boolean sameTarget = target.equals(collection) || target.equals(extCollection);\n    boolean removeSource = message.getBool(REMOVE_SOURCE, false);\n    Cmd command = Cmd.get(message.getStr(COMMAND, Cmd.START.toLower()));\n    if (command == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown command: \" + message.getStr(COMMAND));\n    }\n    Map<String, Object> reindexingState = getReindexingState(ocmh.cloudManager.getDistribStateManager(), collection);\n    if (!reindexingState.containsKey(STATE)) {\n      reindexingState.put(STATE, State.IDLE.toLower());\n    }\n    State state = State.get(reindexingState.get(STATE));\n    if (command == Cmd.ABORT) {\n      log.info(\"Abort requested for collection {}, setting the state to ABORTED.\", collection);\n      // check that it's running\n      if (state != State.RUNNING) {\n        log.debug(\"Abort requested for collection {} but command is not running: {}\", collection, state);\n        return;\n      }\n      setReindexingState(collection, State.ABORTED, null);\n      reindexingState.put(STATE, \"aborting\");\n      results.add(REINDEX_STATUS, reindexingState);\n      // if needed the cleanup will be performed by the running instance of the command\n      return;\n    } else if (command == Cmd.STATUS) {\n      results.add(REINDEX_STATUS, reindexingState);\n      return;\n    }\n    // command == Cmd.START\n\n    // check it's not already running\n    if (state == State.RUNNING) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Reindex is already running for collection \" + collection +\n          \". If you are sure this is not the case you can issue &cmd=abort to clean up this state.\");\n    }\n\n    DocCollection coll = clusterState.getCollection(collection);\n    boolean aborted = false;\n    int batchSize = message.getInt(CommonParams.ROWS, 100);\n    String query = message.getStr(CommonParams.Q, \"*:*\");\n    String fl = message.getStr(CommonParams.FL, \"*\");\n    Integer rf = message.getInt(ZkStateReader.REPLICATION_FACTOR, coll.getReplicationFactor());\n    Integer numNrt = message.getInt(ZkStateReader.NRT_REPLICAS, coll.getNumNrtReplicas());\n    Integer numTlog = message.getInt(ZkStateReader.TLOG_REPLICAS, coll.getNumTlogReplicas());\n    Integer numPull = message.getInt(ZkStateReader.PULL_REPLICAS, coll.getNumPullReplicas());\n    int numShards = message.getInt(ZkStateReader.NUM_SHARDS_PROP, coll.getActiveSlices().size());\n    DocRouter router = coll.getRouter();\n    if (router == null) {\n      router = DocRouter.DEFAULT;\n    }\n\n    String configName = message.getStr(ZkStateReader.CONFIGNAME_PROP, ocmh.zkStateReader.readConfigName(collection));\n    String targetCollection;\n    int seq = tmpCollectionSeq.getAndIncrement();\n    if (sameTarget) {\n      do {\n        targetCollection = TARGET_COL_PREFIX + extCollection + \"_\" + seq;\n        if (!clusterState.hasCollection(targetCollection)) {\n          break;\n        }\n        seq = tmpCollectionSeq.getAndIncrement();\n      } while (clusterState.hasCollection(targetCollection));\n    } else {\n      targetCollection = target;\n    }\n    String chkCollection = CHK_COL_PREFIX + extCollection;\n    String daemonUrl = null;\n    Exception exc = null;\n    boolean createdTarget = false;\n    try {\n      zkHost = ocmh.zkStateReader.getZkClient().getZkServerAddress();\n      // set the running flag\n      reindexingState.clear();\n      reindexingState.put(\"actualSourceCollection\", collection);\n      reindexingState.put(\"actualTargetCollection\", targetCollection);\n      reindexingState.put(\"checkpointCollection\", chkCollection);\n      reindexingState.put(\"inputDocs\", getNumberOfDocs(collection));\n      reindexingState.put(PHASE, \"creating target and checkpoint collections\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      // 0. set up target and checkpoint collections\n      NamedList<Object> cmdResults = new NamedList<>();\n      ZkNodeProps cmd;\n      if (clusterState.hasCollection(targetCollection)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Target collection \" + targetCollection + \" already exists! Delete it first.\");\n      }\n      if (clusterState.hasCollection(chkCollection)) {\n        // delete the checkpoint collection\n        cmd = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n            CommonParams.NAME, chkCollection,\n            CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n        );\n        ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cmd, cmdResults);\n        ocmh.checkResults(\"deleting old checkpoint collection \" + chkCollection, cmdResults, true);\n      }\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      Map<String, Object> propMap = new HashMap<>();\n      propMap.put(Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.CREATE.toLower());\n      propMap.put(CommonParams.NAME, targetCollection);\n      propMap.put(ZkStateReader.NUM_SHARDS_PROP, numShards);\n      propMap.put(CollectionAdminParams.COLL_CONF, configName);\n      // init first from the same router\n      propMap.put(\"router.name\", router.getName());\n      for (String key : coll.keySet()) {\n        if (key.startsWith(\"router.\")) {\n          propMap.put(key, coll.get(key));\n        }\n      }\n      // then apply overrides if present\n      for (String key : message.keySet()) {\n        if (key.startsWith(\"router.\")) {\n          propMap.put(key, message.getStr(key));\n        } else if (COLLECTION_PARAMS.contains(key)) {\n          propMap.put(key, message.get(key));\n        }\n      }\n\n      propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, true);\n      if (rf != null) {\n        propMap.put(ZkStateReader.REPLICATION_FACTOR, rf);\n      }\n      if (numNrt != null) {\n        propMap.put(ZkStateReader.NRT_REPLICAS, numNrt);\n      }\n      if (numTlog != null) {\n        propMap.put(ZkStateReader.TLOG_REPLICAS, numTlog);\n      }\n      if (numPull != null) {\n        propMap.put(ZkStateReader.PULL_REPLICAS, numPull);\n      }\n      // create the target collection\n      cmd = new ZkNodeProps(propMap);\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.CREATE).call(clusterState, cmd, cmdResults);\n      createdTarget = true;\n      ocmh.checkResults(\"creating target collection \" + targetCollection, cmdResults, true);\n\n      // create the checkpoint collection - use RF=1 and 1 shard\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.CREATE.toLower(),\n          CommonParams.NAME, chkCollection,\n          ZkStateReader.NUM_SHARDS_PROP, \"1\",\n          ZkStateReader.REPLICATION_FACTOR, \"1\",\n          CollectionAdminParams.COLL_CONF, \"_default\",\n          CommonAdminParams.WAIT_FOR_FINAL_STATE, \"true\"\n      );\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.CREATE).call(clusterState, cmd, cmdResults);\n      ocmh.checkResults(\"creating checkpoint collection \" + chkCollection, cmdResults, true);\n      // wait for a while until we see both collections\n      TimeOut waitUntil = new TimeOut(30, TimeUnit.SECONDS, ocmh.timeSource);\n      boolean created = false;\n      while (!waitUntil.hasTimedOut()) {\n        waitUntil.sleep(100);\n        // this also refreshes our local var clusterState\n        clusterState = ocmh.cloudManager.getClusterStateProvider().getClusterState();\n        created = clusterState.hasCollection(targetCollection) && clusterState.hasCollection(chkCollection);\n        if (created) break;\n      }\n      if (!created) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Could not fully create temporary collection(s)\");\n      }\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      // 1. put the source collection in read-only mode\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n          ZkStateReader.COLLECTION_PROP, collection,\n          ZkStateReader.READ_ONLY, \"true\");\n      ocmh.overseer.offerStateUpdate(Utils.toJSON(cmd));\n\n      TestInjection.injectReindexLatch();\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      // 2. copy the documents to target\n      // Recipe taken from: http://joelsolr.blogspot.com/2016/10/solr-63-batch-jobs-parallel-etl-and.html\n      ModifiableSolrParams q = new ModifiableSolrParams();\n      q.set(CommonParams.QT, \"/stream\");\n      q.set(\"collection\", collection);\n      q.set(\"expr\",\n          \"daemon(id=\\\"\" + targetCollection + \"\\\",\" +\n              \"terminate=\\\"true\\\",\" +\n              \"commit(\" + targetCollection + \",\" +\n                \"update(\" + targetCollection + \",\" +\n                  \"batchSize=\" + batchSize + \",\" +\n                  \"topic(\" + chkCollection + \",\" +\n                    collection + \",\" +\n                    \"q=\\\"\" + query + \"\\\",\" +\n                    \"fl=\\\"\" + fl + \"\\\",\" +\n                    \"id=\\\"topic_\" + targetCollection + \"\\\",\" +\n                    // some of the documents eg. in .system contain large blobs\n                    \"rows=\\\"\" + batchSize + \"\\\",\" +\n                    \"initialCheckpoint=\\\"0\\\"))))\");\n      log.debug(\"- starting copying documents from {} to {}\", collection, targetCollection);\n      SolrResponse rsp = null;\n      try {\n        rsp = ocmh.cloudManager.request(new QueryRequest(q));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to copy documents from \" +\n            collection + \" to \" + targetCollection, e);\n      }\n      daemonUrl = getDaemonUrl(rsp, coll);\n      if (daemonUrl == null) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to copy documents from \" +\n            collection + \" to \" + targetCollection + \": \" + Utils.toJSONString(rsp));\n      }\n      reindexingState.put(\"daemonUrl\", daemonUrl);\n      reindexingState.put(\"daemonName\", targetCollection);\n      reindexingState.put(PHASE, \"copying documents\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      // wait for the daemon to finish\n      waitForDaemon(targetCollection, daemonUrl, collection, targetCollection, reindexingState);\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n      log.debug(\"- finished copying from {} to {}\", collection, targetCollection);\n      // fail here or earlier during daemon run\n      TestInjection.injectReindexFailure();\n\n      // 5. if (sameTarget) set up an alias to use targetCollection as the source name\n      if (sameTarget) {\n        log.debug(\"- setting up alias from {} to {}\", extCollection, targetCollection);\n        cmd = new ZkNodeProps(\n            CommonParams.NAME, extCollection,\n            \"collections\", targetCollection);\n        cmdResults = new NamedList<>();\n        ocmh.commandMap.get(CollectionParams.CollectionAction.CREATEALIAS).call(clusterState, cmd, cmdResults);\n        ocmh.checkResults(\"setting up alias \" + extCollection + \" -> \" + targetCollection, cmdResults, true);\n        reindexingState.put(\"alias\", extCollection + \" -> \" + targetCollection);\n      }\n\n      reindexingState.remove(\"daemonUrl\");\n      reindexingState.remove(\"daemonName\");\n      reindexingState.put(\"processedDocs\", getNumberOfDocs(targetCollection));\n      reindexingState.put(PHASE, \"copying done, finalizing\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n      // 6. delete the checkpoint collection\n      log.debug(\"- deleting {}\", chkCollection);\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n          CommonParams.NAME, chkCollection,\n          CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n      );\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cmd, cmdResults);\n      ocmh.checkResults(\"deleting checkpoint collection \" + chkCollection, cmdResults, true);\n\n      // 7. optionally delete the source collection\n      if (removeSource) {\n        log.debug(\"- deleting source collection\");\n        cmd = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n            CommonParams.NAME, collection,\n            FOLLOW_ALIASES, \"false\",\n            CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n        );\n        cmdResults = new NamedList<>();\n        ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cmd, cmdResults);\n        ocmh.checkResults(\"deleting source collection \" + collection, cmdResults, true);\n      } else {\n        // 8. clear readOnly on source\n        ZkNodeProps props = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n            ZkStateReader.COLLECTION_PROP, collection,\n            ZkStateReader.READ_ONLY, null);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n      }\n      // 9. set FINISHED state on the target and clear the state on the source\n      ZkNodeProps props = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n          ZkStateReader.COLLECTION_PROP, targetCollection,\n          REINDEXING_STATE, State.FINISHED.toLower());\n      ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n      reindexingState.put(STATE, State.FINISHED.toLower());\n      reindexingState.put(PHASE, \"done\");\n      removeReindexingState(collection);\n    } catch (Exception e) {\n      log.warn(\"Error during reindexing of {}\", extCollection, e);\n      exc = e;\n      aborted = true;\n    } finally {\n      if (aborted) {\n        cleanup(collection, targetCollection, chkCollection, daemonUrl, targetCollection, createdTarget);\n        if (exc != null) {\n          results.add(\"error\", exc.toString());\n        }\n        reindexingState.put(STATE, State.ABORTED.toLower());\n      }\n      results.add(REINDEX_STATUS, reindexingState);\n    }\n  }\n\n","sourceOld":"  @Override\n  @SuppressWarnings({\"unchecked\"})\n  public void call(ClusterState clusterState, CloudConfig cloudConfig, ZkNodeProps message, @SuppressWarnings({\"rawtypes\"})NamedList results) throws Exception {\n\n    log.debug(\"*** called: {}\", message);\n\n    String extCollection = message.getStr(CommonParams.NAME);\n\n    if (extCollection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Source collection name must be specified\");\n    }\n    boolean followAliases = message.getBool(FOLLOW_ALIASES, false);\n    String collection;\n    if (followAliases) {\n      collection = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(extCollection);\n    } else {\n      collection = extCollection;\n    }\n    if (!clusterState.hasCollection(collection)) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Source collection name must exist\");\n    }\n    String target = message.getStr(TARGET);\n    if (target == null) {\n      target = collection;\n    } else {\n      if (followAliases) {\n        target = ocmh.cloudManager.getClusterStateProvider().resolveSimpleAlias(target);\n      }\n    }\n    boolean sameTarget = target.equals(collection) || target.equals(extCollection);\n    boolean removeSource = message.getBool(REMOVE_SOURCE, false);\n    Cmd command = Cmd.get(message.getStr(COMMAND, Cmd.START.toLower()));\n    if (command == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown command: \" + message.getStr(COMMAND));\n    }\n    Map<String, Object> reindexingState = getReindexingState(ocmh.cloudManager.getDistribStateManager(), collection);\n    if (!reindexingState.containsKey(STATE)) {\n      reindexingState.put(STATE, State.IDLE.toLower());\n    }\n    State state = State.get(reindexingState.get(STATE));\n    if (command == Cmd.ABORT) {\n      log.info(\"Abort requested for collection {}, setting the state to ABORTED.\", collection);\n      // check that it's running\n      if (state != State.RUNNING) {\n        log.debug(\"Abort requested for collection {} but command is not running: {}\", collection, state);\n        return;\n      }\n      setReindexingState(collection, State.ABORTED, null);\n      reindexingState.put(STATE, \"aborting\");\n      results.add(REINDEX_STATUS, reindexingState);\n      // if needed the cleanup will be performed by the running instance of the command\n      return;\n    } else if (command == Cmd.STATUS) {\n      results.add(REINDEX_STATUS, reindexingState);\n      return;\n    }\n    // command == Cmd.START\n\n    // check it's not already running\n    if (state == State.RUNNING) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Reindex is already running for collection \" + collection +\n          \". If you are sure this is not the case you can issue &cmd=abort to clean up this state.\");\n    }\n\n    DocCollection coll = clusterState.getCollection(collection);\n    boolean aborted = false;\n    int batchSize = message.getInt(CommonParams.ROWS, 100);\n    String query = message.getStr(CommonParams.Q, \"*:*\");\n    String fl = message.getStr(CommonParams.FL, \"*\");\n    Integer rf = message.getInt(ZkStateReader.REPLICATION_FACTOR, coll.getReplicationFactor());\n    Integer numNrt = message.getInt(ZkStateReader.NRT_REPLICAS, coll.getNumNrtReplicas());\n    Integer numTlog = message.getInt(ZkStateReader.TLOG_REPLICAS, coll.getNumTlogReplicas());\n    Integer numPull = message.getInt(ZkStateReader.PULL_REPLICAS, coll.getNumPullReplicas());\n    int numShards = message.getInt(ZkStateReader.NUM_SHARDS_PROP, coll.getActiveSlices().size());\n    DocRouter router = coll.getRouter();\n    if (router == null) {\n      router = DocRouter.DEFAULT;\n    }\n\n    String configName = message.getStr(ZkStateReader.CONFIGNAME_PROP, ocmh.zkStateReader.readConfigName(collection));\n    String targetCollection;\n    int seq = tmpCollectionSeq.getAndIncrement();\n    if (sameTarget) {\n      do {\n        targetCollection = TARGET_COL_PREFIX + extCollection + \"_\" + seq;\n        if (!clusterState.hasCollection(targetCollection)) {\n          break;\n        }\n        seq = tmpCollectionSeq.getAndIncrement();\n      } while (clusterState.hasCollection(targetCollection));\n    } else {\n      targetCollection = target;\n    }\n    String chkCollection = CHK_COL_PREFIX + extCollection;\n    String daemonUrl = null;\n    Exception exc = null;\n    boolean createdTarget = false;\n    try {\n      zkHost = ocmh.zkStateReader.getZkClient().getZkServerAddress();\n      // set the running flag\n      reindexingState.clear();\n      reindexingState.put(\"actualSourceCollection\", collection);\n      reindexingState.put(\"actualTargetCollection\", targetCollection);\n      reindexingState.put(\"checkpointCollection\", chkCollection);\n      reindexingState.put(\"inputDocs\", getNumberOfDocs(collection));\n      reindexingState.put(PHASE, \"creating target and checkpoint collections\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      // 0. set up target and checkpoint collections\n      NamedList<Object> cmdResults = new NamedList<>();\n      ZkNodeProps cmd;\n      if (clusterState.hasCollection(targetCollection)) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Target collection \" + targetCollection + \" already exists! Delete it first.\");\n      }\n      if (clusterState.hasCollection(chkCollection)) {\n        // delete the checkpoint collection\n        cmd = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n            CommonParams.NAME, chkCollection,\n            CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n        );\n        ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cloudConfig, cmd, cmdResults);\n        ocmh.checkResults(\"deleting old checkpoint collection \" + chkCollection, cmdResults, true);\n      }\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      Map<String, Object> propMap = new HashMap<>();\n      propMap.put(Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.CREATE.toLower());\n      propMap.put(CommonParams.NAME, targetCollection);\n      propMap.put(ZkStateReader.NUM_SHARDS_PROP, numShards);\n      propMap.put(CollectionAdminParams.COLL_CONF, configName);\n      // init first from the same router\n      propMap.put(\"router.name\", router.getName());\n      for (String key : coll.keySet()) {\n        if (key.startsWith(\"router.\")) {\n          propMap.put(key, coll.get(key));\n        }\n      }\n      // then apply overrides if present\n      for (String key : message.keySet()) {\n        if (key.startsWith(\"router.\")) {\n          propMap.put(key, message.getStr(key));\n        } else if (COLLECTION_PARAMS.contains(key)) {\n          propMap.put(key, message.get(key));\n        }\n      }\n\n      propMap.put(CommonAdminParams.WAIT_FOR_FINAL_STATE, true);\n      if (rf != null) {\n        propMap.put(ZkStateReader.REPLICATION_FACTOR, rf);\n      }\n      if (numNrt != null) {\n        propMap.put(ZkStateReader.NRT_REPLICAS, numNrt);\n      }\n      if (numTlog != null) {\n        propMap.put(ZkStateReader.TLOG_REPLICAS, numTlog);\n      }\n      if (numPull != null) {\n        propMap.put(ZkStateReader.PULL_REPLICAS, numPull);\n      }\n      // create the target collection\n      cmd = new ZkNodeProps(propMap);\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.CREATE).call(clusterState, cloudConfig, cmd, cmdResults);\n      createdTarget = true;\n      ocmh.checkResults(\"creating target collection \" + targetCollection, cmdResults, true);\n\n      // create the checkpoint collection - use RF=1 and 1 shard\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.CREATE.toLower(),\n          CommonParams.NAME, chkCollection,\n          ZkStateReader.NUM_SHARDS_PROP, \"1\",\n          ZkStateReader.REPLICATION_FACTOR, \"1\",\n          CollectionAdminParams.COLL_CONF, \"_default\",\n          CommonAdminParams.WAIT_FOR_FINAL_STATE, \"true\"\n      );\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.CREATE).call(clusterState, cloudConfig, cmd, cmdResults);\n      ocmh.checkResults(\"creating checkpoint collection \" + chkCollection, cmdResults, true);\n      // wait for a while until we see both collections\n      TimeOut waitUntil = new TimeOut(30, TimeUnit.SECONDS, ocmh.timeSource);\n      boolean created = false;\n      while (!waitUntil.hasTimedOut()) {\n        waitUntil.sleep(100);\n        // this also refreshes our local var clusterState\n        clusterState = ocmh.cloudManager.getClusterStateProvider().getClusterState();\n        created = clusterState.hasCollection(targetCollection) && clusterState.hasCollection(chkCollection);\n        if (created) break;\n      }\n      if (!created) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Could not fully create temporary collection(s)\");\n      }\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      // 1. put the source collection in read-only mode\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n          ZkStateReader.COLLECTION_PROP, collection,\n          ZkStateReader.READ_ONLY, \"true\");\n      ocmh.overseer.offerStateUpdate(Utils.toJSON(cmd));\n\n      TestInjection.injectReindexLatch();\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n\n      // 2. copy the documents to target\n      // Recipe taken from: http://joelsolr.blogspot.com/2016/10/solr-63-batch-jobs-parallel-etl-and.html\n      ModifiableSolrParams q = new ModifiableSolrParams();\n      q.set(CommonParams.QT, \"/stream\");\n      q.set(\"collection\", collection);\n      q.set(\"expr\",\n          \"daemon(id=\\\"\" + targetCollection + \"\\\",\" +\n              \"terminate=\\\"true\\\",\" +\n              \"commit(\" + targetCollection + \",\" +\n                \"update(\" + targetCollection + \",\" +\n                  \"batchSize=\" + batchSize + \",\" +\n                  \"topic(\" + chkCollection + \",\" +\n                    collection + \",\" +\n                    \"q=\\\"\" + query + \"\\\",\" +\n                    \"fl=\\\"\" + fl + \"\\\",\" +\n                    \"id=\\\"topic_\" + targetCollection + \"\\\",\" +\n                    // some of the documents eg. in .system contain large blobs\n                    \"rows=\\\"\" + batchSize + \"\\\",\" +\n                    \"initialCheckpoint=\\\"0\\\"))))\");\n      log.debug(\"- starting copying documents from {} to {}\", collection, targetCollection);\n      SolrResponse rsp = null;\n      try {\n        rsp = ocmh.cloudManager.request(new QueryRequest(q));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to copy documents from \" +\n            collection + \" to \" + targetCollection, e);\n      }\n      daemonUrl = getDaemonUrl(rsp, coll);\n      if (daemonUrl == null) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unable to copy documents from \" +\n            collection + \" to \" + targetCollection + \": \" + Utils.toJSONString(rsp));\n      }\n      reindexingState.put(\"daemonUrl\", daemonUrl);\n      reindexingState.put(\"daemonName\", targetCollection);\n      reindexingState.put(PHASE, \"copying documents\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      // wait for the daemon to finish\n      waitForDaemon(targetCollection, daemonUrl, collection, targetCollection, reindexingState);\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n      log.debug(\"- finished copying from {} to {}\", collection, targetCollection);\n      // fail here or earlier during daemon run\n      TestInjection.injectReindexFailure();\n\n      // 5. if (sameTarget) set up an alias to use targetCollection as the source name\n      if (sameTarget) {\n        log.debug(\"- setting up alias from {} to {}\", extCollection, targetCollection);\n        cmd = new ZkNodeProps(\n            CommonParams.NAME, extCollection,\n            \"collections\", targetCollection);\n        cmdResults = new NamedList<>();\n        ocmh.commandMap.get(CollectionParams.CollectionAction.CREATEALIAS).call(clusterState, cloudConfig, cmd, cmdResults);\n        ocmh.checkResults(\"setting up alias \" + extCollection + \" -> \" + targetCollection, cmdResults, true);\n        reindexingState.put(\"alias\", extCollection + \" -> \" + targetCollection);\n      }\n\n      reindexingState.remove(\"daemonUrl\");\n      reindexingState.remove(\"daemonName\");\n      reindexingState.put(\"processedDocs\", getNumberOfDocs(targetCollection));\n      reindexingState.put(PHASE, \"copying done, finalizing\");\n      setReindexingState(collection, State.RUNNING, reindexingState);\n\n      if (maybeAbort(collection)) {\n        aborted = true;\n        return;\n      }\n      // 6. delete the checkpoint collection\n      log.debug(\"- deleting {}\", chkCollection);\n      cmd = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n          CommonParams.NAME, chkCollection,\n          CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n      );\n      cmdResults = new NamedList<>();\n      ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cloudConfig, cmd, cmdResults);\n      ocmh.checkResults(\"deleting checkpoint collection \" + chkCollection, cmdResults, true);\n\n      // 7. optionally delete the source collection\n      if (removeSource) {\n        log.debug(\"- deleting source collection\");\n        cmd = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.DELETE.toLower(),\n            CommonParams.NAME, collection,\n            FOLLOW_ALIASES, \"false\",\n            CoreAdminParams.DELETE_METRICS_HISTORY, \"true\"\n        );\n        cmdResults = new NamedList<>();\n        ocmh.commandMap.get(CollectionParams.CollectionAction.DELETE).call(clusterState, cloudConfig, cmd, cmdResults);\n        ocmh.checkResults(\"deleting source collection \" + collection, cmdResults, true);\n      } else {\n        // 8. clear readOnly on source\n        ZkNodeProps props = new ZkNodeProps(\n            Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n            ZkStateReader.COLLECTION_PROP, collection,\n            ZkStateReader.READ_ONLY, null);\n        ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n      }\n      // 9. set FINISHED state on the target and clear the state on the source\n      ZkNodeProps props = new ZkNodeProps(\n          Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.MODIFYCOLLECTION.toLower(),\n          ZkStateReader.COLLECTION_PROP, targetCollection,\n          REINDEXING_STATE, State.FINISHED.toLower());\n      ocmh.overseer.offerStateUpdate(Utils.toJSON(props));\n\n      reindexingState.put(STATE, State.FINISHED.toLower());\n      reindexingState.put(PHASE, \"done\");\n      removeReindexingState(collection);\n    } catch (Exception e) {\n      log.warn(\"Error during reindexing of {}\", extCollection, e);\n      exc = e;\n      aborted = true;\n    } finally {\n      if (aborted) {\n        cleanup(cloudConfig, collection, targetCollection, chkCollection, daemonUrl, targetCollection, createdTarget);\n        if (exc != null) {\n          results.add(\"error\", exc.toString());\n        }\n        reindexingState.put(STATE, State.ABORTED.toLower());\n      }\n      results.add(REINDEX_STATUS, reindexingState);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e7b17e79a71117668ecbf8d3417c876e41396565":["c526352db87264a72a7a9ad68c1b769b81e54305"],"c526352db87264a72a7a9ad68c1b769b81e54305":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e7b17e79a71117668ecbf8d3417c876e41396565"]},"commit2Childs":{"e7b17e79a71117668ecbf8d3417c876e41396565":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c526352db87264a72a7a9ad68c1b769b81e54305":["e7b17e79a71117668ecbf8d3417c876e41396565"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c526352db87264a72a7a9ad68c1b769b81e54305"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}