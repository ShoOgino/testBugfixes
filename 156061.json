{"path":"src/lucene_extras/org/apache/lucene/analysis/TestSynonymFilter#testOffsetsWithOrig().mjava","commits":[{"id":"0c3e228bf650e96f3002a8fb73dd0c13d55af077","date":1138253849,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/lucene_extras/org/apache/lucene/analysis/TestSynonymFilter#testOffsetsWithOrig().mjava","pathOld":"/dev/null","sourceNew":"  public void testOffsetsWithOrig() throws IOException {\n    SynonymMap map = new SynonymMap();\n\n    boolean orig = true;\n    boolean merge = true;\n\n    // test that generated tokens start at the same offset as the original\n    map.add(strings(\"a\"), tokens(\"aa\"), orig, merge);\n    assertTokEqual(getTokList(map,\"a,5\",false), tokens(\"a,5/aa\"));\n    assertTokEqual(getTokList(map,\"a,0\",false), tokens(\"a,0/aa\"));\n\n    // test that offset of first replacement is ignored (always takes the orig offset)\n    map.add(strings(\"b\"), tokens(\"bb,100\"), orig, merge);\n    assertTokEqual(getTokList(map,\"b,5\",false), tokens(\"bb,5/b\"));\n    assertTokEqual(getTokList(map,\"b,0\",false), tokens(\"bb,0/b\"));\n\n    // test that subsequent tokens are adjusted accordingly\n    map.add(strings(\"c\"), tokens(\"cc,100 c2,2\"), orig, merge);\n    assertTokEqual(getTokList(map,\"c,5\",false), tokens(\"cc,5/c c2,2\"));\n    assertTokEqual(getTokList(map,\"c,0\",false), tokens(\"cc,0/c c2,2\"));\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4494badf9b14fe0d151e58b627ae27bb543278b","date":1138579258,"type":4,"author":"Yoav Shapira","isMerge":false,"pathNew":"/dev/null","pathOld":"src/lucene_extras/org/apache/lucene/analysis/TestSynonymFilter#testOffsetsWithOrig().mjava","sourceNew":null,"sourceOld":"  public void testOffsetsWithOrig() throws IOException {\n    SynonymMap map = new SynonymMap();\n\n    boolean orig = true;\n    boolean merge = true;\n\n    // test that generated tokens start at the same offset as the original\n    map.add(strings(\"a\"), tokens(\"aa\"), orig, merge);\n    assertTokEqual(getTokList(map,\"a,5\",false), tokens(\"a,5/aa\"));\n    assertTokEqual(getTokList(map,\"a,0\",false), tokens(\"a,0/aa\"));\n\n    // test that offset of first replacement is ignored (always takes the orig offset)\n    map.add(strings(\"b\"), tokens(\"bb,100\"), orig, merge);\n    assertTokEqual(getTokList(map,\"b,5\",false), tokens(\"bb,5/b\"));\n    assertTokEqual(getTokList(map,\"b,0\",false), tokens(\"bb,0/b\"));\n\n    // test that subsequent tokens are adjusted accordingly\n    map.add(strings(\"c\"), tokens(\"cc,100 c2,2\"), orig, merge);\n    assertTokEqual(getTokList(map,\"c,5\",false), tokens(\"cc,5/c c2,2\"));\n    assertTokEqual(getTokList(map,\"c,0\",false), tokens(\"cc,0/c c2,2\"));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0c3e228bf650e96f3002a8fb73dd0c13d55af077":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"d4494badf9b14fe0d151e58b627ae27bb543278b":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"0c3e228bf650e96f3002a8fb73dd0c13d55af077":["d4494badf9b14fe0d151e58b627ae27bb543278b"],"d4494badf9b14fe0d151e58b627ae27bb543278b":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d4494badf9b14fe0d151e58b627ae27bb543278b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}