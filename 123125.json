{"path":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<FieldInfo, DocFieldConsumerPerField> childFields = new HashMap<FieldInfo, DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo(), f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.numDocs);\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);\n\n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<FieldInfo, DocFieldConsumerPerField> childFields = new HashMap<FieldInfo, DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo(), f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.numDocs);\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);\n\n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cd9165e54429bb5c99e75d5cb1c926cc98772456","date":1337362687,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.numDocs);\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);\n\n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<FieldInfo, DocFieldConsumerPerField> childFields = new HashMap<FieldInfo, DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo(), f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.numDocs);\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);\n\n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0f42e0639920b2e917c9ece35fb68ad83021e38","date":1337629438,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.numDocs);\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.numDocs);\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);\n\n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"129c6e8ac0c0d9a110ba29e4b5f1889374f30076","date":1337725510,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.numDocs);\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.numDocs);\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"16cbef32b882ec68df422af3f08845ec82620335","date":1337802266,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.numDocs);\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<FieldInfo, DocFieldConsumerPerField> childFields = new HashMap<FieldInfo, DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo(), f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.numDocs);\n    }\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);\n\n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18","date":1339188570,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerAndDocID consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6647091125f681395cbde9bb2b7b947cc4ef9bb3","date":1352400554,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null) {\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state.directory, state.segmentInfo, state.fieldInfos, state.context);\n          }\n          field.bytesDVWriter.flush(field.fieldInfo, state,\n                                    dvConsumer.addBinaryField(field.fieldInfo,\n                                                              field.bytesDVWriter.fixedLength >= 0,\n                                                              field.bytesDVWriter.maxLength));\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f44e5a6148908e8393ddd1c2d8b810a385a743c1","date":1352400727,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null) {\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state.directory, state.segmentInfo, state.fieldInfos, state.context);\n          }\n          field.bytesDVWriter.flush(field.fieldInfo, state,\n                                    dvConsumer.addBinaryField(field.fieldInfo,\n                                                              field.bytesDVWriter.fixedLength >= 0,\n                                                              field.bytesDVWriter.maxLength));\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null) {\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state.directory, state.segmentInfo, state.fieldInfos, state.context);\n          }\n          field.bytesDVWriter.flush(field.fieldInfo, state,\n                                    dvConsumer.addBinaryField(field.fieldInfo,\n                                                              field.bytesDVWriter.fixedLength >= 0,\n                                                              field.bytesDVWriter.maxLength));\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9b637fb447c5b4257f6b4532d84ca91e456c1f2a","date":1352405059,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state.directory, state.segmentInfo, state.fieldInfos, state.context);\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n          }\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null) {\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state.directory, state.segmentInfo, state.fieldInfos, state.context);\n          }\n          field.bytesDVWriter.flush(field.fieldInfo, state,\n                                    dvConsumer.addBinaryField(field.fieldInfo,\n                                                              field.bytesDVWriter.fixedLength >= 0,\n                                                              field.bytesDVWriter.maxLength));\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3d3531470649c0bbe4f9d922f7eeb9c3fc88ef15","date":1352476727,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n          }\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state.directory, state.segmentInfo, state.fieldInfos, state.context);\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n          }\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"71df1db89d3a713f022b58111aafd14a4b352da0","date":1352479848,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n          }\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7a8f6a0f831abdaf62496526336f43bbf7c5bbe","date":1352951976,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ce73f585d17f53055185a19beb46db23d76e0ad9","date":1353077110,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f45457a742a53533c348c4b990b1c579ff364467","date":1353197071,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n            // nocommit shouldn't need null check:\n            if (dvConsumer == null) {\n              continue;\n            }\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f176b7bac2a187d69335c079b1f923449fb2881f","date":1353257308,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n            // nocommit shouldn't need null check:\n            if (dvConsumer == null) {\n              continue;\n            }\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n            // nocommit shouldn't need null check:\n            if (dvConsumer == null) {\n              continue;\n            }\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue, state.segmentInfo.getDocCount()));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1ed65f3455364344c6d2ff76ea5421aac754eae7","date":1353261762,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n            // nocommit shouldn't need null check:\n            if (dvConsumer == null) {\n              continue;\n            }\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.finish(state.segmentInfo.getDocCount());\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.finish(state.segmentInfo.getDocCount());\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.finish(state.segmentInfo.getDocCount());\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n            // nocommit shouldn't need null check:\n            if (dvConsumer == null) {\n              continue;\n            }\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6e2893fd5349134af382d33ccc3d84840394c6c1","date":1353682567,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash\n        // order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n            // nocommit shouldn't need null check:\n            if (dvConsumer == null) {\n              continue;\n            }\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.finish(state.segmentInfo.getDocCount());\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.finish(state.segmentInfo.getDocCount());\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.finish(state.segmentInfo.getDocCount());\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n            // nocommit shouldn't need null check:\n            if (dvConsumer == null) {\n              continue;\n            }\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.finish(state.segmentInfo.getDocCount());\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.finish(state.segmentInfo.getDocCount());\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.finish(state.segmentInfo.getDocCount());\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"127981e5a1e1d1425c5fdc816ceacf753ca70ee4","date":1354205321,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    SimpleDVConsumer dvConsumer = null;\n\n    for(int i=0;i<fieldHash.length;i++) {\n      DocFieldProcessorPerField field = fieldHash[i];\n      while(field != null) {\n        // nocommit maybe we should sort by .... somethign?\n        // field name?  field number?  else this is hash\n        // order!!\n        if (field.bytesDVWriter != null || field.numberDVWriter != null || field.sortedBytesDVWriter != null) {\n\n          if (dvConsumer == null) {\n            SimpleDocValuesFormat fmt =  state.segmentInfo.getCodec().simpleDocValuesFormat();\n            // nocommit once we make\n            // Codec.simpleDocValuesFormat abstract, change\n            // this to assert dvConsumer != null!\n            if (fmt == null) {\n              field = field.next;\n              continue;\n            }\n\n            dvConsumer = fmt.fieldsConsumer(state);\n            // nocommit shouldn't need null check:\n            if (dvConsumer == null) {\n              continue;\n            }\n          }\n\n          if (field.bytesDVWriter != null) {\n            field.bytesDVWriter.finish(state.segmentInfo.getDocCount());\n            field.bytesDVWriter.flush(field.fieldInfo, state,\n                                      dvConsumer.addBinaryField(field.fieldInfo,\n                                                                field.bytesDVWriter.fixedLength >= 0,\n                                                                field.bytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.sortedBytesDVWriter != null) {\n            field.sortedBytesDVWriter.finish(state.segmentInfo.getDocCount());\n            field.sortedBytesDVWriter.flush(field.fieldInfo, state,\n                                            dvConsumer.addSortedField(field.fieldInfo,\n                                                                      field.sortedBytesDVWriter.hash.size(),\n                                                                      field.sortedBytesDVWriter.fixedLength >= 0,\n                                                                      field.sortedBytesDVWriter.maxLength));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n\n          if (field.numberDVWriter != null) {\n            field.numberDVWriter.finish(state.segmentInfo.getDocCount());\n            field.numberDVWriter.flush(field.fieldInfo, state,\n                                       dvConsumer.addNumericField(field.fieldInfo,\n                                                                  field.numberDVWriter.minValue,\n                                                                  field.numberDVWriter.maxValue));\n            // nocommit must null it out now else next seg\n            // will flush even if no docs had DV...?\n          }\n        }\n        field = field.next;\n      }\n    }\n\n    assert fields.size() == totalFieldCount;\n\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer, dvConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0837ab0472feecb3a54260729d845f839e1cbd72","date":1358283639,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    // nocommit\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    fieldsWriter.flush(state);\n    consumer.flush(childFields, state);\n\n    for (DocValuesConsumerHolder consumer : docValues.values()) {\n      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());\n    }\n    \n    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS\n    IOUtils.close(perDocConsumer);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":["fa0f44f887719e97183771e977cfc4bfb485b766","cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18","16cbef32b882ec68df422af3f08845ec82620335","a0f42e0639920b2e917c9ece35fb68ad83021e38","6c18273ea5b3974d2f30117f46f1ae416c28f727"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8435160e9702b19398118ddf76b61c846612b6a4","date":1380349140,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":["5f6bd27530a2846413fe2d00030493c0e2d3a072","5f6bd27530a2846413fe2d00030493c0e2d3a072","5f6bd27530a2846413fe2d00030493c0e2d3a072","86a0a50d2d14aaee1e635bbec914468551f7f9a2","86a0a50d2d14aaee1e635bbec914468551f7f9a2","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":5,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":5,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#flush(SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor#flush(SegmentWriteState).mjava","sourceNew":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    // NOTE: caller (DocumentsWriterPerThread) handles\n    // aborting on any exception from this method\n\n    int numDocs = state.segmentInfo.getDocCount();\n    writeNorms(state);\n    writeDocValues(state);\n    \n    fillStoredFields(numDocs);\n    storedFieldsWriter.finish(state.fieldInfos, numDocs);\n    storedFieldsWriter.close();\n\n    Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();\n    for (int i=0;i<fieldHash.length;i++) {\n      PerField perField = fieldHash[i];\n      while (perField != null) {\n        if (perField.invertState != null) {\n          fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);\n        }\n        perField = perField.next;\n      }\n    }\n\n    termsHash.flush(fieldsToFlush, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = docWriter.codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","sourceOld":"  @Override\n  public void flush(SegmentWriteState state) throws IOException {\n\n    Map<String,DocFieldConsumerPerField> childFields = new HashMap<>();\n    Collection<DocFieldConsumerPerField> fields = fields();\n    for (DocFieldConsumerPerField f : fields) {\n      childFields.put(f.getFieldInfo().name, f);\n    }\n\n    assert fields.size() == totalFieldCount;\n\n    storedConsumer.flush(state);\n    consumer.flush(childFields, state);\n\n    // Important to save after asking consumer to flush so\n    // consumer can alter the FieldInfo* if necessary.  EG,\n    // FreqProxTermsWriter does this with\n    // FieldInfo.storePayload.\n    FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();\n    infosWriter.write(state.directory, state.segmentInfo.name, \"\", state.fieldInfos, IOContext.DEFAULT);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"16cbef32b882ec68df422af3f08845ec82620335":["129c6e8ac0c0d9a110ba29e4b5f1889374f30076"],"6647091125f681395cbde9bb2b7b947cc4ef9bb3":["cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18"],"a0f42e0639920b2e917c9ece35fb68ad83021e38":["cd9165e54429bb5c99e75d5cb1c926cc98772456"],"f44e5a6148908e8393ddd1c2d8b810a385a743c1":["6647091125f681395cbde9bb2b7b947cc4ef9bb3"],"cd9165e54429bb5c99e75d5cb1c926cc98772456":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"f45457a742a53533c348c4b990b1c579ff364467":["ce73f585d17f53055185a19beb46db23d76e0ad9"],"129c6e8ac0c0d9a110ba29e4b5f1889374f30076":["a0f42e0639920b2e917c9ece35fb68ad83021e38"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"127981e5a1e1d1425c5fdc816ceacf753ca70ee4":["6e2893fd5349134af382d33ccc3d84840394c6c1"],"0837ab0472feecb3a54260729d845f839e1cbd72":["127981e5a1e1d1425c5fdc816ceacf753ca70ee4"],"3d3531470649c0bbe4f9d922f7eeb9c3fc88ef15":["9b637fb447c5b4257f6b4532d84ca91e456c1f2a"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["8435160e9702b19398118ddf76b61c846612b6a4"],"ce73f585d17f53055185a19beb46db23d76e0ad9":["c7a8f6a0f831abdaf62496526336f43bbf7c5bbe"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","3394716f52b34ab259ad5247e7595d9f9db6e935"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["1ed65f3455364344c6d2ff76ea5421aac754eae7"],"1ed65f3455364344c6d2ff76ea5421aac754eae7":["f176b7bac2a187d69335c079b1f923449fb2881f"],"71df1db89d3a713f022b58111aafd14a4b352da0":["3d3531470649c0bbe4f9d922f7eeb9c3fc88ef15"],"8435160e9702b19398118ddf76b61c846612b6a4":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"f176b7bac2a187d69335c079b1f923449fb2881f":["f45457a742a53533c348c4b990b1c579ff364467"],"c7a8f6a0f831abdaf62496526336f43bbf7c5bbe":["71df1db89d3a713f022b58111aafd14a4b352da0"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","52c7e49be259508735752fba88085255014a6ecf"],"cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","16cbef32b882ec68df422af3f08845ec82620335"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18","0837ab0472feecb3a54260729d845f839e1cbd72"],"9b637fb447c5b4257f6b4532d84ca91e456c1f2a":["f44e5a6148908e8393ddd1c2d8b810a385a743c1"],"52c7e49be259508735752fba88085255014a6ecf":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3394716f52b34ab259ad5247e7595d9f9db6e935"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd9165e54429bb5c99e75d5cb1c926cc98772456","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"16cbef32b882ec68df422af3f08845ec82620335":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"6647091125f681395cbde9bb2b7b947cc4ef9bb3":["f44e5a6148908e8393ddd1c2d8b810a385a743c1"],"a0f42e0639920b2e917c9ece35fb68ad83021e38":["129c6e8ac0c0d9a110ba29e4b5f1889374f30076"],"f44e5a6148908e8393ddd1c2d8b810a385a743c1":["9b637fb447c5b4257f6b4532d84ca91e456c1f2a"],"cd9165e54429bb5c99e75d5cb1c926cc98772456":["a0f42e0639920b2e917c9ece35fb68ad83021e38"],"f45457a742a53533c348c4b990b1c579ff364467":["f176b7bac2a187d69335c079b1f923449fb2881f"],"129c6e8ac0c0d9a110ba29e4b5f1889374f30076":["16cbef32b882ec68df422af3f08845ec82620335"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"127981e5a1e1d1425c5fdc816ceacf753ca70ee4":["0837ab0472feecb3a54260729d845f839e1cbd72"],"0837ab0472feecb3a54260729d845f839e1cbd72":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"3d3531470649c0bbe4f9d922f7eeb9c3fc88ef15":["71df1db89d3a713f022b58111aafd14a4b352da0"],"ce73f585d17f53055185a19beb46db23d76e0ad9":["f45457a742a53533c348c4b990b1c579ff364467"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"6e2893fd5349134af382d33ccc3d84840394c6c1":["127981e5a1e1d1425c5fdc816ceacf753ca70ee4"],"1ed65f3455364344c6d2ff76ea5421aac754eae7":["6e2893fd5349134af382d33ccc3d84840394c6c1"],"71df1db89d3a713f022b58111aafd14a4b352da0":["c7a8f6a0f831abdaf62496526336f43bbf7c5bbe"],"8435160e9702b19398118ddf76b61c846612b6a4":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"f176b7bac2a187d69335c079b1f923449fb2881f":["1ed65f3455364344c6d2ff76ea5421aac754eae7"],"c7a8f6a0f831abdaf62496526336f43bbf7c5bbe":["ce73f585d17f53055185a19beb46db23d76e0ad9"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18":["6647091125f681395cbde9bb2b7b947cc4ef9bb3","d4d69c535930b5cce125cff868d40f6373dc27d4"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["8435160e9702b19398118ddf76b61c846612b6a4"],"9b637fb447c5b4257f6b4532d84ca91e456c1f2a":["3d3531470649c0bbe4f9d922f7eeb9c3fc88ef15"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}