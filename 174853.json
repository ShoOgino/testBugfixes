{"path":"lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","commits":[{"id":"9e8d5a6ffbfa3405d234a87c833741eabed98d13","date":1326725835,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexNormsConsumer.NormsWriter#merge(MergeState).mjava","sourceNew":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.normsPresent()) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","sourceOld":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e92a321a2612261e1010a4038c8586e30b081552","date":1328456689,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","sourceNew":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.normsPresent()) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","sourceOld":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.normsPresent()) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9e8d5a6ffbfa3405d234a87c833741eabed98d13":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e92a321a2612261e1010a4038c8586e30b081552":["9e8d5a6ffbfa3405d234a87c833741eabed98d13"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e92a321a2612261e1010a4038c8586e30b081552"]},"commit2Childs":{"9e8d5a6ffbfa3405d234a87c833741eabed98d13":["e92a321a2612261e1010a4038c8586e30b081552"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9e8d5a6ffbfa3405d234a87c833741eabed98d13"],"e92a321a2612261e1010a4038c8586e30b081552":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}