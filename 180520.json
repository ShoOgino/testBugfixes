{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a78a90fc9701e511308346ea29f4f5e548bb39fe","date":1329489995,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["46ab5314fa7311a6a5c0d9f4913464fe322a48d5"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4a470c93b2b0f8f51241f52705fc110a01f27ad2","date":1337969379,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":["05fe562aa248790944d43cdd478f512572835ba0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29f7cc7c185412da66c1d0089d9e75da01329a00","date":1353364851,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.numOrd());\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(2, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d7e5f3aa5935964617824d1f9b2599ddb334464","date":1353762831,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    assertEquals(4, dti.size());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.numOrd());\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(2, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"32608e0a08e76fe8668cd1dcca0e7a8f6d7f3f0a","date":1357739321,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    assertEquals(4, dti.size());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["4a470c93b2b0f8f51241f52705fc110a01f27ad2","46ab5314fa7311a6a5c0d9f4913464fe322a48d5"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"426a4760316fc52cf79e191cadfcb328dfc2d1ca","date":1394042725,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["05fe562aa248790944d43cdd478f512572835ba0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.shutdown();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.shutdown();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.shutdown();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.shutdown();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.shutdown();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.shutdown();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.shutdown();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.shutdown();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.shutdown();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.shutdown();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.shutdown();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.shutdown();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.shutdown();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.shutdown();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random().nextFloat() * PackedInts.FAST);\n    assertEquals(4, dti.getValueCount());\n    BytesRef br = new BytesRef();\n    dti.lookupOrd(2, br);\n    assertEquals(bigTermBytesRef, br);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.shutdown();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.shutdown();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["05fe562aa248790944d43cdd478f512572835ba0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ef28e085d0747a245bb0e34b72c45d7a05c95c0e","date":1409922824,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"20edfd06a7f4a1a89b2d7e32265f366c64d8463c","date":1435481232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    iwc.setCodec(TestUtil.getDefaultCodec());\n\n    w = new RandomIndexWriter(random(), dir, iwc);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random(), dir);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["05fe562aa248790944d43cdd478f512572835ba0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05fe562aa248790944d43cdd478f512572835ba0","date":1455901667,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document hugeDoc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    hugeDoc.add(new TextField(\"content\", contents, Field.Store.NO));\n    expectThrows(IllegalArgumentException.class, () -> {\n      w.addDocument(hugeDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    iwc.setCodec(TestUtil.getDefaultCodec());\n\n    RandomIndexWriter w2 = new RandomIndexWriter(random(), dir, iwc);\n\n    contentField.setStringValue(\"other\");\n    w2.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w2.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w2.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w2.addDocument(doc);\n\n    reader = w2.getReader();\n    w2.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    try {\n      w.addDocument(doc);\n      fail(\"should have hit exception\");\n    } catch (IllegalArgumentException iae) {\n      // expected\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    iwc.setCodec(TestUtil.getDefaultCodec());\n\n    w = new RandomIndexWriter(random(), dir, iwc);\n\n    contentField.setStringValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["426a4760316fc52cf79e191cadfcb328dfc2d1ca","46ab5314fa7311a6a5c0d9f4913464fe322a48d5","d0ef034a4f10871667ae75181537775ddcf8ade4","20edfd06a7f4a1a89b2d7e32265f366c64d8463c","04f07771a2a7dd3a395700665ed839c3dae2def2"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"49f1924bd448393fbdfef8b5ebed799f938169d3","date":1600069616,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document hugeDoc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    hugeDoc.add(new TextField(\"content\", contents, Field.Store.NO));\n    expectThrows(IllegalArgumentException.class, () -> {\n      w.addDocument(hugeDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    iwc.setCodec(TestUtil.getDefaultCodec());\n\n    RandomIndexWriter w2 = new RandomIndexWriter(random(), dir, iwc);\n\n    contentField.setStringValue(\"other\");\n    w2.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w2.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w2.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w2.addDocument(doc);\n\n    reader = w2.getReader();\n    w2.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document hugeDoc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    hugeDoc.add(new TextField(\"content\", contents, Field.Store.NO));\n    expectThrows(IllegalArgumentException.class, () -> {\n      w.addDocument(hugeDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    iwc.setCodec(TestUtil.getDefaultCodec());\n\n    RandomIndexWriter w2 = new RandomIndexWriter(random(), dir, iwc);\n\n    contentField.setStringValue(\"other\");\n    w2.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w2.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w2.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w2.addDocument(doc);\n\n    reader = w2.getReader();\n    w2.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0dcf8f79417865e5028d753e669fae06457e8369","date":1600073240,"type":3,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document hugeDoc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    hugeDoc.add(new TextField(\"content\", contents, Field.Store.NO));\n    expectThrows(IllegalArgumentException.class, () -> {\n      w.addDocument(hugeDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    iwc.setCodec(TestUtil.getDefaultCodec());\n\n    RandomIndexWriter w2 = new RandomIndexWriter(random(), dir, iwc);\n\n    contentField.setStringValue(\"other\");\n    w2.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w2.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w2.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w2.addDocument(doc);\n\n    reader = w2.getReader();\n    w2.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document hugeDoc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    hugeDoc.add(new TextField(\"content\", contents, Field.Store.NO));\n    expectThrows(IllegalArgumentException.class, () -> {\n      w.addDocument(hugeDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    w.addDocument(doc);\n\n    // So we remove the deleted doc:\n    w.forceMerge(1);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(0, reader.docFreq(new Term(\"content\", \"term\")));\n\n    // Make sure the doc that has the massive term is NOT in\n    // the index:\n    assertEquals(\"document with wicked long term is in the index!\", 1, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    iwc.setCodec(TestUtil.getDefaultCodec());\n\n    RandomIndexWriter w2 = new RandomIndexWriter(random(), dir, iwc);\n\n    contentField.setStringValue(\"other\");\n    w2.addDocument(doc);\n\n    contentField.setStringValue(\"term\");\n    w2.addDocument(doc);\n\n    contentField.setStringValue(bigTerm);\n    w2.addDocument(doc);\n\n    contentField.setStringValue(\"zzz\");\n    w2.addDocument(doc);\n\n    reader = w2.getReader();\n    w2.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["05fe562aa248790944d43cdd478f512572835ba0"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["d4d69c535930b5cce125cff868d40f6373dc27d4","426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"29f7cc7c185412da66c1d0089d9e75da01329a00":["322360ac5185a8446d3e0b530b2068bef67cd3d5"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0dcf8f79417865e5028d753e669fae06457e8369":["05fe562aa248790944d43cdd478f512572835ba0","49f1924bd448393fbdfef8b5ebed799f938169d3"],"05fe562aa248790944d43cdd478f512572835ba0":["20edfd06a7f4a1a89b2d7e32265f366c64d8463c"],"56572ec06f1407c066d6b7399413178b33176cd8":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","93dd449115a9247533e44bab47e8429e5dccbc6d"],"20edfd06a7f4a1a89b2d7e32265f366c64d8463c":["ef28e085d0747a245bb0e34b72c45d7a05c95c0e"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["4a470c93b2b0f8f51241f52705fc110a01f27ad2"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"ef28e085d0747a245bb0e34b72c45d7a05c95c0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["29f7cc7c185412da66c1d0089d9e75da01329a00"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"426a4760316fc52cf79e191cadfcb328dfc2d1ca":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"4a470c93b2b0f8f51241f52705fc110a01f27ad2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"32608e0a08e76fe8668cd1dcca0e7a8f6d7f3f0a":["9d7e5f3aa5935964617824d1f9b2599ddb334464"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["322360ac5185a8446d3e0b530b2068bef67cd3d5","32608e0a08e76fe8668cd1dcca0e7a8f6d7f3f0a"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["04f07771a2a7dd3a395700665ed839c3dae2def2","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["04f07771a2a7dd3a395700665ed839c3dae2def2","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0dcf8f79417865e5028d753e669fae06457e8369"]},"commit2Childs":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["0dcf8f79417865e5028d753e669fae06457e8369"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"29f7cc7c185412da66c1d0089d9e75da01329a00":["9d7e5f3aa5935964617824d1f9b2599ddb334464"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"0dcf8f79417865e5028d753e669fae06457e8369":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"05fe562aa248790944d43cdd478f512572835ba0":["49f1924bd448393fbdfef8b5ebed799f938169d3","0dcf8f79417865e5028d753e669fae06457e8369"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"20edfd06a7f4a1a89b2d7e32265f366c64d8463c":["05fe562aa248790944d43cdd478f512572835ba0"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["322360ac5185a8446d3e0b530b2068bef67cd3d5","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"ef28e085d0747a245bb0e34b72c45d7a05c95c0e":["20edfd06a7f4a1a89b2d7e32265f366c64d8463c"],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["32608e0a08e76fe8668cd1dcca0e7a8f6d7f3f0a"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["56572ec06f1407c066d6b7399413178b33176cd8","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","93dd449115a9247533e44bab47e8429e5dccbc6d"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["29f7cc7c185412da66c1d0089d9e75da01329a00","d4d69c535930b5cce125cff868d40f6373dc27d4","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"426a4760316fc52cf79e191cadfcb328dfc2d1ca":["96ea64d994d340044e0d57aeb6a5871539d10ca5","ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"4a470c93b2b0f8f51241f52705fc110a01f27ad2":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"32608e0a08e76fe8668cd1dcca0e7a8f6d7f3f0a":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["96ea64d994d340044e0d57aeb6a5871539d10ca5","426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["56572ec06f1407c066d6b7399413178b33176cd8","d0ef034a4f10871667ae75181537775ddcf8ade4"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ef28e085d0747a245bb0e34b72c45d7a05c95c0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["4a470c93b2b0f8f51241f52705fc110a01f27ad2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","56572ec06f1407c066d6b7399413178b33176cd8","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}