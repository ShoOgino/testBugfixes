{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,String,String,byte[]).mjava","commits":[{"id":"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","date":1426444850,"type":0,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,String,String,byte[]).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, String leaderCoreNodeName,\n                                       String znodePath, byte[] znodeData) throws KeeperException, InterruptedException {\n    String leaderSeqPath = getLeaderSeqPath(collection, leaderCoreNodeName);\n    if (leaderSeqPath == null) {\n      throw new SolrException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to 'down' for znode: \" + znodePath +\n              \" because the zookeeper leader sequence for leader: \" + leaderCoreNodeName + \" is null\");\n    }\n    if (zkClient.exists(znodePath, true)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, true);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, true);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, true);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,String,String,byte[]).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, String leaderCoreNodeName,\n                                       String znodePath, byte[] znodeData) throws KeeperException, InterruptedException {\n    String leaderSeqPath = getLeaderSeqPath(collection, leaderCoreNodeName);\n    if (leaderSeqPath == null) {\n      throw new SolrException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to 'down' for znode: \" + znodePath +\n              \" because the zookeeper leader sequence for leader: \" + leaderCoreNodeName + \" is null\");\n    }\n    if (zkClient.exists(znodePath, true)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, true);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, true);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, true);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22859cb40e09867e7da8de84a31956c07259f82f","date":1441822065,"type":5,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,String,String,byte[],boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,String,String,byte[]).mjava","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, String leaderCoreNodeName,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    String leaderSeqPath = getLeaderSeqPath(collection, leaderCoreNodeName);\n    if (leaderSeqPath == null) {\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to 'down' for znode: \" + znodePath +\n              \" because the zookeeper leader sequence for leader: \" + leaderCoreNodeName + \" is null\");\n    }\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","sourceOld":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, String leaderCoreNodeName,\n                                       String znodePath, byte[] znodeData) throws KeeperException, InterruptedException {\n    String leaderSeqPath = getLeaderSeqPath(collection, leaderCoreNodeName);\n    if (leaderSeqPath == null) {\n      throw new SolrException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to 'down' for znode: \" + znodePath +\n              \" because the zookeeper leader sequence for leader: \" + leaderCoreNodeName + \" is null\");\n    }\n    if (zkClient.exists(znodePath, true)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, true);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, true);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc"],"22859cb40e09867e7da8de84a31956c07259f82f":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["22859cb40e09867e7da8de84a31956c07259f82f"]},"commit2Childs":{"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","22859cb40e09867e7da8de84a31956c07259f82f"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"22859cb40e09867e7da8de84a31956c07259f82f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}