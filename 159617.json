{"path":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","commits":[{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","pathOld":"/dev/null","sourceNew":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) throws IOException {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    numericDVUpdates = freezeNumericDVUpdates(updates.numericUpdates);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    binaryDVUpdates = freezeBinaryDVUpdates(updates.binaryUpdates);\n    \n    bytesUsed = (int) (deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY \n                       + numericDVUpdates.length + binaryDVUpdates.length);\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.bytesUsed.get(), bytesUsed, 100.*bytesUsed/updates.bytesUsed.get(),\n                                             privateSegment));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","pathOld":"/dev/null","sourceNew":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) throws IOException {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    numericDVUpdates = freezeNumericDVUpdates(updates.numericUpdates);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    binaryDVUpdates = freezeBinaryDVUpdates(updates.binaryUpdates);\n    \n    bytesUsed = (int) (deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY \n                       + numericDVUpdates.length + binaryDVUpdates.length);\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.bytesUsed.get(), bytesUsed, 100.*bytesUsed/updates.bytesUsed.get(),\n                                             privateSegment));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","pathOld":"/dev/null","sourceNew":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) throws IOException {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    numericDVUpdates = freezeNumericDVUpdates(updates.numericUpdates);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    binaryDVUpdates = freezeBinaryDVUpdates(updates.binaryUpdates);\n    \n    bytesUsed = (int) (deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY \n                       + numericDVUpdates.length + binaryDVUpdates.length);\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.bytesUsed.get(), bytesUsed, 100.*bytesUsed/updates.bytesUsed.get(),\n                                             privateSegment));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e129bd6cb34a236558a49edf108a49d5c15e0e1","date":1525081316,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","sourceNew":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) throws IOException {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    Counter counter = Counter.newCounter();\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    numericDVUpdates = freezeDVUpdates(updates.numericUpdates, counter::addAndGet);\n    numericDVUpdateCount = (int)counter.get();\n    counter.addAndGet(-counter.get());\n    assert counter.get() == 0;\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    binaryDVUpdates = freezeDVUpdates(updates.binaryUpdates, counter::addAndGet);\n    binaryDVUpdateCount = (int)counter.get();\n\n    bytesUsed = (int) (deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY \n                       + numericDVUpdates.length + binaryDVUpdates.length);\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.bytesUsed.get(), bytesUsed, 100.*bytesUsed/updates.bytesUsed.get(),\n                                             privateSegment));\n    }\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) throws IOException {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    numericDVUpdates = freezeNumericDVUpdates(updates.numericUpdates);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    binaryDVUpdates = freezeBinaryDVUpdates(updates.binaryUpdates);\n    \n    bytesUsed = (int) (deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY \n                       + numericDVUpdates.length + binaryDVUpdates.length);\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.bytesUsed.get(), bytesUsed, 100.*bytesUsed/updates.bytesUsed.get(),\n                                             privateSegment));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f42883db49d143abc1a0f176ba47e3388dafb608","date":1525083166,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","sourceNew":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) throws IOException {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    Counter counter = Counter.newCounter();\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    numericDVUpdates = freezeDVUpdates(updates.numericUpdates, counter::addAndGet);\n    numericDVUpdateCount = (int)counter.get();\n    counter.addAndGet(-counter.get());\n    assert counter.get() == 0;\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    binaryDVUpdates = freezeDVUpdates(updates.binaryUpdates, counter::addAndGet);\n    binaryDVUpdateCount = (int)counter.get();\n\n    bytesUsed = (int) (deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY \n                       + numericDVUpdates.length + binaryDVUpdates.length);\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.bytesUsed.get(), bytesUsed, 100.*bytesUsed/updates.bytesUsed.get(),\n                                             privateSegment));\n    }\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) throws IOException {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    numericDVUpdates = freezeNumericDVUpdates(updates.numericUpdates);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    binaryDVUpdates = freezeBinaryDVUpdates(updates.binaryUpdates);\n    \n    bytesUsed = (int) (deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY \n                       + numericDVUpdates.length + binaryDVUpdates.length);\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.bytesUsed.get(), bytesUsed, 100.*bytesUsed/updates.bytesUsed.get(),\n                                             privateSegment));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28211671436f185419b3f7e53ccfc3911441ab65","date":1544026960,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","sourceNew":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) throws IOException {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    this.fieldUpdates = Collections.unmodifiableMap(new HashMap<>(updates.fieldUpdates));\n    this.fieldUpdatesCount = updates.numFieldUpdates.get();\n\n    bytesUsed = (int) ((deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY)\n        + updates.fieldUpdatesBytesUsed.get());\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.ramBytesUsed(), bytesUsed, 100.*bytesUsed/updates.ramBytesUsed(),\n                                             privateSegment));\n    }\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) throws IOException {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    Counter counter = Counter.newCounter();\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    numericDVUpdates = freezeDVUpdates(updates.numericUpdates, counter::addAndGet);\n    numericDVUpdateCount = (int)counter.get();\n    counter.addAndGet(-counter.get());\n    assert counter.get() == 0;\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    binaryDVUpdates = freezeDVUpdates(updates.binaryUpdates, counter::addAndGet);\n    binaryDVUpdateCount = (int)counter.get();\n\n    bytesUsed = (int) (deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY \n                       + numericDVUpdates.length + binaryDVUpdates.length);\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.bytesUsed.get(), bytesUsed, 100.*bytesUsed/updates.bytesUsed.get(),\n                                             privateSegment));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83b6ce113ec151d7bf9175578d92d5320f91ab2e","date":1544711434,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","sourceNew":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    this.fieldUpdates = Collections.unmodifiableMap(new HashMap<>(updates.fieldUpdates));\n    this.fieldUpdatesCount = updates.numFieldUpdates.get();\n\n    bytesUsed = (int) ((deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY)\n        + updates.fieldUpdatesBytesUsed.get());\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.ramBytesUsed(), bytesUsed, 100.*bytesUsed/updates.ramBytesUsed(),\n                                             privateSegment));\n    }\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) throws IOException {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    this.fieldUpdates = Collections.unmodifiableMap(new HashMap<>(updates.fieldUpdates));\n    this.fieldUpdatesCount = updates.numFieldUpdates.get();\n\n    bytesUsed = (int) ((deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY)\n        + updates.fieldUpdatesBytesUsed.get());\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.ramBytesUsed(), bytesUsed, 100.*bytesUsed/updates.ramBytesUsed(),\n                                             privateSegment));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd2b7ad87a1172d1d06dc8491991e52534c90f71","date":1555507572,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","sourceNew":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    this.fieldUpdates = Map.copyOf(updates.fieldUpdates);\n    this.fieldUpdatesCount = updates.numFieldUpdates.get();\n\n    bytesUsed = (int) ((deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY)\n        + updates.fieldUpdatesBytesUsed.get());\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.ramBytesUsed(), bytesUsed, 100.*bytesUsed/updates.ramBytesUsed(),\n                                             privateSegment));\n    }\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    this.fieldUpdates = Collections.unmodifiableMap(new HashMap<>(updates.fieldUpdates));\n    this.fieldUpdatesCount = updates.numFieldUpdates.get();\n\n    bytesUsed = (int) ((deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY)\n        + updates.fieldUpdatesBytesUsed.get());\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.ramBytesUsed(), bytesUsed, 100.*bytesUsed/updates.ramBytesUsed(),\n                                             privateSegment));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb0b91dbd7ffa9329d88b6cf6d606e542fed61d8","date":1582222690,"type":3,"author":"Nhat Nguyen","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","sourceNew":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    updates.fieldUpdates.values().forEach(FieldUpdatesBuffer::finish);\n    this.fieldUpdates = Map.copyOf(updates.fieldUpdates);\n    this.fieldUpdatesCount = updates.numFieldUpdates.get();\n\n    bytesUsed = (int) ((deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY)\n        + updates.fieldUpdatesBytesUsed.get());\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.ramBytesUsed(), bytesUsed, 100.*bytesUsed/updates.ramBytesUsed(),\n                                             privateSegment));\n    }\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    this.fieldUpdates = Map.copyOf(updates.fieldUpdates);\n    this.fieldUpdatesCount = updates.numFieldUpdates.get();\n\n    bytesUsed = (int) ((deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY)\n        + updates.fieldUpdatesBytesUsed.get());\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.ramBytesUsed(), bytesUsed, 100.*bytesUsed/updates.ramBytesUsed(),\n                                             privateSegment));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7aff85e853fc8018761caa9a7803b1db411db8c","date":1586893039,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(InfoStream,BufferedUpdates,SegmentCommitInfo).mjava","sourceNew":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\";\n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    updates.fieldUpdates.values().forEach(FieldUpdatesBuffer::finish);\n    this.fieldUpdates = Map.copyOf(updates.fieldUpdates);\n    this.fieldUpdatesCount = updates.numFieldUpdates.get();\n\n    bytesUsed = (int) ((deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY)\n        + updates.fieldUpdatesBytesUsed.get());\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.ramBytesUsed(), bytesUsed, 100.*bytesUsed/updates.ramBytesUsed(),\n                                             privateSegment));\n    }\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(InfoStream infoStream, BufferedUpdates updates, SegmentCommitInfo privateSegment) {\n    this.infoStream = infoStream;\n    this.privateSegment = privateSegment;\n    assert updates.deleteDocIDs.isEmpty();\n    assert privateSegment == null || updates.deleteTerms.isEmpty() : \"segment private packet should only have del queries\"; \n    Term termsArray[] = updates.deleteTerms.keySet().toArray(new Term[updates.deleteTerms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    deleteTerms = builder.finish();\n    \n    deleteQueries = new Query[updates.deleteQueries.size()];\n    deleteQueryLimits = new int[updates.deleteQueries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : updates.deleteQueries.entrySet()) {\n      deleteQueries[upto] = ent.getKey();\n      deleteQueryLimits[upto] = ent.getValue();\n      upto++;\n    }\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated.\n    updates.fieldUpdates.values().forEach(FieldUpdatesBuffer::finish);\n    this.fieldUpdates = Map.copyOf(updates.fieldUpdates);\n    this.fieldUpdatesCount = updates.numFieldUpdates.get();\n\n    bytesUsed = (int) ((deleteTerms.ramBytesUsed() + deleteQueries.length * BYTES_PER_DEL_QUERY)\n        + updates.fieldUpdatesBytesUsed.get());\n    \n    numTermDeletes = updates.numTermDeletes.get();\n    if (infoStream != null && infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\", String.format(Locale.ROOT,\n                                             \"compressed %d to %d bytes (%.2f%%) for deletes/updates; private segment %s\",\n                                             updates.ramBytesUsed(), bytesUsed, 100.*bytesUsed/updates.ramBytesUsed(),\n                                             privateSegment));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fb0b91dbd7ffa9329d88b6cf6d606e542fed61d8":["fd2b7ad87a1172d1d06dc8491991e52534c90f71"],"83b6ce113ec151d7bf9175578d92d5320f91ab2e":["28211671436f185419b3f7e53ccfc3911441ab65"],"28211671436f185419b3f7e53ccfc3911441ab65":["f42883db49d143abc1a0f176ba47e3388dafb608"],"e7aff85e853fc8018761caa9a7803b1db411db8c":["fb0b91dbd7ffa9329d88b6cf6d606e542fed61d8"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f42883db49d143abc1a0f176ba47e3388dafb608":["28288370235ed02234a64753cdbf0c6ec096304a","7e129bd6cb34a236558a49edf108a49d5c15e0e1"],"fd2b7ad87a1172d1d06dc8491991e52534c90f71":["83b6ce113ec151d7bf9175578d92d5320f91ab2e"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e7aff85e853fc8018761caa9a7803b1db411db8c"]},"commit2Childs":{"fb0b91dbd7ffa9329d88b6cf6d606e542fed61d8":["e7aff85e853fc8018761caa9a7803b1db411db8c"],"83b6ce113ec151d7bf9175578d92d5320f91ab2e":["fd2b7ad87a1172d1d06dc8491991e52534c90f71"],"28211671436f185419b3f7e53ccfc3911441ab65":["83b6ce113ec151d7bf9175578d92d5320f91ab2e"],"e7aff85e853fc8018761caa9a7803b1db411db8c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["f42883db49d143abc1a0f176ba47e3388dafb608"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"f42883db49d143abc1a0f176ba47e3388dafb608":["28211671436f185419b3f7e53ccfc3911441ab65"],"fd2b7ad87a1172d1d06dc8491991e52534c90f71":["fb0b91dbd7ffa9329d88b6cf6d606e542fed61d8"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["7e129bd6cb34a236558a49edf108a49d5c15e0e1","f42883db49d143abc1a0f176ba47e3388dafb608"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}