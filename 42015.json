{"path":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","commits":[{"id":"4107dd39b127d892359c5c1d67d0f14d92f1a9bf","date":1351689723,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","pathOld":"/dev/null","sourceNew":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<Term>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    AtomicReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<Term>(terms));\n\n    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","sourceNew":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    AtomicReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<Term>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    AtomicReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<Term>(terms));\n\n    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","sourceNew":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.shutdown();\n    assertEquals(1, reader.leaves().size());\n    AtomicReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    AtomicReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","sourceNew":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    AtomicReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.shutdown();\n    assertEquals(1, reader.leaves().size());\n    AtomicReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","sourceNew":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    AtomicReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80c55596a764e2d397e982828e75fcac5ce430a0","date":1413987559,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","sourceNew":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    FixedBitDocIdSet bits = (FixedBitDocIdSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.bits().cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0abcec02c9851c46c70a75bd42fb6e4d5348ac9e","date":1414135939,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","sourceNew":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    BitDocIdSet bits = (BitDocIdSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.bits().cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    FixedBitDocIdSet bits = (FixedBitDocIdSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.bits().cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c2042d3e27841c5b60112990fc33559e10ccf6dd","date":1424537395,"type":4,"author":"Adrien Grand","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest#testSkipField().mjava","sourceNew":null,"sourceOld":"  public void testSkipField() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int num = atLeast(10);\n    Set<Term> terms = new HashSet<>();\n    for (int i = 0; i < num; i++) {\n      String field = \"field\" + random().nextInt(100);\n      terms.add(new Term(field, \"content1\"));\n      Document doc = new Document();\n      doc.add(newStringField(field, \"content1\", Field.Store.YES));\n      w.addDocument(doc);\n    }\n    int randomFields = random().nextInt(10);\n    for (int i = 0; i < randomFields; i++) {\n      while (true) {\n        String field = \"field\" + random().nextInt(100);\n        Term t = new Term(field, \"content1\");\n        if (!terms.contains(t)) {\n          terms.add(t);\n          break;\n        }\n      }\n    }\n    w.forceMerge(1);\n    IndexReader reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.leaves().size());\n    LeafReaderContext context = reader.leaves().get(0);\n    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));\n\n    BitDocIdSet bits = (BitDocIdSet) tf.getDocIdSet(context, context.reader().getLiveDocs());\n    assertEquals(context.reader().numDocs(), bits.bits().cardinality());  \n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4107dd39b127d892359c5c1d67d0f14d92f1a9bf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["4107dd39b127d892359c5c1d67d0f14d92f1a9bf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"80c55596a764e2d397e982828e75fcac5ce430a0":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"0abcec02c9851c46c70a75bd42fb6e4d5348ac9e":["80c55596a764e2d397e982828e75fcac5ce430a0"],"c2042d3e27841c5b60112990fc33559e10ccf6dd":["0abcec02c9851c46c70a75bd42fb6e4d5348ac9e"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c2042d3e27841c5b60112990fc33559e10ccf6dd"]},"commit2Childs":{"4107dd39b127d892359c5c1d67d0f14d92f1a9bf":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4107dd39b127d892359c5c1d67d0f14d92f1a9bf"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"80c55596a764e2d397e982828e75fcac5ce430a0":["0abcec02c9851c46c70a75bd42fb6e4d5348ac9e"],"0abcec02c9851c46c70a75bd42fb6e4d5348ac9e":["c2042d3e27841c5b60112990fc33559e10ccf6dd"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["80c55596a764e2d397e982828e75fcac5ce430a0"],"c2042d3e27841c5b60112990fc33559e10ccf6dd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}