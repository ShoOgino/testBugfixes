{"path":"modules/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter#DictionaryCompoundWordTokenFilter(Version,TokenStream,CharArraySet,int,int,int,boolean).mjava","commits":[{"id":"c39363fefe2d7f6a6d50ce8e8b758c17a257c58e","date":1328817590,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter#DictionaryCompoundWordTokenFilter(Version,TokenStream,CharArraySet,int,int,int,boolean).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter#DictionaryCompoundWordTokenFilter(Version,TokenStream,Set[#],int,int,int,boolean).mjava","sourceNew":"  /**\n   * Creates a new {@link DictionaryCompoundWordTokenFilter}\n   * \n   * @param matchVersion\n   *          Lucene version to enable correct Unicode 4.0 behavior in the\n   *          dictionaries if Version > 3.0. See <a\n   *          href=\"CompoundWordTokenFilterBase#version\"\n   *          >CompoundWordTokenFilterBase</a> for details.\n   * @param input\n   *          the {@link TokenStream} to process\n   * @param dictionary\n   *          the word dictionary to match against.\n   * @param minWordSize\n   *          only words longer than this get processed\n   * @param minSubwordSize\n   *          only subwords longer than this get to the output stream\n   * @param maxSubwordSize\n   *          only subwords shorter than this get to the output stream\n   * @param onlyLongestMatch\n   *          Add only the longest matching subword to the stream\n   */\n  public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input, CharArraySet dictionary,\n      int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {\n    super(matchVersion, input, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);\n  }\n\n","sourceOld":"  /**\n   * Creates a new {@link DictionaryCompoundWordTokenFilter}\n   * \n   * @param matchVersion\n   *          Lucene version to enable correct Unicode 4.0 behavior in the\n   *          dictionaries if Version > 3.0. See <a\n   *          href=\"CompoundWordTokenFilterBase#version\"\n   *          >CompoundWordTokenFilterBase</a> for details.\n   * @param input\n   *          the {@link TokenStream} to process\n   * @param dictionary\n   *          the word dictionary to match against.\n   * @param minWordSize\n   *          only words longer than this get processed\n   * @param minSubwordSize\n   *          only subwords longer than this get to the output stream\n   * @param maxSubwordSize\n   *          only subwords shorter than this get to the output stream\n   * @param onlyLongestMatch\n   *          Add only the longest matching subword to the stream\n   */\n  public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input, Set<?> dictionary,\n      int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {\n    super(matchVersion, input, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter#DictionaryCompoundWordTokenFilter(Version,TokenStream,CharArraySet,int,int,int,boolean).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter#DictionaryCompoundWordTokenFilter(Version,TokenStream,CharArraySet,int,int,int,boolean).mjava","sourceNew":"  /**\n   * Creates a new {@link DictionaryCompoundWordTokenFilter}\n   * \n   * @param matchVersion\n   *          Lucene version to enable correct Unicode 4.0 behavior in the\n   *          dictionaries if Version > 3.0. See <a\n   *          href=\"CompoundWordTokenFilterBase#version\"\n   *          >CompoundWordTokenFilterBase</a> for details.\n   * @param input\n   *          the {@link TokenStream} to process\n   * @param dictionary\n   *          the word dictionary to match against.\n   * @param minWordSize\n   *          only words longer than this get processed\n   * @param minSubwordSize\n   *          only subwords longer than this get to the output stream\n   * @param maxSubwordSize\n   *          only subwords shorter than this get to the output stream\n   * @param onlyLongestMatch\n   *          Add only the longest matching subword to the stream\n   */\n  public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input, CharArraySet dictionary,\n      int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {\n    super(matchVersion, input, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);\n  }\n\n","sourceOld":"  /**\n   * Creates a new {@link DictionaryCompoundWordTokenFilter}\n   * \n   * @param matchVersion\n   *          Lucene version to enable correct Unicode 4.0 behavior in the\n   *          dictionaries if Version > 3.0. See <a\n   *          href=\"CompoundWordTokenFilterBase#version\"\n   *          >CompoundWordTokenFilterBase</a> for details.\n   * @param input\n   *          the {@link TokenStream} to process\n   * @param dictionary\n   *          the word dictionary to match against.\n   * @param minWordSize\n   *          only words longer than this get processed\n   * @param minSubwordSize\n   *          only subwords longer than this get to the output stream\n   * @param maxSubwordSize\n   *          only subwords shorter than this get to the output stream\n   * @param onlyLongestMatch\n   *          Add only the longest matching subword to the stream\n   */\n  public DictionaryCompoundWordTokenFilter(Version matchVersion, TokenStream input, CharArraySet dictionary,\n      int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {\n    super(matchVersion, input, dictionary, minWordSize, minSubwordSize, maxSubwordSize, onlyLongestMatch);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["c39363fefe2d7f6a6d50ce8e8b758c17a257c58e"],"c39363fefe2d7f6a6d50ce8e8b758c17a257c58e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c39363fefe2d7f6a6d50ce8e8b758c17a257c58e":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c39363fefe2d7f6a6d50ce8e8b758c17a257c58e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}