{"path":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"409324b31a1419d7c05a38211168cf317e39be77","date":1344866765,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"99c9d8533c954f661481ae44273622957dbf572f","date":1380991288,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"090a3a6b4b32e55f8fe1eab3359dbe628a208a0c","date":1399054058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3184874f7f3aca850248483485b4995343066875","date":1413876758,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Field field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a22eafe3f72a4c2945eaad9547e6c78816978f4","date":1413956657,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Field field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"804b857d1066ab5185b3b9101bde41b0b71426ec","date":1435846169,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Field field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.getBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Field field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"770342641f7b505eaa8dccdc666158bff2419109","date":1449868421,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Field field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof LegacyIntField ||\n          field instanceof LegacyLongField ||\n          field instanceof LegacyFloatField ||\n          field instanceof LegacyDoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.getBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Field field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof IntField ||\n          field instanceof LongField ||\n          field instanceof FloatField ||\n          field instanceof DoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.getBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":["067bb525d2e4993889147c508e2ccb5158f409b1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof LegacyIntField ||\n          field instanceof LegacyLongField ||\n          field instanceof LegacyFloatField ||\n          field instanceof LegacyDoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.getBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Field> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Field field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof LegacyIntField ||\n          field instanceof LegacyLongField ||\n          field instanceof LegacyFloatField ||\n          field instanceof LegacyDoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.getBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"067bb525d2e4993889147c508e2ccb5158f409b1","date":1457385185,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (field.fieldType().indexOptions() == IndexOptions.NONE ||\n          field.fieldType().tokenized() == false) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.getBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<IndexableField> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final IndexableField field : fields) {\n      if (!field.fieldType().tokenized() ||\n          field instanceof LegacyIntField ||\n          field instanceof LegacyLongField ||\n          field instanceof LegacyFloatField ||\n          field instanceof LegacyDoubleField) {\n        continue;\n      }\n      \n      final TokenStream stream = field.tokenStream(analyzer, null);\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      while(stream.incrementToken()) {\n        termAtt.getBytesRef();\n        tokenCount++;\n      }\n      stream.end();\n      stream.close();\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":["a78a90fc9701e511308346ea29f4f5e548bb39fe","770342641f7b505eaa8dccdc666158bff2419109"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"804b857d1066ab5185b3b9101bde41b0b71426ec":["3184874f7f3aca850248483485b4995343066875"],"99c9d8533c954f661481ae44273622957dbf572f":["1d028314cced5858683a1bb4741423d0f934257b"],"409324b31a1419d7c05a38211168cf317e39be77":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"770342641f7b505eaa8dccdc666158bff2419109":["804b857d1066ab5185b3b9101bde41b0b71426ec"],"3184874f7f3aca850248483485b4995343066875":["090a3a6b4b32e55f8fe1eab3359dbe628a208a0c"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1d028314cced5858683a1bb4741423d0f934257b":["b89678825b68eccaf09e6ab71675fc0b0af1e099","409324b31a1419d7c05a38211168cf317e39be77"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":["090a3a6b4b32e55f8fe1eab3359dbe628a208a0c","3184874f7f3aca850248483485b4995343066875"],"067bb525d2e4993889147c508e2ccb5158f409b1":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"090a3a6b4b32e55f8fe1eab3359dbe628a208a0c":["99c9d8533c954f661481ae44273622957dbf572f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["067bb525d2e4993889147c508e2ccb5158f409b1"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["770342641f7b505eaa8dccdc666158bff2419109"]},"commit2Childs":{"804b857d1066ab5185b3b9101bde41b0b71426ec":["770342641f7b505eaa8dccdc666158bff2419109"],"99c9d8533c954f661481ae44273622957dbf572f":["090a3a6b4b32e55f8fe1eab3359dbe628a208a0c"],"409324b31a1419d7c05a38211168cf317e39be77":["1d028314cced5858683a1bb4741423d0f934257b"],"3184874f7f3aca850248483485b4995343066875":["804b857d1066ab5185b3b9101bde41b0b71426ec","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"770342641f7b505eaa8dccdc666158bff2419109":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"1d028314cced5858683a1bb4741423d0f934257b":["99c9d8533c954f661481ae44273622957dbf572f"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["409324b31a1419d7c05a38211168cf317e39be77","1d028314cced5858683a1bb4741423d0f934257b"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":[],"067bb525d2e4993889147c508e2ccb5158f409b1":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"090a3a6b4b32e55f8fe1eab3359dbe628a208a0c":["3184874f7f3aca850248483485b4995343066875","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["067bb525d2e4993889147c508e2ccb5158f409b1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["0a22eafe3f72a4c2945eaad9547e6c78816978f4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}