{"path":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsReader#Lucene40BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","commits":[{"id":"0628077afea69a2955260949478afabab8e500d8","date":1413915332,"type":0,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsReader#Lucene40BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"/dev/null","sourceNew":"  /** Sole constructor. */\n  public Lucene40BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = state.segmentInfo.name;\n    String termsFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, Lucene40BlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFileName, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      String indexFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, Lucene40BlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFileName, state.context);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      if (version >= Lucene40BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      if (version >= Lucene40BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= Lucene40BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm, maxTerm;\n        if (version >= Lucene40BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        Lucene40FieldReader previous = fields.put(fieldInfo.name,       \n                                          new Lucene40FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c6d238816bcdf9bbe4ec886226d89bd93834eb7e","date":1413925889,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsReader#Lucene40BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsReader#Lucene40BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public Lucene40BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = state.segmentInfo.name;\n    String termsFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    in = state.directory.openInput(termsFileName, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      String indexFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFileName, state.context);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      if (version >= VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      if (version >= VERSION_CHECKSUM) {\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm, maxTerm;\n        if (version >= VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        Lucene40FieldReader previous = fields.put(fieldInfo.name,       \n                                          new Lucene40FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public Lucene40BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = state.segmentInfo.name;\n    String termsFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, Lucene40BlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFileName, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      String indexFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, Lucene40BlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFileName, state.context);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      if (version >= Lucene40BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      if (version >= Lucene40BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= Lucene40BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm, maxTerm;\n        if (version >= Lucene40BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        Lucene40FieldReader previous = fields.put(fieldInfo.name,       \n                                          new Lucene40FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsReader#Lucene40BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public Lucene40BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = state.segmentInfo.name;\n    String termsFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    in = state.directory.openInput(termsFileName, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      String indexFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFileName, state.context);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      if (version >= VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      if (version >= VERSION_CHECKSUM) {\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm, maxTerm;\n        if (version >= VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        Lucene40FieldReader previous = fields.put(fieldInfo.name,       \n                                          new Lucene40FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/blocktree/Lucene40BlockTreeTermsReader#Lucene40BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":null,"sourceOld":"  /** Sole constructor. */\n  public Lucene40BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = state.segmentInfo.name;\n    String termsFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    in = state.directory.openInput(termsFileName, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      String indexFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFileName, state.context);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      if (version >= VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      if (version >= VERSION_CHECKSUM) {\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm, maxTerm;\n        if (version >= VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        Lucene40FieldReader previous = fields.put(fieldInfo.name,       \n                                          new Lucene40FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c6d238816bcdf9bbe4ec886226d89bd93834eb7e":["0628077afea69a2955260949478afabab8e500d8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c6d238816bcdf9bbe4ec886226d89bd93834eb7e"],"0628077afea69a2955260949478afabab8e500d8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"]},"commit2Childs":{"c6d238816bcdf9bbe4ec886226d89bd93834eb7e":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238","0628077afea69a2955260949478afabab8e500d8"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"0628077afea69a2955260949478afabab8e500d8":["c6d238816bcdf9bbe4ec886226d89bd93834eb7e"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}