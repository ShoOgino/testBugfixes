{"path":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t  tokenStream.end();\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t  tokenStream.end();\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4093b270ba337f9c25a4c0e6cb2ae2c07f697376","date":1347897716,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @param tokenStream\n   * @param text\n   * @param maxNumFragments\n   * @param mergeContiguousFragments\n   * @throws IOException\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n    StringBuilder newText=new StringBuilder();\n\n      CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n      tokenStream.reset();\n\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t  tokenStream.end();\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":["1b7ec747d08b70141f1e52df23dd3c49f8cb4857"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069","date":1348430063,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n    StringBuilder newText=new StringBuilder();\n\n      CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n      tokenStream.reset();\n\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @param tokenStream\n   * @param text\n   * @param maxNumFragments\n   * @param mergeContiguousFragments\n   * @throws IOException\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n    StringBuilder newText=new StringBuilder();\n\n      CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n      tokenStream.reset();\n\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"081b68cb9e8f4b5405b40bfb223fd7c587171aa1","date":1360072766,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.reset();\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n    StringBuilder newText=new StringBuilder();\n\n      CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n      tokenStream.reset();\n\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3e4d4ec39bf5396230748ca859ff05ab024b6fc5","date":1360112310,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.reset();\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n    StringBuilder newText=new StringBuilder();\n\n      CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n      tokenStream.reset();\n\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.reset();\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.reset();\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55244759f906151d96839f8451dee793acb06e75","date":1418999882,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      tokenStream.reset();\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.reset();\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b7ec747d08b70141f1e52df23dd3c49f8cb4857","date":1420327392,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      tokenStream.reset();\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.getNumTokens() >0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.getStartOffset();\n          endOffset = tokenGroup.getEndOffset();\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.getNumTokens() >0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.getStartOffset();\n        endOffset = tokenGroup.getEndOffset();\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      tokenStream.reset();\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.matchStartOffset;\n          endOffset = tokenGroup.matchEndOffset;\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.numTokens>0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.matchStartOffset;\n        endOffset = tokenGroup.matchEndOffset;\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","bugFix":["4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f000d85a7c0d6019fb7ea17eaa661e3f8c47ad4e","date":1500389888,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n\n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      tokenStream.reset();\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.getNumTokens() >0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.getStartOffset();\n          endOffset = tokenGroup.getEndOffset();\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.getNumTokens() >0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.getStartOffset();\n        endOffset = tokenGroup.getEndOffset();\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      tokenStream.reset();\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.getNumTokens() >0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.getStartOffset();\n          endOffset = tokenGroup.getEndOffset();\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.getNumTokens() >0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.getStartOffset();\n        endOffset = tokenGroup.getEndOffset();\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"936cdd5882761db3b844afd6f84ab81cbb011a75","date":1500973524,"type":3,"author":"Cao Manh Dat","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n\n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      tokenStream.reset();\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.getNumTokens() >0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.getStartOffset();\n          endOffset = tokenGroup.getEndOffset();\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.getNumTokens() >0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.getStartOffset();\n        endOffset = tokenGroup.getEndOffset();\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Low level api to get the most relevant (formatted) sections of the document.\n   * This method has been made public to allow visibility of score information held in TextFragment objects.\n   * Thanks to Jason Calabrese for help in redefining the interface.\n   * @throws IOException If there is a low-level I/O error\n   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n   */\n  public final TextFragment[] getBestTextFragments(\n    TokenStream tokenStream,\n    String text,\n    boolean mergeContiguousFragments,\n    int maxNumFragments)\n    throws IOException, InvalidTokenOffsetsException\n  {\n    ArrayList<TextFragment> docFrags = new ArrayList<>();\n    StringBuilder newText=new StringBuilder();\n\n    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());\n\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n    TokenStream newStream = fragmentScorer.init(tokenStream);\n    if(newStream != null) {\n      tokenStream = newStream;\n    }\n    fragmentScorer.startFragment(currentFrag);\n    docFrags.add(currentFrag);\n\n    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n    try\n    {\n\n      String tokenText;\n      int startOffset;\n      int endOffset;\n      int lastEndOffset = 0;\n      textFragmenter.start(text, tokenStream);\n\n      TokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n      tokenStream.reset();\n      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n            next = tokenStream.incrementToken())\n      {\n        if(  (offsetAtt.endOffset()>text.length())\n          ||\n          (offsetAtt.startOffset()>text.length())\n          )\n        {\n          throw new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n              +\" exceeds length of provided text sized \"+text.length());\n        }\n        if((tokenGroup.getNumTokens() >0)&&(tokenGroup.isDistinct()))\n        {\n          //the current token is distinct from previous tokens -\n          // markup the cached token group info\n          startOffset = tokenGroup.getStartOffset();\n          endOffset = tokenGroup.getEndOffset();\n          tokenText = text.substring(startOffset, endOffset);\n          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n          //store any whitespace etc from between this and last group\n          if (startOffset > lastEndOffset)\n            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n          newText.append(markedUpText);\n          lastEndOffset=Math.max(endOffset, lastEndOffset);\n          tokenGroup.clear();\n\n          //check if current token marks the start of a new fragment\n          if(textFragmenter.isNewFragment())\n          {\n            currentFrag.setScore(fragmentScorer.getFragmentScore());\n            //record stats for a new fragment\n            currentFrag.textEndPos = newText.length();\n            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n            fragmentScorer.startFragment(currentFrag);\n            docFrags.add(currentFrag);\n          }\n        }\n\n        tokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//        if(lastEndOffset>maxDocBytesToAnalyze)\n//        {\n//          break;\n//        }\n      }\n      currentFrag.setScore(fragmentScorer.getFragmentScore());\n\n      if(tokenGroup.getNumTokens() >0)\n      {\n        //flush the accumulated text (same code as in above loop)\n        startOffset = tokenGroup.getStartOffset();\n        endOffset = tokenGroup.getEndOffset();\n        tokenText = text.substring(startOffset, endOffset);\n        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n        //store any whitespace etc from between this and last group\n        if (startOffset > lastEndOffset)\n          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n        newText.append(markedUpText);\n        lastEndOffset=Math.max(lastEndOffset,endOffset);\n      }\n\n      //Test what remains of the original text beyond the point where we stopped analyzing\n      if (\n//          if there is text beyond the last token considered..\n          (lastEndOffset < text.length())\n          &&\n//          and that text is not too large...\n          (text.length()<= maxDocCharsToAnalyze)\n        )\n      {\n        //append it to the last fragment\n        newText.append(encoder.encodeText(text.substring(lastEndOffset)));\n      }\n\n      currentFrag.textEndPos = newText.length();\n\n      //sort the most relevant sections of the text\n      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n      {\n        currentFrag = i.next();\n\n        //If you are running with a version of Lucene before 11th Sept 03\n        // you do not have PriorityQueue.insert() - so uncomment the code below\n        /*\n                  if (currentFrag.getScore() >= minScore)\n                  {\n                    fragQueue.put(currentFrag);\n                    if (fragQueue.size() > maxNumFragments)\n                    { // if hit queue overfull\n                      fragQueue.pop(); // remove lowest in hit queue\n                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n                    }\n\n\n                  }\n        */\n        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n        //fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n        // USE ABOVE CODE IF THIS DOES NOT COMPILE!\n        fragQueue.insertWithOverflow(currentFrag);\n      }\n\n      //return the most relevant fragments\n      TextFragment frag[] = new TextFragment[fragQueue.size()];\n      for (int i = frag.length - 1; i >= 0; i--)\n      {\n        frag[i] = fragQueue.pop();\n      }\n\n      //merge any contiguous fragments to improve readability\n      if(mergeContiguousFragments)\n      {\n        mergeContiguousFragments(frag);\n        ArrayList<TextFragment> fragTexts = new ArrayList<>();\n        for (int i = 0; i < frag.length; i++)\n        {\n          if ((frag[i] != null) && (frag[i].getScore() > 0))\n          {\n            fragTexts.add(frag[i]);\n          }\n        }\n        frag= fragTexts.toArray(new TextFragment[0]);\n      }\n\n      return frag;\n\n    }\n    finally\n    {\n      if (tokenStream != null)\n      {\n        try\n        {\n          tokenStream.end();\n          tokenStream.close();\n        }\n        catch (Exception e)\n        {\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069":["4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["081b68cb9e8f4b5405b40bfb223fd7c587171aa1"],"081b68cb9e8f4b5405b40bfb223fd7c587171aa1":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3e4d4ec39bf5396230748ca859ff05ab024b6fc5":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069","081b68cb9e8f4b5405b40bfb223fd7c587171aa1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"936cdd5882761db3b844afd6f84ab81cbb011a75":["1b7ec747d08b70141f1e52df23dd3c49f8cb4857","f000d85a7c0d6019fb7ea17eaa661e3f8c47ad4e"],"f000d85a7c0d6019fb7ea17eaa661e3f8c47ad4e":["1b7ec747d08b70141f1e52df23dd3c49f8cb4857"],"1b7ec747d08b70141f1e52df23dd3c49f8cb4857":["55244759f906151d96839f8451dee793acb06e75"],"55244759f906151d96839f8451dee793acb06e75":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f000d85a7c0d6019fb7ea17eaa661e3f8c47ad4e"],"4093b270ba337f9c25a4c0e6cb2ae2c07f697376":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069":["081b68cb9e8f4b5405b40bfb223fd7c587171aa1","3e4d4ec39bf5396230748ca859ff05ab024b6fc5"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["55244759f906151d96839f8451dee793acb06e75"],"081b68cb9e8f4b5405b40bfb223fd7c587171aa1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","3e4d4ec39bf5396230748ca859ff05ab024b6fc5"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["4093b270ba337f9c25a4c0e6cb2ae2c07f697376"],"3e4d4ec39bf5396230748ca859ff05ab024b6fc5":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"936cdd5882761db3b844afd6f84ab81cbb011a75":[],"f000d85a7c0d6019fb7ea17eaa661e3f8c47ad4e":["936cdd5882761db3b844afd6f84ab81cbb011a75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1b7ec747d08b70141f1e52df23dd3c49f8cb4857":["936cdd5882761db3b844afd6f84ab81cbb011a75","f000d85a7c0d6019fb7ea17eaa661e3f8c47ad4e"],"55244759f906151d96839f8451dee793acb06e75":["1b7ec747d08b70141f1e52df23dd3c49f8cb4857"],"4093b270ba337f9c25a4c0e6cb2ae2c07f697376":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3e4d4ec39bf5396230748ca859ff05ab024b6fc5","936cdd5882761db3b844afd6f84ab81cbb011a75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}