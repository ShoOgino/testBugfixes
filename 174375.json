{"path":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#TermsQuery(FieldAndTermEnum,int).mjava","commits":[{"id":"c2042d3e27841c5b60112990fc33559e10ccf6dd","date":1424537395,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#TermsQuery(FieldAndTermEnum,int).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsFilter#TermsFilter(FieldAndTermEnum,int).mjava","sourceNew":"  private TermsQuery(FieldAndTermEnum iter, int length) {\n    // TODO: maybe use oal.index.PrefixCodedTerms instead?\n    // If number of terms is more than a few hundred it\n    // should be a win\n\n    // TODO: we also pack terms in FieldCache/DocValues\n    // ... maybe we can refactor to share that code\n\n    // TODO: yet another option is to build the union of the terms in\n    // an automaton an call intersect on the termsenum if the density is high\n\n    int hash = 9;\n    byte[] serializedTerms = new byte[0];\n    this.offsets = new int[length+1];\n    int lastEndOffset = 0;\n    int index = 0;\n    ArrayList<TermsAndField> termsAndFields = new ArrayList<>();\n    TermsAndField lastTermsAndField = null;\n    BytesRef previousTerm = null;\n    String previousField = null;\n    BytesRef currentTerm;\n    String currentField;\n    while((currentTerm = iter.next()) != null) {\n      currentField = iter.field();\n      if (currentField == null) {\n        throw new IllegalArgumentException(\"Field must not be null\");\n      }\n      if (previousField != null) {\n        // deduplicate\n        if (previousField.equals(currentField)) {\n          if (previousTerm.bytesEquals(currentTerm)){\n            continue;\n          }\n        } else {\n          final int start = lastTermsAndField == null ? 0 : lastTermsAndField.end;\n          lastTermsAndField = new TermsAndField(start, index, previousField);\n          termsAndFields.add(lastTermsAndField);\n        }\n      }\n      hash = 31 *  hash + currentField.hashCode();\n      hash = 31 *  hash + currentTerm.hashCode();\n      if (serializedTerms.length < lastEndOffset+currentTerm.length) {\n        serializedTerms = ArrayUtil.grow(serializedTerms, lastEndOffset+currentTerm.length);\n      }\n      System.arraycopy(currentTerm.bytes, currentTerm.offset, serializedTerms, lastEndOffset, currentTerm.length);\n      offsets[index] = lastEndOffset;\n      lastEndOffset += currentTerm.length;\n      index++;\n      previousTerm = currentTerm;\n      previousField = currentField;\n    }\n    offsets[index] = lastEndOffset;\n    final int start = lastTermsAndField == null ? 0 : lastTermsAndField.end;\n    lastTermsAndField = new TermsAndField(start, index, previousField);\n    termsAndFields.add(lastTermsAndField);\n    this.termsBytes = ArrayUtil.shrink(serializedTerms, lastEndOffset);\n    this.termsAndFields = termsAndFields.toArray(new TermsAndField[termsAndFields.size()]);\n    this.hashCode = hash;\n  }\n\n","sourceOld":"  private TermsFilter(FieldAndTermEnum iter, int length) {\n    // TODO: maybe use oal.index.PrefixCodedTerms instead?\n    // If number of terms is more than a few hundred it\n    // should be a win\n\n    // TODO: we also pack terms in FieldCache/DocValues\n    // ... maybe we can refactor to share that code\n\n    // TODO: yet another option is to build the union of the terms in\n    // an automaton an call intersect on the termsenum if the density is high\n\n    int hash = 9;\n    byte[] serializedTerms = new byte[0];\n    this.offsets = new int[length+1];\n    int lastEndOffset = 0;\n    int index = 0;\n    ArrayList<TermsAndField> termsAndFields = new ArrayList<>();\n    TermsAndField lastTermsAndField = null;\n    BytesRef previousTerm = null;\n    String previousField = null;\n    BytesRef currentTerm;\n    String currentField;\n    while((currentTerm = iter.next()) != null) {\n      currentField = iter.field();\n      if (currentField == null) {\n        throw new IllegalArgumentException(\"Field must not be null\");\n      }\n      if (previousField != null) {\n        // deduplicate\n        if (previousField.equals(currentField)) {\n          if (previousTerm.bytesEquals(currentTerm)){\n            continue;            \n          }\n        } else {\n          final int start = lastTermsAndField == null ? 0 : lastTermsAndField.end;\n          lastTermsAndField = new TermsAndField(start, index, previousField);\n          termsAndFields.add(lastTermsAndField);\n        }\n      }\n      hash = PRIME *  hash + currentField.hashCode();\n      hash = PRIME *  hash + currentTerm.hashCode();\n      if (serializedTerms.length < lastEndOffset+currentTerm.length) {\n        serializedTerms = ArrayUtil.grow(serializedTerms, lastEndOffset+currentTerm.length);\n      }\n      System.arraycopy(currentTerm.bytes, currentTerm.offset, serializedTerms, lastEndOffset, currentTerm.length);\n      offsets[index] = lastEndOffset; \n      lastEndOffset += currentTerm.length;\n      index++;\n      previousTerm = currentTerm;\n      previousField = currentField;\n    }\n    offsets[index] = lastEndOffset;\n    final int start = lastTermsAndField == null ? 0 : lastTermsAndField.end;\n    lastTermsAndField = new TermsAndField(start, index, previousField);\n    termsAndFields.add(lastTermsAndField);\n    this.termsBytes = ArrayUtil.shrink(serializedTerms, lastEndOffset);\n    this.termsAndFields = termsAndFields.toArray(new TermsAndField[termsAndFields.size()]);\n    this.hashCode = hash;\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a54410d37fe11baed59cc55dcad44db795f732c2","date":1430995912,"type":4,"author":"Adrien Grand","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#TermsQuery(FieldAndTermEnum,int).mjava","sourceNew":null,"sourceOld":"  private TermsQuery(FieldAndTermEnum iter, int length) {\n    // TODO: maybe use oal.index.PrefixCodedTerms instead?\n    // If number of terms is more than a few hundred it\n    // should be a win\n\n    // TODO: we also pack terms in FieldCache/DocValues\n    // ... maybe we can refactor to share that code\n\n    // TODO: yet another option is to build the union of the terms in\n    // an automaton an call intersect on the termsenum if the density is high\n\n    int hash = 9;\n    byte[] serializedTerms = new byte[0];\n    this.offsets = new int[length+1];\n    int lastEndOffset = 0;\n    int index = 0;\n    ArrayList<TermsAndField> termsAndFields = new ArrayList<>();\n    TermsAndField lastTermsAndField = null;\n    BytesRef previousTerm = null;\n    String previousField = null;\n    BytesRef currentTerm;\n    String currentField;\n    while((currentTerm = iter.next()) != null) {\n      currentField = iter.field();\n      if (currentField == null) {\n        throw new IllegalArgumentException(\"Field must not be null\");\n      }\n      if (previousField != null) {\n        // deduplicate\n        if (previousField.equals(currentField)) {\n          if (previousTerm.bytesEquals(currentTerm)){\n            continue;\n          }\n        } else {\n          final int start = lastTermsAndField == null ? 0 : lastTermsAndField.end;\n          lastTermsAndField = new TermsAndField(start, index, previousField);\n          termsAndFields.add(lastTermsAndField);\n        }\n      }\n      hash = 31 *  hash + currentField.hashCode();\n      hash = 31 *  hash + currentTerm.hashCode();\n      if (serializedTerms.length < lastEndOffset+currentTerm.length) {\n        serializedTerms = ArrayUtil.grow(serializedTerms, lastEndOffset+currentTerm.length);\n      }\n      System.arraycopy(currentTerm.bytes, currentTerm.offset, serializedTerms, lastEndOffset, currentTerm.length);\n      offsets[index] = lastEndOffset;\n      lastEndOffset += currentTerm.length;\n      index++;\n      previousTerm = currentTerm;\n      previousField = currentField;\n    }\n    offsets[index] = lastEndOffset;\n    final int start = lastTermsAndField == null ? 0 : lastTermsAndField.end;\n    lastTermsAndField = new TermsAndField(start, index, previousField);\n    termsAndFields.add(lastTermsAndField);\n    this.termsBytes = ArrayUtil.shrink(serializedTerms, lastEndOffset);\n    this.termsAndFields = termsAndFields.toArray(new TermsAndField[termsAndFields.size()]);\n    this.hashCode = hash;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a54410d37fe11baed59cc55dcad44db795f732c2":["c2042d3e27841c5b60112990fc33559e10ccf6dd"],"c2042d3e27841c5b60112990fc33559e10ccf6dd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a54410d37fe11baed59cc55dcad44db795f732c2"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c2042d3e27841c5b60112990fc33559e10ccf6dd"],"a54410d37fe11baed59cc55dcad44db795f732c2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c2042d3e27841c5b60112990fc33559e10ccf6dd":["a54410d37fe11baed59cc55dcad44db795f732c2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}