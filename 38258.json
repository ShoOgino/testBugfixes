{"path":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","commits":[{"id":"849494cf2f3a96af5c8c84995108ddd8456fcd04","date":1372277913,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"/dev/null","sourceNew":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dataDir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dataDir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dataDir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dataDir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["7c88c33fee958027b9192ef2c6bb54836618b165","7c88c33fee958027b9192ef2c6bb54836618b165"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a73d20bc1dc667e3a86198ec1aadd353510216e","date":1372352930,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests on Windows require Cygwin\", Constants.WINDOWS);\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dataDir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dataDir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dataDir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dataDir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"da849b9d9ae3f85f6be7ecc82eb7a199992e2cad","date":1372353469,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests on Windows require Cygwin\", Constants.WINDOWS);\n    LuceneTestCase.assumeFalse(\"HDFS do not work well with FreeBSD blackhole setup\", Constants.FREE_BSD);\n   // LuceneTestCase.assumeFalse(\"HDFS tests on Windows require Cygwin\", Constants.F);\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests on Windows require Cygwin\", Constants.WINDOWS);\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f965a58508de2621165dd693e9ebafd7a7360a5c","date":1373378928,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests on Windows require Cygwin\", Constants.WINDOWS);\n    LuceneTestCase.assumeFalse(\"HDFS do not work well with FreeBSD blackhole setup\", Constants.FREE_BSD);\n   // LuceneTestCase.assumeFalse(\"HDFS tests on Windows require Cygwin\", Constants.F);\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"/dev/null","sourceNew":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"62eb0905fa8d584070a6091f650b6601f7c9c574","date":1388362044,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c88c33fee958027b9192ef2c6bb54836618b165","date":1390430350,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19389fe47925b510b2811e2b385a75f7ad19dcca","date":1393903127,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0d579490a72f2e6297eaa648940611234c57cf1","date":1395917140,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass() throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    File dir = TestUtil.createTempDir(LuceneTestCase.getTestClass().getSimpleName());\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb1f22cfa77230b5f05b7784feae5367f6bbb488","date":1395968145,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass().mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass() throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    File dir = TestUtil.createTempDir(LuceneTestCase.getTestClass().getSimpleName());\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a0f5bb79c600763ffe7b8141df59a3169d31e48","date":1396689440,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad7bdba3e91cf3373cda2e52239cb761fc0b452","date":1408019547,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ce6c1f997c135ab2e3d211580d089de539d7e20","date":1421161966,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir) throws Exception {\n    return setupClass(dir, true);\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a","date":1429888091,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir) throws Exception {\n    return setupClass(dir, true, true);\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir) throws Exception {\n    return setupClass(dir, true);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7c88c33fee958027b9192ef2c6bb54836618b165":["62eb0905fa8d584070a6091f650b6601f7c9c574"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["7c88c33fee958027b9192ef2c6bb54836618b165","19389fe47925b510b2811e2b385a75f7ad19dcca"],"bb1f22cfa77230b5f05b7784feae5367f6bbb488":["d0d579490a72f2e6297eaa648940611234c57cf1"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["19389fe47925b510b2811e2b385a75f7ad19dcca","bb1f22cfa77230b5f05b7784feae5367f6bbb488"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f965a58508de2621165dd693e9ebafd7a7360a5c"],"d0d579490a72f2e6297eaa648940611234c57cf1":["19389fe47925b510b2811e2b385a75f7ad19dcca"],"0ce6c1f997c135ab2e3d211580d089de539d7e20":["0ad7bdba3e91cf3373cda2e52239cb761fc0b452"],"19389fe47925b510b2811e2b385a75f7ad19dcca":["7c88c33fee958027b9192ef2c6bb54836618b165"],"f965a58508de2621165dd693e9ebafd7a7360a5c":["da849b9d9ae3f85f6be7ecc82eb7a199992e2cad"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"da849b9d9ae3f85f6be7ecc82eb7a199992e2cad":["0a73d20bc1dc667e3a86198ec1aadd353510216e"],"0ad7bdba3e91cf3373cda2e52239cb761fc0b452":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0a73d20bc1dc667e3a86198ec1aadd353510216e":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"62eb0905fa8d584070a6091f650b6601f7c9c574":["f965a58508de2621165dd693e9ebafd7a7360a5c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3def6e0e7b7566dd7f04a3514e77ee97a40fc78a"],"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a":["0ce6c1f997c135ab2e3d211580d089de539d7e20"]},"commit2Childs":{"7c88c33fee958027b9192ef2c6bb54836618b165":["96ea64d994d340044e0d57aeb6a5871539d10ca5","19389fe47925b510b2811e2b385a75f7ad19dcca"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"bb1f22cfa77230b5f05b7784feae5367f6bbb488":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["0ad7bdba3e91cf3373cda2e52239cb761fc0b452"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"d0d579490a72f2e6297eaa648940611234c57cf1":["bb1f22cfa77230b5f05b7784feae5367f6bbb488"],"0ce6c1f997c135ab2e3d211580d089de539d7e20":["3def6e0e7b7566dd7f04a3514e77ee97a40fc78a"],"19389fe47925b510b2811e2b385a75f7ad19dcca":["96ea64d994d340044e0d57aeb6a5871539d10ca5","2a0f5bb79c600763ffe7b8141df59a3169d31e48","d0d579490a72f2e6297eaa648940611234c57cf1"],"f965a58508de2621165dd693e9ebafd7a7360a5c":["37a0f60745e53927c4c876cfe5b5a58170f0646c","62eb0905fa8d584070a6091f650b6601f7c9c574"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["0a73d20bc1dc667e3a86198ec1aadd353510216e"],"da849b9d9ae3f85f6be7ecc82eb7a199992e2cad":["f965a58508de2621165dd693e9ebafd7a7360a5c"],"0ad7bdba3e91cf3373cda2e52239cb761fc0b452":["0ce6c1f997c135ab2e3d211580d089de539d7e20"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["37a0f60745e53927c4c876cfe5b5a58170f0646c","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"0a73d20bc1dc667e3a86198ec1aadd353510216e":["da849b9d9ae3f85f6be7ecc82eb7a199992e2cad"],"62eb0905fa8d584070a6091f650b6601f7c9c574":["7c88c33fee958027b9192ef2c6bb54836618b165"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}