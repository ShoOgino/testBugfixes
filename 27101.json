{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter#testLotsOfConcatenating2().mjava","commits":[{"id":"098528909bb70948871fd7ed865fafb87ed73964","date":1484667487,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter#testLotsOfConcatenating2().mjava","pathOld":"/dev/null","sourceNew":"  /** concat numbers + words + all + preserve original */\n  public void testLotsOfConcatenating2() throws Exception {\n    final int flags = PRESERVE_ORIGINAL | GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_WORDS | CATENATE_NUMBERS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;    \n\n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterGraphFilter(tokenizer, flags, null));\n      }\n    };\n    \n    assertAnalyzesTo(a, \"abc-def-123-456\", \n                     new String[] { \"abcdef123456\", \"abc-def-123-456\", \"abcdef\", \"abc\", \"def\", \"123456\", \"123\", \"456\" }, \n                     new int[] { 0, 0, 0, 0, 4, 8, 8, 12 }, \n                     new int[] { 15, 15, 7, 3, 7, 15, 11, 15 },\n                     null,\n                     new int[] { 1, 0, 0, 0, 1, 1, 0, 1 },\n                     null,\n                     false);\n    a.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"302d34f2c66e8d489ee13078305c330cbf67b226","date":1484754357,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter#testLotsOfConcatenating2().mjava","pathOld":"/dev/null","sourceNew":"  /** concat numbers + words + all + preserve original */\n  public void testLotsOfConcatenating2() throws Exception {\n    final int flags = PRESERVE_ORIGINAL | GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_WORDS | CATENATE_NUMBERS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;    \n\n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterGraphFilter(tokenizer, flags, null));\n      }\n    };\n    \n    assertAnalyzesTo(a, \"abc-def-123-456\", \n                     new String[] { \"abcdef123456\", \"abc-def-123-456\", \"abcdef\", \"abc\", \"def\", \"123456\", \"123\", \"456\" }, \n                     new int[] { 0, 0, 0, 0, 4, 8, 8, 12 }, \n                     new int[] { 15, 15, 7, 3, 7, 15, 11, 15 },\n                     null,\n                     new int[] { 1, 0, 0, 0, 1, 1, 0, 1 },\n                     null,\n                     false);\n    a.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"47e241984c8185946746fd8e18cff4200659091e","date":1543916862,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter#testLotsOfConcatenating2().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter#testLotsOfConcatenating2().mjava","sourceNew":"  /** concat numbers + words + all + preserve original */\n  public void testLotsOfConcatenating2() throws Exception {\n    final int flags = PRESERVE_ORIGINAL | GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_WORDS | CATENATE_NUMBERS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;    \n\n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterGraphFilter(tokenizer, flags, null));\n      }\n    };\n    \n    assertAnalyzesTo(a, \"abc-def-123-456\", \n                     new String[] { \"abcdef123456\", \"abc-def-123-456\", \"abcdef\", \"abc\", \"def\", \"123456\", \"123\", \"456\" }, \n                     new int[] { 0, 0, 0, 0, 0, 0, 0, 0 },\n                     new int[] { 15, 15, 15, 15, 15, 15, 15, 15 },\n                     null,\n                     new int[] { 1, 0, 0, 0, 1, 1, 0, 1 },\n                     null,\n                     false);\n    a.close();\n  }\n\n","sourceOld":"  /** concat numbers + words + all + preserve original */\n  public void testLotsOfConcatenating2() throws Exception {\n    final int flags = PRESERVE_ORIGINAL | GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_WORDS | CATENATE_NUMBERS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;    \n\n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterGraphFilter(tokenizer, flags, null));\n      }\n    };\n    \n    assertAnalyzesTo(a, \"abc-def-123-456\", \n                     new String[] { \"abcdef123456\", \"abc-def-123-456\", \"abcdef\", \"abc\", \"def\", \"123456\", \"123\", \"456\" }, \n                     new int[] { 0, 0, 0, 0, 4, 8, 8, 12 }, \n                     new int[] { 15, 15, 7, 3, 7, 15, 11, 15 },\n                     null,\n                     new int[] { 1, 0, 0, 0, 1, 1, 0, 1 },\n                     null,\n                     false);\n    a.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9a2e54c3916a617394d9b279fef949760a349a75","date":1554191929,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter#testLotsOfConcatenating2().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter#testLotsOfConcatenating2().mjava","sourceNew":"  /** concat numbers + words + all + preserve original */\n  public void testLotsOfConcatenating2() throws Exception {\n    final int flags = PRESERVE_ORIGINAL | GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_WORDS | CATENATE_NUMBERS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;    \n\n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterGraphFilter(tokenizer, flags, null));\n      }\n    };\n    \n    assertAnalyzesTo(a, \"abc-def-123-456\", \n                     new String[] { \"abc-def-123-456\", \"abcdef123456\", \"abcdef\", \"abc\", \"def\", \"123456\", \"123\", \"456\" },\n                     new int[] { 0, 0, 0, 0, 0, 0, 0, 0 },\n                     new int[] { 15, 15, 15, 15, 15, 15, 15, 15 },\n                     null,\n                     new int[] { 1, 0, 0, 0, 1, 1, 0, 1 },\n                     null,\n                     false);\n    a.close();\n  }\n\n","sourceOld":"  /** concat numbers + words + all + preserve original */\n  public void testLotsOfConcatenating2() throws Exception {\n    final int flags = PRESERVE_ORIGINAL | GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_WORDS | CATENATE_NUMBERS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;    \n\n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new WordDelimiterGraphFilter(tokenizer, flags, null));\n      }\n    };\n    \n    assertAnalyzesTo(a, \"abc-def-123-456\", \n                     new String[] { \"abcdef123456\", \"abc-def-123-456\", \"abcdef\", \"abc\", \"def\", \"123456\", \"123\", \"456\" }, \n                     new int[] { 0, 0, 0, 0, 0, 0, 0, 0 },\n                     new int[] { 15, 15, 15, 15, 15, 15, 15, 15 },\n                     null,\n                     new int[] { 1, 0, 0, 0, 1, 1, 0, 1 },\n                     null,\n                     false);\n    a.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"098528909bb70948871fd7ed865fafb87ed73964":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9a2e54c3916a617394d9b279fef949760a349a75":["47e241984c8185946746fd8e18cff4200659091e"],"47e241984c8185946746fd8e18cff4200659091e":["098528909bb70948871fd7ed865fafb87ed73964"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9a2e54c3916a617394d9b279fef949760a349a75"],"302d34f2c66e8d489ee13078305c330cbf67b226":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","098528909bb70948871fd7ed865fafb87ed73964"]},"commit2Childs":{"098528909bb70948871fd7ed865fafb87ed73964":["47e241984c8185946746fd8e18cff4200659091e","302d34f2c66e8d489ee13078305c330cbf67b226"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["098528909bb70948871fd7ed865fafb87ed73964","302d34f2c66e8d489ee13078305c330cbf67b226"],"9a2e54c3916a617394d9b279fef949760a349a75":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"47e241984c8185946746fd8e18cff4200659091e":["9a2e54c3916a617394d9b279fef949760a349a75"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"302d34f2c66e8d489ee13078305c330cbf67b226":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","302d34f2c66e8d489ee13078305c330cbf67b226"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}