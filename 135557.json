{"path":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","commits":[{"id":"e885d2b1e112b1d9db6a2dae82b3b493dfba1df1","date":1342716838,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID=\" + docID);\n    }\n\n    // nocommit do this in finishDoc... but does it fail...?\n    // is it not always called...?\n    if (posOut != null && saveNextPosBlock) {\n      lastBlockPosFP = posOut.getFilePointer();\n      if (payOut != null) {\n        lastBlockPayFP = payOut.getFilePointer();\n      }\n      lastBlockPosBufferUpto = posBufferUpto;\n      lastBlockEndOffset = lastEndOffset;\n      lastBlockPayloadByteUpto = payloadByteUpto;\n      saveNextPosBlock = false;\n      if (DEBUG) {\n        System.out.println(\"  now save lastBlockPosFP=\" + lastBlockPosFP + \" lastBlockPosBufferUpto=\" + lastBlockPosBufferUpto + \" lastBlockPayloadByteUpto=\" + lastBlockPayloadByteUpto);\n      }\n    }\n\n    final int docDelta = docID - lastDocID;\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n    lastDocID = docID;\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n    if (DEBUG) {\n      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == blockSize) {\n      // nocommit maybe instead of buffering skip before\n      // writing a block based on last block's end data\n      // ... we could buffer after writing the block?  only\n      // iffiness with that approach is it could be a\n      // pointlness skip?  like we may stop adding docs\n      // right after that, then we have skip point AFTER\n      // last doc.  the thing is, in finishTerm we are\n      // already sometimes adding a skip point AFTER the\n      // last doc?\n      if (lastBlockDocID != -1) {\n        if (DEBUG) {\n          System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-blockSize));\n        }\n        skipWriter.bufferSkip(lastBlockDocID, docCount-blockSize, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockEndOffset, lastBlockPayloadByteUpto);\n      }\n      lastBlockDocID = docID;\n      saveNextPosBlock = true;\n\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      writeBlock(docDeltaBuffer, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        writeBlock(freqBuffer, docOut);\n      }\n      docBufferUpto = 0;\n    }\n\n    lastPosition = 0;\n    lastEndOffset = 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["f81e20fb7784c464473faba4f1fdd4f775e8ee87","9a70ce9bddc6f985feb8e5e182aebe20872328d4","9a70ce9bddc6f985feb8e5e182aebe20872328d4","9a70ce9bddc6f985feb8e5e182aebe20872328d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f81e20fb7784c464473faba4f1fdd4f775e8ee87","date":1343937502,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","sourceNew":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID=\" + docID);\n    }\n\n    // nocommit do this in finishDoc... but does it fail...?\n    // is it not always called...?\n    if (posOut != null && saveNextPosBlock) {\n      lastBlockPosFP = posOut.getFilePointer();\n      if (payOut != null) {\n        lastBlockPayFP = payOut.getFilePointer();\n      }\n      lastBlockPosBufferUpto = posBufferUpto;\n      lastBlockStartOffset = lastStartOffset;\n      lastBlockPayloadByteUpto = payloadByteUpto;\n      saveNextPosBlock = false;\n      if (DEBUG) {\n        System.out.println(\"  now save lastBlockPosFP=\" + lastBlockPosFP + \" lastBlockPosBufferUpto=\" + lastBlockPosBufferUpto + \" lastBlockPayloadByteUpto=\" + lastBlockPayloadByteUpto);\n      }\n    }\n\n    final int docDelta = docID - lastDocID;\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n    lastDocID = docID;\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n    if (DEBUG) {\n      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == blockSize) {\n      // nocommit maybe instead of buffering skip before\n      // writing a block based on last block's end data\n      // ... we could buffer after writing the block?  only\n      // iffiness with that approach is it could be a\n      // pointlness skip?  like we may stop adding docs\n      // right after that, then we have skip point AFTER\n      // last doc.  the thing is, in finishTerm we are\n      // already sometimes adding a skip point AFTER the\n      // last doc?\n      if (lastBlockDocID != -1) {\n        if (DEBUG) {\n          System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-blockSize));\n        }\n        skipWriter.bufferSkip(lastBlockDocID, docCount-blockSize, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);\n      }\n      lastBlockDocID = docID;\n      saveNextPosBlock = true;\n\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      writeBlock(docDeltaBuffer, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        writeBlock(freqBuffer, docOut);\n      }\n      docBufferUpto = 0;\n    }\n\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","sourceOld":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID=\" + docID);\n    }\n\n    // nocommit do this in finishDoc... but does it fail...?\n    // is it not always called...?\n    if (posOut != null && saveNextPosBlock) {\n      lastBlockPosFP = posOut.getFilePointer();\n      if (payOut != null) {\n        lastBlockPayFP = payOut.getFilePointer();\n      }\n      lastBlockPosBufferUpto = posBufferUpto;\n      lastBlockEndOffset = lastEndOffset;\n      lastBlockPayloadByteUpto = payloadByteUpto;\n      saveNextPosBlock = false;\n      if (DEBUG) {\n        System.out.println(\"  now save lastBlockPosFP=\" + lastBlockPosFP + \" lastBlockPosBufferUpto=\" + lastBlockPosBufferUpto + \" lastBlockPayloadByteUpto=\" + lastBlockPayloadByteUpto);\n      }\n    }\n\n    final int docDelta = docID - lastDocID;\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n    lastDocID = docID;\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n    if (DEBUG) {\n      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == blockSize) {\n      // nocommit maybe instead of buffering skip before\n      // writing a block based on last block's end data\n      // ... we could buffer after writing the block?  only\n      // iffiness with that approach is it could be a\n      // pointlness skip?  like we may stop adding docs\n      // right after that, then we have skip point AFTER\n      // last doc.  the thing is, in finishTerm we are\n      // already sometimes adding a skip point AFTER the\n      // last doc?\n      if (lastBlockDocID != -1) {\n        if (DEBUG) {\n          System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-blockSize));\n        }\n        skipWriter.bufferSkip(lastBlockDocID, docCount-blockSize, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockEndOffset, lastBlockPayloadByteUpto);\n      }\n      lastBlockDocID = docID;\n      saveNextPosBlock = true;\n\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      writeBlock(docDeltaBuffer, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        writeBlock(freqBuffer, docOut);\n      }\n      docBufferUpto = 0;\n    }\n\n    lastPosition = 0;\n    lastEndOffset = 0;\n  }\n\n","bugFix":["e885d2b1e112b1d9db6a2dae82b3b493dfba1df1"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8081d5018e8846bf4080f2809432c759996e749f","date":1344206372,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","sourceNew":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    }\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n//    if (DEBUG) {\n//      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n//    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == blockSize) {\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      writeBlock(docDeltaBuffer, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        writeBlock(freqBuffer, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","sourceOld":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID=\" + docID);\n    }\n\n    // nocommit do this in finishDoc... but does it fail...?\n    // is it not always called...?\n    if (posOut != null && saveNextPosBlock) {\n      lastBlockPosFP = posOut.getFilePointer();\n      if (payOut != null) {\n        lastBlockPayFP = payOut.getFilePointer();\n      }\n      lastBlockPosBufferUpto = posBufferUpto;\n      lastBlockStartOffset = lastStartOffset;\n      lastBlockPayloadByteUpto = payloadByteUpto;\n      saveNextPosBlock = false;\n      if (DEBUG) {\n        System.out.println(\"  now save lastBlockPosFP=\" + lastBlockPosFP + \" lastBlockPosBufferUpto=\" + lastBlockPosBufferUpto + \" lastBlockPayloadByteUpto=\" + lastBlockPayloadByteUpto);\n      }\n    }\n\n    final int docDelta = docID - lastDocID;\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n    lastDocID = docID;\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n    if (DEBUG) {\n      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == blockSize) {\n      // nocommit maybe instead of buffering skip before\n      // writing a block based on last block's end data\n      // ... we could buffer after writing the block?  only\n      // iffiness with that approach is it could be a\n      // pointlness skip?  like we may stop adding docs\n      // right after that, then we have skip point AFTER\n      // last doc.  the thing is, in finishTerm we are\n      // already sometimes adding a skip point AFTER the\n      // last doc?\n      if (lastBlockDocID != -1) {\n        if (DEBUG) {\n          System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-blockSize));\n        }\n        skipWriter.bufferSkip(lastBlockDocID, docCount-blockSize, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);\n      }\n      lastBlockDocID = docID;\n      saveNextPosBlock = true;\n\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      writeBlock(docDeltaBuffer, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        writeBlock(freqBuffer, docOut);\n      }\n      docBufferUpto = 0;\n    }\n\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d2f8624b27ae4159f0d53a55e5c9c5c6f6fa8f51","date":1344355078,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","sourceNew":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    }\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n//    if (DEBUG) {\n//      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n//    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      writeBlock(docDeltaBuffer, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        writeBlock(freqBuffer, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","sourceOld":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    }\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n//    if (DEBUG) {\n//      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n//    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == blockSize) {\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      writeBlock(docDeltaBuffer, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        writeBlock(freqBuffer, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e6b41208259e8566cba0ecac7da6a331ea9732dd","date":1344551376,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","sourceNew":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    }\n\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n//    if (DEBUG) {\n//      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n//    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        forUtil.writeBlock(freqBuffer, encoded, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","sourceOld":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    }\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n//    if (DEBUG) {\n//      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n//    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      writeBlock(docDeltaBuffer, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        writeBlock(freqBuffer, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3e4439268f3e73dd6840cec2281bd73e76b3eb28","date":1345033656,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","sourceNew":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    }\n    // Have collected a block of docs, and get a new doc. \n    // Should write skip data as well as postings list for\n    // current block.\n    if (lastBlockDocID != -1 && docBufferUpto == 0) {\n      if (DEBUG) {\n        System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-1));\n      }\n      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);\n    }\n\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n//    if (DEBUG) {\n//      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n//    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        forUtil.writeBlock(freqBuffer, encoded, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n\n\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","sourceOld":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    }\n\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n//    if (DEBUG) {\n//      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n//    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        forUtil.writeBlock(freqBuffer, encoded, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","bugFix":null,"bugIntro":["c2808fd811f853d032fff6b48bb83a4b7b6f48e7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fcc6cbc7e8693270112e8d40ca98226199c0288e","date":1345039720,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","sourceNew":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    // if (DEBUG) {\n    //   System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    // }\n    // Have collected a block of docs, and get a new doc. \n    // Should write skip data as well as postings list for\n    // current block.\n    if (lastBlockDocID != -1 && docBufferUpto == 0) {\n      // if (DEBUG) {\n      //   System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-1));\n      // }\n      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);\n    }\n\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n    // if (DEBUG) {\n    //   System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n    // }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      // if (DEBUG) {\n      //   System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      // }\n      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);\n      if (fieldHasFreqs) {\n        // if (DEBUG) {\n        //   System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        // }\n        forUtil.writeBlock(freqBuffer, encoded, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n\n\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","sourceOld":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    if (DEBUG) {\n      System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    }\n    // Have collected a block of docs, and get a new doc. \n    // Should write skip data as well as postings list for\n    // current block.\n    if (lastBlockDocID != -1 && docBufferUpto == 0) {\n      if (DEBUG) {\n        System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-1));\n      }\n      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);\n    }\n\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n//    if (DEBUG) {\n//      System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n//    }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      if (DEBUG) {\n        System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      }\n      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);\n      if (fieldHasFreqs) {\n        if (DEBUG) {\n          System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        }\n        forUtil.writeBlock(freqBuffer, encoded, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n\n\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f8615860cb50aefb8eebca1d1b3893dbe21cf126","date":1345550448,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    // if (DEBUG) {\n    //   System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    // }\n    // Have collected a block of docs, and get a new doc. \n    // Should write skip data as well as postings list for\n    // current block.\n    if (lastBlockDocID != -1 && docBufferUpto == 0) {\n      // if (DEBUG) {\n      //   System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-1));\n      // }\n      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);\n    }\n\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n    // if (DEBUG) {\n    //   System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n    // }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      // if (DEBUG) {\n      //   System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      // }\n      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);\n      if (fieldHasFreqs) {\n        // if (DEBUG) {\n        //   System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        // }\n        forUtil.writeBlock(freqBuffer, encoded, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n\n\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"001b25b42373b22a52f399dbf072f1224632e8e6","date":1345889167,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    // if (DEBUG) {\n    //   System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    // }\n    // Have collected a block of docs, and get a new doc. \n    // Should write skip data as well as postings list for\n    // current block.\n    if (lastBlockDocID != -1 && docBufferUpto == 0) {\n      // if (DEBUG) {\n      //   System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-1));\n      // }\n      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);\n    }\n\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n    // if (DEBUG) {\n    //   System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n    // }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      // if (DEBUG) {\n      //   System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      // }\n      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);\n      if (fieldHasFreqs) {\n        // if (DEBUG) {\n        //   System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        // }\n        forUtil.writeBlock(freqBuffer, encoded, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n\n\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e45d45bc3730ddd1341f4eb6025f33b8482e6e2","date":1346834651,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter#startDoc(int,int).mjava","sourceNew":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    // if (DEBUG) {\n    //   System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    // }\n    // Have collected a block of docs, and get a new doc. \n    // Should write skip data as well as postings list for\n    // current block.\n    if (lastBlockDocID != -1 && docBufferUpto == 0) {\n      // if (DEBUG) {\n      //   System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-1));\n      // }\n      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);\n    }\n\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n    // if (DEBUG) {\n    //   System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n    // }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      // if (DEBUG) {\n      //   System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      // }\n      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);\n      if (fieldHasFreqs) {\n        // if (DEBUG) {\n        //   System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        // }\n        forUtil.writeBlock(freqBuffer, encoded, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n\n\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","sourceOld":"  @Override\n  public void startDoc(int docID, int termDocFreq) throws IOException {\n    // if (DEBUG) {\n    //   System.out.println(\"FPW.startDoc docID[\"+docBufferUpto+\"]=\" + docID);\n    // }\n    // Have collected a block of docs, and get a new doc. \n    // Should write skip data as well as postings list for\n    // current block.\n    if (lastBlockDocID != -1 && docBufferUpto == 0) {\n      // if (DEBUG) {\n      //   System.out.println(\"  bufferSkip at writeBlock: lastDocID=\" + lastBlockDocID + \" docCount=\" + (docCount-1));\n      // }\n      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);\n    }\n\n    final int docDelta = docID - lastDocID;\n\n    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {\n      throw new CorruptIndexException(\"docs out of order (\" + docID + \" <= \" + lastDocID + \" ) (docOut: \" + docOut + \")\");\n    }\n\n    docDeltaBuffer[docBufferUpto] = docDelta;\n    // if (DEBUG) {\n    //   System.out.println(\"  docDeltaBuffer[\" + docBufferUpto + \"]=\" + docDelta);\n    // }\n    if (fieldHasFreqs) {\n      freqBuffer[docBufferUpto] = termDocFreq;\n    }\n    docBufferUpto++;\n    docCount++;\n\n    if (docBufferUpto == BLOCK_SIZE) {\n      // if (DEBUG) {\n      //   System.out.println(\"  write docDelta block @ fp=\" + docOut.getFilePointer());\n      // }\n      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);\n      if (fieldHasFreqs) {\n        // if (DEBUG) {\n        //   System.out.println(\"  write freq block @ fp=\" + docOut.getFilePointer());\n        // }\n        forUtil.writeBlock(freqBuffer, encoded, docOut);\n      }\n      // NOTE: don't set docBufferUpto back to 0 here;\n      // finishDoc will do so (because it needs to see that\n      // the block was filled so it can save skip data)\n    }\n\n\n    lastDocID = docID;\n    lastPosition = 0;\n    lastStartOffset = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"001b25b42373b22a52f399dbf072f1224632e8e6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f8615860cb50aefb8eebca1d1b3893dbe21cf126"],"3e45d45bc3730ddd1341f4eb6025f33b8482e6e2":["f8615860cb50aefb8eebca1d1b3893dbe21cf126"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f8615860cb50aefb8eebca1d1b3893dbe21cf126":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","fcc6cbc7e8693270112e8d40ca98226199c0288e"],"d2f8624b27ae4159f0d53a55e5c9c5c6f6fa8f51":["8081d5018e8846bf4080f2809432c759996e749f"],"e885d2b1e112b1d9db6a2dae82b3b493dfba1df1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8081d5018e8846bf4080f2809432c759996e749f":["f81e20fb7784c464473faba4f1fdd4f775e8ee87"],"fcc6cbc7e8693270112e8d40ca98226199c0288e":["3e4439268f3e73dd6840cec2281bd73e76b3eb28"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3e45d45bc3730ddd1341f4eb6025f33b8482e6e2"],"3e4439268f3e73dd6840cec2281bd73e76b3eb28":["e6b41208259e8566cba0ecac7da6a331ea9732dd"],"e6b41208259e8566cba0ecac7da6a331ea9732dd":["d2f8624b27ae4159f0d53a55e5c9c5c6f6fa8f51"],"f81e20fb7784c464473faba4f1fdd4f775e8ee87":["e885d2b1e112b1d9db6a2dae82b3b493dfba1df1"]},"commit2Childs":{"001b25b42373b22a52f399dbf072f1224632e8e6":[],"3e45d45bc3730ddd1341f4eb6025f33b8482e6e2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["001b25b42373b22a52f399dbf072f1224632e8e6","f8615860cb50aefb8eebca1d1b3893dbe21cf126","e885d2b1e112b1d9db6a2dae82b3b493dfba1df1"],"f8615860cb50aefb8eebca1d1b3893dbe21cf126":["001b25b42373b22a52f399dbf072f1224632e8e6","3e45d45bc3730ddd1341f4eb6025f33b8482e6e2"],"d2f8624b27ae4159f0d53a55e5c9c5c6f6fa8f51":["e6b41208259e8566cba0ecac7da6a331ea9732dd"],"e885d2b1e112b1d9db6a2dae82b3b493dfba1df1":["f81e20fb7784c464473faba4f1fdd4f775e8ee87"],"8081d5018e8846bf4080f2809432c759996e749f":["d2f8624b27ae4159f0d53a55e5c9c5c6f6fa8f51"],"fcc6cbc7e8693270112e8d40ca98226199c0288e":["f8615860cb50aefb8eebca1d1b3893dbe21cf126"],"f81e20fb7784c464473faba4f1fdd4f775e8ee87":["8081d5018e8846bf4080f2809432c759996e749f"],"3e4439268f3e73dd6840cec2281bd73e76b3eb28":["fcc6cbc7e8693270112e8d40ca98226199c0288e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"e6b41208259e8566cba0ecac7da6a331ea9732dd":["3e4439268f3e73dd6840cec2281bd73e76b3eb28"]},"heads":["001b25b42373b22a52f399dbf072f1224632e8e6","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}