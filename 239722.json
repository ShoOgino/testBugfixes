{"path":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","commits":[{"id":"5a902fceaa7c10b5669d1ed631319bd619378ca7","date":1169742721,"type":1,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see SolrParams#FACET_LIMIT\n   * @see SolrParams#FACET_ZEROS\n   * @see SolrParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      if (df>0) { /* check df since all docs may be deleted */\n        int c = searcher.numDocs(new TermQuery(t), docs);\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see SolrParams#FACET_LIMIT\n   * @see SolrParams#FACET_ZEROS\n   * @see SolrParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n    TermEnum te = r.terms(new Term(field,\"\"));\n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      int df = te.docFreq();\n\n      if (df>0) { /* check df since all docs may be deleted */\n        int c = searcher.numDocs(new TermQuery(t), docs);\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"090a3d6e41dac3d2e4ba2efd822d75846a41eca8","date":1179509667,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see SolrParams#FACET_LIMIT\n   * @see SolrParams#FACET_ZEROS\n   * @see SolrParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, SolrParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see SolrParams#FACET_LIMIT\n   * @see SolrParams#FACET_ZEROS\n   * @see SolrParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      if (df>0) { /* check df since all docs may be deleted */\n        int c = searcher.numDocs(new TermQuery(t), docs);\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4fe52b26cfcf59d6b3f30b128e9f5985f2fa4ef","date":1185993405,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see SolrParams#FACET_LIMIT\n   * @see SolrParams#FACET_ZEROS\n   * @see SolrParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see SolrParams#FACET_LIMIT\n   * @see SolrParams#FACET_ZEROS\n   * @see SolrParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, SolrParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71e34bb5aeffd6bfbe524b81edb939aaabbfa643","date":1189215503,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see SolrParams#FACET_LIMIT\n   * @see SolrParams#FACET_ZEROS\n   * @see SolrParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see SolrParams#FACET_LIMIT\n   * @see SolrParams#FACET_ZEROS\n   * @see SolrParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b6124b1a8da43c086b34ee5005d89e20f4f0764","date":1189219182,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see SolrParams#FACET_LIMIT\n   * @see SolrParams#FACET_ZEROS\n   * @see SolrParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e5a95ce1d7a3779af6db59b6b39d3b89172d7445","date":1228620032,"type":5,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFacetTermEnumCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,boolean,String).mjava","sourceNew":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = (sort.equals(\"count\") || sort.equals(\"true\")) ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort.equals(\"count\") || sort.equals(\"true\")) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort.equals(\"count\") || sort.equals(\"true\")) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","sourceOld":"  /**\n   * Returns a list of terms in the specified field along with the \n   * corresponding count of documents in the set that match that constraint.\n   * This method uses the FilterCache to get the intersection count between <code>docs</code>\n   * and the DocSet for each term in the filter.\n   *\n   * @see FacetParams#FACET_LIMIT\n   * @see FacetParams#FACET_ZEROS\n   * @see FacetParams#FACET_MISSING\n   */\n  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, boolean sort, String prefix)\n    throws IOException {\n\n    /* :TODO: potential optimization...\n    * cache the Terms with the highest docFreq and try them first\n    * don't enum if we get our max from them\n    */\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);\n\n    IndexSchema schema = searcher.getSchema();\n    IndexReader r = searcher.getReader();\n    FieldType ft = schema.getFieldType(field);\n\n    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;    \n    final BoundedTreeSet<CountPair<String,Integer>> queue = sort ? new BoundedTreeSet<CountPair<String,Integer>>(maxsize) : null;\n    final NamedList res = new NamedList();\n\n    int min=mincount-1;  // the smallest value in the top 'N' values    \n    int off=offset;\n    int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n    String startTerm = prefix==null ? \"\" : ft.toInternal(prefix);\n    TermEnum te = r.terms(new Term(field,startTerm));\n    TermDocs td = r.termDocs();\n\n    if (docs.size() >= mincount) { \n    do {\n      Term t = te.term();\n\n      if (null == t || ! t.field().equals(field))\n        break;\n\n      if (prefix!=null && !t.text().startsWith(prefix)) break;\n\n      int df = te.docFreq();\n\n      // If we are sorting, we can use df>min (rather than >=) since we\n      // are going in index order.  For certain term distributions this can\n      // make a large difference (for example, many terms with df=1).\n      if (df>0 && df>min) {\n        int c;\n\n        if (df >= minDfFilterCache) {\n          // use the filter cache\n          c = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          // iterate over TermDocs to calculate the intersection\n          td.seek(te);\n          c=0;\n          while (td.next()) {\n            if (docs.exists(td.doc())) c++;\n          }\n        }\n\n        if (sort) {\n          if (c>min) {\n            queue.add(new CountPair<String,Integer>(t.text(), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        } else {\n          if (c >= mincount && --off<0) {\n            if (--lim<0) break;\n            res.add(ft.indexedToReadable(t.text()), c);\n          }\n        }\n      }\n    } while (te.next());\n    }\n\n    if (sort) {\n      for (CountPair<String,Integer> p : queue) {\n        if (--off>=0) continue;\n        if (--lim<0) break;\n        res.add(ft.indexedToReadable(p.key), p.val);\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,field));\n    }\n\n    te.close();\n    td.close();    \n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c4fe52b26cfcf59d6b3f30b128e9f5985f2fa4ef":["090a3d6e41dac3d2e4ba2efd822d75846a41eca8"],"71e34bb5aeffd6bfbe524b81edb939aaabbfa643":["c4fe52b26cfcf59d6b3f30b128e9f5985f2fa4ef"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"090a3d6e41dac3d2e4ba2efd822d75846a41eca8":["5a902fceaa7c10b5669d1ed631319bd619378ca7"],"5a902fceaa7c10b5669d1ed631319bd619378ca7":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"4b6124b1a8da43c086b34ee5005d89e20f4f0764":["71e34bb5aeffd6bfbe524b81edb939aaabbfa643"],"e5a95ce1d7a3779af6db59b6b39d3b89172d7445":["4b6124b1a8da43c086b34ee5005d89e20f4f0764"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"c4fe52b26cfcf59d6b3f30b128e9f5985f2fa4ef":["71e34bb5aeffd6bfbe524b81edb939aaabbfa643"],"71e34bb5aeffd6bfbe524b81edb939aaabbfa643":["4b6124b1a8da43c086b34ee5005d89e20f4f0764"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["5a902fceaa7c10b5669d1ed631319bd619378ca7"],"090a3d6e41dac3d2e4ba2efd822d75846a41eca8":["c4fe52b26cfcf59d6b3f30b128e9f5985f2fa4ef"],"5a902fceaa7c10b5669d1ed631319bd619378ca7":["090a3d6e41dac3d2e4ba2efd822d75846a41eca8"],"4b6124b1a8da43c086b34ee5005d89e20f4f0764":["e5a95ce1d7a3779af6db59b6b39d3b89172d7445"],"e5a95ce1d7a3779af6db59b6b39d3b89172d7445":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e5a95ce1d7a3779af6db59b6b39d3b89172d7445","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}