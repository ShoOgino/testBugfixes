{"path":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ed4c1297d7566d06b37bf0571ead16ec3cb9f8a7","date":1375992438,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i < 0){//be defensive here, even though we checked before hand, something could have changed\n            throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" pos: \" + pos + \" limit: \" + limit + \" end: \" + end);\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0dcf83b1542b01ad5cddb41b01dc51f751215919","date":1376231222,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        bb = byteBuf;\n        byteBuf.clear().position(offset);\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        int readLength = len;\n        while (readLength > 0) {\n          final int toRead = Math.min(CHUNK_SIZE, readLength);\n          bb.limit(bb.position() + toRead);\n          assert bb.remaining() == toRead;\n          final int i = channel.read(bb, pos);\n          if (i < 0) { // be defensive here, even though we checked before hand, something could have changed\n            throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" pos: \" + pos + \" chunkLen: \" + toRead + \" end: \" + end);\n          }\n          assert i > 0 : \"FileChannel.read with non zero-length bb.remaining() must always read at least one byte (FileChannel is in blocking mode, see spec of ReadableByteChannel)\";\n          pos += i;\n          readLength -= i;\n        }\n        assert readLength == 0;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i < 0){//be defensive here, even though we checked before hand, something could have changed\n            throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" pos: \" + pos + \" limit: \" + limit + \" end: \" + end);\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        bb = byteBuf;\n        byteBuf.clear().position(offset);\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        int readLength = len;\n        while (readLength > 0) {\n          final int toRead = Math.min(CHUNK_SIZE, readLength);\n          bb.limit(bb.position() + toRead);\n          assert bb.remaining() == toRead;\n          final int i = channel.read(bb, pos);\n          if (i < 0) { // be defensive here, even though we checked before hand, something could have changed\n            throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" pos: \" + pos + \" chunkLen: \" + toRead + \" end: \" + end);\n          }\n          assert i > 0 : \"FileChannel.read with non zero-length bb.remaining() must always read at least one byte (FileChannel is in blocking mode, see spec of ReadableByteChannel)\";\n          pos += i;\n          readLength -= i;\n        }\n        assert readLength == 0;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"af9ce763a9d61cb88781e638d593430b71efa7dc","date":1586259009,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(ByteBuffer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(ByteBuffer b) throws IOException {\n      long pos = getFilePointer() + off;\n      \n      if (pos + b.remaining() > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        int readLength = b.remaining();\n        while (readLength > 0) {\n          final int toRead = Math.min(CHUNK_SIZE, readLength);\n          b.limit(b.position() + toRead);\n          assert b.remaining() == toRead;\n          final int i = channel.read(b, pos);\n          if (i < 0) { // be defensive here, even though we checked before hand, something could have changed\n            throw new EOFException(\"read past EOF: \" + this + \" buffer: \" + b + \" chunkLen: \" + toRead + \" end: \" + end);\n          }\n          assert i > 0 : \"FileChannel.read with non zero-length bb.remaining() must always read at least one byte (FileChannel is in blocking mode, see spec of ReadableByteChannel)\";\n          pos += i;\n          readLength -= i;\n        }\n        assert readLength == 0;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        bb = byteBuf;\n        byteBuf.clear().position(offset);\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        int readLength = len;\n        while (readLength > 0) {\n          final int toRead = Math.min(CHUNK_SIZE, readLength);\n          bb.limit(bb.position() + toRead);\n          assert bb.remaining() == toRead;\n          final int i = channel.read(bb, pos);\n          if (i < 0) { // be defensive here, even though we checked before hand, something could have changed\n            throw new EOFException(\"read past EOF: \" + this + \" off: \" + offset + \" len: \" + len + \" pos: \" + pos + \" chunkLen: \" + toRead + \" end: \" + end);\n          }\n          assert i > 0 : \"FileChannel.read with non zero-length bb.remaining() must always read at least one byte (FileChannel is in blocking mode, see spec of ReadableByteChannel)\";\n          pos += i;\n          readLength -= i;\n        }\n        assert readLength == 0;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"af9ce763a9d61cb88781e638d593430b71efa7dc":["0dcf83b1542b01ad5cddb41b01dc51f751215919"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"0dcf83b1542b01ad5cddb41b01dc51f751215919":["ed4c1297d7566d06b37bf0571ead16ec3cb9f8a7"],"ed4c1297d7566d06b37bf0571ead16ec3cb9f8a7":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["af9ce763a9d61cb88781e638d593430b71efa7dc"]},"commit2Childs":{"af9ce763a9d61cb88781e638d593430b71efa7dc":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","ed4c1297d7566d06b37bf0571ead16ec3cb9f8a7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"0dcf83b1542b01ad5cddb41b01dc51f751215919":["af9ce763a9d61cb88781e638d593430b71efa7dc"],"ed4c1297d7566d06b37bf0571ead16ec3cb9f8a7":["0dcf83b1542b01ad5cddb41b01dc51f751215919"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}