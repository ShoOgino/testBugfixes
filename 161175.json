{"path":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","commits":[{"id":"decc8a7344e9231708f9991fa09db2cafec7a2dd","date":1201187153,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","pathOld":"/dev/null","sourceNew":"  public void testBoth() throws Exception {\n    Set untoks = new HashSet();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    Token token;\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"a b c d\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"a b c d\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 11, token.startOffset() == 11);\n    assertTrue(token.endOffset() + \" does not equal: \" + 18, token.endOffset() == 18);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"a\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"a\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 0, token.getPositionIncrement() == 0);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", token.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 11, token.startOffset() == 11);\n    assertTrue(token.endOffset() + \" does not equal: \" + 12, token.endOffset() == 12);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"b\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"b\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 13, token.startOffset() == 13);\n    assertTrue(token.endOffset() + \" does not equal: \" + 14, token.endOffset() == 14);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"c\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"c\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 15, token.startOffset() == 15);\n    assertTrue(token.endOffset() + \" does not equal: \" + 16, token.endOffset() == 16);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"d\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"d\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 17, token.startOffset() == 17);\n    assertTrue(token.endOffset() + \" does not equal: \" + 18, token.endOffset() == 18);\n\n\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"e f g\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"e f g\") == true);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 32, token.startOffset() == 32);\n    assertTrue(token.endOffset() + \" does not equal: \" + 37, token.endOffset() == 37);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"e\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"e\") == true);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 0, token.getPositionIncrement() == 0);\n    assertTrue(token.startOffset() + \" does not equal: \" + 32, token.startOffset() == 32);\n    assertTrue(token.endOffset() + \" does not equal: \" + 33, token.endOffset() == 33);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"f\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"f\") == true);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.startOffset() + \" does not equal: \" + 34, token.startOffset() == 34);\n    assertTrue(token.endOffset() + \" does not equal: \" + 35, token.endOffset() == 35);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"g\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"g\") == true);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.startOffset() + \" does not equal: \" + 36, token.startOffset() == 36);\n    assertTrue(token.endOffset() + \" does not equal: \" + 37, token.endOffset() == 37);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"link\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"link\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 42, token.startOffset() == 42);\n    assertTrue(token.endOffset() + \" does not equal: \" + 46, token.endOffset() == 46);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"here\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"here\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 47, token.startOffset() == 47);\n    assertTrue(token.endOffset() + \" does not equal: \" + 51, token.endOffset() == 51);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"link\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"link\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.startOffset() + \" does not equal: \" + 56, token.startOffset() == 56);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(token.endOffset() + \" does not equal: \" + 60, token.endOffset() == 60);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"there\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"there\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 61, token.startOffset() == 61);\n    assertTrue(token.endOffset() + \" does not equal: \" + 66, token.endOffset() == 66);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"italics here\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"italics here\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(token.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 71, token.startOffset() == 71);\n    assertTrue(token.endOffset() + \" does not equal: \" + 83, token.endOffset() == 83);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"italics\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"italics\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 0, token.getPositionIncrement() == 0);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 71, token.startOffset() == 71);\n    assertTrue(token.endOffset() + \" does not equal: \" + 78, token.endOffset() == 78);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"here\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"here\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 79, token.startOffset() == 79);\n    assertTrue(token.endOffset() + \" does not equal: \" + 83, token.endOffset() == 83);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"something\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"something\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.startOffset() + \" does not equal: \" + 86, token.startOffset() == 86);\n    assertTrue(token.endOffset() + \" does not equal: \" + 95, token.endOffset() == 95);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"more italics\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"more italics\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(token.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 98, token.startOffset() == 98);\n    assertTrue(token.endOffset() + \" does not equal: \" + 110, token.endOffset() == 110);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"more\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"more\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 0, token.getPositionIncrement() == 0);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 98, token.startOffset() == 98);\n    assertTrue(token.endOffset() + \" does not equal: \" + 102, token.endOffset() == 102);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"italics\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"italics\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n        assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(token.startOffset() + \" does not equal: \" + 103, token.startOffset() == 103);\n    assertTrue(token.endOffset() + \" does not equal: \" + 110, token.endOffset() == 110);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"h   i   j\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"h   i   j\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 124, token.startOffset() == 124);\n    assertTrue(token.endOffset() + \" does not equal: \" + 133, token.endOffset() == 133);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"h\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"h\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 0, token.getPositionIncrement() == 0);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 124, token.startOffset() == 124);\n    assertTrue(token.endOffset() + \" does not equal: \" + 125, token.endOffset() == 125);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"i\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"i\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 128, token.startOffset() == 128);\n    assertTrue(token.endOffset() + \" does not equal: \" + 129, token.endOffset() == 129);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"j\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"j\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 132, token.startOffset() == 132);\n    assertTrue(token.endOffset() + \" does not equal: \" + 133, token.endOffset() == 133);\n\n    token = tf.next();\n    assertTrue(\"token is not null and it should be\", token == null);\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","pathOld":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","sourceNew":"  public void testBoth() throws Exception {\n    Set untoks = new HashSet();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    final Token reusableToken = new Token();\n    Token nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"a b c d\",\n            nextToken.term().equals(\"a b c d\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 11, nextToken.startOffset() == 11);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 18, nextToken.endOffset() == 18);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"a\",\n            nextToken.term().equals(\"a\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 0, nextToken.getPositionIncrement() == 0);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", nextToken.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 11, nextToken.startOffset() == 11);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 12, nextToken.endOffset() == 12);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"b\",\n            nextToken.term().equals(\"b\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 13, nextToken.startOffset() == 13);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 14, nextToken.endOffset() == 14);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"c\",\n            nextToken.term().equals(\"c\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 15, nextToken.startOffset() == 15);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 16, nextToken.endOffset() == 16);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"d\",\n            nextToken.term().equals(\"d\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 17, nextToken.startOffset() == 17);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 18, nextToken.endOffset() == 18);\n\n\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"e f g\",\n            nextToken.term().equals(\"e f g\") == true);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 32, nextToken.startOffset() == 32);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 37, nextToken.endOffset() == 37);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"e\",\n            nextToken.term().equals(\"e\") == true);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 0, nextToken.getPositionIncrement() == 0);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 32, nextToken.startOffset() == 32);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 33, nextToken.endOffset() == 33);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"f\",\n            nextToken.term().equals(\"f\") == true);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 34, nextToken.startOffset() == 34);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 35, nextToken.endOffset() == 35);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"g\",\n            nextToken.term().equals(\"g\") == true);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 36, nextToken.startOffset() == 36);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 37, nextToken.endOffset() == 37);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"link\",\n            nextToken.term().equals(\"link\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 42, nextToken.startOffset() == 42);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 46, nextToken.endOffset() == 46);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"here\",\n            nextToken.term().equals(\"here\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 47, nextToken.startOffset() == 47);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 51, nextToken.endOffset() == 51);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"link\",\n            nextToken.term().equals(\"link\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 56, nextToken.startOffset() == 56);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 60, nextToken.endOffset() == 60);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"there\",\n            nextToken.term().equals(\"there\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 61, nextToken.startOffset() == 61);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 66, nextToken.endOffset() == 66);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"italics here\",\n            nextToken.term().equals(\"italics here\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(nextToken.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 71, nextToken.startOffset() == 71);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 83, nextToken.endOffset() == 83);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"italics\",\n            nextToken.term().equals(\"italics\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 0, nextToken.getPositionIncrement() == 0);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 71, nextToken.startOffset() == 71);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 78, nextToken.endOffset() == 78);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"here\",\n            nextToken.term().equals(\"here\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 79, nextToken.startOffset() == 79);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 83, nextToken.endOffset() == 83);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"something\",\n            nextToken.term().equals(\"something\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 86, nextToken.startOffset() == 86);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 95, nextToken.endOffset() == 95);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"more italics\",\n            nextToken.term().equals(\"more italics\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(nextToken.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 98, nextToken.startOffset() == 98);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 110, nextToken.endOffset() == 110);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"more\",\n            nextToken.term().equals(\"more\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 0, nextToken.getPositionIncrement() == 0);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 98, nextToken.startOffset() == 98);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 102, nextToken.endOffset() == 102);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"italics\",\n            nextToken.term().equals(\"italics\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n        assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 103, nextToken.startOffset() == 103);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 110, nextToken.endOffset() == 110);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"h   i   j\",\n            nextToken.term().equals(\"h   i   j\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 124, nextToken.startOffset() == 124);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 133, nextToken.endOffset() == 133);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"h\",\n            nextToken.term().equals(\"h\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 0, nextToken.getPositionIncrement() == 0);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 124, nextToken.startOffset() == 124);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 125, nextToken.endOffset() == 125);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"i\",\n            nextToken.term().equals(\"i\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 128, nextToken.startOffset() == 128);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 129, nextToken.endOffset() == 129);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"j\",\n            nextToken.term().equals(\"j\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 132, nextToken.startOffset() == 132);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 133, nextToken.endOffset() == 133);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is not null and it should be\", nextToken == null);\n\n  }\n\n","sourceOld":"  public void testBoth() throws Exception {\n    Set untoks = new HashSet();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    Token token;\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"a b c d\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"a b c d\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 11, token.startOffset() == 11);\n    assertTrue(token.endOffset() + \" does not equal: \" + 18, token.endOffset() == 18);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"a\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"a\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 0, token.getPositionIncrement() == 0);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", token.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 11, token.startOffset() == 11);\n    assertTrue(token.endOffset() + \" does not equal: \" + 12, token.endOffset() == 12);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"b\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"b\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 13, token.startOffset() == 13);\n    assertTrue(token.endOffset() + \" does not equal: \" + 14, token.endOffset() == 14);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"c\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"c\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 15, token.startOffset() == 15);\n    assertTrue(token.endOffset() + \" does not equal: \" + 16, token.endOffset() == 16);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"d\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"d\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 17, token.startOffset() == 17);\n    assertTrue(token.endOffset() + \" does not equal: \" + 18, token.endOffset() == 18);\n\n\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"e f g\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"e f g\") == true);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 32, token.startOffset() == 32);\n    assertTrue(token.endOffset() + \" does not equal: \" + 37, token.endOffset() == 37);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"e\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"e\") == true);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 0, token.getPositionIncrement() == 0);\n    assertTrue(token.startOffset() + \" does not equal: \" + 32, token.startOffset() == 32);\n    assertTrue(token.endOffset() + \" does not equal: \" + 33, token.endOffset() == 33);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"f\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"f\") == true);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.startOffset() + \" does not equal: \" + 34, token.startOffset() == 34);\n    assertTrue(token.endOffset() + \" does not equal: \" + 35, token.endOffset() == 35);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"g\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"g\") == true);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.startOffset() + \" does not equal: \" + 36, token.startOffset() == 36);\n    assertTrue(token.endOffset() + \" does not equal: \" + 37, token.endOffset() == 37);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"link\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"link\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 42, token.startOffset() == 42);\n    assertTrue(token.endOffset() + \" does not equal: \" + 46, token.endOffset() == 46);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"here\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"here\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 47, token.startOffset() == 47);\n    assertTrue(token.endOffset() + \" does not equal: \" + 51, token.endOffset() == 51);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"link\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"link\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.startOffset() + \" does not equal: \" + 56, token.startOffset() == 56);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(token.endOffset() + \" does not equal: \" + 60, token.endOffset() == 60);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"there\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"there\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 61, token.startOffset() == 61);\n    assertTrue(token.endOffset() + \" does not equal: \" + 66, token.endOffset() == 66);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"italics here\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"italics here\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(token.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 71, token.startOffset() == 71);\n    assertTrue(token.endOffset() + \" does not equal: \" + 83, token.endOffset() == 83);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"italics\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"italics\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 0, token.getPositionIncrement() == 0);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 71, token.startOffset() == 71);\n    assertTrue(token.endOffset() + \" does not equal: \" + 78, token.endOffset() == 78);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"here\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"here\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 79, token.startOffset() == 79);\n    assertTrue(token.endOffset() + \" does not equal: \" + 83, token.endOffset() == 83);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"something\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"something\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.startOffset() + \" does not equal: \" + 86, token.startOffset() == 86);\n    assertTrue(token.endOffset() + \" does not equal: \" + 95, token.endOffset() == 95);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"more italics\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"more italics\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(token.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 98, token.startOffset() == 98);\n    assertTrue(token.endOffset() + \" does not equal: \" + 110, token.endOffset() == 110);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"more\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"more\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 0, token.getPositionIncrement() == 0);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 98, token.startOffset() == 98);\n    assertTrue(token.endOffset() + \" does not equal: \" + 102, token.endOffset() == 102);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"italics\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"italics\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n        assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(token.startOffset() + \" does not equal: \" + 103, token.startOffset() == 103);\n    assertTrue(token.endOffset() + \" does not equal: \" + 110, token.endOffset() == 110);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"h   i   j\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"h   i   j\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(token.startOffset() + \" does not equal: \" + 124, token.startOffset() == 124);\n    assertTrue(token.endOffset() + \" does not equal: \" + 133, token.endOffset() == 133);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"h\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"h\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 0, token.getPositionIncrement() == 0);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 124, token.startOffset() == 124);\n    assertTrue(token.endOffset() + \" does not equal: \" + 125, token.endOffset() == 125);\n\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"i\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"i\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 128, token.startOffset() == 128);\n    assertTrue(token.endOffset() + \" does not equal: \" + 129, token.endOffset() == 129);\n    token = tf.next();\n    assertTrue(\"token is null and it shouldn't be\", token != null);\n    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + \" is not equal to \" + \"j\",\n            new String(token.termBuffer(), 0, token.termLength()).equals(\"j\") == true);\n    assertTrue(token.getPositionIncrement() + \" does not equal: \" + 1, token.getPositionIncrement() == 1);\n    assertTrue(token.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(token.startOffset() + \" does not equal: \" + 132, token.startOffset() == 132);\n    assertTrue(token.endOffset() + \" does not equal: \" + 133, token.endOffset() == 133);\n\n    token = tf.next();\n    assertTrue(\"token is not null and it should be\", token == null);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9b5756469957918cac40a831acec9cf01c8c2bb3","date":1249167152,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","pathOld":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","sourceNew":"  public void testBoth() throws Exception {\n    Set untoks = new HashSet();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    TermAttribute termAtt = (TermAttribute) tf.addAttribute(TermAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) tf.addAttribute(TypeAttribute.class);\n    PositionIncrementAttribute posIncrAtt = (PositionIncrementAttribute) tf.addAttribute(PositionIncrementAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) tf.addAttribute(OffsetAttribute.class);\n    FlagsAttribute flagsAtt = (FlagsAttribute) tf.addAttribute(FlagsAttribute.class);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a b c d\",\n            termAtt.term().equals(\"a b c d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a\",\n            termAtt.term().equals(\"a\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", flagsAtt.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 12, offsetAtt.endOffset() == 12);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"b\",\n            termAtt.term().equals(\"b\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 13, offsetAtt.startOffset() == 13);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 14, offsetAtt.endOffset() == 14);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"c\",\n            termAtt.term().equals(\"c\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 15, offsetAtt.startOffset() == 15);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 16, offsetAtt.endOffset() == 16);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"d\",\n            termAtt.term().equals(\"d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 17, offsetAtt.startOffset() == 17);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n\n\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e f g\",\n            termAtt.term().equals(\"e f g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e\",\n            termAtt.term().equals(\"e\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 33, offsetAtt.endOffset() == 33);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"f\",\n            termAtt.term().equals(\"f\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 34, offsetAtt.startOffset() == 34);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 35, offsetAtt.endOffset() == 35);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"g\",\n            termAtt.term().equals(\"g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 36, offsetAtt.startOffset() == 36);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 42, offsetAtt.startOffset() == 42);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 46, offsetAtt.endOffset() == 46);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 47, offsetAtt.startOffset() == 47);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 51, offsetAtt.endOffset() == 51);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 56, offsetAtt.startOffset() == 56);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 60, offsetAtt.endOffset() == 60);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"there\",\n            termAtt.term().equals(\"there\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 61, offsetAtt.startOffset() == 61);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 66, offsetAtt.endOffset() == 66);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics here\",\n            termAtt.term().equals(\"italics here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 78, offsetAtt.endOffset() == 78);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 79, offsetAtt.startOffset() == 79);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"something\",\n            termAtt.term().equals(\"something\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 86, offsetAtt.startOffset() == 86);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 95, offsetAtt.endOffset() == 95);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more italics\",\n            termAtt.term().equals(\"more italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more\",\n            termAtt.term().equals(\"more\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 102, offsetAtt.endOffset() == 102);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n        assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 103, offsetAtt.startOffset() == 103);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h   i   j\",\n            termAtt.term().equals(\"h   i   j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h\",\n            termAtt.term().equals(\"h\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 125, offsetAtt.endOffset() == 125);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"i\",\n            termAtt.term().equals(\"i\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 128, offsetAtt.startOffset() == 128);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 129, offsetAtt.endOffset() == 129);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"j\",\n            termAtt.term().equals(\"j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 132, offsetAtt.startOffset() == 132);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertFalse(tf.incrementToken());\n  }\n\n","sourceOld":"  public void testBoth() throws Exception {\n    Set untoks = new HashSet();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    final Token reusableToken = new Token();\n    Token nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"a b c d\",\n            nextToken.term().equals(\"a b c d\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 11, nextToken.startOffset() == 11);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 18, nextToken.endOffset() == 18);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"a\",\n            nextToken.term().equals(\"a\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 0, nextToken.getPositionIncrement() == 0);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", nextToken.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 11, nextToken.startOffset() == 11);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 12, nextToken.endOffset() == 12);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"b\",\n            nextToken.term().equals(\"b\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 13, nextToken.startOffset() == 13);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 14, nextToken.endOffset() == 14);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"c\",\n            nextToken.term().equals(\"c\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 15, nextToken.startOffset() == 15);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 16, nextToken.endOffset() == 16);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"d\",\n            nextToken.term().equals(\"d\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 17, nextToken.startOffset() == 17);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 18, nextToken.endOffset() == 18);\n\n\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"e f g\",\n            nextToken.term().equals(\"e f g\") == true);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 32, nextToken.startOffset() == 32);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 37, nextToken.endOffset() == 37);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"e\",\n            nextToken.term().equals(\"e\") == true);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 0, nextToken.getPositionIncrement() == 0);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 32, nextToken.startOffset() == 32);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 33, nextToken.endOffset() == 33);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"f\",\n            nextToken.term().equals(\"f\") == true);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 34, nextToken.startOffset() == 34);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 35, nextToken.endOffset() == 35);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"g\",\n            nextToken.term().equals(\"g\") == true);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 36, nextToken.startOffset() == 36);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 37, nextToken.endOffset() == 37);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"link\",\n            nextToken.term().equals(\"link\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 42, nextToken.startOffset() == 42);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 46, nextToken.endOffset() == 46);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"here\",\n            nextToken.term().equals(\"here\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 47, nextToken.startOffset() == 47);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 51, nextToken.endOffset() == 51);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"link\",\n            nextToken.term().equals(\"link\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 56, nextToken.startOffset() == 56);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 60, nextToken.endOffset() == 60);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"there\",\n            nextToken.term().equals(\"there\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 61, nextToken.startOffset() == 61);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 66, nextToken.endOffset() == 66);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"italics here\",\n            nextToken.term().equals(\"italics here\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(nextToken.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 71, nextToken.startOffset() == 71);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 83, nextToken.endOffset() == 83);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"italics\",\n            nextToken.term().equals(\"italics\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 0, nextToken.getPositionIncrement() == 0);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 71, nextToken.startOffset() == 71);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 78, nextToken.endOffset() == 78);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"here\",\n            nextToken.term().equals(\"here\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 79, nextToken.startOffset() == 79);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 83, nextToken.endOffset() == 83);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"something\",\n            nextToken.term().equals(\"something\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 86, nextToken.startOffset() == 86);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 95, nextToken.endOffset() == 95);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"more italics\",\n            nextToken.term().equals(\"more italics\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(nextToken.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 98, nextToken.startOffset() == 98);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 110, nextToken.endOffset() == 110);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"more\",\n            nextToken.term().equals(\"more\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 0, nextToken.getPositionIncrement() == 0);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 98, nextToken.startOffset() == 98);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 102, nextToken.endOffset() == 102);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"italics\",\n            nextToken.term().equals(\"italics\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n        assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 103, nextToken.startOffset() == 103);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 110, nextToken.endOffset() == 110);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"h   i   j\",\n            nextToken.term().equals(\"h   i   j\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 124, nextToken.startOffset() == 124);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 133, nextToken.endOffset() == 133);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"h\",\n            nextToken.term().equals(\"h\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 0, nextToken.getPositionIncrement() == 0);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 124, nextToken.startOffset() == 124);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 125, nextToken.endOffset() == 125);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"i\",\n            nextToken.term().equals(\"i\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 128, nextToken.startOffset() == 128);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 129, nextToken.endOffset() == 129);\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is null and it shouldn't be\", nextToken != null);\n    assertTrue(nextToken.term() + \" is not equal to \" + \"j\",\n            nextToken.term().equals(\"j\") == true);\n    assertTrue(nextToken.getPositionIncrement() + \" does not equal: \" + 1, nextToken.getPositionIncrement() == 1);\n    assertTrue(nextToken.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(nextToken.startOffset() + \" does not equal: \" + 132, nextToken.startOffset() == 132);\n    assertTrue(nextToken.endOffset() + \" does not equal: \" + 133, nextToken.endOffset() == 133);\n\n    nextToken = tf.next(reusableToken);\n    assertTrue(\"nextToken is not null and it should be\", nextToken == null);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","pathOld":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","sourceNew":"  public void testBoth() throws Exception {\n    Set untoks = new HashSet();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);\n    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);\n    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);\n    OffsetAttribute offsetAtt = tf.addAttribute(OffsetAttribute.class);\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a b c d\",\n            termAtt.term().equals(\"a b c d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a\",\n            termAtt.term().equals(\"a\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", flagsAtt.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 12, offsetAtt.endOffset() == 12);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"b\",\n            termAtt.term().equals(\"b\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 13, offsetAtt.startOffset() == 13);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 14, offsetAtt.endOffset() == 14);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"c\",\n            termAtt.term().equals(\"c\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 15, offsetAtt.startOffset() == 15);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 16, offsetAtt.endOffset() == 16);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"d\",\n            termAtt.term().equals(\"d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 17, offsetAtt.startOffset() == 17);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n\n\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e f g\",\n            termAtt.term().equals(\"e f g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e\",\n            termAtt.term().equals(\"e\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 33, offsetAtt.endOffset() == 33);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"f\",\n            termAtt.term().equals(\"f\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 34, offsetAtt.startOffset() == 34);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 35, offsetAtt.endOffset() == 35);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"g\",\n            termAtt.term().equals(\"g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 36, offsetAtt.startOffset() == 36);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 42, offsetAtt.startOffset() == 42);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 46, offsetAtt.endOffset() == 46);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 47, offsetAtt.startOffset() == 47);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 51, offsetAtt.endOffset() == 51);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 56, offsetAtt.startOffset() == 56);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 60, offsetAtt.endOffset() == 60);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"there\",\n            termAtt.term().equals(\"there\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 61, offsetAtt.startOffset() == 61);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 66, offsetAtt.endOffset() == 66);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics here\",\n            termAtt.term().equals(\"italics here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 78, offsetAtt.endOffset() == 78);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 79, offsetAtt.startOffset() == 79);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"something\",\n            termAtt.term().equals(\"something\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 86, offsetAtt.startOffset() == 86);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 95, offsetAtt.endOffset() == 95);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more italics\",\n            termAtt.term().equals(\"more italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more\",\n            termAtt.term().equals(\"more\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 102, offsetAtt.endOffset() == 102);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n        assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 103, offsetAtt.startOffset() == 103);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h   i   j\",\n            termAtt.term().equals(\"h   i   j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h\",\n            termAtt.term().equals(\"h\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 125, offsetAtt.endOffset() == 125);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"i\",\n            termAtt.term().equals(\"i\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 128, offsetAtt.startOffset() == 128);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 129, offsetAtt.endOffset() == 129);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"j\",\n            termAtt.term().equals(\"j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 132, offsetAtt.startOffset() == 132);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertFalse(tf.incrementToken());\n  }\n\n","sourceOld":"  public void testBoth() throws Exception {\n    Set untoks = new HashSet();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    TermAttribute termAtt = (TermAttribute) tf.addAttribute(TermAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) tf.addAttribute(TypeAttribute.class);\n    PositionIncrementAttribute posIncrAtt = (PositionIncrementAttribute) tf.addAttribute(PositionIncrementAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) tf.addAttribute(OffsetAttribute.class);\n    FlagsAttribute flagsAtt = (FlagsAttribute) tf.addAttribute(FlagsAttribute.class);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a b c d\",\n            termAtt.term().equals(\"a b c d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a\",\n            termAtt.term().equals(\"a\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", flagsAtt.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 12, offsetAtt.endOffset() == 12);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"b\",\n            termAtt.term().equals(\"b\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 13, offsetAtt.startOffset() == 13);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 14, offsetAtt.endOffset() == 14);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"c\",\n            termAtt.term().equals(\"c\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 15, offsetAtt.startOffset() == 15);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 16, offsetAtt.endOffset() == 16);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"d\",\n            termAtt.term().equals(\"d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 17, offsetAtt.startOffset() == 17);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n\n\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e f g\",\n            termAtt.term().equals(\"e f g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e\",\n            termAtt.term().equals(\"e\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 33, offsetAtt.endOffset() == 33);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"f\",\n            termAtt.term().equals(\"f\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 34, offsetAtt.startOffset() == 34);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 35, offsetAtt.endOffset() == 35);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"g\",\n            termAtt.term().equals(\"g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 36, offsetAtt.startOffset() == 36);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 42, offsetAtt.startOffset() == 42);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 46, offsetAtt.endOffset() == 46);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 47, offsetAtt.startOffset() == 47);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 51, offsetAtt.endOffset() == 51);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 56, offsetAtt.startOffset() == 56);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 60, offsetAtt.endOffset() == 60);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"there\",\n            termAtt.term().equals(\"there\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 61, offsetAtt.startOffset() == 61);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 66, offsetAtt.endOffset() == 66);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics here\",\n            termAtt.term().equals(\"italics here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 78, offsetAtt.endOffset() == 78);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 79, offsetAtt.startOffset() == 79);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"something\",\n            termAtt.term().equals(\"something\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 86, offsetAtt.startOffset() == 86);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 95, offsetAtt.endOffset() == 95);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more italics\",\n            termAtt.term().equals(\"more italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more\",\n            termAtt.term().equals(\"more\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 102, offsetAtt.endOffset() == 102);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n        assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 103, offsetAtt.startOffset() == 103);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h   i   j\",\n            termAtt.term().equals(\"h   i   j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h\",\n            termAtt.term().equals(\"h\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 125, offsetAtt.endOffset() == 125);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"i\",\n            termAtt.term().equals(\"i\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 128, offsetAtt.startOffset() == 128);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 129, offsetAtt.endOffset() == 129);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"j\",\n            termAtt.term().equals(\"j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 132, offsetAtt.startOffset() == 132);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertFalse(tf.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a","date":1267298041,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","pathOld":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","sourceNew":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);\n    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);\n    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);\n    OffsetAttribute offsetAtt = tf.addAttribute(OffsetAttribute.class);\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a b c d\",\n            termAtt.term().equals(\"a b c d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a\",\n            termAtt.term().equals(\"a\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", flagsAtt.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 12, offsetAtt.endOffset() == 12);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"b\",\n            termAtt.term().equals(\"b\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 13, offsetAtt.startOffset() == 13);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 14, offsetAtt.endOffset() == 14);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"c\",\n            termAtt.term().equals(\"c\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 15, offsetAtt.startOffset() == 15);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 16, offsetAtt.endOffset() == 16);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"d\",\n            termAtt.term().equals(\"d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 17, offsetAtt.startOffset() == 17);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n\n\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e f g\",\n            termAtt.term().equals(\"e f g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e\",\n            termAtt.term().equals(\"e\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 33, offsetAtt.endOffset() == 33);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"f\",\n            termAtt.term().equals(\"f\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 34, offsetAtt.startOffset() == 34);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 35, offsetAtt.endOffset() == 35);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"g\",\n            termAtt.term().equals(\"g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 36, offsetAtt.startOffset() == 36);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 42, offsetAtt.startOffset() == 42);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 46, offsetAtt.endOffset() == 46);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 47, offsetAtt.startOffset() == 47);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 51, offsetAtt.endOffset() == 51);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 56, offsetAtt.startOffset() == 56);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 60, offsetAtt.endOffset() == 60);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"there\",\n            termAtt.term().equals(\"there\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 61, offsetAtt.startOffset() == 61);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 66, offsetAtt.endOffset() == 66);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics here\",\n            termAtt.term().equals(\"italics here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 78, offsetAtt.endOffset() == 78);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 79, offsetAtt.startOffset() == 79);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"something\",\n            termAtt.term().equals(\"something\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 86, offsetAtt.startOffset() == 86);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 95, offsetAtt.endOffset() == 95);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more italics\",\n            termAtt.term().equals(\"more italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more\",\n            termAtt.term().equals(\"more\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 102, offsetAtt.endOffset() == 102);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n        assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 103, offsetAtt.startOffset() == 103);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h   i   j\",\n            termAtt.term().equals(\"h   i   j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h\",\n            termAtt.term().equals(\"h\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 125, offsetAtt.endOffset() == 125);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"i\",\n            termAtt.term().equals(\"i\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 128, offsetAtt.startOffset() == 128);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 129, offsetAtt.endOffset() == 129);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"j\",\n            termAtt.term().equals(\"j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 132, offsetAtt.startOffset() == 132);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertFalse(tf.incrementToken());\n  }\n\n","sourceOld":"  public void testBoth() throws Exception {\n    Set untoks = new HashSet();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);\n    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);\n    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);\n    OffsetAttribute offsetAtt = tf.addAttribute(OffsetAttribute.class);\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a b c d\",\n            termAtt.term().equals(\"a b c d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a\",\n            termAtt.term().equals(\"a\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", flagsAtt.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 12, offsetAtt.endOffset() == 12);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"b\",\n            termAtt.term().equals(\"b\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 13, offsetAtt.startOffset() == 13);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 14, offsetAtt.endOffset() == 14);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"c\",\n            termAtt.term().equals(\"c\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 15, offsetAtt.startOffset() == 15);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 16, offsetAtt.endOffset() == 16);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"d\",\n            termAtt.term().equals(\"d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 17, offsetAtt.startOffset() == 17);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n\n\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e f g\",\n            termAtt.term().equals(\"e f g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e\",\n            termAtt.term().equals(\"e\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 33, offsetAtt.endOffset() == 33);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"f\",\n            termAtt.term().equals(\"f\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 34, offsetAtt.startOffset() == 34);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 35, offsetAtt.endOffset() == 35);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"g\",\n            termAtt.term().equals(\"g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 36, offsetAtt.startOffset() == 36);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 42, offsetAtt.startOffset() == 42);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 46, offsetAtt.endOffset() == 46);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 47, offsetAtt.startOffset() == 47);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 51, offsetAtt.endOffset() == 51);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 56, offsetAtt.startOffset() == 56);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 60, offsetAtt.endOffset() == 60);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"there\",\n            termAtt.term().equals(\"there\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 61, offsetAtt.startOffset() == 61);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 66, offsetAtt.endOffset() == 66);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics here\",\n            termAtt.term().equals(\"italics here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 78, offsetAtt.endOffset() == 78);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 79, offsetAtt.startOffset() == 79);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"something\",\n            termAtt.term().equals(\"something\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 86, offsetAtt.startOffset() == 86);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 95, offsetAtt.endOffset() == 95);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more italics\",\n            termAtt.term().equals(\"more italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more\",\n            termAtt.term().equals(\"more\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 102, offsetAtt.endOffset() == 102);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n        assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 103, offsetAtt.startOffset() == 103);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h   i   j\",\n            termAtt.term().equals(\"h   i   j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h\",\n            termAtt.term().equals(\"h\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 125, offsetAtt.endOffset() == 125);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"i\",\n            termAtt.term().equals(\"i\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 128, offsetAtt.startOffset() == 128);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 129, offsetAtt.endOffset() == 129);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"j\",\n            termAtt.term().equals(\"j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 132, offsetAtt.startOffset() == 132);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertFalse(tf.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","pathOld":"contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest#testBoth().mjava","sourceNew":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);\n    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);\n    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);\n    OffsetAttribute offsetAtt = tf.addAttribute(OffsetAttribute.class);\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a b c d\",\n            termAtt.term().equals(\"a b c d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a\",\n            termAtt.term().equals(\"a\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", flagsAtt.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 12, offsetAtt.endOffset() == 12);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"b\",\n            termAtt.term().equals(\"b\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 13, offsetAtt.startOffset() == 13);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 14, offsetAtt.endOffset() == 14);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"c\",\n            termAtt.term().equals(\"c\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 15, offsetAtt.startOffset() == 15);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 16, offsetAtt.endOffset() == 16);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"d\",\n            termAtt.term().equals(\"d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 17, offsetAtt.startOffset() == 17);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n\n\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e f g\",\n            termAtt.term().equals(\"e f g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e\",\n            termAtt.term().equals(\"e\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 33, offsetAtt.endOffset() == 33);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"f\",\n            termAtt.term().equals(\"f\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 34, offsetAtt.startOffset() == 34);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 35, offsetAtt.endOffset() == 35);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"g\",\n            termAtt.term().equals(\"g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 36, offsetAtt.startOffset() == 36);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 42, offsetAtt.startOffset() == 42);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 46, offsetAtt.endOffset() == 46);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 47, offsetAtt.startOffset() == 47);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 51, offsetAtt.endOffset() == 51);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 56, offsetAtt.startOffset() == 56);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 60, offsetAtt.endOffset() == 60);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"there\",\n            termAtt.term().equals(\"there\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 61, offsetAtt.startOffset() == 61);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 66, offsetAtt.endOffset() == 66);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics here\",\n            termAtt.term().equals(\"italics here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 78, offsetAtt.endOffset() == 78);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 79, offsetAtt.startOffset() == 79);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"something\",\n            termAtt.term().equals(\"something\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 86, offsetAtt.startOffset() == 86);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 95, offsetAtt.endOffset() == 95);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more italics\",\n            termAtt.term().equals(\"more italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more\",\n            termAtt.term().equals(\"more\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 102, offsetAtt.endOffset() == 102);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n        assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 103, offsetAtt.startOffset() == 103);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h   i   j\",\n            termAtt.term().equals(\"h   i   j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h\",\n            termAtt.term().equals(\"h\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 125, offsetAtt.endOffset() == 125);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"i\",\n            termAtt.term().equals(\"i\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 128, offsetAtt.startOffset() == 128);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 129, offsetAtt.endOffset() == 129);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"j\",\n            termAtt.term().equals(\"j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 132, offsetAtt.startOffset() == 132);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertFalse(tf.incrementToken());\n  }\n\n","sourceOld":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);\n    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);\n    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);\n    OffsetAttribute offsetAtt = tf.addAttribute(OffsetAttribute.class);\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a b c d\",\n            termAtt.term().equals(\"a b c d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"a\",\n            termAtt.term().equals(\"a\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" equals: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + \" and it shouldn't\", flagsAtt.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 11, offsetAtt.startOffset() == 11);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 12, offsetAtt.endOffset() == 12);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"b\",\n            termAtt.term().equals(\"b\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 13, offsetAtt.startOffset() == 13);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 14, offsetAtt.endOffset() == 14);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"c\",\n            termAtt.term().equals(\"c\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 15, offsetAtt.startOffset() == 15);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 16, offsetAtt.endOffset() == 16);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"d\",\n            termAtt.term().equals(\"d\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 17, offsetAtt.startOffset() == 17);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 18, offsetAtt.endOffset() == 18);\n\n\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e f g\",\n            termAtt.term().equals(\"e f g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"e\",\n            termAtt.term().equals(\"e\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 32, offsetAtt.startOffset() == 32);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 33, offsetAtt.endOffset() == 33);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"f\",\n            termAtt.term().equals(\"f\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 34, offsetAtt.startOffset() == 34);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 35, offsetAtt.endOffset() == 35);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"g\",\n            termAtt.term().equals(\"g\") == true);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 36, offsetAtt.startOffset() == 36);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 37, offsetAtt.endOffset() == 37);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 42, offsetAtt.startOffset() == 42);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 46, offsetAtt.endOffset() == 46);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 47, offsetAtt.startOffset() == 47);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 51, offsetAtt.endOffset() == 51);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"link\",\n            termAtt.term().equals(\"link\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 56, offsetAtt.startOffset() == 56);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 60, offsetAtt.endOffset() == 60);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"there\",\n            termAtt.term().equals(\"there\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 61, offsetAtt.startOffset() == 61);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 66, offsetAtt.endOffset() == 66);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics here\",\n            termAtt.term().equals(\"italics here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 71, offsetAtt.startOffset() == 71);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 78, offsetAtt.endOffset() == 78);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"here\",\n            termAtt.term().equals(\"here\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 79, offsetAtt.startOffset() == 79);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 83, offsetAtt.endOffset() == 83);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"something\",\n            termAtt.term().equals(\"something\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 86, offsetAtt.startOffset() == 86);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 95, offsetAtt.endOffset() == 95);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more italics\",\n            termAtt.term().equals(\"more italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"more\",\n            termAtt.term().equals(\"more\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 98, offsetAtt.startOffset() == 98);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 102, offsetAtt.endOffset() == 102);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"italics\",\n            termAtt.term().equals(\"italics\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n        assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);\n\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 103, offsetAtt.startOffset() == 103);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 110, offsetAtt.endOffset() == 110);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h   i   j\",\n            termAtt.term().equals(\"h   i   j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(flagsAtt.getFlags() + \" does not equal: \" + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"h\",\n            termAtt.term().equals(\"h\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 0, posIncrAtt.getPositionIncrement() == 0);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 124, offsetAtt.startOffset() == 124);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 125, offsetAtt.endOffset() == 125);\n\n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"i\",\n            termAtt.term().equals(\"i\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 128, offsetAtt.startOffset() == 128);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 129, offsetAtt.endOffset() == 129);\n    \n    assertTrue(tf.incrementToken());\n    assertTrue(termAtt.term() + \" is not equal to \" + \"j\",\n            termAtt.term().equals(\"j\") == true);\n    assertTrue(posIncrAtt.getPositionIncrement() + \" does not equal: \" + 1, posIncrAtt.getPositionIncrement() == 1);\n    assertTrue(typeAtt.type() + \" is not equal to \" + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);\n    assertTrue(offsetAtt.startOffset() + \" does not equal: \" + 132, offsetAtt.startOffset() == 132);\n    assertTrue(offsetAtt.endOffset() + \" does not equal: \" + 133, offsetAtt.endOffset() == 133);\n\n    assertFalse(tf.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["decc8a7344e9231708f9991fa09db2cafec7a2dd"],"decc8a7344e9231708f9991fa09db2cafec7a2dd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9b5756469957918cac40a831acec9cf01c8c2bb3":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a9ac13b5f0ce5ef1b2ce168367d993a79594b23a"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["9b5756469957918cac40a831acec9cf01c8c2bb3"],"decc8a7344e9231708f9991fa09db2cafec7a2dd":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["decc8a7344e9231708f9991fa09db2cafec7a2dd"],"9b5756469957918cac40a831acec9cf01c8c2bb3":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["a9ac13b5f0ce5ef1b2ce168367d993a79594b23a"],"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}