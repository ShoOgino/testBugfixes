{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"    /**\n     * Make sure we skip wicked long terms.\n    */\n    public void testWickedLongTerm() throws IOException {\n      RAMDirectory dir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n      char[] chars = new char[DocumentsWriter.CHAR_BLOCK_SIZE-1];\n      Arrays.fill(chars, 'x');\n      Document doc = new Document();\n      final String bigTerm = new String(chars);\n\n      // Max length term is 16383, so this contents produces\n      // a too-long term:\n      String contents = \"abc xyz x\" + bigTerm + \" another term\";\n      doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n\n      // Make sure we can add another normal document\n      doc = new Document();\n      doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n      writer.close();\n\n      IndexReader reader = IndexReader.open(dir, true);\n\n      // Make sure all terms < max size were indexed\n      assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n      // Make sure position is still incremented when\n      // massive term is skipped:\n      TermPositions tps = reader.termPositions(new Term(\"content\", \"another\"));\n      assertTrue(tps.next());\n      assertEquals(1, tps.freq());\n      assertEquals(3, tps.nextPosition());\n\n      // Make sure the doc that has the massive term is in\n      // the index:\n      assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n      reader.close();\n\n      // Make sure we can add a document with exactly the\n      // maximum length term, and search on that term:\n      doc = new Document();\n      doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n      StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n      sa.setMaxTokenLength(100000);\n      writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n      writer.addDocument(doc);\n      writer.close();\n      reader = IndexReader.open(dir, true);\n      assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n      reader.close();\n\n      dir.close();\n    }\n\n","sourceOld":"    /**\n     * Make sure we skip wicked long terms.\n    */\n    public void testWickedLongTerm() throws IOException {\n      RAMDirectory dir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n      char[] chars = new char[DocumentsWriter.CHAR_BLOCK_SIZE-1];\n      Arrays.fill(chars, 'x');\n      Document doc = new Document();\n      final String bigTerm = new String(chars);\n\n      // Max length term is 16383, so this contents produces\n      // a too-long term:\n      String contents = \"abc xyz x\" + bigTerm + \" another term\";\n      doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n\n      // Make sure we can add another normal document\n      doc = new Document();\n      doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n      writer.close();\n\n      IndexReader reader = IndexReader.open(dir, true);\n\n      // Make sure all terms < max size were indexed\n      assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n      // Make sure position is still incremented when\n      // massive term is skipped:\n      TermPositions tps = reader.termPositions(new Term(\"content\", \"another\"));\n      assertTrue(tps.next());\n      assertEquals(1, tps.freq());\n      assertEquals(3, tps.nextPosition());\n\n      // Make sure the doc that has the massive term is in\n      // the index:\n      assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n      reader.close();\n\n      // Make sure we can add a document with exactly the\n      // maximum length term, and search on that term:\n      doc = new Document();\n      doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n      StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n      sa.setMaxTokenLength(100000);\n      writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n      writer.addDocument(doc);\n      writer.close();\n      reader = IndexReader.open(dir, true);\n      assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n      reader.close();\n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"    /**\n     * Make sure we skip wicked long terms.\n    */\n    public void testWickedLongTerm() throws IOException {\n      RAMDirectory dir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n      char[] chars = new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];\n      Arrays.fill(chars, 'x');\n      Document doc = new Document();\n      final String bigTerm = new String(chars);\n\n      // This produces a too-long term:\n      String contents = \"abc xyz x\" + bigTerm + \" another term\";\n      doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n\n      // Make sure we can add another normal document\n      doc = new Document();\n      doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n      writer.close();\n\n      IndexReader reader = IndexReader.open(dir, true);\n\n      // Make sure all terms < max size were indexed\n      assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n      // Make sure position is still incremented when\n      // massive term is skipped:\n      TermPositions tps = reader.termPositions(new Term(\"content\", \"another\"));\n      assertTrue(tps.next());\n      assertEquals(1, tps.freq());\n      assertEquals(3, tps.nextPosition());\n\n      // Make sure the doc that has the massive term is in\n      // the index:\n      assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n      reader.close();\n\n      // Make sure we can add a document with exactly the\n      // maximum length term, and search on that term:\n      doc = new Document();\n      doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n      StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n      sa.setMaxTokenLength(100000);\n      writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n      writer.addDocument(doc);\n      writer.close();\n      reader = IndexReader.open(dir, true);\n      assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n      reader.close();\n\n      dir.close();\n    }\n\n","sourceOld":"    /**\n     * Make sure we skip wicked long terms.\n    */\n    public void testWickedLongTerm() throws IOException {\n      RAMDirectory dir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n      char[] chars = new char[DocumentsWriter.CHAR_BLOCK_SIZE-1];\n      Arrays.fill(chars, 'x');\n      Document doc = new Document();\n      final String bigTerm = new String(chars);\n\n      // Max length term is 16383, so this contents produces\n      // a too-long term:\n      String contents = \"abc xyz x\" + bigTerm + \" another term\";\n      doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n\n      // Make sure we can add another normal document\n      doc = new Document();\n      doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n      writer.close();\n\n      IndexReader reader = IndexReader.open(dir, true);\n\n      // Make sure all terms < max size were indexed\n      assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n      // Make sure position is still incremented when\n      // massive term is skipped:\n      TermPositions tps = reader.termPositions(new Term(\"content\", \"another\"));\n      assertTrue(tps.next());\n      assertEquals(1, tps.freq());\n      assertEquals(3, tps.nextPosition());\n\n      // Make sure the doc that has the massive term is in\n      // the index:\n      assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n      reader.close();\n\n      // Make sure we can add a document with exactly the\n      // maximum length term, and search on that term:\n      doc = new Document();\n      doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n      StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n      sa.setMaxTokenLength(100000);\n      writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n      writer.addDocument(doc);\n      writer.close();\n      reader = IndexReader.open(dir, true);\n      assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n      reader.close();\n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790c3f61c9b891d66d919c5d10db9fa5216eb0f1","date":1274818604,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":null,"sourceOld":"    /**\n     * Make sure we skip wicked long terms.\n    */\n    public void testWickedLongTerm() throws IOException {\n      RAMDirectory dir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n      char[] chars = new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];\n      Arrays.fill(chars, 'x');\n      Document doc = new Document();\n      final String bigTerm = new String(chars);\n\n      // This produces a too-long term:\n      String contents = \"abc xyz x\" + bigTerm + \" another term\";\n      doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n\n      // Make sure we can add another normal document\n      doc = new Document();\n      doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n      writer.close();\n\n      IndexReader reader = IndexReader.open(dir, true);\n\n      // Make sure all terms < max size were indexed\n      assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n      assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n      // Make sure position is still incremented when\n      // massive term is skipped:\n      TermPositions tps = reader.termPositions(new Term(\"content\", \"another\"));\n      assertTrue(tps.next());\n      assertEquals(1, tps.freq());\n      assertEquals(3, tps.nextPosition());\n\n      // Make sure the doc that has the massive term is in\n      // the index:\n      assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n      reader.close();\n\n      // Make sure we can add a document with exactly the\n      // maximum length term, and search on that term:\n      doc = new Document();\n      doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n      StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n      sa.setMaxTokenLength(100000);\n      writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n      writer.addDocument(doc);\n      writer.close();\n      reader = IndexReader.open(dir, true);\n      assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n      reader.close();\n\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"46ab5314fa7311a6a5c0d9f4913464fe322a48d5","date":1300205481,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    Field contentField = new Field(\"content\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe","d4d69c535930b5cce125cff868d40f6373dc27d4","05fe562aa248790944d43cdd478f512572835ba0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    Field contentField = new Field(\"content\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    Field contentField = new Field(\"content\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    Field contentField = new Field(\"content\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    Field contentField = new Field(\"content\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    Field contentField = new Field(\"content\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    Field contentField = new Field(\"content\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    Field contentField = new Field(\"content\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    Field contentField = new Field(\"content\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", customType, \"\");\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    Field contentField = new Field(\"content\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd","date":1317197236,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", customType, \"\");\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"));\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2dd6ecb8250c497ed227653279d6a4f470bfbb31","date":1326814483,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(reader, \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"386d1b0dcb065f1bfc494b1407cb41c536b95485","date":1327848512,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n   */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, new StringSplitAnalyzer());\n\n    char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n    final BytesRef bigTermBytesRef = new BytesRef(bigTerm);\n\n    // This contents produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    w.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    w.addDocument(doc);\n\n    IndexReader reader = w.getReader();\n    w.close();\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader, null, \"content\", new BytesRef(\"another\"), false);\n    assertEquals(0, tps.nextDoc());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n    dir.close();\n    dir = newDirectory();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_UNSTORED);\n    customType.setTokenized(false);\n    Field contentField = new Field(\"content\", \"\", customType);\n    doc.add(contentField);\n\n    w = new RandomIndexWriter(random, dir);\n\n    contentField.setValue(\"other\");\n    w.addDocument(doc);\n\n    contentField.setValue(\"term\");\n    w.addDocument(doc);\n\n    contentField.setValue(bigTerm);\n    w.addDocument(doc);\n\n    contentField.setValue(\"zzz\");\n    w.addDocument(doc);\n\n    reader = w.getReader();\n    w.close();\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n\n    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), \"content\", random.nextBoolean());\n    assertEquals(5, dti.numOrd());                // +1 for null ord\n    assertEquals(4, dti.size());\n    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["46ab5314fa7311a6a5c0d9f4913464fe322a48d5","c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"2dd6ecb8250c497ed227653279d6a4f470bfbb31":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["2dd6ecb8250c497ed227653279d6a4f470bfbb31","386d1b0dcb065f1bfc494b1407cb41c536b95485"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1","46ab5314fa7311a6a5c0d9f4913464fe322a48d5"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1","46ab5314fa7311a6a5c0d9f4913464fe322a48d5"],"a3776dccca01c11e7046323cfad46a3b4a471233":["46ab5314fa7311a6a5c0d9f4913464fe322a48d5","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"386d1b0dcb065f1bfc494b1407cb41c536b95485":["2dd6ecb8250c497ed227653279d6a4f470bfbb31"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"46ab5314fa7311a6a5c0d9f4913464fe322a48d5":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"790c3f61c9b891d66d919c5d10db9fa5216eb0f1":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"2dd6ecb8250c497ed227653279d6a4f470bfbb31":["5cab9a86bd67202d20b6adc463008c8e982b070a","386d1b0dcb065f1bfc494b1407cb41c536b95485"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"386d1b0dcb065f1bfc494b1407cb41c536b95485":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"46ab5314fa7311a6a5c0d9f4913464fe322a48d5":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["2dd6ecb8250c497ed227653279d6a4f470bfbb31"],"790c3f61c9b891d66d919c5d10db9fa5216eb0f1":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","46ab5314fa7311a6a5c0d9f4913464fe322a48d5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["31f025ae60076ae95274433f3fe8e6ace2857a87"]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}