{"path":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c18273ea5b3974d2f30117f46f1ae416c28f727","date":1279708040,"type":5,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a186ae8733084223c22044e935e4ef848a143d1","date":1289694819,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    if (state.numDocsInStore > 0) {\n      // It's possible that all documents seen in this segment\n      // hit non-aborting exceptions, in which case we will\n      // not have yet init'd the TermVectorsWriter:\n      initTermVectorsWriter();\n    }\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":["bafd5d8871af22a215defb79410396c64a671126"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c498d3f8d75170b121f5eda2c6210ac5beb5d411","date":1289726298,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    if (state.numDocsInStore > 0) {\n      // It's possible that all documents seen in this segment\n      // hit non-aborting exceptions, in which case we will\n      // not have yet init'd the TermVectorsWriter:\n      initTermVectorsWriter();\n    }\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bafd5d8871af22a215defb79410396c64a671126","date":1290770742,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    // NOTE: it's possible that all documents seen in this segment\n    // hit non-aborting exceptions, in which case we will\n    // not have yet init'd the TermVectorsWriter.  This is\n    // actually OK (unlike in the stored fields case)\n    // because, although IieldInfos.hasVectors() will return\n    // true, the TermVectorsReader gracefully handles\n    // non-existence of the term vectors files.\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    if (state.numDocsInStore > 0) {\n      // It's possible that all documents seen in this segment\n      // hit non-aborting exceptions, in which case we will\n      // not have yet init'd the TermVectorsWriter:\n      initTermVectorsWriter();\n    }\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":["2a186ae8733084223c22044e935e4ef848a143d1"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    // NOTE: it's possible that all documents seen in this segment\n    // hit non-aborting exceptions, in which case we will\n    // not have yet init'd the TermVectorsWriter.  This is\n    // actually OK (unlike in the stored fields case)\n    // because, although IieldInfos.hasVectors() will return\n    // true, the TermVectorsReader gracefully handles\n    // non-existence of the term vectors files.\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    if (state.numDocsInStore > 0) {\n      // It's possible that all documents seen in this segment\n      // hit non-aborting exceptions, in which case we will\n      // not have yet init'd the TermVectorsWriter:\n      initTermVectorsWriter();\n    }\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d36aa87c4d74123a26f107bafdecfb5cb6b48dd0","date":1292191891,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    // NOTE: it's possible that all documents seen in this segment\n    // hit non-aborting exceptions, in which case we will\n    // not have yet init'd the TermVectorsWriter.  This is\n    // actually OK (unlike in the stored fields case)\n    // because, although FieldInfos.hasVectors() will return\n    // true, the TermVectorsReader gracefully handles\n    // non-existence of the term vectors files.\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    // NOTE: it's possible that all documents seen in this segment\n    // hit non-aborting exceptions, in which case we will\n    // not have yet init'd the TermVectorsWriter.  This is\n    // actually OK (unlike in the stored fields case)\n    // because, although IieldInfos.hasVectors() will return\n    // true, the TermVectorsReader gracefully handles\n    // non-existence of the term vectors files.\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e1cbd7e289dc1243c7a59e1a83d078163a147fe","date":1292268032,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    // NOTE: it's possible that all documents seen in this segment\n    // hit non-aborting exceptions, in which case we will\n    // not have yet init'd the TermVectorsWriter.  This is\n    // actually OK (unlike in the stored fields case)\n    // because, although FieldInfos.hasVectors() will return\n    // true, the TermVectorsReader gracefully handles\n    // non-existence of the term vectors files.\n    state.hasVectors = hasVectors;\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    // NOTE: it's possible that all documents seen in this segment\n    // hit non-aborting exceptions, in which case we will\n    // not have yet init'd the TermVectorsWriter.  This is\n    // actually OK (unlike in the stored fields case)\n    // because, although FieldInfos.hasVectors() will return\n    // true, the TermVectorsReader gracefully handles\n    // non-existence of the term vectors files.\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      String fldName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_FIELDS_EXTENSION);\n      String docName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n\n      if (4 + ((long) state.numDocs) * 16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      state.flushedFiles.add(idxName);\n      state.flushedFiles.add(fldName);\n      state.flushedFiles.add(docName);\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    // NOTE: it's possible that all documents seen in this segment\n    // hit non-aborting exceptions, in which case we will\n    // not have yet init'd the TermVectorsWriter.  This is\n    // actually OK (unlike in the stored fields case)\n    // because, although FieldInfos.hasVectors() will return\n    // true, the TermVectorsReader gracefully handles\n    // non-existence of the term vectors files.\n    state.hasVectors = hasVectors;\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      String fldName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_FIELDS_EXTENSION);\n      String docName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n\n      if (4 + ((long) state.numDocs) * 16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      state.flushedFiles.add(idxName);\n      state.flushedFiles.add(fldName);\n      state.flushedFiles.add(docName);\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    // NOTE: it's possible that all documents seen in this segment\n    // hit non-aborting exceptions, in which case we will\n    // not have yet init'd the TermVectorsWriter.  This is\n    // actually OK (unlike in the stored fields case)\n    // because, although IieldInfos.hasVectors() will return\n    // true, the TermVectorsReader gracefully handles\n    // non-existence of the term vectors files.\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe","date":1294227869,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      if (4 + ((long) state.numDocs) * 16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      String fldName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_FIELDS_EXTENSION);\n      String docName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n\n      if (4 + ((long) state.numDocs) * 16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      state.flushedFiles.add(idxName);\n      state.flushedFiles.add(fldName);\n      state.flushedFiles.add(docName);\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      if (4 + ((long) state.numDocs) * 16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      String fldName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_FIELDS_EXTENSION);\n      String docName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n\n      if (4 + ((long) state.numDocs) * 16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      state.flushedFiles.add(idxName);\n      state.flushedFiles.add(fldName);\n      state.flushedFiles.add(docName);\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":5,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      if (4 + ((long) state.numDocs) * 16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":null,"sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      if (4 + ((long) state.numDocs) * 16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":4,"author":"Steven Rowe","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":null,"sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      if (4 + ((long) state.numDocs) * 16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70ad682703b8585f5d0a637efec044d57ec05efb":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"2a186ae8733084223c22044e935e4ef848a143d1":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"c498d3f8d75170b121f5eda2c6210ac5beb5d411":["9454a6510e2db155fb01faa5c049b06ece95fab9","2a186ae8733084223c22044e935e4ef848a143d1"],"bafd5d8871af22a215defb79410396c64a671126":["2a186ae8733084223c22044e935e4ef848a143d1"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["70ad682703b8585f5d0a637efec044d57ec05efb","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe","6c18273ea5b3974d2f30117f46f1ae416c28f727"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["3bb13258feba31ab676502787ab2e1779f129b7a","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"d36aa87c4d74123a26f107bafdecfb5cb6b48dd0":["bafd5d8871af22a215defb79410396c64a671126"],"a3776dccca01c11e7046323cfad46a3b4a471233":["fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["d36aa87c4d74123a26f107bafdecfb5cb6b48dd0"],"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"3bb13258feba31ab676502787ab2e1779f129b7a":["c498d3f8d75170b121f5eda2c6210ac5beb5d411","bafd5d8871af22a215defb79410396c64a671126"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"70ad682703b8585f5d0a637efec044d57ec05efb":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"2a186ae8733084223c22044e935e4ef848a143d1":["c498d3f8d75170b121f5eda2c6210ac5beb5d411","bafd5d8871af22a215defb79410396c64a671126"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"c498d3f8d75170b121f5eda2c6210ac5beb5d411":["3bb13258feba31ab676502787ab2e1779f129b7a"],"bafd5d8871af22a215defb79410396c64a671126":["d36aa87c4d74123a26f107bafdecfb5cb6b48dd0","3bb13258feba31ab676502787ab2e1779f129b7a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["70ad682703b8585f5d0a637efec044d57ec05efb"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d36aa87c4d74123a26f107bafdecfb5cb6b48dd0":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe":["70ad682703b8585f5d0a637efec044d57ec05efb","b3e06be49006ecac364d39d12b9c9f74882f9b9f","a3776dccca01c11e7046323cfad46a3b4a471233"],"3bb13258feba31ab676502787ab2e1779f129b7a":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["2a186ae8733084223c22044e935e4ef848a143d1","6c18273ea5b3974d2f30117f46f1ae416c28f727","c498d3f8d75170b121f5eda2c6210ac5beb5d411"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}