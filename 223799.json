{"path":"modules/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","commits":[{"id":"e230a61047bc041516c811baa08a7174d6f8322a","date":1306175633,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"modules/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","pathOld":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","sourceNew":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = new ArrayList<Entry>();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","sourceOld":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = Lists.newArrayList();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","pathOld":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","sourceNew":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = new ArrayList<Entry>();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","sourceOld":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = Lists.newArrayList();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":1,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","pathOld":"solr/src/java/org/apache/solr/spelling/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","sourceNew":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = new ArrayList<Entry>();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","sourceOld":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = Lists.newArrayList();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7bf07f4ebadc7539de83a531e64678350d78ca26","date":1322776167,"type":4,"author":"Dawid Weiss","isMerge":false,"pathNew":"/dev/null","pathOld":"modules/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTLookup#build(TermFreqIterator).mjava","sourceNew":null,"sourceOld":"  /* */\n  @Override\n  public void build(TermFreqIterator tfit) throws IOException {\n    // Buffer the input because we will need it twice: for calculating\n    // weights distribution and for the actual automata building.\n    List<Entry> entries = new ArrayList<Entry>();\n    while (tfit.hasNext()) {\n      String term = tfit.next();\n      char [] termChars = new char [term.length() + 1]; // add padding for weight.\n      for (int i = 0; i < term.length(); i++)\n        termChars[i + 1] = term.charAt(i);\n      entries.add(new Entry(termChars, tfit.freq()));\n    }\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n    if (entries.size() > 0) {\n      redistributeWeightsProportionalMinMax(entries, buckets);\n      encodeWeightPrefix(entries);\n    }\n\n    // Build the automaton (includes input sorting) and cache root arcs in order from the highest,\n    // to the lowest weight.\n    this.automaton = buildAutomaton(entries);\n    cacheRootArcs();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7bf07f4ebadc7539de83a531e64678350d78ca26":["e230a61047bc041516c811baa08a7174d6f8322a"],"e230a61047bc041516c811baa08a7174d6f8322a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e230a61047bc041516c811baa08a7174d6f8322a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7bf07f4ebadc7539de83a531e64678350d78ca26"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e230a61047bc041516c811baa08a7174d6f8322a"]},"commit2Childs":{"7bf07f4ebadc7539de83a531e64678350d78ca26":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e230a61047bc041516c811baa08a7174d6f8322a":["7bf07f4ebadc7539de83a531e64678350d78ca26","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e230a61047bc041516c811baa08a7174d6f8322a","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}