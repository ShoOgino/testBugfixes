{"path":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"/dev/null","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":null,"sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"/dev/null","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term.utf8ToString())));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum td = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = td.getBulkResult();\n\n      for(;;) {\n        int n = td.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17","date":1277233255,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term.utf8ToString())), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term.utf8ToString())));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum td = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = td.getBulkResult();\n\n      for(;;) {\n        int n = td.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term.utf8ToString())), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c220849f876de24a79f756f65b3eb045db59f63f","date":1294902803,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getIndexReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getIndexReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getIndexReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"95ae76773bf2b95987d5f9c8f566ab3738953fb4","date":1301758351,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    DocsEnum docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getIndexReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":5,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocTermOrds#uninvert(IndexReader,BytesRef).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  // Call this only once (if you subclass!)\n  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {\n    //System.out.println(\"DTO uninvert field=\" + field + \" prefix=\" + termPrefix);\n    final long startTime = System.currentTimeMillis();\n    prefix = termPrefix == null ? null : new BytesRef(termPrefix);\n\n    final int maxDoc = reader.maxDoc();\n    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n\n    final Terms terms = MultiFields.getTerms(reader, field);\n    if (terms == null) {\n      // No terms\n      return;\n    }\n\n    final TermsEnum te = terms.iterator();\n    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();\n    //System.out.println(\"seekStart=\" + seekStart.utf8ToString());\n    if (te.seek(seekStart) == TermsEnum.SeekStatus.END) {\n      // No terms match\n      return;\n    }\n\n    // If we need our \"term index wrapper\", these will be\n    // init'd below:\n    List<BytesRef> indexedTerms = null;\n    PagedBytes indexedTermsBytes = null;\n\n    boolean testedOrd = false;\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    int termNum = 0;\n    DocsEnum docsEnum = null;\n\n    // Loop begins with te positioned to first term (we call\n    // seek above):\n    for (;;) {\n      final BytesRef t = te.term();\n      if (t == null || (termPrefix != null && !t.startsWith(termPrefix))) {\n        break;\n      }\n      //System.out.println(\"visit term=\" + t.utf8ToString() + \" \" + t + \" termNum=\" + termNum);\n\n      if (!testedOrd) {\n        try {\n          ordBase = (int) te.ord();\n          //System.out.println(\"got ordBase=\" + ordBase);\n        } catch (UnsupportedOperationException uoe) {\n          // Reader cannot provide ord support, so we wrap\n          // our own support by creating our own terms index:\n          indexedTerms = new ArrayList<BytesRef>();\n          indexedTermsBytes = new PagedBytes(15);\n          //System.out.println(\"NO ORDS\");\n        }\n        testedOrd = true;\n      }\n\n      visitTerm(te, termNum);\n\n      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {\n        // Index this term\n        sizeOfIndexedStrings += t.length;\n        BytesRef indexedTerm = new BytesRef();\n        indexedTermsBytes.copy(t, indexedTerm);\n        // TODO: really should 1) strip off useless suffix,\n        // and 2) use FST not array/PagedBytes\n        indexedTerms.add(indexedTerm);\n      }\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\n        docsEnum = te.docs(delDocs, docsEnum);\n\n        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n        // dF, but takes deletions into account\n        int actualDF = 0;\n\n        for (;;) {\n          int chunk = docsEnum.read();\n          if (chunk <= 0) {\n            break;\n          }\n          //System.out.println(\"  chunk=\" + chunk + \" docs\");\n\n          actualDF += chunk;\n\n          for (int i=0; i<chunk; i++) {\n            termInstances++;\n            int doc = bulkResult.docs.ints[i];\n            //System.out.println(\"    docID=\" + doc);\n            // add TNUM_OFFSET to the term number to make room for special reserved values:\n            // 0 (end term) and 1 (index into byte array follows)\n            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n            lastTerm[doc] = termNum;\n            int val = index[doc];\n\n            if ((val & 0xff)==1) {\n              // index into byte array (actually the end of\n              // the doc-specific byte[] when building)\n              int pos = val >>> 8;\n              int ilen = vIntSize(delta);\n              byte[] arr = bytes[doc];\n              int newend = pos+ilen;\n              if (newend > arr.length) {\n                // We avoid a doubling strategy to lower memory usage.\n                // this faceting method isn't for docs with many terms.\n                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n                // TODO: figure out what array lengths we can round up to w/o actually using more memory\n                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n                // It should be safe to round up to the nearest 32 bits in any case.\n                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n                byte[] newarr = new byte[newLen];\n                System.arraycopy(arr, 0, newarr, 0, pos);\n                arr = newarr;\n                bytes[doc] = newarr;\n              }\n              pos = writeInt(delta, arr, pos);\n              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n            } else {\n              // OK, this int has data in it... find the end (a zero starting byte - not\n              // part of another number, hence not following a byte with the high bit set).\n              int ipos;\n              if (val==0) {\n                ipos=0;\n              } else if ((val & 0x0000ff80)==0) {\n                ipos=1;\n              } else if ((val & 0x00ff8000)==0) {\n                ipos=2;\n              } else if ((val & 0xff800000)==0) {\n                ipos=3;\n              } else {\n                ipos=4;\n              }\n\n              //System.out.println(\"      ipos=\" + ipos);\n\n              int endPos = writeInt(delta, tempArr, ipos);\n              //System.out.println(\"      endpos=\" + endPos);\n              if (endPos <= 4) {\n                //System.out.println(\"      fits!\");\n                // value will fit in the integer... move bytes back\n                for (int j=ipos; j<endPos; j++) {\n                  val |= (tempArr[j] & 0xff) << (j<<3);\n                }\n                index[doc] = val;\n              } else {\n                // value won't fit... move integer into byte[]\n                for (int j=0; j<ipos; j++) {\n                  tempArr[j] = (byte)val;\n                  val >>>=8;\n                }\n                // point at the end index in the byte[]\n                index[doc] = (endPos<<8) | 1;\n                bytes[doc] = tempArr;\n                tempArr = new byte[12];\n              }\n            }\n          }\n        }\n        setActualDocFreq(termNum, actualDF);\n      }\n\n      termNum++;\n      if (te.next() == null) {\n        break;\n      }\n    }\n\n    numTermsInField = termNum;\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      tnums = null;\n    } else {\n\n      this.index = index;\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            //System.out.println(\"  pass=\" + pass + \" process docID=\" + doc);\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              //System.out.println(\"    ptr pos=\" + pos);\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new IllegalStateException(\"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              /*\n              for(byte b : arr) {\n                //System.out.println(\"      b=\" + Integer.toHexString((int) b));\n              }\n              */\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n\n      if (indexedTerms != null) {\n        indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getIndexReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":null,"sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getIndexReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":4,"author":"Steven Rowe","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":null,"sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getIndexReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermsEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    SolrIndexSearcher.DocsEnumState deState = null;\n\n    for (;;) {\n      BytesRef t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = new BytesRef(t);\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        if (deState == null) {\n          deState = new SolrIndexSearcher.DocsEnumState();\n          deState.termsEnum = te.tenum;\n          deState.reuse = te.docsEnum;\n        }\n        DocSet set = searcher.getDocSet(new TermQuery(new Term(ti.field, topTerm.term)), deState);\n        te.docsEnum = deState.reuse;\n\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      DocsEnum docsEnum = te.getDocsEnum();\n\n      DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();\n\n      for(;;) {\n        int n = docsEnum.read();\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = bulkResult.docs.ints[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17"],"b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"5f4e87790277826a2aea119328600dfb07761f32":["1da8d55113b689b06716246649de6f62430f15c0","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"c220849f876de24a79f756f65b3eb045db59f63f":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"a3776dccca01c11e7046323cfad46a3b4a471233":["c220849f876de24a79f756f65b3eb045db59f63f","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","c220849f876de24a79f756f65b3eb045db59f63f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"28427ef110c4c5bf5b4057731b83110bd1e13724":["1da8d55113b689b06716246649de6f62430f15c0"],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"95ae76773bf2b95987d5f9c8f566ab3738953fb4":["c220849f876de24a79f756f65b3eb045db59f63f"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["5f4e87790277826a2aea119328600dfb07761f32","c220849f876de24a79f756f65b3eb045db59f63f"],"45669a651c970812a680841b97a77cce06af559f":["868da859b43505d9d2a023bfeae6dd0c795f5295","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["95ae76773bf2b95987d5f9c8f566ab3738953fb4"]},"commit2Childs":{"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["5f4e87790277826a2aea119328600dfb07761f32","c220849f876de24a79f756f65b3eb045db59f63f","29ef99d61cda9641b6250bf9567329a6e65f901d"],"b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"5f4e87790277826a2aea119328600dfb07761f32":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"1da8d55113b689b06716246649de6f62430f15c0":["5f4e87790277826a2aea119328600dfb07761f32","28427ef110c4c5bf5b4057731b83110bd1e13724"],"c220849f876de24a79f756f65b3eb045db59f63f":["a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d","95ae76773bf2b95987d5f9c8f566ab3738953fb4","868da859b43505d9d2a023bfeae6dd0c795f5295"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["b4cd50ad7936c0f3a0851313b4e0ab861c7a6b17"],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"95ae76773bf2b95987d5f9c8f566ab3738953fb4":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","45669a651c970812a680841b97a77cce06af559f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["45669a651c970812a680841b97a77cce06af559f"],"45669a651c970812a680841b97a77cce06af559f":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","45669a651c970812a680841b97a77cce06af559f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}