{"path":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeBinaryDVUpdates(Map[String,LinkedHashMap[Term,BinaryDocValuesUpdate]]).mjava","commits":[{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeBinaryDVUpdates(Map[String,LinkedHashMap[Term,BinaryDocValuesUpdate]]).mjava","pathOld":"/dev/null","sourceNew":"  private byte[] freezeBinaryDVUpdates(Map<String,LinkedHashMap<Term,BinaryDocValuesUpdate>> binaryDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    RAMOutputStream out = new RAMOutputStream();\n    String lastTermField = null;\n    String lastUpdateField = null;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : binaryDVUpdates.values()) {\n      binaryDVUpdateCount += binaryUpdates.size();\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n\n        int code = update.term.bytes().length << 2;\n\n        String termField = update.term.field();\n        if (termField.equals(lastTermField) == false) {\n          code |= 1;\n        }\n        String updateField = update.field;\n        if (updateField.equals(lastUpdateField) == false) {\n          code |= 2;\n        }\n        out.writeVInt(code);\n        out.writeVInt(update.docIDUpto);\n        if (termField.equals(lastTermField) == false) {\n          out.writeString(termField);\n          lastTermField = termField;\n        }\n        if (updateField.equals(lastUpdateField) == false) {\n          out.writeString(updateField);\n          lastUpdateField = updateField;\n        }\n        out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n\n        BytesRef value = (BytesRef) update.value;\n        out.writeVInt(value.length);\n        out.writeBytes(value.bytes, value.offset, value.length);\n      }\n    }\n    byte[] bytes = new byte[(int) out.getFilePointer()];\n    out.writeTo(bytes, 0);\n    return bytes;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeBinaryDVUpdates(Map[String,LinkedHashMap[Term,BinaryDocValuesUpdate]]).mjava","pathOld":"/dev/null","sourceNew":"  private byte[] freezeBinaryDVUpdates(Map<String,LinkedHashMap<Term,BinaryDocValuesUpdate>> binaryDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    RAMOutputStream out = new RAMOutputStream();\n    String lastTermField = null;\n    String lastUpdateField = null;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : binaryDVUpdates.values()) {\n      binaryDVUpdateCount += binaryUpdates.size();\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n\n        int code = update.term.bytes().length << 2;\n\n        String termField = update.term.field();\n        if (termField.equals(lastTermField) == false) {\n          code |= 1;\n        }\n        String updateField = update.field;\n        if (updateField.equals(lastUpdateField) == false) {\n          code |= 2;\n        }\n        out.writeVInt(code);\n        out.writeVInt(update.docIDUpto);\n        if (termField.equals(lastTermField) == false) {\n          out.writeString(termField);\n          lastTermField = termField;\n        }\n        if (updateField.equals(lastUpdateField) == false) {\n          out.writeString(updateField);\n          lastUpdateField = updateField;\n        }\n        out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n\n        BytesRef value = (BytesRef) update.value;\n        out.writeVInt(value.length);\n        out.writeBytes(value.bytes, value.offset, value.length);\n      }\n    }\n    byte[] bytes = new byte[(int) out.getFilePointer()];\n    out.writeTo(bytes, 0);\n    return bytes;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeBinaryDVUpdates(Map[String,LinkedHashMap[Term,BinaryDocValuesUpdate]]).mjava","pathOld":"/dev/null","sourceNew":"  private byte[] freezeBinaryDVUpdates(Map<String,LinkedHashMap<Term,BinaryDocValuesUpdate>> binaryDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    RAMOutputStream out = new RAMOutputStream();\n    String lastTermField = null;\n    String lastUpdateField = null;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : binaryDVUpdates.values()) {\n      binaryDVUpdateCount += binaryUpdates.size();\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n\n        int code = update.term.bytes().length << 2;\n\n        String termField = update.term.field();\n        if (termField.equals(lastTermField) == false) {\n          code |= 1;\n        }\n        String updateField = update.field;\n        if (updateField.equals(lastUpdateField) == false) {\n          code |= 2;\n        }\n        out.writeVInt(code);\n        out.writeVInt(update.docIDUpto);\n        if (termField.equals(lastTermField) == false) {\n          out.writeString(termField);\n          lastTermField = termField;\n        }\n        if (updateField.equals(lastUpdateField) == false) {\n          out.writeString(updateField);\n          lastUpdateField = updateField;\n        }\n        out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n\n        BytesRef value = (BytesRef) update.value;\n        out.writeVInt(value.length);\n        out.writeBytes(value.bytes, value.offset, value.length);\n      }\n    }\n    byte[] bytes = new byte[(int) out.getFilePointer()];\n    out.writeTo(bytes, 0);\n    return bytes;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"110125c995236a7f61057dd04b039ed2d267f3a1","date":1521014987,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeBinaryDVUpdates(Map[String,LinkedHashMap[Term,BinaryDocValuesUpdate]]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeBinaryDVUpdates(Map[String,LinkedHashMap[Term,BinaryDocValuesUpdate]]).mjava","sourceNew":"  private byte[] freezeBinaryDVUpdates(Map<String,LinkedHashMap<Term,BinaryDocValuesUpdate>> binaryDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, BinaryDocValuesUpdate> binaryUpdates : binaryDVUpdates.values()) {\n        binaryDVUpdateCount += binaryUpdates.size();\n        for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n\n          BytesRef value = (BytesRef) update.value;\n          out.writeVInt(value.length);\n          out.writeBytes(value.bytes, value.offset, value.length);\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","sourceOld":"  private byte[] freezeBinaryDVUpdates(Map<String,LinkedHashMap<Term,BinaryDocValuesUpdate>> binaryDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    RAMOutputStream out = new RAMOutputStream();\n    String lastTermField = null;\n    String lastUpdateField = null;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : binaryDVUpdates.values()) {\n      binaryDVUpdateCount += binaryUpdates.size();\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n\n        int code = update.term.bytes().length << 2;\n\n        String termField = update.term.field();\n        if (termField.equals(lastTermField) == false) {\n          code |= 1;\n        }\n        String updateField = update.field;\n        if (updateField.equals(lastUpdateField) == false) {\n          code |= 2;\n        }\n        out.writeVInt(code);\n        out.writeVInt(update.docIDUpto);\n        if (termField.equals(lastTermField) == false) {\n          out.writeString(termField);\n          lastTermField = termField;\n        }\n        if (updateField.equals(lastUpdateField) == false) {\n          out.writeString(updateField);\n          lastUpdateField = updateField;\n        }\n        out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n\n        BytesRef value = (BytesRef) update.value;\n        out.writeVInt(value.length);\n        out.writeBytes(value.bytes, value.offset, value.length);\n      }\n    }\n    byte[] bytes = new byte[(int) out.getFilePointer()];\n    out.writeTo(bytes, 0);\n    return bytes;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e129bd6cb34a236558a49edf108a49d5c15e0e1","date":1525081316,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#[T-extends-DocValuesUpdate]_freezeDVUpdates(Map[String,LinkedHashMap[Term,T]],IntConsumer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeBinaryDVUpdates(Map[String,LinkedHashMap[Term,BinaryDocValuesUpdate]]).mjava","sourceNew":"  private static <T extends DocValuesUpdate> byte[] freezeDVUpdates(Map<String,LinkedHashMap<Term, T>> dvUpdates,\n                                                                    IntConsumer updateSizeConsumer)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, T> updates : dvUpdates.values()) {\n        updateSizeConsumer.accept(updates.size());\n        for (T update : updates.values()) {\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n          update.writeTo(out);\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","sourceOld":"  private byte[] freezeBinaryDVUpdates(Map<String,LinkedHashMap<Term,BinaryDocValuesUpdate>> binaryDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, BinaryDocValuesUpdate> binaryUpdates : binaryDVUpdates.values()) {\n        binaryDVUpdateCount += binaryUpdates.size();\n        for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n\n          BytesRef value = (BytesRef) update.value;\n          out.writeVInt(value.length);\n          out.writeBytes(value.bytes, value.offset, value.length);\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f42883db49d143abc1a0f176ba47e3388dafb608","date":1525083166,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#[T-extends-DocValuesUpdate]_freezeDVUpdates(Map[String,LinkedHashMap[Term,T]],IntConsumer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeBinaryDVUpdates(Map[String,LinkedHashMap[Term,BinaryDocValuesUpdate]]).mjava","sourceNew":"  private static <T extends DocValuesUpdate> byte[] freezeDVUpdates(Map<String,LinkedHashMap<Term, T>> dvUpdates,\n                                                                    IntConsumer updateSizeConsumer)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, T> updates : dvUpdates.values()) {\n        updateSizeConsumer.accept(updates.size());\n        for (T update : updates.values()) {\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n          update.writeTo(out);\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","sourceOld":"  private byte[] freezeBinaryDVUpdates(Map<String,LinkedHashMap<Term,BinaryDocValuesUpdate>> binaryDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, BinaryDocValuesUpdate> binaryUpdates : binaryDVUpdates.values()) {\n        binaryDVUpdateCount += binaryUpdates.size();\n        for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n\n          BytesRef value = (BytesRef) update.value;\n          out.writeVInt(value.length);\n          out.writeBytes(value.bytes, value.offset, value.length);\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["110125c995236a7f61057dd04b039ed2d267f3a1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f42883db49d143abc1a0f176ba47e3388dafb608":["110125c995236a7f61057dd04b039ed2d267f3a1","7e129bd6cb34a236558a49edf108a49d5c15e0e1"],"110125c995236a7f61057dd04b039ed2d267f3a1":["28288370235ed02234a64753cdbf0c6ec096304a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f42883db49d143abc1a0f176ba47e3388dafb608"]},"commit2Childs":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["f42883db49d143abc1a0f176ba47e3388dafb608"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"f42883db49d143abc1a0f176ba47e3388dafb608":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"110125c995236a7f61057dd04b039ed2d267f3a1":["7e129bd6cb34a236558a49edf108a49d5c15e0e1","f42883db49d143abc1a0f176ba47e3388dafb608"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["110125c995236a7f61057dd04b039ed2d267f3a1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}