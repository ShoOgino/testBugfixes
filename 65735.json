{"path":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","commits":[{"id":"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5","date":1556572478,"type":1,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","sourceNew":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          s.getReplicas().forEach(r -> {\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), r.getProperties());\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n            }\n          });\n        });\n      });\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","sourceOld":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          s.getReplicas().forEach(r -> {\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), r.getProperties());\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n            }\n          });\n        });\n      });\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"edf5b262a72d10530eb2f01dc8f19060355b213e","date":1557765866,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","sourceNew":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      colShardReplicaMap.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          Replica leader = s.getLeader();\n          s.getReplicas().forEach(r -> {\n            Map<String, Object> props = new HashMap<>(r.getProperties());\n            if (leader != null && r.getName().equals(leader.getName())) {\n              props.put(\"leader\", \"true\");\n            }\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), props);\n            if (leader != null && r.getName().equals(leader.getName())) {\n              ri.getVariables().put(\"leader\", \"true\");\n            }\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n              colShardReplicaMap.computeIfAbsent(ri.getCollection(), name -> new ConcurrentHashMap<>())\n                  .computeIfAbsent(ri.getShard(), shard -> new ArrayList<>()).add(ri);\n            } else {\n              log.warn(\"- dropping replica because its node \" + r.getNodeName() + \" is not live: \" + r);\n            }\n          });\n        });\n      });\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","sourceOld":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          s.getReplicas().forEach(r -> {\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), r.getProperties());\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n            }\n          });\n        });\n      });\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e35f2dde06b35aa9904949a3a93fabd090371077","date":1587906921,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","sourceNew":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      colShardReplicaMap.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          Replica leader = s.getLeader();\n          s.getReplicas().forEach(r -> {\n            Map<String, Object> props = new HashMap<>(r.getProperties());\n            if (leader != null && r.getName().equals(leader.getName())) {\n              props.put(\"leader\", \"true\");\n            }\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), props);\n            if (leader != null && r.getName().equals(leader.getName())) {\n              ri.getVariables().put(\"leader\", \"true\");\n            }\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n              colShardReplicaMap.computeIfAbsent(ri.getCollection(), name -> new ConcurrentHashMap<>())\n                  .computeIfAbsent(ri.getShard(), shard -> new ArrayList<>()).add(ri);\n            } else {\n              log.warn(\"- dropping replica because its node {} is not live: {}\", r.getNodeName(), r);\n            }\n          });\n        });\n      });\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","sourceOld":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      colShardReplicaMap.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          Replica leader = s.getLeader();\n          s.getReplicas().forEach(r -> {\n            Map<String, Object> props = new HashMap<>(r.getProperties());\n            if (leader != null && r.getName().equals(leader.getName())) {\n              props.put(\"leader\", \"true\");\n            }\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), props);\n            if (leader != null && r.getName().equals(leader.getName())) {\n              ri.getVariables().put(\"leader\", \"true\");\n            }\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n              colShardReplicaMap.computeIfAbsent(ri.getCollection(), name -> new ConcurrentHashMap<>())\n                  .computeIfAbsent(ri.getShard(), shard -> new ArrayList<>()).add(ri);\n            } else {\n              log.warn(\"- dropping replica because its node \" + r.getNodeName() + \" is not live: \" + r);\n            }\n          });\n        });\n      });\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eb7a329be123e1f46f9d78d74f6d23f33ec81b0a","date":1589907167,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","sourceNew":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      colShardReplicaMap.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      collectionsStatesRef.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        // DocCollection will be created later\n        collectionsStatesRef.put(dc.getName(), new CachedCollectionRef(dc.getName(), dc.getZNodeVersion()));\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          Replica leader = s.getLeader();\n          s.getReplicas().forEach(r -> {\n            Map<String, Object> props = new HashMap<>(r.getProperties());\n            if (leader != null && r.getName().equals(leader.getName())) {\n              props.put(\"leader\", \"true\");\n            }\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), props);\n            if (leader != null && r.getName().equals(leader.getName())) {\n              ri.getVariables().put(\"leader\", \"true\");\n            }\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n              colShardReplicaMap.computeIfAbsent(ri.getCollection(), name -> new ConcurrentHashMap<>())\n                  .computeIfAbsent(ri.getShard(), shard -> new ArrayList<>()).add(ri);\n            } else {\n              log.warn(\"- dropping replica because its node {} is not live: {}\", r.getNodeName(), r);\n            }\n          });\n        });\n      });\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","sourceOld":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      colShardReplicaMap.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          Replica leader = s.getLeader();\n          s.getReplicas().forEach(r -> {\n            Map<String, Object> props = new HashMap<>(r.getProperties());\n            if (leader != null && r.getName().equals(leader.getName())) {\n              props.put(\"leader\", \"true\");\n            }\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), props);\n            if (leader != null && r.getName().equals(leader.getName())) {\n              ri.getVariables().put(\"leader\", \"true\");\n            }\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n              colShardReplicaMap.computeIfAbsent(ri.getCollection(), name -> new ConcurrentHashMap<>())\n                  .computeIfAbsent(ri.getShard(), shard -> new ArrayList<>()).add(ri);\n            } else {\n              log.warn(\"- dropping replica because its node {} is not live: {}\", r.getNodeName(), r);\n            }\n          });\n        });\n      });\n      collectionsStatesRef.set(null);\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"daa0f21a44e235a2299ea1fa913898b182dd7cce","date":1590952026,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","sourceNew":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  @SuppressWarnings({\"unchecked\"})\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      colShardReplicaMap.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      collectionsStatesRef.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        // DocCollection will be created later\n        collectionsStatesRef.put(dc.getName(), new CachedCollectionRef(dc.getName(), dc.getZNodeVersion()));\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          Replica leader = s.getLeader();\n          s.getReplicas().forEach(r -> {\n            Map<String, Object> props = new HashMap<>(r.getProperties());\n            if (leader != null && r.getName().equals(leader.getName())) {\n              props.put(\"leader\", \"true\");\n            }\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), props);\n            if (leader != null && r.getName().equals(leader.getName())) {\n              ri.getVariables().put(\"leader\", \"true\");\n            }\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n              colShardReplicaMap.computeIfAbsent(ri.getCollection(), name -> new ConcurrentHashMap<>())\n                  .computeIfAbsent(ri.getShard(), shard -> new ArrayList<>()).add(ri);\n            } else {\n              log.warn(\"- dropping replica because its node {} is not live: {}\", r.getNodeName(), r);\n            }\n          });\n        });\n      });\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","sourceOld":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      colShardReplicaMap.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      collectionsStatesRef.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        // DocCollection will be created later\n        collectionsStatesRef.put(dc.getName(), new CachedCollectionRef(dc.getName(), dc.getZNodeVersion()));\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          Replica leader = s.getLeader();\n          s.getReplicas().forEach(r -> {\n            Map<String, Object> props = new HashMap<>(r.getProperties());\n            if (leader != null && r.getName().equals(leader.getName())) {\n              props.put(\"leader\", \"true\");\n            }\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), props);\n            if (leader != null && r.getName().equals(leader.getName())) {\n              ri.getVariables().put(\"leader\", \"true\");\n            }\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n              colShardReplicaMap.computeIfAbsent(ri.getCollection(), name -> new ConcurrentHashMap<>())\n                  .computeIfAbsent(ri.getShard(), shard -> new ArrayList<>()).add(ri);\n            } else {\n              log.warn(\"- dropping replica because its node {} is not live: {}\", r.getNodeName(), r);\n            }\n          });\n        });\n      });\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd","date":1594731683,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","sourceNew":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  @SuppressWarnings({\"unchecked\"})\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      colShardReplicaMap.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      collectionsStatesRef.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        // DocCollection will be created later\n        collectionsStatesRef.put(dc.getName(), new CachedCollectionRef(dc.getName(), dc.getZNodeVersion()));\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          Replica leader = s.getLeader();\n          s.getReplicas().forEach(r -> {\n            Map<String, Object> props = new HashMap<>(r.getProperties());\n            if (leader != null && r.getName().equals(leader.getName())) {\n              props.put(\"leader\", \"true\");\n            }\n            Replica ri = new Replica(r.getName(), r.getNodeName(), dc.getName(), s.getName(), r.getCoreName(),\n                r.getState(), r.getType(), props);\n            if (leader != null && r.getName().equals(leader.getName())) {\n              ri.getProperties().put(\"leader\", \"true\");\n            }\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n              colShardReplicaMap.computeIfAbsent(ri.getCollection(), name -> new ConcurrentHashMap<>())\n                  .computeIfAbsent(ri.getShard(), shard -> new ArrayList<>()).add(ri);\n            } else {\n              log.warn(\"- dropping replica because its node {} is not live: {}\", r.getNodeName(), r);\n            }\n          });\n        });\n      });\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","sourceOld":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  @SuppressWarnings({\"unchecked\"})\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      colShardReplicaMap.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      collectionsStatesRef.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        // DocCollection will be created later\n        collectionsStatesRef.put(dc.getName(), new CachedCollectionRef(dc.getName(), dc.getZNodeVersion()));\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          Replica leader = s.getLeader();\n          s.getReplicas().forEach(r -> {\n            Map<String, Object> props = new HashMap<>(r.getProperties());\n            if (leader != null && r.getName().equals(leader.getName())) {\n              props.put(\"leader\", \"true\");\n            }\n            ReplicaInfo ri = new ReplicaInfo(r.getName(), r.getCoreName(), dc.getName(), s.getName(), r.getType(), r.getNodeName(), props);\n            if (leader != null && r.getName().equals(leader.getName())) {\n              ri.getVariables().put(\"leader\", \"true\");\n            }\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n              colShardReplicaMap.computeIfAbsent(ri.getCollection(), name -> new ConcurrentHashMap<>())\n                  .computeIfAbsent(ri.getShard(), shard -> new ArrayList<>()).add(ri);\n            } else {\n              log.warn(\"- dropping replica because its node {} is not live: {}\", r.getNodeName(), r);\n            }\n          });\n        });\n      });\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f504512a03d978990cbff30db0522b354e846db","date":1595247421,"type":4,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simSetClusterState(ClusterState).mjava","sourceNew":null,"sourceOld":"  /**\n   * Initialize from an existing cluster state\n   * @param initialState initial cluster state\n   */\n  @SuppressWarnings({\"unchecked\"})\n  public void simSetClusterState(ClusterState initialState) throws Exception {\n    lock.lockInterruptibly();\n    try {\n      collProperties.clear();\n      colShardReplicaMap.clear();\n      sliceProperties.clear();\n      nodeReplicaMap.clear();\n      liveNodes.clear();\n      collectionsStatesRef.clear();\n      for (String nodeId : stateManager.listData(ZkStateReader.LIVE_NODES_ZKNODE)) {\n        if (stateManager.hasData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.LIVE_NODES_ZKNODE + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_ADDED_PATH + \"/\" + nodeId, -1);\n        }\n        if (stateManager.hasData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId)) {\n          stateManager.removeData(ZkStateReader.SOLR_AUTOSCALING_NODE_LOST_PATH + \"/\" + nodeId, -1);\n        }\n      }\n      liveNodes.addAll(initialState.getLiveNodes());\n      for (String nodeId : liveNodes.get()) {\n        createEphemeralLiveNode(nodeId);\n      }\n      initialState.forEachCollection(dc -> {\n        // DocCollection will be created later\n        collectionsStatesRef.put(dc.getName(), new CachedCollectionRef(dc.getName(), dc.getZNodeVersion()));\n        collProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>()).putAll(dc.getProperties());\n        opDelays.computeIfAbsent(dc.getName(), Utils.NEW_HASHMAP_FUN).putAll(defaultOpDelays);\n        dc.getSlices().forEach(s -> {\n          sliceProperties.computeIfAbsent(dc.getName(), name -> new ConcurrentHashMap<>())\n              .computeIfAbsent(s.getName(), Utils.NEW_HASHMAP_FUN).putAll(s.getProperties());\n          Replica leader = s.getLeader();\n          s.getReplicas().forEach(r -> {\n            Map<String, Object> props = new HashMap<>(r.getProperties());\n            if (leader != null && r.getName().equals(leader.getName())) {\n              props.put(\"leader\", \"true\");\n            }\n            Replica ri = new Replica(r.getName(), r.getNodeName(), dc.getName(), s.getName(), r.getCoreName(),\n                r.getState(), r.getType(), props);\n            if (leader != null && r.getName().equals(leader.getName())) {\n              ri.getProperties().put(\"leader\", \"true\");\n            }\n            if (liveNodes.get().contains(r.getNodeName())) {\n              nodeReplicaMap.computeIfAbsent(r.getNodeName(), Utils.NEW_SYNCHRONIZED_ARRAYLIST_FUN).add(ri);\n              colShardReplicaMap.computeIfAbsent(ri.getCollection(), name -> new ConcurrentHashMap<>())\n                  .computeIfAbsent(ri.getShard(), shard -> new ArrayList<>()).add(ri);\n            } else {\n              log.warn(\"- dropping replica because its node {} is not live: {}\", r.getNodeName(), r);\n            }\n          });\n        });\n      });\n    } finally {\n      lock.unlock();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd":["daa0f21a44e235a2299ea1fa913898b182dd7cce"],"3f504512a03d978990cbff30db0522b354e846db":["7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd"],"daa0f21a44e235a2299ea1fa913898b182dd7cce":["eb7a329be123e1f46f9d78d74f6d23f33ec81b0a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"eb7a329be123e1f46f9d78d74f6d23f33ec81b0a":["e35f2dde06b35aa9904949a3a93fabd090371077"],"e35f2dde06b35aa9904949a3a93fabd090371077":["edf5b262a72d10530eb2f01dc8f19060355b213e"],"edf5b262a72d10530eb2f01dc8f19060355b213e":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f504512a03d978990cbff30db0522b354e846db"]},"commit2Childs":{"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd":["3f504512a03d978990cbff30db0522b354e846db"],"3f504512a03d978990cbff30db0522b354e846db":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"daa0f21a44e235a2299ea1fa913898b182dd7cce":["7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"eb7a329be123e1f46f9d78d74f6d23f33ec81b0a":["daa0f21a44e235a2299ea1fa913898b182dd7cce"],"e35f2dde06b35aa9904949a3a93fabd090371077":["eb7a329be123e1f46f9d78d74f6d23f33ec81b0a"],"edf5b262a72d10530eb2f01dc8f19060355b213e":["e35f2dde06b35aa9904949a3a93fabd090371077"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["edf5b262a72d10530eb2f01dc8f19060355b213e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}