{"path":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","commits":[{"id":"01f60198ece724a6e96cd0b45f289cf42ff83d4f","date":1286864103,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"/dev/null","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      if (count == 0)\n        return;\n      initIndexOut();\n      initDataOut();\n      int[] sortedEntries = hash.sort(comp);\n\n      // first dump bytes data, recording index & offset as\n      // we go\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e] = 1 + i;\n\n        final BytesRef bytes = hash.get(e);\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        lastOffset = offset;\n        offset += bytes.length;\n      }\n\n      // total bytes of data\n      idxOut.writeLong(offset);\n\n      // write index -- first doc -> 1+ord\n      // nocommit -- allow not -1:\n      final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n          docCount, PackedInts.bitsRequired(count));\n      final int limit = docCount > docToEntry.length ? docToEntry.length\n          : docCount;\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        indexWriter.add(e == -1 ? 0 : index[e]);\n      }\n      for (int i = limit; i < docCount; i++) {\n        indexWriter.add(0);\n      }\n      indexWriter.finish();\n\n      // next ord (0-based) -> offset\n      // nocommit -- allow not -1:\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n          PackedInts.bitsRequired(lastOffset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ecea1664e8617d82eca3b8055a3c37cb4da8511","date":1287578668,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      if (count == 0)\n        return;\n      initIndexOut();\n      initDataOut();\n      int[] sortedEntries = hash.sort(comp);\n\n      // first dump bytes data, recording index & offset as\n      // we go\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e] = 1 + i;\n\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        lastOffset = offset;\n        offset += bytes.length;\n      }\n\n      // total bytes of data\n      idxOut.writeLong(offset);\n\n      // write index -- first doc -> 1+ord\n      // nocommit -- allow not -1:\n      final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n          docCount, PackedInts.bitsRequired(count));\n      final int limit = docCount > docToEntry.length ? docToEntry.length\n          : docCount;\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        indexWriter.add(e == -1 ? 0 : index[e]);\n      }\n      for (int i = limit; i < docCount; i++) {\n        indexWriter.add(0);\n      }\n      indexWriter.finish();\n\n      // next ord (0-based) -> offset\n      // nocommit -- allow not -1:\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n          PackedInts.bitsRequired(lastOffset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      if (count == 0)\n        return;\n      initIndexOut();\n      initDataOut();\n      int[] sortedEntries = hash.sort(comp);\n\n      // first dump bytes data, recording index & offset as\n      // we go\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e] = 1 + i;\n\n        final BytesRef bytes = hash.get(e);\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        lastOffset = offset;\n        offset += bytes.length;\n      }\n\n      // total bytes of data\n      idxOut.writeLong(offset);\n\n      // write index -- first doc -> 1+ord\n      // nocommit -- allow not -1:\n      final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n          docCount, PackedInts.bitsRequired(count));\n      final int limit = docCount > docToEntry.length ? docToEntry.length\n          : docCount;\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        indexWriter.add(e == -1 ? 0 : index[e]);\n      }\n      for (int i = limit; i < docCount; i++) {\n        indexWriter.add(0);\n      }\n      indexWriter.finish();\n\n      // next ord (0-based) -> offset\n      // nocommit -- allow not -1:\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n          PackedInts.bitsRequired(lastOffset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"39f3757037aa8f710c0cbf9a76a332de735f58b0","date":1288384416,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      if (count == 0)\n        return;\n      initIndexOut();\n      initDataOut();\n      int[] sortedEntries = hash.sort(comp);\n\n      // first dump bytes data, recording index & offset as\n      // we go\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e] = 1 + i;\n\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        lastOffset = offset;\n        offset += bytes.length;\n      }\n\n      // total bytes of data\n      idxOut.writeLong(offset);\n\n      // write index -- first doc -> 1+ord\n      // TODO(simonw): allow not -1:\n      final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n          docCount, PackedInts.bitsRequired(count));\n      final int limit = docCount > docToEntry.length ? docToEntry.length\n          : docCount;\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        indexWriter.add(e == -1 ? 0 : index[e]);\n      }\n      for (int i = limit; i < docCount; i++) {\n        indexWriter.add(0);\n      }\n      indexWriter.finish();\n\n      // next ord (0-based) -> offset\n      // TODO(simonw): -- allow not -1:\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n          PackedInts.bitsRequired(lastOffset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      if (count == 0)\n        return;\n      initIndexOut();\n      initDataOut();\n      int[] sortedEntries = hash.sort(comp);\n\n      // first dump bytes data, recording index & offset as\n      // we go\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e] = 1 + i;\n\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        lastOffset = offset;\n        offset += bytes.length;\n      }\n\n      // total bytes of data\n      idxOut.writeLong(offset);\n\n      // write index -- first doc -> 1+ord\n      // nocommit -- allow not -1:\n      final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n          docCount, PackedInts.bitsRequired(count));\n      final int limit = docCount > docToEntry.length ? docToEntry.length\n          : docCount;\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        indexWriter.add(e == -1 ? 0 : index[e]);\n      }\n      for (int i = limit; i < docCount; i++) {\n        indexWriter.add(0);\n      }\n      indexWriter.finish();\n\n      // next ord (0-based) -> offset\n      // nocommit -- allow not -1:\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n          PackedInts.bitsRequired(lastOffset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4","date":1291128345,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      if (count == 0)\n        return;\n      initIndexOut();\n      initDataOut();\n      int[] sortedEntries = hash.sort(comp);\n\n      // first dump bytes data, recording index & offset as\n      // we go\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e] = 1 + i;\n\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        lastOffset = offset;\n        offset += bytes.length;\n      }\n\n      // total bytes of data\n      idxOut.writeLong(offset);\n\n      // write index -- first doc -> 1+ord\n      // TODO(simonw): allow not -1:\n      final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n          docCount, PackedInts.bitsRequired(count));\n      final int limit = docCount > docToEntry.length ? docToEntry.length\n          : docCount;\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        indexWriter.add(e == -1 ? 0 : index[e]);\n      }\n      for (int i = limit; i < docCount; i++) {\n        indexWriter.add(0);\n      }\n      indexWriter.finish();\n\n      // next ord (0-based) -> offset\n      // TODO(simonw): -- allow not -1:\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n          PackedInts.bitsRequired(lastOffset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      hash.close();\n\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      if (count == 0)\n        return;\n      initIndexOut();\n      initDataOut();\n      int[] sortedEntries = hash.sort(comp);\n\n      // first dump bytes data, recording index & offset as\n      // we go\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e] = 1 + i;\n\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        lastOffset = offset;\n        offset += bytes.length;\n      }\n\n      // total bytes of data\n      idxOut.writeLong(offset);\n\n      // write index -- first doc -> 1+ord\n      // TODO(simonw): allow not -1:\n      final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n          docCount, PackedInts.bitsRequired(count));\n      final int limit = docCount > docToEntry.length ? docToEntry.length\n          : docCount;\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        indexWriter.add(e == -1 ? 0 : index[e]);\n      }\n      for (int i = limit; i < docCount; i++) {\n        indexWriter.add(0);\n      }\n      indexWriter.finish();\n\n      // next ord (0-based) -> offset\n      // TODO(simonw): -- allow not -1:\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n          PackedInts.bitsRequired(lastOffset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n\n        // first dump bytes data, recording index & offset as\n        // we go\n        long offset = 0;\n        long lastOffset = 0;\n        final int[] index = new int[count];\n        final long[] offsets = new long[count];\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        hash.close();\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      if (count == 0)\n        return;\n      initIndexOut();\n      initDataOut();\n      int[] sortedEntries = hash.sort(comp);\n\n      // first dump bytes data, recording index & offset as\n      // we go\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        offsets[i] = offset;\n        index[e] = 1 + i;\n\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        // TODO: we could prefix code...\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        lastOffset = offset;\n        offset += bytes.length;\n      }\n\n      // total bytes of data\n      idxOut.writeLong(offset);\n\n      // write index -- first doc -> 1+ord\n      // TODO(simonw): allow not -1:\n      final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n          docCount, PackedInts.bitsRequired(count));\n      final int limit = docCount > docToEntry.length ? docToEntry.length\n          : docCount;\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        indexWriter.add(e == -1 ? 0 : index[e]);\n      }\n      for (int i = limit; i < docCount; i++) {\n        indexWriter.add(0);\n      }\n      indexWriter.finish();\n\n      // next ord (0-based) -> offset\n      // TODO(simonw): -- allow not -1:\n      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n          PackedInts.bitsRequired(lastOffset));\n      for (int i = 0; i < count; i++) {\n        offsetWriter.add(offsets[i]);\n      }\n      offsetWriter.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      hash.close();\n\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16dc5aef9648bb8fedce2ef55874f52e62c2766d","date":1293994163,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        long offset = 0;\n        long lastOffset = 0;\n        final int[] index = new int[count];\n        final long[] offsets = new long[count];\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        hash.close();\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n\n        // first dump bytes data, recording index & offset as\n        // we go\n        long offset = 0;\n        long lastOffset = 0;\n        final int[] index = new int[count];\n        final long[] offsets = new long[count];\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        hash.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9235b9d4454a46c066cda47fed7ca0a34e614529","date":1304414372,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        long offset = 0;\n        long lastOffset = 0;\n        final int[] index = new int[count];\n        final long[] offsets = new long[count];\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        hash.close();\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        long offset = 0;\n        long lastOffset = 0;\n        final int[] index = new int[count];\n        final long[] offsets = new long[count];\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        hash.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e8d7ba2175f47e280231533f7d3016249cea88b","date":1307711934,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"/dev/null","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        long offset = 0;\n        long lastOffset = 0;\n        final int[] index = new int[count];\n        final long[] offsets = new long[count];\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        hash.close();\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"/dev/null","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        long offset = 0;\n        long lastOffset = 0;\n        final int[] index = new int[count];\n        final long[] offsets = new long[count];\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        hash.close();\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b7a068f550e13e49517c6899cc3b94c8eeb72e5","date":1309354772,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      final IndexOutput datOut = getDataOut();\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      boolean success = false;\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n        success = true;\n      } finally {\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        IOUtils.closeSafely(!success, idxOut);\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        long offset = 0;\n        long lastOffset = 0;\n        final int[] index = new int[count];\n        final long[] offsets = new long[count];\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        hash.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      final IndexOutput datOut = getDataOut();\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      boolean success = false;\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n        success = true;\n      } finally {\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        IOUtils.closeSafely(!success, idxOut);\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        long offset = 0;\n        long lastOffset = 0;\n        final int[] index = new int[count];\n        final long[] offsets = new long[count];\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        hash.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      final IndexOutput datOut = getDataOut();\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      boolean success = false;\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n        success = true;\n      } finally {\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        IOUtils.closeSafely(!success, idxOut);\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        long offset = 0;\n        long lastOffset = 0;\n        final int[] index = new int[count];\n        final long[] offsets = new long[count];\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        hash.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24230fe54121f9be9d85f2c2067536296785e421","date":1314462346,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      final IndexOutput datOut = getDataOut();\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      boolean success = false;\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(datOut);\n        } else {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n        success = true;\n      } finally {\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        if (success) {\n          IOUtils.close(idxOut);\n        } else {\n          IOUtils.closeWhileHandlingException(idxOut);\n        }\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      final IndexOutput datOut = getDataOut();\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      boolean success = false;\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n        success = true;\n      } finally {\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        IOUtils.closeSafely(!success, idxOut);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85eb75e0c0203e44dcf686f35876cf6080f3a671","date":1317221550,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.Writer#finish(int).mjava","sourceNew":null,"sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final int count = hash.size();\n      final IndexOutput datOut = getDataOut();\n      long offset = 0;\n      long lastOffset = 0;\n      final int[] index = new int[count];\n      final long[] offsets = new long[count];\n      boolean success = false;\n      try {\n        final int[] sortedEntries = hash.sort(comp);\n        // first dump bytes data, recording index & offset as\n        // we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          offsets[i] = offset;\n          index[e] = 1 + i;\n\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          // TODO: we could prefix code...\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          lastOffset = offset;\n          offset += bytes.length;\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(datOut);\n        } else {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        // total bytes of data\n        idxOut.writeLong(offset);\n\n        // write index -- first doc -> 1+ord\n        // TODO(simonw): allow not -1:\n        final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,\n            docCount, PackedInts.bitsRequired(count));\n        final int limit = docCount > docToEntry.length ? docToEntry.length\n            : docCount;\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          indexWriter.add(e == -1 ? 0 : index[e]);\n        }\n        for (int i = limit; i < docCount; i++) {\n          indexWriter.add(0);\n        }\n        indexWriter.finish();\n\n        // next ord (0-based) -> offset\n        // TODO(simonw): -- allow not -1:\n        PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,\n            PackedInts.bitsRequired(lastOffset));\n        for (int i = 0; i < count; i++) {\n          offsetWriter.add(offsets[i]);\n        }\n        offsetWriter.finish();\n        success = true;\n      } finally {\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        if (success) {\n          IOUtils.close(idxOut);\n        } else {\n          IOUtils.closeWhileHandlingException(idxOut);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"24230fe54121f9be9d85f2c2067536296785e421":["3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4":["39f3757037aa8f710c0cbf9a76a332de735f58b0"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2e8d7ba2175f47e280231533f7d3016249cea88b"],"39f3757037aa8f710c0cbf9a76a332de735f58b0":["4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"3b7a068f550e13e49517c6899cc3b94c8eeb72e5":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"01f60198ece724a6e96cd0b45f289cf42ff83d4f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["2e8d7ba2175f47e280231533f7d3016249cea88b","3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"9235b9d4454a46c066cda47fed7ca0a34e614529":["16dc5aef9648bb8fedce2ef55874f52e62c2766d"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9235b9d4454a46c066cda47fed7ca0a34e614529"],"85eb75e0c0203e44dcf686f35876cf6080f3a671":["24230fe54121f9be9d85f2c2067536296785e421"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"16dc5aef9648bb8fedce2ef55874f52e62c2766d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["01f60198ece724a6e96cd0b45f289cf42ff83d4f","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["85eb75e0c0203e44dcf686f35876cf6080f3a671"]},"commit2Childs":{"24230fe54121f9be9d85f2c2067536296785e421":["85eb75e0c0203e44dcf686f35876cf6080f3a671"],"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"39f3757037aa8f710c0cbf9a76a332de735f58b0":["5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4"],"3b7a068f550e13e49517c6899cc3b94c8eeb72e5":["24230fe54121f9be9d85f2c2067536296785e421","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["16dc5aef9648bb8fedce2ef55874f52e62c2766d"],"01f60198ece724a6e96cd0b45f289cf42ff83d4f":["4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"9235b9d4454a46c066cda47fed7ca0a34e614529":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","3b7a068f550e13e49517c6899cc3b94c8eeb72e5","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","ab5cb6a74aefb78aa0569857970b9151dfe2e787","01f60198ece724a6e96cd0b45f289cf42ff83d4f","2e8d7ba2175f47e280231533f7d3016249cea88b","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"85eb75e0c0203e44dcf686f35876cf6080f3a671":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"16dc5aef9648bb8fedce2ef55874f52e62c2766d":["9235b9d4454a46c066cda47fed7ca0a34e614529"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["39f3757037aa8f710c0cbf9a76a332de735f58b0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}