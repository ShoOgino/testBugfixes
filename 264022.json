{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","commits":[{"id":"6c18273ea5b3974d2f30117f46f1ae416c28f727","date":1279708040,"type":1,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexingThenDeleting().mjava","sourceNew":"  // nocommit - TODO: enable when flushing by RAM is implemented\n  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = newRandom();\n\n    Directory dir = new MockRAMDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.5));\n    //w.setInfoStream(System.out);\n    Document doc = new Document();\n    doc.add(new Field(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int iter=0;iter<6*_TestUtil.getRandomMultiplier();iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = newRandom();\n\n    Directory dir = new MockRAMDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.5));\n    //w.setInfoStream(System.out);\n    Document doc = new Document();\n    doc.add(new Field(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int iter=0;iter<6*_TestUtil.getRandomMultiplier();iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3242a09f703274d3b9283f2064a1a33064b53a1b","date":1280263474,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","sourceNew":"  // nocommit - TODO: enable when flushing by RAM is implemented\n  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = newRandom();\n\n    Directory dir = new MockRAMDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.5));\n    //w.setInfoStream(System.out);\n    Document doc = new Document();\n    doc.add(new Field(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = 6 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // nocommit - TODO: enable when flushing by RAM is implemented\n  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = newRandom();\n\n    Directory dir = new MockRAMDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.5));\n    //w.setInfoStream(System.out);\n    Document doc = new Document();\n    doc.add(new Field(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int iter=0;iter<6*_TestUtil.getRandomMultiplier();iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":4,"author":"Michael Busch","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","sourceNew":null,"sourceOld":"  // nocommit - TODO: enable when flushing by RAM is implemented\n  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = newRandom();\n\n    Directory dir = new MockRAMDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.5));\n    //w.setInfoStream(System.out);\n    Document doc = new Document();\n    doc.add(new Field(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = 6 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54cdb3f871937873dd85ae388202af7a5efd5584","date":1294960319,"type":1,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexingThenDeleting().mjava","sourceNew":"  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n\n    Directory dir = newDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, false)).setRAMBufferSizeMB(0.5).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = 6 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n\n    Directory dir = newDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, false)).setRAMBufferSizeMB(0.5).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = 6 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","sourceNew":"  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n\n    Directory dir = newDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, false)).setRAMBufferSizeMB(0.5).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = 6 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 1500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n\n    Directory dir = newDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, false)).setRAMBufferSizeMB(0.5).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = 6 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","sourceNew":"  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n\n    Directory dir = newDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, false)).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = 6 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n\n    Directory dir = newDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, false)).setRAMBufferSizeMB(0.5).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = 6 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 1500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","sourceNew":"  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n\n    Directory dir = newDirectory();\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, false)).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = 6 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e9214d7961d272de9c4c9795f247025fd4f47b2","date":1303971736,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#_testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void _testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.flushCount;\n        while(w.flushCount == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2e9214d7961d272de9c4c9795f247025fd4f47b2":["962d04139994fce5193143ef35615499a9a96d78"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["6c18273ea5b3974d2f30117f46f1ae416c28f727","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["bde51b089eb7f86171eb3406e38a274743f9b7ac","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"54cdb3f871937873dd85ae388202af7a5efd5584":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"962d04139994fce5193143ef35615499a9a96d78":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["3242a09f703274d3b9283f2064a1a33064b53a1b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["54cdb3f871937873dd85ae388202af7a5efd5584","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"2e9214d7961d272de9c4c9795f247025fd4f47b2":[],"3242a09f703274d3b9283f2064a1a33064b53a1b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["962d04139994fce5193143ef35615499a9a96d78"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["3242a09f703274d3b9283f2064a1a33064b53a1b"],"54cdb3f871937873dd85ae388202af7a5efd5584":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3242a09f703274d3b9283f2064a1a33064b53a1b","c0ef0193974807e4bddf5432a6b0287fe4d6c9df","6c18273ea5b3974d2f30117f46f1ae416c28f727","962d04139994fce5193143ef35615499a9a96d78","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"962d04139994fce5193143ef35615499a9a96d78":["2e9214d7961d272de9c4c9795f247025fd4f47b2"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["54cdb3f871937873dd85ae388202af7a5efd5584"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2e9214d7961d272de9c4c9795f247025fd4f47b2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}